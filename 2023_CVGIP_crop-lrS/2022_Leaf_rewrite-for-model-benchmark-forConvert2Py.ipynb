{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bf2b0fd",
   "metadata": {},
   "source": [
    "### Leaf benchmark for SOTA models and search better ensamble \n",
    "\n",
    "    2021-12-01\n",
    "    This nb is adopt by KPTK_build_models_BaseModelsBuilder.ipynb and tf.data_Loadandpreprocessdata_image_tf2.3to2.2_twcc_clean_GPUS-Temp-Leaf-EFN-2021-11-18-forReviewOnlyTwcc.ipynb\n",
    "    \n",
    "    (1) Rewrite tfds creation to read from CSV rather than from the name of directories. [12/03 done]\n",
    "    (2)\n",
    "\n",
    "12/08\n",
    "VGG16/16 seems not works? train fail at step-2/epoch-1, need use tf 21.08 docker.\n",
    "\n",
    "\n",
    "## 12/27\n",
    "####    rewrite ipynb to scrip version for production run. \n",
    "        1. convert to py\n",
    "            restart kernel and clean all output.\n",
    "            jupyter nbconvert --to script 2022_Leaf_rewrite-for-model-benchmark-forConvert2Py.ipynb \n",
    "            \n",
    "            [NbConvertApp] Converting notebook 2022_Leaf_rewrite-for-model-benchmark-forConvert2Py.ipynb to script\n",
    "            [NbConvertApp] Writing 80449 bytes to 2022_Leaf_rewrite-for-model-benchmark-forConvert2Py.py\n",
    "            \n",
    "        2. parameterize the ds, lr, aug, models and witght selection.\n",
    "            LeafTK.py : repository for custom funtion. \n",
    "            Train_Leaf.py : train loop for benchmark.\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc3828a",
   "metadata": {},
   "source": [
    "### 1. ENV\n",
    "#### <font color=#FF6600>[ENV] Turn off the error from twcc's AMP issue</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564ddb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export TF_ENABLE_AUTO_MIXED_PRECISION=0\n",
    "\n",
    "#no work\n",
    "!export TF_FORCE_GPU_ALLOW_GROWTH=1\n",
    "# !export drop_remainder=False\n",
    "\n",
    "# !export TF_ENABLE_AUTO_MIXED_PRECISION=1\n",
    "# !export TF_ENABLE_AUTO_MIXED_PRECISION_GRAPH_REWRITE=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3715215",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !sh install_env.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3745094",
   "metadata": {},
   "source": [
    "#### <font color=#FF6600>[ENV] Moduls importing</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84280ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# set log level should be before import tf\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "os.environ[\"AUTOGRAPH_VERBOSITY\"] = \"0\"\n",
    "\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "import json\n",
    "import errno\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "from cycler import cycler\n",
    "from datetime import datetime\n",
    "\n",
    "# albumentations\n",
    "from functools import partial\n",
    "import albumentations as A\n",
    "\n",
    "# RandAugment, AutoAugment, augment.py from TF github\n",
    "from augment import RandAugment,AutoAugment\n",
    "\n",
    "\n",
    "from pytictoc import TicToc\n",
    "\n",
    "t = TicToc() #create instance of class\n",
    "\n",
    "t.tic() #Start time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50df4f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'tf: {tf.__version__} \\ncv2: {cv2.__version__} \\nnp: {np.__version__} \\npd: {pd.__version__} \\nmatplotlib: {matplotlib.__version__}')\n",
    "t.toc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056d56b9",
   "metadata": {},
   "source": [
    "#### <font color=#FF6600>[ENV] Parameters</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e051a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bench models\n",
    "log_dir_name = \"TrainSaveDir-1227-toPytest\" # Put all results in same dir with different file name.\n",
    "\"\"\"Ex: best_model_name: ./TrainSaveDir/ft_RRNET_imagenet1k_resize_120x120_CDCLR_AA_bs32_best_val_accuracy.h5\"\"\"\n",
    "\n",
    "\n",
    "# hyper setting\n",
    "weight=\"imagenet1k\" #random, maybe just 1k is enough\n",
    "crop= \"crop\" #\"resize\" #crop=center crop\n",
    "lr_name= 'CDR' #'fixed' # 'CDR' #'WCD' #'plateau'#'lrdump' #\"WCD\" # WCD, WCDC, lrdump, platrure\n",
    "augment= 'RA' #None # None, 'AA', 'RA', 'NoisyStudent', 'all'\n",
    "\n",
    "\n",
    "# hyper models\n",
    "top_dropout_rate = 0.4 #less dp rate, say 0.1, train_loss will lower than val_loss # for flood 0.2 is ok. for leaf 0.4 is better. for foot 0.8 is fine.\n",
    "drop_connect_rate = 0.9 #for efnet This parameter serves as a toggle for extra regularization in finetuning, but does not affect loaded weights.\n",
    "outputnum = 5 # classes of 5\n",
    "\n",
    "\n",
    "# Image size\n",
    "BATCH_SIZE = 4 #32#4 #2 # 8# 32 #64 #64:512*8 OOM, B7+bs8:RecvAsync is cancelled\n",
    "img_height = 512 #600 #512 #120\n",
    "img_width = 512 #600 #512 #120\n",
    "\n",
    "patience_1 = 3\n",
    "patience_2 = 5\n",
    "\n",
    "# 自動調節tf.data管道\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# visible/logical device (able to be used)\n",
    "print(tf.config.experimental.list_physical_devices('GPU'),'\\n')\n",
    "print(tf.config.experimental.list_logical_devices('GPU'),'\\n')\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num logical_gpus  :\", len(tf.config.experimental.list_logical_devices('GPU')))\n",
    "\n",
    "# tf MirroredStrategy seting\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "REPLICAS = strategy.num_replicas_in_sync\n",
    "print('\\nNumber of REPLICAS: {}\\n'.format(REPLICAS))\n",
    "\n",
    "\n",
    "MULTI_BATCH_SIZE = BATCH_SIZE * strategy.num_replicas_in_sync\n",
    "print('BATCH_SIZE: {}, MULTI_BATCH_SIZE: {}'.format(BATCH_SIZE, MULTI_BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3e51ab",
   "metadata": {},
   "source": [
    "### 2. Dataset (DS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a7986b",
   "metadata": {},
   "source": [
    "#### <font color=orange>[DS] Create the training dataset W/ croped</font>\n",
    "#### <font color=#00FF00>[DS] Create the training dataset W/ croped</font>\n",
    "\n",
    "    label_num_to_disease_map.json    {\n",
    "    \"0\": \"Cassava Bacterial Blight (CBB)\", \n",
    "    \"1\": \"Cassava Brown Streak Disease (CBSD)\", \n",
    "    \"2\": \"Cassava Green Mottle (CGM)\", \n",
    "    \"3\": \"Cassava Mosaic Disease (CMD)\", \n",
    "    \"4\": \"Healthy\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d0c6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = ['CBB', \n",
    "           'CBSD', \n",
    "           'CGM', \n",
    "           'CMD', \n",
    "           'Healthy']\n",
    "LABELS = {\"0\": \"CBB\", \n",
    "          \"1\": \"CBSD\", \n",
    "          \"2\": \"CGM\", \n",
    "          \"3\": \"CMD\", \n",
    "          \"4\": \"Healthy\"}\n",
    "\n",
    "data_dir = '/home/u3148947/.keras/datasets/leaf/'\n",
    "# leaf_dir = 'leaf/leaf_labels/'\n",
    "leaf_dir = '/home/u3148947/.keras/datasets/leaf/train_images/'\n",
    "\n",
    "df_train = pd.read_csv(data_dir + '/train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6846a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check lables\n",
    "for i in range(5):\n",
    "    print(i, CLASSES[i])\n",
    "print([(i,l) for i,l in zip(LABELS.keys(), LABELS.values())])\n",
    "\n",
    "for i,l in zip(LABELS.keys(), LABELS.values()):\n",
    "    print(i,l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a585aea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8c71b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d7f3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and reset index\n",
    "# fixed shuffle for compare later, random_state=42\n",
    "df_train = df_train.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ece876",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cad011",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669c86d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = df_train.groupby(['label']).count() \n",
    "print(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e72f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get no key freq\n",
    "freq = df_train['label'].value_counts() \n",
    "ax = freq.plot.bar(x='image_id', y='label', rot=0) #no key so x y no matter.\n",
    "print(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f0008b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check image size\n",
    "\n",
    "import imageio\n",
    "for tmp in ['827159844.jpg', '795383461.jpg', '851791464.jpg', '923880010.jpg']:\n",
    "    tmp_img = imageio.imread(leaf_dir + '/' + tmp)\n",
    "    print(type(tmp_img), tmp_img.shape, tmp_img.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c67563",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ffac851",
   "metadata": {},
   "source": [
    "\n",
    "#### <font color=#00FF00>[DS] Create tf.dataset (DS)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37611553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataframe\n",
    "list_ds = tf.data.Dataset.from_tensor_slices((df_train['image_id'], df_train['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b9e8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Python iterator\n",
    "\n",
    "it_list_ds = iter(list_ds) # Make sure iter ds only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfa8a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using iter and consuming its elements using next: every print different image name.\n",
    "\n",
    "for i in range(4):\n",
    "    image_id, label = next(it_list_ds)\n",
    "    print(image_id.numpy(), label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7480ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# map list to ds.\n",
    "#\n",
    "def process_path_label(image_id, label):\n",
    "    file_path = leaf_dir + image_id\n",
    "    # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)#can read the byte string paths b'image_0001.png'\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    \n",
    "    if crop == \"resize\":\n",
    "        print(\"--resize\")\n",
    "        img = tf.cast(tf.image.resize(img, [img_height, img_width]), tf.uint8) # for resize the training image for faster checing! tf.image.resize return a float!\n",
    "    if crop == \"crop\":\n",
    "        print(\"--crop\")\n",
    "        # crop the toe from top-left corner [image, offset_height y1, offset_width x1, target_height, target_width]\n",
    "        y1=(600-img_height)/2;    x1=(800-img_width)/2;    h=img_height;    w=img_width # not the pp location\n",
    "        img = tf.image.crop_to_bounding_box(img, int(y1), int(x1), h, w)\n",
    "\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2b5d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leaf train ds\n",
    "train_ds_map = list_ds.map(process_path_label, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7ebdca",
   "metadata": {},
   "source": [
    "#### <font color=#00FF00>[DS] Split TVT</font>\n",
    "train/val/test with ratio 7. 1.5 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174c33c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split TVT train/val/test 7 1.5 1.5\n",
    "\n",
    "\n",
    "val_size = int(tf.data.experimental.cardinality(train_ds_map).numpy() * 0.15)\n",
    "# val_size = int(tf.data.experimental.cardinality(train_ds_map_toe).numpy() * 0.1)#no help\n",
    "\n",
    "print(\"val size:\", val_size)\n",
    "\n",
    "train_ds_map_s = train_ds_map.skip(val_size+val_size)\n",
    "temp_s = train_ds_map.take(val_size+val_size)\n",
    "\n",
    "valid_ds_map_s = temp_s.take(val_size)\n",
    "test_ds_map_s = temp_s.skip(val_size)\n",
    "\n",
    "print(\"total size:\", len(train_ds_map))\n",
    "print(\"\\ntrain\", tf.data.experimental.cardinality(train_ds_map_s).numpy())\n",
    "print(\"valid\", tf.data.experimental.cardinality(valid_ds_map_s).numpy())\n",
    "print(\"test\", tf.data.experimental.cardinality(test_ds_map_s).numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d82068b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create iterator once\n",
    "# iter_map = iter(train_ds_map_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2d9d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # get new image every next time\n",
    "# image, label = next(iter_map)\n",
    "\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# plt.imshow(image)\n",
    "# plt.title(f'Image dtype: {image.dtype} and the range: {image.numpy().min()} to {image.numpy().max()}, shape:{image.shape}')\n",
    "\n",
    "# print(f'Lable: {label}')\n",
    "\n",
    "# \"\"\"\n",
    "# png\n",
    "# Lable: 4\n",
    "# CPU times: user 18.8 s, sys: 934 ms, total: 19.7 s\n",
    "# Wall time: 36.6 s\n",
    "\n",
    "# jpg\n",
    "# Lable: 4\n",
    "# CPU times: user 13.4 s, sys: 370 ms, total: 13.8 s\n",
    "# Wall time: 3.57 s\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b509ad65",
   "metadata": {},
   "source": [
    "#### <font color=#00FF00>[DS] Augmentation and performance cache pipeline</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae44787",
   "metadata": {},
   "outputs": [],
   "source": [
    "## AA, auto aug test\n",
    "\n",
    "# image = tf.io.read_file(str(CMD[1]))\n",
    "# image = tf.image.decode_jpeg(image, channels=3)\n",
    "\n",
    "## RA, is upgraded version of AA.\n",
    "\n",
    "def AA(image, label):\n",
    "    Auto_Aug = AutoAugment()\n",
    "#     auto_img = Auto_Aug.distort(image)\n",
    "    return Auto_Aug.distort(image), label\n",
    "\n",
    "\n",
    "def RA(image, label):\n",
    "    Rand_Aug = RandAugment()\n",
    "#     auto_img = Auto_Aug.distort(image)\n",
    "    return Rand_Aug.distort(image), label\n",
    "\n",
    "\n",
    "## DS performance cache\n",
    "\n",
    "def configure_for_performance_cache(ds, cache=True, augment=None):  \n",
    "    \"\"\"#TODO: need to check the parse logic of ds.cache.\n",
    "    if cache:\n",
    "        print(\"Check cache-f1 to file:\", cache)\n",
    "        if isinstance(cache, str):\n",
    "            ds = ds.cache(cache)\n",
    "            print(\"Check cache-f2 to file:\", cache)\n",
    "    else:\n",
    "        ds = ds.cache()\n",
    "        print(\"Check cache in memory:\", cache)\n",
    "    \"\"\"    \n",
    "    if cache:\n",
    "        ds = ds.cache()\n",
    "        print(\"Check cache in memory:Y\", cache)\n",
    "    else:\n",
    "        print(\"Check cache in memory:N\", cache)\n",
    "\n",
    "        \n",
    "#     if augment== \"Albu\":\n",
    "#         ds = ds.map(data_augment, num_parallel_calls=AUTOTUNE)        \n",
    "    if augment==\"AA\":\n",
    "        ds = ds.map(AA, num_parallel_calls=AUTOTUNE)\n",
    "        print(\"Check augment :Y\", augment)\n",
    "    if augment==\"RA\":\n",
    "        ds = ds.map(RA, num_parallel_calls=AUTOTUNE)\n",
    "        print(\"Check augment :Y\", augment)\n",
    "    if augment==None:\n",
    "        print(\"Check augment :N\", augment)\n",
    "    \n",
    "    #ds = ds.repeat()#TODO:2020-12-14: test\n",
    "    ds = ds.shuffle(buffer_size=MULTI_BATCH_SIZE, reshuffle_each_iteration=True) #buffer_size=MULTI_BATCH_SIZE*2 10sec. # (buffer_size=MULTI_BATCH_SIZE*5) ~10sec,buffer_size=1000 take few sec. or buffer_size=image_count <- take too long # each take ds take 30~45 sec, TODO!!\n",
    "    \"\"\"Note: While large buffer_sizes shuffle more thoroughly, they can take a lot of memory, and \n",
    "        significant time to fill. Consider using Dataset.interleave across files if this becomes a problem.\"\"\"\n",
    "    \n",
    "    ds = ds.batch(MULTI_BATCH_SIZE)#MULTI_BATCH_SIZE for multi-GPUs\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE) #buffer_size=AUTOTUNE seem no speed improve\n",
    "    \n",
    "    print(\"Check ds cache[{}] and augment[{}]\".format(cache, augment))\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13384820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment= #True#False #aug, AA, RA, ADA\n",
    "\n",
    "train_ds_pre = configure_for_performance_cache(train_ds_map_s, cache=True, augment=augment)\n",
    "valid_ds_pre = configure_for_performance_cache(valid_ds_map_s)\n",
    "test_ds_pre = configure_for_performance_cache(test_ds_map_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aadcd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "print('AUTOTUNE=', AUTOTUNE)\n",
    "\n",
    "import math\n",
    "col_row = math.sqrt(int(BATCH_SIZE))\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "for images, labels in train_ds_pre.take(1):\n",
    "    print('batch * multi:', len(labels))\n",
    "    for i in range(4): # for batch size to show 4 32\n",
    "        ax = plt.subplot(4, 4, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(f'labels:{labels[i]}, {CLASSES[labels[i]]}')\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "#why take so long\n",
    "\"\"\"\n",
    "tf.io.decode_png\n",
    "AUTOTUNE= -1\n",
    "batch * multi: 64\n",
    "CPU times: user 9min 52s, sys: 23.5 s, total: 10min 15s\n",
    "Wall time: 5min 10s\n",
    "\n",
    "tf.io.decode_jpeg\n",
    "AUTOTUNE= -1\n",
    "batch * multi: 64\n",
    "CPU times: user 9min 40s, sys: 22.6 s, total: 10min 2s\n",
    "Wall time: 3min 39s\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acaa3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# print('AUTOTUNE=', AUTOTUNE)\n",
    "# # too long\n",
    "\n",
    "# #J 新版：tf2.3\n",
    "\n",
    "# image_batch, label_batch = next(iter(train_ds_pre))\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# plt.imshow(image_batch[1].numpy().astype(\"uint8\"))\n",
    "# # image_batch[1]\n",
    "\n",
    "# print(f'Image dtype: {image_batch[1].dtype} and the range: {image_batch[1].numpy().min()} to {image_batch[1].numpy().max()}')\n",
    "\n",
    "# \"\"\"tf.cast(uint8) will hard fit source to 0-255\"\"\"\n",
    "\n",
    "# \"\"\"AUTOTUNE= -1\n",
    "# Image dtype: <dtype: 'uint8'> and the range: 0 to 255\n",
    "# CPU times: user 8min 56s, sys: 5.43 s, total: 9min 1s\n",
    "# Wall time: 3min 28s\n",
    "\n",
    "# AUTOTUNE= -1\n",
    "# Image dtype: <dtype: 'uint8'> and the range: 0 to 255\n",
    "# CPU times: user 8min 59s, sys: 5.81 s, total: 9min 5s\n",
    "# Wall time: 3min 29s\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ad8048",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.tic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a105f051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create it_ds once\n",
    "# it_train_ds_pre_toe_s = iter(train_ds_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2f75c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# image_batch, label_batch = next(it_train_ds_pre_toe_s)\n",
    "\n",
    "# fig = plt.figure(figsize=(8, 8))\n",
    "# print('batch * multi:', len(label_batch), ', MULTI_BATCH_SIZE=', MULTI_BATCH_SIZE)\n",
    "# for i in range(16):\n",
    "#     ax = plt.subplot(4, 4, i + 1)\n",
    "#     plt.imshow(image_batch[i].numpy().astype(\"uint8\"))\n",
    "#     plt.title(label_batch[i].numpy())\n",
    "#     plt.axis(\"off\")\n",
    "   \n",
    "#     # 2021-11-05\n",
    "#     # Check jpg, png agter ds is [0-255]\n",
    "#     print(' [min,max]:', image_batch[i].numpy().min() , image_batch[i].numpy().max()) \n",
    "    \n",
    "#     print(f'Check lables: {label_batch[i]}')\n",
    "\n",
    "# \"\"\"CPU times: user 8min 54s, sys: 4.8 s, total: 8min 58s\n",
    "# Wall time: 3min 27s\n",
    "\n",
    "# CPU times: user 9min 2s, sys: 5.46 s, total: 9min 7s\n",
    "# Wall time: 3min 30s\n",
    "\n",
    "# ds.shuffle buffer_sizes  = MULTI_BATCH_SIZE\n",
    "# CPU times: user 22 s, sys: 552 ms, total: 22.6 s\n",
    "# Wall time: 7.75 s\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115c4473",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f40deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# t.tic()\n",
    "# num_bs=0\n",
    "# for bs in tqdm(train_ds_pre):\n",
    "# #     print(type(bs))\n",
    "# #     print(\".\",end=\"\")\n",
    "# #     print(len(bs))\n",
    "#     print([len(a) for a in bs])\n",
    "#     num_bs += 1\n",
    "# print(\"num_bs=\", num_bs)\n",
    "# t.toc()\n",
    "\n",
    "\n",
    "\"\"\"跑完整個ds需要3min33s\n",
    "100%|██████████| 235/235 [03:33<00:00,  1.10it/s]\n",
    "num_bs= 235\n",
    "Elapsed time is 213.461381 seconds.\n",
    "CPU times: user 9min 1s, sys: 8.04 s, total: 9min 9s\n",
    "Wall time: 3min 33s\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b2f5b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5e1cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad02e03",
   "metadata": {},
   "source": [
    "### 4. Models\n",
    "\n",
    "#### <font color=\"yellow\"> [Models] </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cc6272",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "aps: module 'tensorflow.keras.applications' from '/usr/local/lib/python3.8/dist-packages/tensorflow/keras/applications/__init__.py\n",
    "vim /usr/local/lib/python3.8/dist-packages/tensorflow/keras/applications/__init__.py\n",
    "\n",
    "可以實現由動態字串變數載入特定的基本模型<但是太多處理>還不如直接表列每項寫出來的清楚簡單!!!!\n",
    "\n",
    "Model_List = [\"Xception\", \"ResNet50\", \"ResNet101\", \"ResNet152\", \"InceptionV3\", \"MobileNet\", \"MobileNetV2\", \n",
    "\"DenseNet121\",\"DenseNet169\",\"DenseNet201\",\n",
    "\"NASNetMobile\",\"NASNetLarge\", \n",
    "\"EfficientNetB0\",\n",
    "\"EfficientNetB1\", #13\n",
    "\"EfficientNetB3\",\n",
    "\"EfficientNetB5\", #15\n",
    "\"EfficientNetB7\",\n",
    "]\n",
    "\"\"\"\n",
    "import importlib \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def build_efn_model(model_name, outputnum, top_dropout_rate, drop_connect_rate):\n",
    "    \n",
    "    # move to Top \n",
    "    inputs = tf.keras.Input(shape=(img_height, img_width, 3)) #shape=(120, 120, 3), img_height, img_width shape=(img_height, img_width, 3)\n",
    "    \n",
    "    # EfficientNetB@# #\n",
    "#     # OK efn\n",
    "#     if model_name.startswith('EfficientNetB'):# == \"EfficientNetB0\":\n",
    "#         root_m_name = 'efficientnet'\n",
    "#         fullnameofmodel = \"tensorflow.keras.applications.\" + root_m_name #model_name #model_name.lower()\n",
    "#         model = importlib.import_module(fullnameofmodel)\n",
    "#         BaseCnn = getattr(model,model_name)       \n",
    "#         base_model = BaseCnn(include_top=False, weights=\"imagenet\", input_shape=(120,120,3),drop_connect_rate=drop_connect_rate) #{'imagenet', None}\n",
    "    # shorter version of OK efn\n",
    "    if model_name.startswith('EfficientNetB'):\n",
    "        \"\"\"For EfficientNet, input preprocessing is included as part of the model (as a Rescaling layer).\"\"\"\n",
    "        BaseCnn = getattr(importlib.import_module(\"tensorflow.keras.applications.efficientnet\"), model_name)       \n",
    "        base_model = BaseCnn(include_top=False, weights=\"imagenet\", drop_connect_rate=drop_connect_rate) #{'imagenet', None}\n",
    "        # NO extra rescale need, efn already include the scaling inside the model\n",
    "        rescaling_input = inputs\n",
    "        \n",
    "    # Xception #\n",
    "    \"\"\"When run in \"tf\" mode it actuallly expect the input to be uint8 between 0 and 255 and scales it to the range from -1.0 to 1.0. \n",
    "    Check the docstring and the source code.\"\"\" #NOT TRUE\n",
    "    \"\"\" For Xception, call tf.keras.applications.xception.preprocess_input on your inputs before passing them to the model. \n",
    "    xception.preprocess_input will scale input pixels between -1 and 1.\"\"\"\n",
    "    if model_name.startswith('Xception'):\n",
    "        BaseCnn = getattr(importlib.import_module(\"tensorflow.python.keras.applications.xception\"), model_name)     \n",
    "        base_model = BaseCnn(include_top=False, weights=\"imagenet\") #{'imagenet', None}\n",
    "        \n",
    "        rescaling_input = tf.keras.applications.xception.preprocess_input(inputs)\n",
    "\n",
    "    # ResNet50 ResNet101 ResNet152 #\n",
    "    if model_name.startswith('ResNet'):\n",
    "        \"\"\"For ResNet, call tf.keras.applications.resnet.preprocess_input on your inputs before passing them to the model. \n",
    "        resnet.preprocess_input will convert the input images from RGB to BGR, then will zero-center each color channel with \n",
    "        respect to the ImageNet dataset, without scaling.\"\"\"\n",
    "        BaseCnn = getattr(importlib.import_module(\"tensorflow.python.keras.applications.resnet\"), model_name)     \n",
    "        base_model = BaseCnn(include_top=False, weights=\"imagenet\") #{'imagenet', None}    \n",
    "        \n",
    "        rescaling_input = tf.keras.applications.resnet.preprocess_input(inputs)\n",
    "        \n",
    "    # InceptionV3 #\n",
    "    if model_name.startswith('InceptionV3'):\n",
    "        \"\"\"For InceptionV3, call tf.keras.applications.inception_v3.preprocess_input on your inputs before passing them to the model. \n",
    "        inception_v3.preprocess_input will scale input pixels between -1 and 1.\"\"\"\n",
    "        BaseCnn = getattr(importlib.import_module(\"tensorflow.python.keras.applications.inception_v3\"), model_name)     \n",
    "        base_model = BaseCnn(include_top=False, weights=\"imagenet\") #{'imagenet', None}    \n",
    "        \n",
    "        rescaling_input = tf.keras.applications.inception_v3.preprocess_input(inputs)        \n",
    "\n",
    "    # MobileNet #\n",
    "    if model_name.endswith('MobileNet'):\n",
    "        \"\"\" For MobileNet, call tf.keras.applications.mobilenet.preprocess_input on your inputs before passing them to the model. \n",
    "        mobilenet.preprocess_input will scale input pixels between -1 and 1.\"\"\"\n",
    "        \"\"\"\n",
    "        The weight of trained (224, 224) will be load for fine turn, bcs the mbnet preteing size (). But it not matter in FT task.\n",
    "        \"\"\"\n",
    "        BaseCnn = getattr(importlib.import_module(\"tensorflow.python.keras.applications.mobilenet\"), model_name)     \n",
    "        base_model = BaseCnn(include_top=False, weights=\"imagenet\") #{'imagenet', None}    \n",
    "        \n",
    "        rescaling_input = tf.keras.applications.mobilenet.preprocess_input(inputs)             \n",
    "        \n",
    "    # MobileNetV2 #\n",
    "    if model_name.startswith('MobileNetV2'):\n",
    "        \"\"\" For MobileNetV2, call tf.keras.applications.mobilenet_v2.preprocess_input on your inputs before passing them to the model. \n",
    "        mobilenet_v2.preprocess_input will scale input pixels between -1 and 1.\"\"\"\n",
    "        BaseCnn = getattr(importlib.import_module(\"tensorflow.python.keras.applications.mobilenet_v2\"), model_name)     \n",
    "        base_model = BaseCnn(include_top=False, weights=\"imagenet\") #{'imagenet', None}    \n",
    "        \n",
    "        rescaling_input = tf.keras.applications.mobilenet_v2.preprocess_input(inputs)            \n",
    "        \n",
    "    # DenseNet121 DenseNet169 DenseNet201 #\n",
    "    if model_name.startswith('DenseNet'):\n",
    "        \"\"\" For DenseNet, call tf.keras.applications.densenet.preprocess_input on your inputs before passing them to the model.\"\"\"\n",
    "        BaseCnn = getattr(importlib.import_module(\"tensorflow.python.keras.applications.densenet\"), model_name)     \n",
    "        base_model = BaseCnn(include_top=False, weights=\"imagenet\") #{'imagenet', None}    \n",
    "        \n",
    "        rescaling_input = tf.keras.applications.densenet.preprocess_input(inputs)            \n",
    "        \n",
    "    # NASNet: NASNetMobile #\n",
    "    if model_name.startswith('NASNetMobile'):\n",
    "        \"\"\"For NASNet, call tf.keras.applications.nasnet.preprocess_input on your inputs before passing them to the model.\"\"\"\n",
    "        \"\"\"Optional shape tuple, only to be specified if include_top is False (otherwise the input shape has to be (224, 224, 3)\n",
    "        for NASNetMobile It should have exactly 3 inputs channels, and width and height should be no smaller than 32. E.g. (224, 224, 3) \n",
    "        would be one valid value. For loading imagenet weights, input_shape should be (224, 224, 3)\"\"\"\n",
    "        \n",
    "        \"\"\"otherwise the input shape has to be (331, 331, 3) for NASNetLarge. It should have exactly 3 inputs channels, and width and height should \n",
    "        be no smaller than 32. E.g. (224, 224, 3) would be one valid value.  For loading imagenet weights, input_shape should be (331, 331, 3)\"\"\"\n",
    "        \"\"\" NASNetMobile imagenet with 224: ted 10.x/5.x \n",
    "            NASNetMobile None with 120: ted 12.x/4.x,  seem no different at fine tune phase. \"\"\"\n",
    "        \n",
    "        \"\"\"pre-set use inputs = tf.keras.Input([None, None, 3]) to fake run build mode to get the model weight.  Then, run\n",
    "        on normal build with specific input size without imagenet-weight, and reload the weight by hand.\n",
    "        Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/nasnet/nasnet_mobile_no_top.h5\n",
    "        Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/nasnet/NASNet-large-no-top.h5\n",
    "        https://github.com/keras-team/keras-applications/issues/78\n",
    "        \"\"\"\n",
    "        \"\"\"Very large model, NASNetLarge take 8xM parameters, take 800~300 sec for one epoch.\n",
    "        Epoch 00015: val_accuracy did not improve from 0.87036\n",
    "        [306.6111526489258] of epoch 15\n",
    "        CPU times: user 7h 30min 14s, sys: 32min 2s, total: 8h 2min 16s\n",
    "        Wall time: 1h 26min 32s\n",
    "        \n",
    "        NASNetMobile: but loss: nan after epcoh 1, need reduce the lr!!!!!!!!!!!!!!!!!!!!!!!!!!!!!![12/14]Fixed by seprating mobile/large to two functions.\n",
    "        Epoch 00011: val_accuracy did not improve from 0.64444\n",
    "        [124.91568398475647] of epoch 11\n",
    "        CPU times: user 3h 19min 49s, sys: 3min 41s, total: 3h 23min 31s\n",
    "        Wall time: 29min 27s\n",
    "        \"\"\"\n",
    "        # Pre download the model first for it weight later we need to reload it.\n",
    "#         inputs = tf.keras.Input([None, None, 3])\n",
    "\n",
    "        rescaling_input = tf.keras.applications.nasnet.preprocess_input(inputs)     \n",
    "\n",
    "        BaseCnn = getattr(importlib.import_module(\"tensorflow.python.keras.applications.nasnet\"), model_name)     \n",
    "#         base_model = BaseCnn(include_top=False, weights=\"imagenet\") #{'imagenet', None}    \n",
    "#         base_model = BaseCnn(include_top=False, weights='imagenet', input_shape=(img_height, img_width, 3)) #{'imagenet', None} for set input to 120x120    \n",
    "#         base_model = BaseCnn(include_top=False, weights='imagenet') #{'imagenet', None} for set input to 120x120    \n",
    "\n",
    "        # load weight by hand.\n",
    "        base_model = BaseCnn(include_top=False, weights=None, input_shape=(img_height, img_width, 3))\n",
    "        \n",
    "        # forgote where got it@@\n",
    "        #base_model.load_weights('/home/u3148947/.keras/models/nasnet_mobile_no_top.h5') # If no load_weights, model.fit will take very long long time for initial when into the epoch 1.\n",
    "        \n",
    "        #NASNet-mobile-no-top.h5 https://github.com/fchollet/deep-learning-models/releases/ #same as above, epoch 2 loss nan\n",
    "        base_model.load_weights('/home/u3148947/.keras/models/NASNet-mobile-no-top.h5') # If no load_weights, model.fit will take very long long time for initial when into the epoch 1.\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "#         inputs = tf.keras.layers.Resizing(224, 224) # tf >= 2.6.0, but currnet TWCC newest 21.08 is tf=2.5.0\n",
    "#         i = tf.compat.v1.keras.layers.experimental.preprocessing.Resizing(224, 224)(inputs)\n",
    "#         x = tf.cast(i, tf.float32)\n",
    "\n",
    "\n",
    "    # NASNet: NASNetLarge #\n",
    "    if model_name.startswith('NASNetLarge'):\n",
    "        rescaling_input = tf.keras.applications.nasnet.preprocess_input(inputs)     \n",
    "\n",
    "        BaseCnn = getattr(importlib.import_module(\"tensorflow.python.keras.applications.nasnet\"), model_name)    \n",
    "        \n",
    "        # load weight by hand.\n",
    "        base_model = BaseCnn(include_top=False, weights=None, input_shape=(img_height, img_width, 3))\n",
    "        base_model.load_weights('/home/u3148947/.keras/models/nasnet_large_no_top.h5')\n",
    "\n",
    "\n",
    "        \n",
    "    # VGG16 # work with tf21.08\n",
    "    \"\"\"VGG16 not train even with None or Imagenet. pooling= is not the factor\"\"\"\n",
    "    if model_name.startswith('VGG16'):\n",
    "        \"\"\"For VGG16, call tf.keras.applications.vgg16.preprocess_input on your inputs before passing them to the model. \n",
    "        vgg16.preprocess_input will convert the input images from RGB to BGR, then will zero-center each color channel \n",
    "        with respect to the ImageNet dataset, without scaling.\"\"\"\n",
    "        BaseCnn = getattr(importlib.import_module(\"tensorflow.python.keras.applications.vgg16\"), model_name)     \n",
    "        base_model = BaseCnn(include_top=False, weights='imagenet') #{'imagenet', None}\n",
    "        \n",
    "        rescaling_input = tf.keras.applications.vgg16.preprocess_input(inputs)  \n",
    "\n",
    "    # VGG19 #\n",
    "    if model_name.startswith('VGG19'):\n",
    "        \"\"\"For VGG19, call tf.keras.applications.vgg19.preprocess_input on your inputs before passing them to the model. \n",
    "        vgg16.preprocess_input will convert the input images from RGB to BGR, then will zero-center each color channel \n",
    "        with respect to the ImageNet dataset, without scaling.\"\"\"\n",
    "        \n",
    "        BaseCnn = getattr(importlib.import_module(\"tensorflow.python.keras.applications.vgg19\"), model_name)     \n",
    "        base_model = BaseCnn(include_top=False, weights=\"imagenet\") #{'imagenet', None}    \n",
    "        \n",
    "        rescaling_input = tf.keras.applications.vgg19.preprocess_input(inputs)  \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     # ViT # tf.hub version [Waiting for twcc update CCS image for version 21.11]\n",
    "#     if model_name.startswith('ViT'):\n",
    "#         \"\"\"Inputs to the model must:\n",
    "#             1.be four dimensional Tensors of the shape (batch_size, height, width, num_channels). Note that the model expects images with channels_last property. num_channels must be 3.\n",
    "#             2.be resized to 224x224 resolution.\n",
    "#             3.have pixel values in the range [-1, 1].\n",
    "#         \"\"\"\n",
    "#         \"\"\"ValueError: Unknown SavedObject type: None\n",
    "#         but work in wth tf2.6.0, tf2.7.0\n",
    "\n",
    "#         \"\"\"\n",
    "#         if model_name.startswith('ViT_b8'):\n",
    "#             handle=\"https://tfhub.dev/sayakpaul/vit_b8_fe/1\"\n",
    "#         if model_name.startswith('ViT_s16'):\n",
    "#             handle=\"https://tfhub.dev/sayakpaul/vit_s16_fe/1\"\n",
    "#         num_classes=5\n",
    "        \n",
    "#         # ViT model as a layer\n",
    "#         hub_layer = hub.KerasLayer(\"https://tfhub.dev/sayakpaul/vit_b8_fe/1\", trainable=True)\n",
    "#         model = tf.keras.Sequential([\n",
    "#                                         inputs,\n",
    "#                                         hub_layer,\n",
    "#                                         keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "#                                         ])\n",
    "\n",
    "    # ViT # ViT-keras, but seems need more epoch to train.\n",
    "    if model_name.startswith('ViT'):\n",
    "        \"\"\"For ViT (vit-keras)\n",
    "        There are models pre-trained on imagenet21k for the following architectures: ViT-B/16, ViT-B/32, ViT-L/16, ViT-L/32 and ViT-H/14. \n",
    "        There are also the same models pre-trained on imagenet21k and fine-tuned on imagenet2012.\n",
    "        \n",
    "        pip install -U --quiet vit-keras # for imagenet21k pre-trained weight.\n",
    "        pip install -U tensorflow-addons # for scratch\n",
    "        \"\"\"\n",
    "        \"\"\"base_model = vit.vit_b32\n",
    "        Downloading data from https://github.com/faustomorales/vit-keras/releases/download/dl/ViT-B_32_imagenet21k+imagenet2012.npz\n",
    "        \n",
    "        base_model = vit.vit_b16\n",
    "        Downloading data from https://github.com/faustomorales/vit-keras/releases/download/dl/ViT-B_16_imagenet21k+imagenet2012.npz\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        ValueError: Input 0 of layer global_average_pooling2d is incompatible with the layer: expected ndim=4, found ndim=2. Full shape received: (None, 768)\n",
    "        seems need use Flatten() to capture laster feature output.\n",
    "        \"\"\"\n",
    "        \"\"\"AssertionError: image_size must be a multiple of patch_size\n",
    "        600 / 16 = 37.5\n",
    "        512 / 16 = 32, only works\n",
    "        \"\"\"\n",
    "        from vit_keras import vit\n",
    "        #import tensorflow_addons as tfa\n",
    "        \n",
    "        #vit_b16 vit_b32  vit_L16 vit_L32  \n",
    "        base_model = vit.vit_b16(\n",
    "            image_size = img_width,\n",
    "            activation = 'softmax',\n",
    "            pretrained = True, #True,\n",
    "            include_top = True,\n",
    "            pretrained_top = False,\n",
    "            classes = 5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     # template #\n",
    "#     if model_name.startswith(''):\n",
    "#         \"\"\"For.\"\"\"\n",
    "#         BaseCnn = getattr(importlib.import_module(\"tensorflow.python.keras.applications.\"), model_name)     \n",
    "#         base_model = BaseCnn(include_top=False, weights=\"imagenet\", input_shape=(120,120,3)) #{'imagenet', None}    \n",
    "        \n",
    "#         rescaling_input = tf.keras.applications.inception_v3.preprocess_input(inputs)  \n",
    "\n",
    "\n",
    "#     # template #\n",
    "#     if model_name.startswith(''):\n",
    "#         \"\"\"For.\"\"\"\n",
    "#         BaseCnn = getattr(importlib.import_module(\"tensorflow.python.keras.applications.\"), model_name)     \n",
    "#         base_model = BaseCnn(include_top=False, weights=\"imagenet\", input_shape=(120,120,3)) #{'imagenet', None}    \n",
    "        \n",
    "#         rescaling_input = tf.keras.applications.inception_v3.preprocess_input(inputs)  \n",
    "\n",
    "\n",
    "    # ViT # tf.hub version [Waiting for twcc update CCS image for version 21.11]\n",
    "    if model_name.startswith('ViT'):\n",
    "        \"\"\"ViT was loaded above already.\"\"\"\n",
    "        print(f'Set {model_name}')\n",
    "        pass    \n",
    "    \n",
    "    elif model_name.startswith('ConvMixer'):\n",
    "        \"\"\"TEST\"\"\"\n",
    "        print(f'Set {model_name}')\n",
    "        pass  \n",
    "        \n",
    "    else:\n",
    "        # Freeze the pretrained weights\n",
    "        base_model.trainable = True #False #skip the TL so it shuld be change to True. and remove the free_model()\n",
    "        print(\"base_model.trainable : \", base_model.trainable)\n",
    "        print(f'Set others')\n",
    "\n",
    "        # move to Top \n",
    "        # How to add training=False in base_model create\n",
    "        #inputs = tf.keras.Input(shape=(120, 120, 3))\n",
    "        #rescal = rescaling_input()(inputs)\n",
    "        #b_m_output = base_model(inputs, training=False)\n",
    "\n",
    "        b_m_output = base_model(rescaling_input, training=False)\n",
    "\n",
    "        # Rebuild top\n",
    "        gap2d = tf.keras.layers.GlobalAveragePooling2D()(b_m_output) #(base_model.output)\n",
    "        #BNL = tf.keras.layers.BatchNormalization()(gap2d) #tood: remove#\n",
    "        dropout = tf.keras.layers.Dropout(top_dropout_rate)(gap2d)#tood: remove# J add dropout, for flood 0.2 is ok. for leaf 0.4 is better. for foot 0.8 is fine.\n",
    "        #outputs = tf.keras.layers.Dense(outputnum)(dropout)# remove activation for regression output (to default, the linear), , activation = 'relu' no help\n",
    "        outputs = tf.keras.layers.Dense(outputnum, activation=\"softmax\")(dropout)#todo: activation=\"softmax\", default is \"linear\" activation: a(x) = x\n",
    "\n",
    "\n",
    "        # Compile new model\n",
    "        model = tf.keras.Model(inputs, outputs, name=model_name)\n",
    "\n",
    "    \n",
    "    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.00001), #0.0001 1e-4 #RMSprop , Adam, SGD Adadelta(learning_rate=0.001), if set lr_callback the learning_rate=0.001 will not effeced.\n",
    "                loss=tf.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "    return model, base_model\n",
    "\n",
    "\n",
    "# Pick a model #\n",
    "Model_List = [\"Xception\", \"ResNet50\", \"ResNet101\", \"ResNet152\", \"InceptionV3\", \"MobileNet\", \"MobileNetV2\", # 0-6 \n",
    "\"DenseNet121\",\"DenseNet169\",\"DenseNet201\", # 7 8 9\n",
    "\"NASNetMobile\",\"NASNetLarge\", # 10 11 (hard code of size!224 331!)\n",
    "\"EfficientNetB0\", #12\n",
    "\"EfficientNetB1\", #13\n",
    "\"EfficientNetB3\",\n",
    "\"EfficientNetB5\", #15\n",
    "\"EfficientNetB7\", #16\n",
    "'VGG16', # 17 #at leass twcc21.11\n",
    "'VGG19', # 18\n",
    "'ViT_b8', #19 Vision Transformer\n",
    "'ViT_s16',\n",
    "'EANet',\n",
    "'ConvMixer', #\n",
    "'BiT', # BigTransfer\n",
    "]\n",
    "\n",
    "Model_List = Model_List[:19]\n",
    "print(Model_List)\n",
    "\n",
    "# model_name = Model_List[10]\n",
    "# # print(Model_List[1:3])\n",
    "# print(model_name)\n",
    "\n",
    "# # Pick a model #\n",
    "# Model_List = [#\"MobileNet\", \"MobileNetV2\", # 0-6 \n",
    "# # \"NASNetMobile\",\n",
    "#     \"NASNetLarge\",\n",
    "# ]\n",
    "\n",
    "model_name = Model_List[-1]\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabae6d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7aa41020",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\"> [Models] Train misc. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb36cbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor = 'val_accuracy' #'val_loss' 'val_accuracy' if use ed_loss it still the loss here.\n",
    "\n",
    "def mk_log_dir(log_dir_name):\n",
    "    try:\n",
    "        os.makedirs(log_dir_name)\n",
    "    except OSError as e:\n",
    "        print(\"This log dir exist.\")\n",
    "        if e.errno != errno.EEXIST:\n",
    "            raise ValueError(\"we got problem.\")\n",
    "\n",
    "def get_best_model_name(th):\n",
    "    return './' + log_dir_name + '/' + th + '_' + model_name + '_bs' + str(BATCH_SIZE) + '_w' + str(img_width) + '_best_' + monitor + '.h5'\n",
    "\n",
    "\n",
    "\n",
    "def get_best_model_name_bench():\n",
    "#     return './' + log_dir_name + '/' + th + '_' + model_name + '_' + crop + \"_\" + str(img_width)+ '_bs' + str(BATCH_SIZE)  + '_best_' + monitor + '.h5'\n",
    "    return f'./{log_dir_name}/{th}_{model_name}_{weight}_{crop}_{img_width}x{img_height}_{lr_name}_{augment}_bs{MULTI_BATCH_SIZE}_best_{monitor}.h5'\n",
    "\n",
    "\n",
    "# weight=\"imagenet1k\" #random\n",
    "# crop= \"resize\" #cencrop :centercrop\n",
    "# lr= \"CDCLR\" # WCD, platrure\n",
    "# aug= \"AA\" # AA, RA, NoisyStudent, all\n",
    "\n",
    "\n",
    "# th = 'toe'\n",
    "# th = 'heel'\n",
    "\n",
    "# th = \"stage1\"\n",
    "th = \"ft\" # fine tune only without transfer learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0812976e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use once at the time\n",
    "# log_dir_name = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# log_dir_name = \"TrainSaveDir\"\n",
    "\n",
    "mk_log_dir(log_dir_name)\n",
    "\n",
    "best_model_name = get_best_model_name(th)\n",
    "\n",
    "best_model_save = tf.keras.callbacks.ModelCheckpoint(filepath=best_model_name, \n",
    "                             save_best_only = True, \n",
    "                             save_weights_only = False,\n",
    "                             monitor = monitor, \n",
    "                             mode = 'auto', verbose = 1)\n",
    "print('best_model_name:', best_model_name)\n",
    "\n",
    "logdir = log_dir_name + \"/logs/toe/\"\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c25819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"RRNET\"\n",
    "# img_width = img_height =120\n",
    "# BATCH_SIZE = 32\n",
    "\n",
    "# log_dir_name = \"TrainSaveDir\"\n",
    "\n",
    "# best_model_name = get_best_model_name_bench(th)\n",
    "# print('best_model_name:', best_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b577f1a2",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\"> [Models] Learning Rate Scheduler </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bbb07f",
   "metadata": {},
   "source": [
    "## WCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb43697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StepWise warmup cosine decay learning rate [optimazer]\n",
    "\n",
    "# Reference:\n",
    "# https://colab.research.google.com/github/sayakpaul/ViT-jax2tf/blob/main/fine_tune.ipynb\n",
    "# https://www.kaggle.com/ashusma/training-rfcx-tensorflow-tpu-effnet-b2\n",
    "# https://colab.research.google.com/github/google-research/vision_transformer/blob/linen/vit_jax.ipynb\n",
    "\n",
    "class WarmUpCosine(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(\n",
    "        self, learning_rate_base, total_steps, warmup_learning_rate, warmup_steps\n",
    "    ):\n",
    "        super(WarmUpCosine, self).__init__()\n",
    "\n",
    "        self.learning_rate_base = learning_rate_base\n",
    "        self.total_steps = total_steps\n",
    "        self.warmup_learning_rate = warmup_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.pi = tf.constant(np.pi)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        if self.total_steps < self.warmup_steps:\n",
    "            raise ValueError(\"Total_steps must be larger or equal to warmup_steps.\")\n",
    "        learning_rate = (\n",
    "            0.5\n",
    "            * self.learning_rate_base\n",
    "            * (\n",
    "                1\n",
    "                + tf.cos(\n",
    "                    self.pi\n",
    "                    * (tf.cast(step, tf.float32) - self.warmup_steps)\n",
    "                    / float(self.total_steps - self.warmup_steps)\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if self.warmup_steps > 0:\n",
    "            if self.learning_rate_base < self.warmup_learning_rate:\n",
    "                raise ValueError(\n",
    "                    \"Learning_rate_base must be larger or equal to \"\n",
    "                    \"warmup_learning_rate.\"\n",
    "                )\n",
    "            slope = (\n",
    "                self.learning_rate_base - self.warmup_learning_rate\n",
    "            ) / self.warmup_steps\n",
    "            warmup_rate = slope * tf.cast(step, tf.float32) + self.warmup_learning_rate\n",
    "            learning_rate = tf.where(\n",
    "                step < self.warmup_steps, warmup_rate, learning_rate\n",
    "            )\n",
    "        return tf.where(\n",
    "            step > self.total_steps, 0.0, learning_rate, name=\"learning_rate\"\n",
    "        )\n",
    "\n",
    "    # 2021-12-19\n",
    "    # If use optimizers.schedules.LearningRateSchedule and to save full model, need to rewrite the get_config()\n",
    "    # https://stackoverflow.com/questions/61557024/notimplementederror-learning-rate-schedule-must-override-get-config\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'learning_rate_base':self.learning_rate_base,\n",
    "            'total_steps':self.total_steps,\n",
    "            'warmup_learning_rate':self.warmup_learning_rate,\n",
    "            'warmup_steps':self.warmup_steps,\n",
    "        }    \n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd905ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds_map_s: = all_train_img\n",
    "# train_ds_pre: = all_train_img/batch\n",
    "\n",
    "num_train = tf.data.experimental.cardinality(train_ds_map_s)\n",
    "num_val = tf.data.experimental.cardinality(valid_ds_map_s)\n",
    "print(f\"Number of training examples: {num_train}\")\n",
    "print(f\"Number of validation examples: {num_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b85b2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "EPOCHS_WARM = 10\n",
    "TOTAL_STEPS = int((num_train / MULTI_BATCH_SIZE) * EPOCHS)\n",
    "WARMUP_STEPS = int((num_train / MULTI_BATCH_SIZE) * EPOCHS_WARM) #10 # warmup 2 epochs\n",
    "INIT_LR = 0.004 #4e-3\n",
    "WAMRUP_LR = 0.00001 #1e-5\n",
    "\n",
    "print(f'total step/EPOCHS: {TOTAL_STEPS}/{EPOCHS}, MULTI_BATCH_SIZE:{MULTI_BATCH_SIZE}')\n",
    "\n",
    "print(f'total step= (num_train / MULTI_BATCH_SIZE) * EPOCHS : {TOTAL_STEPS}=({num_train}/{MULTI_BATCH_SIZE})*{EPOCHS}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da14316b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "WCD = WarmUpCosine(\n",
    "    learning_rate_base=INIT_LR,\n",
    "    total_steps=TOTAL_STEPS,\n",
    "    warmup_learning_rate=WAMRUP_LR,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    ")\n",
    "\n",
    "lrs = [WCD(step) for step in range(TOTAL_STEPS)]\n",
    "plt.plot(lrs)\n",
    "plt.xlabel(\"Step\", fontsize=14)\n",
    "plt.ylabel(\"LR\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9c8417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "rng = [i for i in range(TOTAL_STEPS)]\n",
    "y = [WCD(step) for step in rng]\n",
    "sns.set(style='darkgrid')\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "plt.plot(rng, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f53b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# treate step as epoch \n",
    "\n",
    "# need to reduce the lr, cause InceptionV3,\n",
    "EPOCHS = 20\n",
    "EPOCHS_WARM = 5\n",
    "\n",
    "TOTAL_STEPS = EPOCHS\n",
    "WARMUP_STEPS = EPOCHS_WARM\n",
    "\n",
    "# INIT_LR = 0.001 #4e-3\n",
    "# WAMRUP_LR = 0.00001 #1e-5\n",
    "\n",
    "# for some models need very small lr.\n",
    "INIT_LR = 0.00001 #1e-5\n",
    "WAMRUP_LR = 0.0000001 #1e-7\n",
    "\n",
    "WCD = WarmUpCosine(\n",
    "    learning_rate_base=INIT_LR,\n",
    "    total_steps=TOTAL_STEPS,\n",
    "    warmup_learning_rate=WAMRUP_LR,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "lrs = [WCD(step) for step in range(TOTAL_STEPS)]\n",
    "y = lrs\n",
    "x = [x for x in range(TOTAL_STEPS)]\n",
    "\n",
    "sns.set(style='white') # will affect the further plot include plt or sns. \n",
    "\n",
    "plt.plot(x,lrs)\n",
    "plt.xticks(x)\n",
    "\n",
    "plt.xlabel(\"Epoch\", fontsize=14)\n",
    "plt.ylabel(\"Learning rate\", fontsize=14)\n",
    "\n",
    "for a,b in zip(x, y): \n",
    "    #plt.text(a, b, str(b))\n",
    "    plt.scatter(a,b, color='black', alpha=0.2)\n",
    "    plt.annotate(f'{b:.8f}',xy=(a,b)) # offest text:, xytext=(10,10), textcoords='offset points'\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02ea623",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{} ~ {}'.format(min(y), max(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97733ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in y:\n",
    "    print(i.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a78eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_list=[0.00001,\n",
    "# 0.0002,\n",
    "# 0.0004,\n",
    "# 0.0006,\n",
    "# 0.0008,\n",
    "# 0.001,\n",
    "# 0.000989,\n",
    "# 0.000956,\n",
    "# 0.000904,\n",
    "# 0.000834,\n",
    "# 0.00075,\n",
    "# 0.000654,\n",
    "# 0.000552,\n",
    "# 0.00044,\n",
    "# 0.00034,\n",
    "# 0.00024,\n",
    "# 0.00016,\n",
    "# 0.0001,\n",
    "# 0.00004,\n",
    "# 0.00001,\n",
    "# ]\n",
    "\n",
    "# sns.set(style='white') # will affect the further plot include plt or sns. \n",
    "\n",
    "# x = [x for x in range(1, 21, 1)]\n",
    "\n",
    "# plt.plot(x,lr_list)\n",
    "# plt.xticks(x)\n",
    "\n",
    "# plt.xlabel(\"Epoch\", fontsize=14)\n",
    "# plt.ylabel(\"Learning rate\", fontsize=14)\n",
    "\n",
    "# for a,b in zip(x, lr_list): \n",
    "#     #plt.text(a, b, str(b))\n",
    "#     plt.scatter(a,b, color='black', alpha=0.2)\n",
    "#     plt.annotate(f'{b:.8f}',xy=(a,b)) # offest text:, xytext=(10,10), textcoords='offset points'\n",
    "\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96070bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(20):\n",
    "#     print(i, lr_list[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f696795",
   "metadata": {},
   "source": [
    "## CDR\n",
    "#### warm Cosine Ddecay Rrestart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba74ddab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "ep_num = EPOCHS\n",
    "\n",
    "initial_learning_rate = 1e-5\n",
    "first_decay_steps = 5\n",
    "\n",
    "CosineDecayCLRWarmUpLSW = tf.keras.experimental.CosineDecayRestarts(\n",
    "          initial_learning_rate,\n",
    "          first_decay_steps,\n",
    "          t_mul=1.0,\n",
    "          m_mul=1.0,\n",
    "          alpha = initial_learning_rate,\n",
    "          name=\"CCosineDecayRestarts\")\n",
    "\n",
    "rng = [i for i in range(ep_num)]\n",
    "y = [CosineDecayCLRWarmUpLSW(x) for x in rng]\n",
    "sns.set(style='darkgrid')\n",
    "fig, ax = plt.subplots(figsize=(20, 6))\n",
    "# plt.ylim(.0000000000000001, .01)# for too large loss\n",
    "ax.yaxis.set_major_formatter(FormatStrFormatter('%.12f'))# for too small loss\n",
    "plt.plot(rng, y)\n",
    "plt.xticks(rng)\n",
    "\n",
    "for a,b in zip(rng, y): \n",
    "    #plt.text(a, b, str(b))\n",
    "    plt.scatter(a,b, color='black', alpha=0.2)\n",
    "    plt.annotate(f'{b:.8f}',xy=(a,b)) # offest text:, xytext=(10,10), textcoords='offset points'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38323bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CDR = CosineDecayCLRWarmUpLSW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a00b3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    print(i, y[i].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb75066",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f97a7d9",
   "metadata": {},
   "source": [
    "#### fixed_WCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33476800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed_WCD(epoch):\n",
    "    return lr_list[epoch]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5218cdcb",
   "metadata": {},
   "source": [
    "#### lrdump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5321bef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"dump lr (Epochwise: callback)\n",
    "\"\"\"\n",
    "ep_num_transf = 500\n",
    "\n",
    "\n",
    "\n",
    "def lrdump(epoch):\n",
    "    \n",
    "    #step_size = 100\n",
    "    lr_max = 0.004\n",
    "    lr_min = 0.00001\n",
    "    lr_start = 0.00001\n",
    "\n",
    "    lr_init_ep = 0\n",
    "    lr_ramp_ep = 50\n",
    "    lr_sus_ep  = 0 #10\n",
    "    lr_decay   = 0.95\n",
    "\n",
    "    \n",
    "    # warm up\n",
    "    if epoch < lr_init_ep:\n",
    "        lr = (lr_max - lr_min) / lr_ramp_ep * epoch + lr_min    \n",
    "        \n",
    "    elif lr_init_ep -1 < epoch < lr_ramp_ep:\n",
    "        lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "\n",
    "    elif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "        lr = lr_max\n",
    "\n",
    "    else:\n",
    "        lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
    "\n",
    "    return lr\n",
    "\n",
    "rng = [i for i in range(ep_num_transf)]\n",
    "y = [lrdump(x) for x in rng]\n",
    "sns.set(style='darkgrid')\n",
    "fig, ax = plt.subplots(figsize=(20, 6))\n",
    "plt.plot(rng, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc7e34c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dcc45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{} ~ {}'.format(min(y), max(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bfe59a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for e, lr in zip(rng,y):\n",
    "    print('{}\\t {}\\n'.format(e, lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf3c67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed_scheduler(epoch):\n",
    "    return 0.0001 # 1e4\n",
    "#     return 0.00001 # 1e5\n",
    "#     return 0.000001 # 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bcc586",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e5ad4c6",
   "metadata": {},
   "source": [
    "## Select LR_SCHEDULER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5726af9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select LR_SCHEDULER [move inside the build_efn()]\n",
    "\n",
    "if lr_name=='fixed':\n",
    "    scheduled_lr = tf.keras.callbacks.LearningRateScheduler(fixed_scheduler)\n",
    "    print(fixed_scheduler(1))\n",
    "    \n",
    "if lr_name=='lrdump':\n",
    "    scheduled_lr = tf.keras.callbacks.LearningRateScheduler(lrdump)\n",
    "\n",
    "if lr_name=='plateau':\n",
    "    #learning_rate=0.0001 in the tf.keras.optimizers.Adam()\n",
    "    scheduled_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=monitor, factor=0.5, patience=1, verbose=1, \n",
    "        mode='auto', min_delta=0.0001, cooldown=0, min_lr=1e-8)\n",
    "    \n",
    "if lr_name=='WCD':\n",
    "    # tk.optimizer\n",
    "    scheduled_lr = tf.keras.callbacks.LearningRateScheduler(WCD)\n",
    "    # tk.callback\n",
    "#     scheduled_lr = tf.keras.callbacks.LearningRateScheduler(fixed_WCD)\n",
    "\n",
    "if lr_name=='CDR':\n",
    "    # tk.optimizer\n",
    "    scheduled_lr = tf.keras.callbacks.LearningRateScheduler(CDR)\n",
    "    \n",
    "\n",
    "    \n",
    "if lr_name=='WCDCR': # warmup cosin decay with cycle\n",
    "    pass\n",
    "\n",
    "print(f'Set scheduler LR : {lr_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e67a212",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b295f1d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc39d122",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\"> [Models] Callback </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b6c294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2021-11-25 #\n",
    "#TODO: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback 改寫為callbacks.on_epoch_begin(epoch) 並加入datetime方便在訓練時觀察時間\n",
    "class PrintLR(tf.keras.callbacks.Callback):\n",
    "#     def on_epoch_begin(self, epoch, logs=None):\n",
    "#         self.epoch=epoch\n",
    "#         print(f\"第 {epoch} 執行週期開始...\")\n",
    "#     def on_epoch_end(self, epoch, logs=None):\n",
    "#         print('\\nLearning rate for epoch {} is {}'.format(epoch + 1, model.optimizer.lr.numpy()))\n",
    "\n",
    "\n",
    "#     def on_epoch_begin(self, epoch, logs=None):\n",
    "#         print(f'[{datetime.now()}] Learning rate for epoch {epoch + 1} is {model.optimizer.lr.numpy()}')\n",
    "        \n",
    "    # time of epoch\n",
    "    def on_train_begin(self,  epoch, logs=None):\n",
    "        self.times = []\n",
    "    def on_epoch_begin(self,  epoch, logs=None):\n",
    "        self.epoch_time_start = time.time()\n",
    "        #print(f'[{datetime.now()}] Learning rate for epoch {epoch + 1} is {model_toe.optimizer.lr.numpy()}') #for epochwise\n",
    "        print(f'[{datetime.now()}] Learning rate for epoch {epoch + 1} is {self.model.optimizer._decayed_lr(tf.float32).numpy()}') #for stepwise\n",
    "        \n",
    "#         s_time = time.time()\n",
    "#         print(\"start\")\n",
    "\n",
    "    def on_epoch_end(self,  epoch, logs=None):\n",
    "        wall_time = time.time() - self.epoch_time_start\n",
    "        self.times.append(wall_time)\n",
    "        #print(f'[{self.times}] of epoch {epoch + 1}')\n",
    "        print(f'[{wall_time}] of epoch {epoch + 1}')\n",
    "\n",
    "        \n",
    "callback_lr_time = PrintLR() #return a object of callback, not use the Classs PrintLR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4536f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks = [\n",
    "# #     tensorboard_callback,\n",
    "#     best_model_save,\n",
    "#     tf.keras.callbacks.EarlyStopping(monitor=monitor, patience=patience_1), #patience=step_size or ep_num\n",
    "# #     lr_reduceonplateau,\n",
    "#     tf.keras.callbacks.LearningRateScheduler(lrdump),#lrdump, decay or lrfn or lrfn2. clr\n",
    "#     callback_lr_time\n",
    "# #     tensorboard_callback,\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef16de9",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\"> [Models] Build models </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380a0a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with strategy.scope():\n",
    "#     # Model building/compiling need to be within `strategy.scope()`.\n",
    "#     model_toe, base_model = build_efn_model(model_name, outputnum, top_dropout_rate, drop_connect_rate) # for efnet, Xincept\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d906d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(model_toe.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61014ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = 0\n",
    "nt = 0\n",
    "# for layer in model_toe.layers:\n",
    "#     if layer.trainable:\n",
    "#         tt +=1\n",
    "#         print(f'{layer.name}')\n",
    "#     else:\n",
    "#         nt +=1\n",
    "# print(f'tt: {tt}, nt:{nt}')\n",
    "\n",
    "def count_model_trainOrNot_layers(model, printlayers=False):\n",
    "    tt = 0\n",
    "    nt = 0\n",
    "    for layer in model.layers:\n",
    "        if layer.trainable:\n",
    "            tt +=1\n",
    "            if printlayers:\n",
    "                print(f'{layer.name}')\n",
    "        else:\n",
    "            nt +=1\n",
    "    print('\\n*********************************** Start fine tune ***********************************')\n",
    "    print(f'tt: {tt}, nt:{nt}, total layers:{tt+nt}')\n",
    "    print('*********************************** Start fine tune ***********************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18266300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_model_trainOrNot_layers(model_toe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047c0909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc842ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_toe.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e467d9",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\"> [Models] Train top layers (transfer learning)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288d2278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # fit the model on all data\n",
    "# history_toe = model_toe.fit(train_ds_pre, \n",
    "#                       verbose=1, \n",
    "#                       epochs=5, #ep_num_transf, \n",
    "#                       validation_data=valid_ds_pre, \n",
    "#                       callbacks=callbacks)#, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94adb7a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75566ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33e1c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EFntB7 \n",
    "# inputs = 600*600, 8 gpu, bs4x8\n",
    "\n",
    "# TL seems not work!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc8295d",
   "metadata": {},
   "source": [
    "EFntB7 \n",
    "inputs = 512*512, 8 gpu, bs32x8\n",
    "\n",
    "Epoch 00012: val_accuracy improved from 0.73917 to 0.74478, saving model to ./TrainSaveDir/stage1_EfficientNetB7_bs32_w512_best_val_accuracy.h5\n",
    "[40.45903658866882] of epoch 12\n",
    "Epoch 13/500\n",
    "[2021-12-07 13:03:08.241426] Learning rate for epoch 13 is 0.0009675999754108489\n",
    "59/59 [==============================] - 39s 618ms/step - loss: 0.7936 - accuracy: 0.7024 - val_loss: 0.6922 - val_accuracy: 0.7445\n",
    "\n",
    "Epoch 00013: val_accuracy did not improve from 0.74478\n",
    "[38.982457876205444] of epoch 13\n",
    "CPU times: user 1h 30min 59s, sys: 9min 19s, total: 1h 40min 19s\n",
    "Wall time: 13min 11s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d7d4e7",
   "metadata": {},
   "source": [
    "MBnetV2\n",
    "DS, inputs = 512*512, 8 gpu, bs32x8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c96a65",
   "metadata": {},
   "source": [
    "MBnetV2\n",
    "DS, inputs = 512*512, 8 gpu, bs64x8\n",
    "\n",
    "\n",
    "Epoch 00032: val_accuracy did not improve from 0.72608\n",
    "[34.99274802207947] of epoch 32\n",
    "CPU times: user 3h 1min 32s, sys: 8min 26s, total: 3h 9min 59s\n",
    "Wall time: 21min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94efc685",
   "metadata": {},
   "source": [
    "MBnetV2\n",
    "DS, inputs = 120*120\n",
    "\n",
    "Epoch 00016: val_accuracy did not improve from 0.68027\n",
    "[11.85263991355896] of epoch 16\n",
    "Epoch 17/500\n",
    "235/235 [==============================] - 12s 44ms/step - loss: 1.1775 - accuracy: 0.6076 - val_loss: 0.9085 - val_accuracy: 0.6768\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460e32ca",
   "metadata": {},
   "source": [
    "## 5. Fine tune\n",
    "\n",
    "#### <font color=\"yellow\"> [Models] Train bench models (Fine tune)</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1f1013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def unfreeze_model(model, base_model):\n",
    "# #     # We unfreeze the top 20 layers while leaving BatchNorm layers frozen\n",
    "# #     for layer in model.layers[-20:]:\n",
    "# #         if not isinstance(layer, layers.BatchNormalization):\n",
    "# #             layer.trainable = True\n",
    "\n",
    "# #     model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),#RMSprop , Adam, SGD Adadelta(learning_rate=0.001), if set lr_callback the learning_rate=0.001 will not effeced.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #     fine_tune_at = 20 #10 #241 #20\n",
    "# #     print('[Note] Now create model fine tuneing at Top-{} layers!'.format(fine_tune_at))\n",
    "# #     for layer in model_toe.layers[-fine_tune_at:]:\n",
    "# #         if not isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "# #             layer.trainable = True\n",
    "\n",
    "\n",
    "# #\n",
    "# #'block7a_expand_conv'20 'block6c_expand_conv'50 'block6a_expand_conv'79 'block5b_expand_conv'109 'block4a_expand_conv' 166 \n",
    "# #\n",
    "\n",
    "#     # Set All layers trainable first\n",
    "#     #model.trainable = True #範例似乎是指 base_model.trainable = True 而不是新建model！！！！！！(2021-11-08)\n",
    "#     base_model.trainable = True\n",
    "    \n",
    "    \n",
    "    \n",
    "#     # Then, set layers NOT trainable below specific layer.\n",
    "# #     set_trainable = False\n",
    "# #     for layer in model.layers:\n",
    "# #         if layer.name == 'block5b_expand_conv': \n",
    "# #             set_trainable = True\n",
    "# #         if set_trainable:\n",
    "# #             layer.trainable = True\n",
    "# #         else:\n",
    "# #             layer.trainable = False\n",
    "\n",
    "# #     model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),#RMSprop , Adam, SGD Adadelta(learning_rate=0.001), if set lr_callback the learning_rate=0.001 will not effeced.\n",
    "# #                     loss=ed_metric_2d_mean)\n",
    "#     model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),#RMSprop , Adam, SGD Adadelta(learning_rate=0.001), if set lr_callback the learning_rate=0.001 will not effeced.\n",
    "#                 loss=tf.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "#                  metrics=['accuracy'])\n",
    "\n",
    "\"\"\"NOTE THAT: if re-compile model again, the model seems will loss the weight it learned before!?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b88d425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfreeze for finetune the toe model  \n",
    "# unfreeze_model(model_toe,base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3296c104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_model_trainOrNot_layers(model_toe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15f77e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_toe.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566bb799",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# best_model_name_s2 = get_best_model_name(th)\n",
    "\n",
    "# best_model_save_s2 = tf.keras.callbacks.ModelCheckpoint(filepath=best_model_name_s2, \n",
    "#                              save_best_only = True, \n",
    "#                              save_weights_only = False,\n",
    "#                              monitor = monitor, \n",
    "#                              mode = 'auto', verbose = 1)\n",
    "# print('best_model_name_s2:', best_model_name_s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033614ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_reduceonplateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "#     monitor=monitor, factor=0.1, patience=5, verbose=1, \n",
    "#     mode='auto', min_delta=0.0001, cooldown=0, min_lr=1e-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bf41a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks_s2 = [\n",
    "# #     tensorboard_callback,\n",
    "#     best_model_save_s2,\n",
    "#     tf.keras.callbacks.EarlyStopping(monitor=monitor, patience=patience_2), #patience=step_size or ep_num\n",
    "# #     lr_reduceonplateau,\n",
    "#     tf.keras.callbacks.LearningRateScheduler(lrdump),#lrdump, decay or lrfn or lrfn2. clr\n",
    "#     callback_lr_time\n",
    "# #     tensorboard_callback,\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259a0e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # fit the model on all data\n",
    "# history_toe_finetune = model_toe.fit(train_ds_pre, \n",
    "#                       verbose=1, \n",
    "#                       epochs= 100,#ep_num_transf, \n",
    "#                       validation_data=valid_ds_pre, \n",
    "#                       callbacks=callbacks_s2)#, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fe3418",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0456818a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ad5fd84",
   "metadata": {},
   "source": [
    "## <font color=\"yellow\"> [Models] Train bench models (Fine tune)</font>\n",
    "\n",
    "copy from K10L195"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef59d2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "takenote=[\"Xception\", \"ResNet50\", \"ResNet101\", \"ResNet152\", \"InceptionV3\", \"MobileNet\", \"MobileNetV2\", # 0-6 \n",
    "\"DenseNet121\",\"DenseNet169\",\"DenseNet201\", # 7 8 9\n",
    "\"NASNetMobile\",\"NASNetLarge\", # 10 11 (hard code of size!224 331!)\n",
    "\"EfficientNetB0\", #12\n",
    "\"EfficientNetB1\", #13\n",
    "\"EfficientNetB3\",\n",
    "\"EfficientNetB5\", #15\n",
    "\"EfficientNetB7\", #16\n",
    "'VGG16', # 17 #at leass twcc21.11\n",
    "'VGG19', # 18\n",
    "'ViT_b8', #19 Vision Transformer\n",
    "'ViT_s16',\n",
    "'EANet',\n",
    "'ConvMixer', #\n",
    "'BiT', # BigTransfer\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c9a747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model_List = takenote[:19]\n",
    "Model_List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4f26c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# history_toe = []\n",
    "# history_toe_finetune = []\n",
    "\n",
    "# use dict of dict to store hist\n",
    "history_toe_finetune = {}\n",
    "\n",
    "# Model_List = [\"Xception\",\"MobileNet\", \n",
    "# \"EfficientNetB0\", \"NASNetMobile\",\n",
    "# ]\n",
    "\n",
    "# Model_List = [\"MobileNet\", \n",
    "# \"EfficientNetB0\",\n",
    "# ]\n",
    "\n",
    "for model_name in Model_List:\n",
    "    print(\"\\n \\n K = \", model_name, \"\\n\")\n",
    "    best_model_name = get_best_model_name_bench()\n",
    "    best_model_save = tf.keras.callbacks.ModelCheckpoint(filepath=best_model_name, \n",
    "                                 save_best_only = True, \n",
    "                                 save_weights_only = False,\n",
    "                                 monitor = monitor, \n",
    "                                 mode = 'auto', verbose = 1)\n",
    "\n",
    "    callbacks_s2 = [\n",
    "                #     tensorboard_callback,\n",
    "                    best_model_save,\n",
    "                    tf.keras.callbacks.EarlyStopping(monitor=monitor, patience=patience_2), #patience=step_size or ep_num\n",
    "                #     lr_reduceonplateau,\n",
    "#                     tf.keras.callbacks.LearningRateScheduler(lrdump),#lrdump, decay or lrfn or lrfn2. clr\n",
    "                    scheduled_lr,\n",
    "                    callback_lr_time,\n",
    "                #     tensorboard_callback,\n",
    "                ]\n",
    "    print('best_model_name:', best_model_name)\n",
    "\n",
    "\n",
    "    with strategy.scope():\n",
    "        model_toe, base_model = build_efn_model(model_name, outputnum, top_dropout_rate, drop_connect_rate)\n",
    "    \n",
    "#     # Train K-Model with transfer learning # IF HAVE!\n",
    "#     hist = model_toe.fit(train_ds_pre_toe_s, \n",
    "#                           verbose=1, \n",
    "#                           epochs=ep_num_transf, \n",
    "#                           validation_data=valid_ds_pre_toe_s, \n",
    "#                           callbacks=callbacks_toe_tl)#, validation_split=0.1)\n",
    "#     history_toe.append(hist)\n",
    "    \n",
    "    \n",
    "    # Train K-Model with fine tune #\n",
    "    \n",
    "    # bench models, FT\n",
    "#     unfreeze_model(model_toe,base_model) # skip the TL so unfreeze when build_EFN()\n",
    "    count_model_trainOrNot_layers(model_toe)\n",
    "    print(model_toe.summary())\n",
    "    # fit the model on all data\n",
    "    hist = model_toe.fit(train_ds_pre, \n",
    "                          verbose=1, #'auto' or 0 1 2 , 0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
    "                          epochs=EPOCHS, \n",
    "                          validation_data=valid_ds_pre, \n",
    "                          callbacks=callbacks_s2)#, validation_split=0.1)\n",
    "    # add the epoch timeing\n",
    "    hist.history['epoch_time_secs'] = callback_lr_time.times\n",
    "    history_toe_finetune[model_name] = hist.history #hist # what if use hist.history\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba03b18d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3a95f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0a34c26",
   "metadata": {},
   "source": [
    "# MobileNetV2, NASNetMobile\n",
    "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/utils/generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
    "  warnings.warn('Custom mask layers require a config and must override '\n",
    "  \n",
    "  https://stackoverflow.com/questions/69590274/warning-custom-mask-layers-require-a-config-and-must-override-when-saving-the\n",
    "  it was an issue with old versions (from TF2.5, TF2.6, and TF2.7).\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0fa03a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e012edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68478790",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5c25a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# history_toe_finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a54182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # hist is dict\n",
    "# # but history_toe_finetune is a list of dict without the name\n",
    "\n",
    "# history_toe_finetune['EfficientNetB0']['loss']\n",
    "# # history_toe_finetune[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf03cc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ED sum\n",
    "def get_valloss(his_v_l):  \n",
    "    return np.min(his_v_l), np.argmin(his_v_l)\n",
    "\n",
    "t_vl = []\n",
    "# h_vl = []\n",
    "for k in Model_List:\n",
    "    print(f'K:{k}')\n",
    "    t_v, _ = get_valloss(history_toe_finetune[k]['val_loss'])\n",
    "#     h_v, _ = get_valloss(history_heel_finetune[k].history['val_loss'])\n",
    "    \n",
    "    t_vl.append(t_v)\n",
    "#     h_vl.append(h_v)\n",
    "\n",
    "# t_vl = np.mean(t_vl, axis=0)\n",
    "# h_vl = np.mean(h_vl, axis=0)\n",
    "# print(f'{round(t_vl,5)} + {round(h_vl,5)} = {round(t_vl + h_vl,5)}')\n",
    "\n",
    "t_vl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a15cd32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b689a7e0",
   "metadata": {},
   "source": [
    "## <font color=\"yellow\"> [Models] Result Ploting</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af53a650",
   "metadata": {},
   "source": [
    "### Save hist to np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7666c886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist save to npy\n",
    "# This bench log name\n",
    "bench_log_name = f'./{log_dir_name}/{th}_bench_{weight}_{crop}_{img_width}x{img_height}_{lr_name}_{augment}_bs{MULTI_BATCH_SIZE}_best_{monitor}.npy'\n",
    "np.save(bench_log_name, history_toe_finetune)\n",
    "\n",
    "# reload hist from npy\n",
    "history_np_load = np.load(bench_log_name, allow_pickle='TRUE').item()\n",
    "hisnp = history_np_load.copy()\n",
    "\n",
    "# ploting train log\n",
    "\n",
    "\n",
    "# ploting test acc %"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5622267",
   "metadata": {},
   "source": [
    "### Reload np to hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c4f38a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d266f72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draft reload his from saved np\n",
    "bench_log_name = f'./{log_dir_name}/{th}_bench_{weight}_{crop}_{img_width}x{img_height}_{lr_name}_{augment}_bs{MULTI_BATCH_SIZE}_best_{monitor}.npy'\n",
    "\n",
    "# Note that, if change bs4 to bs32, when load hist by one gpu container #\n",
    "# bench_log_name = './TrainSaveDir-1223-CDR/ft_bench_imagenet1k_crop_512x512_CDR_RA_bs32_best_val_accuracy.npy'\n",
    "\n",
    "\n",
    "# reload hist from npy\n",
    "history_np_load = np.load(bench_log_name, allow_pickle='TRUE').item()\n",
    "hisnp = history_np_load.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfbac9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "handles = [handle for handle in Model_List]\n",
    "handles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942afb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ploting train log\n",
    "plt.figure(figsize=(25, 10))\n",
    "\n",
    "for k in Model_List:\n",
    "\n",
    "    x = len(hisnp[k]['loss']) + 1\n",
    "    x = range(1,x,1)\n",
    "    y = hisnp[k]['loss']\n",
    "    #y = [f'{z:.4f}' for z in y]\n",
    "    plt.plot(x, y, label=f'{k}_loss')\n",
    "    \n",
    "    \n",
    "    for a,b in zip(x, y): \n",
    "        #plt.text(a, b, str(b))\n",
    "        plt.scatter(a,b, color='black', alpha=0.2)\n",
    "        plt.annotate(f'{b:.3f}',xy=(a,b)) # offest text:, xytext=(10,10), textcoords='offset points'\n",
    "    \n",
    "    y = hisnp[k]['val_loss']\n",
    "    plt.plot(x, y, label=f'{k}_val_loss')\n",
    "\n",
    "    \n",
    "plt.title('K-model ed loss toe-TL')\n",
    "plt.ylabel('ed loss'), plt.ylim(0, 2)# for too large loss\n",
    "plt.xlabel('epoch')\n",
    "# plt.legend(['k0 loss', 'k0 val_loss', 'k1 loss', 'k1 val_loss', 'k2 loss', 'k2 val_loss', \n",
    "#             'k3 loss', 'k3 val_loss', 'k4 loss', 'k4 val_loss'], loc='upper left')\n",
    "# plt.show()\n",
    "\n",
    "# plt.legend(['x','y'], title='models', loc='best')\n",
    "\n",
    "# get handles\n",
    "# handles = [handle for handle in Model_List]\n",
    "# plt.legend(handles, title='Nets')\n",
    "\n",
    "plt.legend(title='Nets')\n",
    "\n",
    "\n",
    "# save plot : comment plo.show in jupyter notebook.  dpi=600 is good for journal.\n",
    "dpi=300 # for quickly check\n",
    "print(f'Save to {pgn} \\n')\n",
    "pgn=f'{log_dir_name}/{th}_bench_{weight}_{crop}_{img_width}x{img_height}_{lr_name}_{augment}_bs{MULTI_BATCH_SIZE}_best_{monitor}_hisnpTL_loss.png'\n",
    "plt.savefig(pgn, bbox_inches = 'tight', dpi=dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8530e287",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in Model_List:\n",
    "\n",
    "    print(f\"{k}: {hisnp[k]['val_accuracy']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2002e6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ploting train log\n",
    "plt.figure(figsize=(25, 10))\n",
    "\n",
    "for k in Model_List:\n",
    "\n",
    "    x = len(hisnp[k]['val_accuracy']) + 1\n",
    "    x = range(1,x,1)\n",
    "    y = hisnp[k]['val_accuracy']\n",
    "    #y = [f'{z:.4f}' for z in y]\n",
    "    plt.plot(x, y, label=f'{k}_val_accuracy')\n",
    "    \n",
    "    \n",
    "    for a,b in zip(x, y): \n",
    "        #plt.text(a, b, str(b))\n",
    "        plt.scatter(a,b, color='black', alpha=0.2)\n",
    "        plt.annotate(f'{b:.3f}',xy=(a,b)) # offest text:, xytext=(10,10), textcoords='offset points'\n",
    "    \n",
    "    #plt.plot(hisnp[k]['val_accuracy'])\n",
    "\n",
    "    \n",
    "plt.title('K-model-TL val_accuracy', fontsize='xx-large')\n",
    "plt.ylabel('val_accuracy'), plt.ylim(0.1, 0.9)# for too large loss\n",
    "plt.xlabel('epoch')\n",
    "# plt.legend(['k0 loss', 'k0 val_loss', 'k1 loss', 'k1 val_loss', 'k2 loss', 'k2 val_loss', \n",
    "#             'k3 loss', 'k3 val_loss', 'k4 loss', 'k4 val_loss'], loc='upper left')\n",
    "# plt.show()\n",
    "\n",
    "# plt.legend(['x','y'], title='models', loc='best')\n",
    "\n",
    "# get handles\n",
    "# handles = [handle for handle in Model_List]\n",
    "# plt.legend(handles, title='Nets')\n",
    "\n",
    "plt.legend(title='Nets:', title_fontsize='x-large', fontsize='large')\n",
    "\"\"\"title_fontsize: int or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# save plot : comment plo.show in jupyter notebook.  dpi=600 is good for journal.\n",
    "dpi=300\n",
    "pgn= f'{log_dir_name}/{th}_bench_{weight}_{crop}_{img_width}x{img_height}_{lr_name}_{augment}_bs{MULTI_BATCH_SIZE}_best_{monitor}_hisnpTL_val_acc.png'\n",
    "print(f'Save to {pgn} \\n')\n",
    "plt.savefig(pgn, bbox_inches = 'tight', dpi=dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab1dc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in Model_List:\n",
    "\n",
    "    print(f\"{k}: {hisnp[k]['val_accuracy']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2c9e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hisnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35dd907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ploting train log with lr\n",
    "# plt.figure(figsize=(25, 10))\n",
    "\n",
    "# for different scales (different Y-axes)\n",
    "# fig, ax1 = plt.subplots()\n",
    "fig, ax1 = plt.subplots(figsize=(25, 10))\n",
    "\n",
    "# ax1 for val_accuracy #\n",
    "# nice to have this colorful tip.\n",
    "color = 'tab:red'\n",
    "ax1.set_title('K-model-TL val_accuracy with lr', fontsize='xx-large')\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.set_ylabel('val_accuracy', color=color)\n",
    "\n",
    "for k in Model_List:\n",
    "\n",
    "    x = len(hisnp[k]['val_accuracy']) + 1\n",
    "    x = range(1,x,1)\n",
    "    y = hisnp[k]['val_accuracy']\n",
    "    #y = [f'{z:.4f}' for z in y]\n",
    "    plt.plot(x, y, label=f'{k}_val_accuracy')\n",
    "    \n",
    "    \n",
    "    for a,b in zip(x, y): \n",
    "        #plt.text(a, b, str(b))\n",
    "        plt.scatter(a,b, color='black', alpha=0.2)\n",
    "        plt.annotate(f'{b:.3f}',xy=(a,b)) # offest text:, xytext=(10,10), textcoords='offset points'\n",
    "\n",
    "ax1.tick_params(axis='y', labelcolor=color)    \n",
    "ax1.legend(title='Nets:', title_fontsize='x-large', fontsize='large')\n",
    "\"\"\"title_fontsize: int or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}\"\"\"\n",
    "\n",
    "\n",
    "# ax2 for learning rate #\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "color = 'tab:green'\n",
    "ax2.set_ylabel('learning rate', color=color)\n",
    "# ax2.plot(hisnp[k]['lr'], color=color)\n",
    "x = len(hisnp[k]['lr']) + 1\n",
    "x = range(1,x,1)\n",
    "y = hisnp[k]['lr']\n",
    "#y = [f'{z:.4f}' for z in y]\n",
    "plt.plot(x, y, color='green', label=f'learning rate')\n",
    "\n",
    "for a,b in zip(x, y): \n",
    "    #plt.text(a, b, str(b))\n",
    "    plt.scatter(a,b, color='green', alpha=0.2)\n",
    "    plt.annotate(f'{b:.7f}',xy=(a,b)) # offest text:, xytext=(10,10), textcoords='offset points'\n",
    "\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.legend(fontsize='large', loc='upper center') \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "# save plot : comment plo.show in jupyter notebook.  dpi=600 is good for journal.\n",
    "dpi=300\n",
    "pgn= f'{log_dir_name}/{th}_bench_{weight}_{crop}_{img_width}x{img_height}_{lr_name}_{augment}_bs{MULTI_BATCH_SIZE}_best_{monitor}_hisnpTL_val_acc_lr.png'\n",
    "print(f'Save to {pgn} \\n')\n",
    "plt.savefig(pgn, bbox_inches = 'tight', dpi=dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67bc964",
   "metadata": {},
   "source": [
    "### with cycle color/marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfce3445",
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['lines.linewidth'] = 1.5\n",
    "matplotlib.rcParams[\"markers.fillstyle\"] = 'left' # 'full', 'left', 'right', 'bottom', 'top', 'none'\n",
    "\n",
    "\n",
    "auto_custom_cycler_01 = (cycler(color=[plt.get_cmap('jet')(i/13) for i in range(24)]) + # 24 colors\n",
    "                  cycler(linestyle=['-', '--', ':', '-.'] * 6) + # [4]*6 = 24\n",
    "                    cycler(marker=['o', 'v', '^', '<', '>', '8', 's', 'p', '*', 'h', 'H', 'D'] * 2)) # [12]*2 = 24\n",
    "\n",
    "\n",
    "# ploting train log with lr\n",
    "# plt.figure(figsize=(25, 10))\n",
    "\n",
    "# for different scales (different Y-axes)\n",
    "# fig, ax1 = plt.subplots()\n",
    "fig, ax1 = plt.subplots(figsize=(25, 15))\n",
    "\n",
    "\n",
    "ax1.set_prop_cycle(auto_custom_cycler_01) # set to use custom_cycler\n",
    "\n",
    "\n",
    "# ax1 for val_accuracy #\n",
    "# nice to have this colorful tip.\n",
    "color = 'tab:red'\n",
    "ax1.set_title('K-model-TL val_accuracy with lr', fontsize='xx-large')\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.set_ylabel('val_accuracy', color=color)\n",
    "\n",
    "for k in Model_List:\n",
    "\n",
    "    x = len(hisnp[k]['val_accuracy']) + 1\n",
    "    x = range(1,x,1)\n",
    "    y = hisnp[k]['val_accuracy']\n",
    "    #y = [f'{z:.4f}' for z in y]\n",
    "    plt.plot(x, y, label=f'{k}_val_accuracy')\n",
    "    \n",
    "#     # v_a value #\n",
    "#     for a,b in zip(x, y): \n",
    "#         #plt.text(a, b, str(b))\n",
    "#         plt.scatter(a,b, color='black', alpha=0.2)\n",
    "#         plt.annotate(f'{b:.3f}',xy=(a,b)) # offest text:, xytext=(10,10), textcoords='offset points'\n",
    "\n",
    "        \n",
    "ax1.tick_params(axis='y', labelcolor=color)    \n",
    "# ax1.legend(title='Nets:', title_fontsize='x-large', fontsize='large')\n",
    "\"\"\"title_fontsize: int or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}\"\"\"\n",
    "ax1.legend(title='Nets:', title_fontsize='x-large', fontsize='large', bbox_to_anchor=(1.23, 1.0))\n",
    "\n",
    "\n",
    "\n",
    "# ax2 for learning rate #\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "color = 'tab:green'\n",
    "ax2.set_ylabel('learning rate', color=color)\n",
    "# ax2.plot(hisnp[k]['lr'], color=color)\n",
    "x = len(hisnp[k]['lr']) + 1\n",
    "x = range(1,x,1)\n",
    "y = hisnp[k]['lr']\n",
    "#y = [f'{z:.4f}' for z in y]\n",
    "plt.plot(x, y, color='green', label=f'learning rate')\n",
    "\n",
    "# lr value #\n",
    "# for a,b in zip(x, y): \n",
    "#     #plt.text(a, b, str(b))\n",
    "#     plt.scatter(a,b, color='green', alpha=0.2)\n",
    "#     plt.annotate(f'{b:.7f}',xy=(a,b)) # offest text:, xytext=(10,10), textcoords='offset points'\n",
    "\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.legend(fontsize='large', loc='lower right', bbox_to_anchor=(1.15, .4)) \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "# save plot : comment plo.show in jupyter notebook.  dpi=600 is good for journal.\n",
    "dpi=300\n",
    "pgn= f'{log_dir_name}/{th}_bench_{weight}_{crop}_{img_width}x{img_height}_{lr_name}_{augment}_bs{MULTI_BATCH_SIZE}_best_{monitor}_hisnpTL_val_acc_lr_cyc.png'\n",
    "print(f'Save to {pgn} \\n')\n",
    "plt.savefig(pgn, bbox_inches = 'tight', dpi=dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edd500b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best val_acc and the epoch #\n",
    "\n",
    "print(\"[max val_acc]\")\n",
    "tmp_acc_his = []\n",
    "for k in Model_List:\n",
    "    v_a = hisnp[k]['val_accuracy']\n",
    "    b_e = np.argmax(v_a)\n",
    "    print(f'  {k}-------------------------:\\n\\t       {v_a[b_e]} Epoch@P{b_e}')\n",
    "    \n",
    "    tmp_acc_his.append(v_a[b_e])\n",
    "\n",
    "print('\\n[Best model and val_acc]')\n",
    "b_m_e = np.argmax(tmp_acc_his)\n",
    "print(f'  {Model_List[b_m_e]} : {tmp_acc_his[b_m_e]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4e7aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "hisnp[k]['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04819af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hisnp[k]['lr'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26612cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "float(hisnp[k]['lr'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a840554e",
   "metadata": {},
   "outputs": [],
   "source": [
    "float('1.426282e-05') # e-05無法轉float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c229a282",
   "metadata": {},
   "outputs": [],
   "source": [
    "float('1.426282e-04') #e-04可以表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8de57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "float(3.25753e+10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d44e8f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9433b496",
   "metadata": {},
   "source": [
    "## Evaluate the valid and test accuracy\n",
    "\n",
    "    for DenseNet201_imagenet1k_crop_512x512_CDR_RA_bs32\n",
    "     the eval/test accuracy is match the training result.\n",
    "\n",
    "    803/803 [==============================] - 45s 56ms/step - loss: 0.3456 - accuracy: 0.8962\n",
    "    Test accuracy : 0.8962293863296509\n",
    "    803/803 [==============================] - 31s 39ms/step - loss: 0.3273 - accuracy: 0.8965\n",
    "    Test accuracy : 0.8965409994125366"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbe58ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evl_m_path = log_dir_name + \"/\" + \"ft_DenseNet201_imagenet1k_crop_512x512_CDR_RA_bs32_best_val_accuracy.h5\"\n",
    "# evl_model = tf.keras.models.load_model(evl_m_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114cb813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss, accuracy = evl_model.evaluate(valid_ds_pre)\n",
    "# print('Test accuracy :', accuracy)\n",
    "\n",
    "# # print(\"count roughly ds size: \", tf.data.experimental.cardinality(valid_ds_pre).numpy() * BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6936b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss, accuracy = evl_model.evaluate(test_ds_pre)\n",
    "# print('Test accuracy :', accuracy)\n",
    "\n",
    "# # print(\"count roughly ds size: \", tf.data.experimental.cardinality(val_ds_pre).numpy() * BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41223b70",
   "metadata": {},
   "source": [
    "### confusion matrix move to finianl script\n",
    "\n",
    "    * model-eval_acc_confusion.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab17ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72d9924c",
   "metadata": {},
   "source": [
    "## misc test log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8284b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 1.1232483386993408\n",
    "\n",
    "ll = \"{:10.4f}\".format(l)\n",
    "print(ll)\n",
    "print(f'{l:10.4f}')\n",
    "\n",
    "ll = \"{:0.4f}\".format(l)\n",
    "print(ll)\n",
    "print(f'{l:0.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b27f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = hisnp[k]['loss']\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306a66a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "[\"{:.4f}\".format(z) for z in y] #note: 0.71758 -> 0.7176"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ba8386",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([f'{z:.4f}' for z in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a793cbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(0.7175814509391785, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f004b3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0.7175814509391785\n",
    "str(n)[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9170e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = '0.7175814509391785'\n",
    "d, f = n.split(\".\")\n",
    "str(d + \".\" + f[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f895a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trunc(num, digits):\n",
    "    l = str(float(num)).split('.')\n",
    "    digits = min(len(l[1]), digits)\n",
    "    return l[0] + '.' + l[1][:digits]\n",
    "\n",
    "print(trunc(n, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f60bd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507ebc0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f631b173",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c898878",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2592f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,3,1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba2be72",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(range(1,3,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37475ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dict test\n",
    "dic1={'loss': [1.2079896926879883,\n",
    "  1.0480238199234009,\n",
    "  1.0053461790084839,\n",
    "  0.9194557070732117,\n",
    "  0.9075971841812134],\n",
    " 'accuracy': [0.5908271670341492,\n",
    "  0.6259430050849915,\n",
    "  0.6331531405448914,\n",
    "  0.6556512713432312,\n",
    "  0.658588707447052],\n",
    " 'val_loss': [0.9052042365074158,\n",
    "  0.8515479564666748,\n",
    "  0.8730301260948181,\n",
    "  0.7421011328697205,\n",
    "  0.7377873659133911],\n",
    " 'val_accuracy': [0.6650046706199646,\n",
    "  0.6687441468238831,\n",
    "  0.6656278967857361,\n",
    "  0.6983484029769897,\n",
    "  0.6974135041236877],\n",
    " 'lr': [1e-05, 8.98e-05, 0.0001696, 0.0002494, 0.0003292]}\n",
    "\n",
    "dic2={'loss': [1.2529504299163818,\n",
    "  0.9882707595825195,\n",
    "  0.8480234742164612,\n",
    "  0.7772594094276428,\n",
    "  0.7426433563232422],\n",
    " 'accuracy': [0.5742039084434509,\n",
    "  0.6400961875915527,\n",
    "  0.7013819813728333,\n",
    "  0.7340276837348938,\n",
    "  0.7386341094970703],\n",
    " 'val_loss': [0.9145353436470032,\n",
    "  0.7174426317214966,\n",
    "  0.5882198214530945,\n",
    "  0.6258523464202881,\n",
    "  0.6130486726760864],\n",
    " 'val_accuracy': [0.6400747895240784,\n",
    "  0.7323153614997864,\n",
    "  0.7890308499336243,\n",
    "  0.768463671207428,\n",
    "  0.7678404450416565],\n",
    " 'lr': [1e-05, 8.98e-05, 0.0001696, 0.0002494, 0.0003292]}\n",
    "dic3={'loss': [1.2426950931549072,\n",
    "  0.8300295472145081,\n",
    "  0.6558055281639099,\n",
    "  0.6005755662918091,\n",
    "  0.5661617517471313],\n",
    " 'accuracy': [0.5582482218742371,\n",
    "  0.6869617700576782,\n",
    "  0.7658722400665283,\n",
    "  0.7941785454750061,\n",
    "  0.8049936890602112],\n",
    " 'val_loss': [0.9312921762466431,\n",
    "  0.6461496949195862,\n",
    "  0.6557254791259766,\n",
    "  0.5172611474990845,\n",
    "  0.4652194380760193],\n",
    " 'val_accuracy': [0.6378934383392334,\n",
    "  0.7843564748764038,\n",
    "  0.7834216356277466,\n",
    "  0.8267372846603394,\n",
    "  0.8404487371444702],\n",
    " 'lr': [1e-05, 8.98e-05, 0.0001696, 0.0002494, 0.0003292]}\n",
    "\n",
    "big_hist = {}\n",
    "\n",
    "big_hist['dic1']=dic1\n",
    "big_hist['dic2']=dic2\n",
    "\n",
    "big_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414b37fc",
   "metadata": {},
   "source": [
    "# 注意!不能執行的CELL會使nbconvert失敗不能存檔即便是在最尾端<整個運行都部會儲存>\n",
    "\n",
    "nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:\n",
    "------------------\n",
    "for different DS: 800*600 ->> 512resize/600resize/512crop/600crop\n",
    "\n",
    "\u001b[0;31m for different DS: 800*600 ->> 512resize/600resize/512crop/600crop\u001b[0m\n",
    "\u001b[0m ^\u001b[0m\n",
    "\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n",
    "SyntaxError: invalid syntax (2723153103.py, line 1)\n",
    "2021-12-18 03:20:38\n",
    "[End]\n",
    "\n",
    "[APT ABOUT 3 mins]\n",
    "\n",
    "[Start]\n",
    "2021-12-17 17:48:22\n",
    "2021-12-18 03:20:38\n",
    "[End]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34a91f1",
   "metadata": {},
   "source": [
    "for different DS: 800*600 ->> 512resize/600resize/512crop/600crop\n",
    "\n",
    "EFntB7 \n",
    "inputs = 600*600, 8 gpu, bs4x8\n",
    "\n",
    "Epoch 00014: val_accuracy did not improve from 0.88252\n",
    "[205.34342765808105] of epoch 14\n",
    "CPU times: user 5h 35min 30s, sys: 20min 23s, total: 5h 55min 53s\n",
    "Wall time: 56min 51s\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321bf3ea",
   "metadata": {},
   "source": [
    "ResNet152 inputs = 512*512, 8 gpu, bs4x8\n",
    "\n",
    "Epoch 00011: val_accuracy did not improve from 0.83640\n",
    "[134.74419260025024] of epoch 11\n",
    "CPU times: user 2h 26min 57s, sys: 4min 11s, total: 2h 31min 9s\n",
    "Wall time: 29min 14s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9cf168",
   "metadata": {},
   "source": [
    "Xception inputs = 512*512, 8 gpu, bs4x8\n",
    "\n",
    "somehow TL not work??? cause FT train from inf!\n",
    "\n",
    "Epoch 00005: val_accuracy did not improve from 0.83017\n",
    "[79.85516905784607] of epoch 5\n",
    "CPU times: user 48min 24s, sys: 2min 27s, total: 50min 51s\n",
    "Wall time: 8min 13s\n",
    "    \n",
    "Epoch 00018: val_accuracy did not improve from 0.86600\n",
    "[72.12753582000732] of epoch 18\n",
    "CPU times: user 2h 37min 51s, sys: 4min 40s, total: 2h 42min 32s\n",
    "Wall time: 23min 47s    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfbca17",
   "metadata": {},
   "source": [
    "DenseNet201  inputs = 512*512, 8 gpu, bs4x8\n",
    "\n",
    "after 4 mins, it ran. (without TL [stage1])\n",
    "\n",
    "Epoch 00005: val_accuracy did not improve from 0.81739\n",
    "[113.5602593421936] of epoch 5\n",
    "Epoch 6/500\n",
    "[2021-12-07 17:04:56.111606] Learning rate for epoch 6 is 0.0004090000002179295\n",
    "469/469 [==============================] - 113s 237ms/step - loss: 0.6786 - accuracy: 0.7655 - val_loss: 0.6962 - val_accuracy: 0.7918\n",
    "\n",
    "Epoch 00006: val_accuracy did not improve from 0.81739\n",
    "[113.0628821849823] of epoch 6\n",
    "CPU times: user 1h 34min 3s, sys: 2min 42s, total: 1h 36min 46s\n",
    "Wall time: 17min 21s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa64c3ef",
   "metadata": {},
   "source": [
    "EFntB7 inputs = 512*512, 8 gpu, bs4x8\n",
    "\n",
    "after 5 mins, it ran. (without TL [stage1])\n",
    "\n",
    "[592.2609107494354] of epoch 1\n",
    "[210.42193007469177] of epoch 3\n",
    "Epoch 00004: val_accuracy did not improve from 0.88221\n",
    "[181.73404240608215] of epoch 4\n",
    "\n",
    "Epoch 5/500\n",
    "[2021-12-07 15:54:04.367304] Learning rate for epoch 5 is 0.00032920000376179814\n",
    "469/469 [==============================] - 180s 381ms/step - loss: 0.3675 - accuracy: 0.8766 - val_loss: nan - val_accuracy: 0.0000e+00\n",
    "\n",
    "Epoch 00006: val_accuracy did not improve from 0.88221\n",
    "[181.85825777053833] of epoch 6\n",
    "CPU times: user 2h 12min 22s, sys: 7min 30s, total: 2h 19min 53s\n",
    "Wall time: 25min 43s                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9456224e",
   "metadata": {},
   "source": [
    "EFntB7 inputs = 512*512, 8 gpu, bs2x8\n",
    "\n",
    "after 4 mins, it ran. then 10min epoch 1 val_loss: nan. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61ac8e7",
   "metadata": {},
   "source": [
    "EFntB7 inputs = 512*512, 8 gpu, bs16x8 and bs8x8\n",
    "\n",
    "\n",
    "CancelledError:  [_Derived_]RecvAsync is cancelled.\n",
    "\t [[{{node div_no_nan_1/ReadVariableOp_6/_3732}}]] [Op:__inference_train_function_778904]\n",
    "\n",
    "Function call stack:\n",
    "train_function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e01c98",
   "metadata": {},
   "source": [
    "EFntB7 \n",
    "inputs = 512*512, 8 gpu, bs32x8\n",
    "\n",
    "CancelledError:  [_Derived_]RecvAsync is cancelled.\n",
    "\t [[{{node div_no_nan/ReadVariableOp_6/_3768}}]] [Op:__inference_train_function_843519]\n",
    "\n",
    "Function call stack:\n",
    "train_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad3f121",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f2557e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62ef848c",
   "metadata": {},
   "source": [
    "MBnetV2 8 gpu FT 512x512 BS32 patience5\n",
    "\n",
    "Epoch 00010: val_accuracy did not improve from 0.86974\n",
    "[33.78622841835022] of epoch 10\n",
    "Epoch 11/500\n",
    "[2021-12-07 12:39:57.489634] Learning rate for epoch 11 is 0.0008079999824985862\n",
    "59/59 [==============================] - 34s 542ms/step - loss: 0.5160 - accuracy: 0.8253 - val_loss: 0.4515 - val_accuracy: 0.8672\n",
    "\n",
    "Epoch 00011: val_accuracy did not improve from 0.86974\n",
    "[34.4537308216095] of epoch 11\n",
    "CPU times: user 1h 7min 24s, sys: 2min 38s, total: 1h 10min 2s\n",
    "Wall time: 8min 10s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b4da5d",
   "metadata": {},
   "source": [
    "MBnetV2 8 gpu FT 512x512 BS64\n",
    "\n",
    "OOM when allocating tensor with shape[64,96,256,256]\n",
    "if img is int8 (1B) then\n",
    "64x96x256x256 x1B = 402.653MB\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00fa3ba",
   "metadata": {},
   "source": [
    "MBnetV2 8 gpu FT 120x120\n",
    "\n",
    "Epoch 00024: val_accuracy did not improve from 0.77470\n",
    "[4.618327617645264] of epoch 24\n",
    "Epoch 25/500\n",
    "[2021-12-07 11:09:18.232609] Learning rate for epoch 25 is 0.0019251999910920858\n",
    "30/30 [==============================] - 5s 102ms/step - loss: 0.7740 - accuracy: 0.7228 - val_loss: 0.8236 - val_accuracy: 0.7018\n",
    "\n",
    "Epoch 00025: val_accuracy did not improve from 0.77470\n",
    "[4.819160223007202] of epoch 25\n",
    "CPU times: user 23min 26s, sys: 1min 10s, total: 24min 37s\n",
    "Wall time: 4min 10s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8459f1ab",
   "metadata": {},
   "source": [
    "MBnetV2 two gpu FT 120x120\n",
    "\n",
    "Epoch 00003: val_accuracy did not improve from 0.74945\n",
    "[9.133079528808594] of epoch 3\n",
    "118/118 [==============================] - 9s 59ms/step - loss: 1.1131 - accuracy: 0.6141 - val_loss: 2.5206 - val_accuracy: 0.1399\n",
    "\n",
    "Epoch 00019: val_accuracy did not improve from 0.74945\n",
    "CPU times: user 15min 1s, sys: 21.1 s, total: 15min 22s\n",
    "Wall time: 3min 52s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaa158c",
   "metadata": {},
   "source": [
    "MBnetV2 one gpu FT 120x120\n",
    "Epoch 00012: val_accuracy did not improve from 0.74135\n",
    "[13.294872760772705] of epoch 12\n",
    "CPU times: user 8min 18s, sys: 5.84 s, total: 8min 24s\n",
    "Wall time: 3min 18s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a7bf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e003f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ef18e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2021-11-26\n",
    "# # hist save and load test https://stackoverflow.com/questions/41061457\n",
    "\n",
    "\n",
    "# history_fine = history_toe_finetune\n",
    "\n",
    "# \"\"\"Then history is a dictionary and you can retrieve all desirable values using the keys.\"\"\"\n",
    "# # add the epoch timeing\n",
    "# history_fine.history['epoch_time_secs'] = callback_lr_time.times\n",
    "\n",
    "# np.save('test_history.npy', history_fine.history)\n",
    "# history_np_load = np.load('test_history.npy', allow_pickle='TRUE').item()\n",
    "\n",
    "\n",
    "\n",
    "# \"\"\"As history.history is a dict, you can convert it as well to a pandas DataFrame object, which can then be saved to suit your needs.\"\"\"\n",
    "# # convert the history.history dict to a pandas DataFrame:     \n",
    "# hist_df = pd.DataFrame(history_fine.history) \n",
    "\n",
    "# # save to json:  \n",
    "# hist_json_file = 'test_history.json' \n",
    "# with open(hist_json_file, mode='w') as f:\n",
    "#     hist_df.to_json(f)\n",
    "# # with open('data.json') as f:\n",
    "# #     data = json.load(f)\n",
    "# with open(hist_json_file) as f:\n",
    "#     t_h_json = json.load(f)\n",
    "    \n",
    "# # or save to csv: \n",
    "# hist_csv_file = 'test_history.csv'\n",
    "# with open(hist_csv_file, mode='w') as f:\n",
    "#     hist_df.to_csv(f)\n",
    "\n",
    "# t_h_csv = pd.read_csv(hist_csv_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f107a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(history_np_load['loss'])\n",
    "# print(history_np_load['loss'][3]) # the loss at epoch n+1\n",
    "\n",
    "# print(history_np_load['val_accuracy'])\n",
    "# print(history_np_load['val_accuracy'][0]) # the loss at epoch n+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4d5372",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(type(t_h_json))\n",
    "# print(\"train_loss:\", t_h_json[\"loss\"], \"\\n\") #take a sub dict\n",
    "\n",
    "# t_h_json # all dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b6ae64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t_h_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d6eb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for only fine tune\n",
    "# acc = history_fine.history['accuracy']\n",
    "# val_acc = history_fine.history['val_accuracy']\n",
    "\n",
    "# loss = history_fine.history['loss']\n",
    "# val_loss = history_fine.history['val_loss']\n",
    "\n",
    "\n",
    "# # for only fine tune\n",
    "# fig = plt.figure(figsize=(8, 8), num=model_name)\n",
    "# fig.suptitle(model_name + '(top-100 layers)', fontsize=14, fontweight='bold')\n",
    "# print(\"base model and training pahse: {}\".format(model_name + '_top-100layer' + '_lr' + str(lr) + '_e' + str(ep_num_transf) ))\n",
    "\n",
    "\n",
    "# plt.subplot(2, 1, 1)\n",
    "# plt.plot(acc, label='Training Accuracy')\n",
    "# plt.plot(val_acc, label='Validation Accuracy')\n",
    "# plt.ylim([0.1, 1])\n",
    "# # plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "# #           plt.ylim(), label='Start Fine Tuning')\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.title('Training and Validation Accuracy')\n",
    "\n",
    "# plt.subplot(2, 1, 2)\n",
    "# plt.plot(loss, label='Training Loss')\n",
    "# plt.plot(val_loss, label='Validation Loss')\n",
    "# plt.ylim([0, 5.0])\n",
    "# # plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "# #          plt.ylim(), label='Start Fine Tuning')\n",
    "# plt.legend(loc='upper right')\n",
    "# plt.title('Training and Validation Loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beefafdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load hist from saved npy\n",
    "\n",
    "# # for only fine tune\n",
    "# acc = history_np_load['accuracy']\n",
    "# val_acc = history_np_load['val_accuracy']\n",
    "\n",
    "# loss = history_np_load['loss']\n",
    "# val_loss = history_np_load['val_loss']\n",
    "\n",
    "\n",
    "# # for only fine tune\n",
    "# fig = plt.figure(figsize=(8, 8), num=model_name)\n",
    "# fig.suptitle(model_name + '(top-100 layers)', fontsize=14, fontweight='bold')\n",
    "# print(\"base model and training pahse: {}\".format(model_name + '_top-100layer' + '_lr' + str(lr) + '_e' + str(ep_num_transf) ))\n",
    "\n",
    "\n",
    "# plt.subplot(2, 1, 1)\n",
    "# plt.plot(acc, label='Training Accuracy')\n",
    "# plt.plot(val_acc, label='Validation Accuracy')\n",
    "# plt.ylim([0.1, 1])\n",
    "# # plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "# #           plt.ylim(), label='Start Fine Tuning')\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.title('Training and Validation Accuracy')\n",
    "\n",
    "# plt.subplot(2, 1, 2)\n",
    "# plt.plot(loss, label='Training Loss')\n",
    "# plt.plot(val_loss, label='Validation Loss')\n",
    "# plt.ylim([0, 5.0])\n",
    "# # plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "# #          plt.ylim(), label='Start Fine Tuning')\n",
    "# plt.legend(loc='upper right')\n",
    "# plt.title('Training and Validation Loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bf34a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333002cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# loss, accuracy = model_toe.evaluate(test_ds_pre)\n",
    "# print('Test accuracy :', accuracy)\n",
    "\n",
    "# print(\"count roughly ds size: \", tf.data.experimental.cardinality(test_ds_pre).numpy() * MULTI_BATCH_SIZE)\n",
    "# \"\"\"the last model may is nan not best one\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0eb3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # current_model_name = 'leaf-2020-12-01-EfficientNetB7_top-layer50_lr_lrfn_val-acc.8352_wh512_e37.h5'\n",
    "# model_back = tf.keras.models.load_model(best_model_name_s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7d4b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# loss, accuracy = model_back.evaluate(test_ds_pre)\n",
    "# print('Test accuracy :', accuracy)\n",
    "\n",
    "# print(\"count roughly ds size: \", tf.data.experimental.cardinality(test_ds_pre).numpy() * MULTI_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f982f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# efnetB7 epoch 6, val_acc:0.882, test_acc:0.891\n",
    "\n",
    "# 101/101 [==============================] - 36s 356ms/step - loss: 0.3229 - accuracy: 0.8916\n",
    "# Test accuracy : 0.8915550112724304\n",
    "# count roughly ds size:  3232\n",
    "# CPU times: user 10.1 s, sys: 4.25 s, total: 14.4 s\n",
    "# Wall time: 36 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6cdd60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5e7295",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f532361",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5873eba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb88d87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6716dc74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02113b30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b63d5bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6201a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6c43ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716b0999",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc1d495",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c851aece",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44197a91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ab184d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4143fead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c33e92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
