{
 "cells": [
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V8-yl-s-WKMG"
   },
   "source": [
    "2019-12-24 with TF2.0\n",
    "\n",
    "The kernel appears to have died. It will restart automatically.\n",
    "通常都是套件、環境衝突，導致jupyter執行中斷，要逐步檢查套件版本、位置、呼叫正確！\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "2019-12-26 with TF1.14\n",
    "\n",
    " 看來此jpupyter是在TF2跑，反而不能用tf1.x!!!!\n",
    " \n",
    " \n",
    "\n",
    "# Object Detection API Demo\n",
    "\n",
    "<table align=\"left\"><td>\n",
    "  <a target=\"_blank\"  href=\"https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab\n",
    "  </a>\n",
    "</td><td>\n",
    "  <a target=\"_blank\"  href=\"https://colab.sandbox.google.com/github/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb\">\n",
    "    <img width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "</td></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3cIrseUv6WKz"
   },
   "source": [
    "Welcome to the [Object Detection API](https://github.com/tensorflow/models/tree/master/research/object_detection). This notebook will walk you step by step through the process of using a pre-trained model to detect objects in an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VrJaG0cYN9yh"
   },
   "source": [
    "> **Important**: This tutorial is to help you through the first step towards using [Object Detection API](https://github.com/tensorflow/models/tree/master/research/object_detection) to build models. If you just just need an off the shelf model that does the job, see the [TFHub object detection example](https://colab.sandbox.google.com/github/tensorflow/hub/blob/master/examples/colab/object_detection.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kFSqkTCdWKMI"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "awjrpqy-6MaQ"
   },
   "source": [
    "Important: If you're running on a local machine, be sure to follow the [installation instructions](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md). This notebook includes only what's necessary to run in Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p3UGXxUii5Ym"
   },
   "source": [
    "### Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hGL97-GXjSUw"
   },
   "outputs": [],
   "source": [
    "#!pip install -U --pre tensorflow==\"2.*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n_ap_s9ajTHH"
   },
   "source": [
    "Make sure you have `pycocotools` installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bg8ZyA47i3pY",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 http://archive.ubuntu.com/ubuntu bionic InRelease [242 kB]\n",
      "Get:2 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
      "Get:3 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [777 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]      \u001b[0m\u001b[33m\n",
      "Get:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]    \u001b[0m\u001b[33m\n",
      "Get:6 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB]33m\u001b[33m\n",
      "Get:7 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB]0m\u001b[33m\u001b[33m\n",
      "Get:9 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [21.8 kB]\n",
      "Get:10 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [6781 B]\n",
      "Get:11 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [796 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB]   \u001b[0m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\n",
      "Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [35.5 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [10.8 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1325 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [1074 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [2496 B]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [4241 B]\n",
      "Fetched 17.4 MB in 6s (3066 kB/s)33m            \u001b[33m\u001b[33m\u001b[33m\u001b[33m\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "20 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "Collecting pycocotools\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/96/84/9a07b1095fd8555ba3f3d519517c8743c2554a245f9476e5e39869f948d2/pycocotools-2.0.0.tar.gz (1.5MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5MB 1.2MB/s eta 0:00:01\n",
      "\u001b[31m    ERROR: Command errored out with exit status 1:\n",
      "     command: /usr/bin/python -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-anbpg49v/pycocotools/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-anbpg49v/pycocotools/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /tmp/pip-install-anbpg49v/pycocotools/pip-egg-info\n",
      "         cwd: /tmp/pip-install-anbpg49v/pycocotools/\n",
      "    Complete output (5 lines):\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"/tmp/pip-install-anbpg49v/pycocotools/setup.py\", line 2, in <module>\n",
      "        from Cython.Build import cythonize\n",
      "    ModuleNotFoundError: No module named 'Cython'\n",
      "    ----------------------------------------\u001b[0m\n",
      "\u001b[31mERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# !sudo apt update;\n",
    "# !sudo apt-get install protobuf-compiler python-pil python-lxml python-tk;\n",
    "# !sudo pip install Cython;\n",
    "# !sudo pip install pycocotools;\n",
    "# !sudo pip install contextlib2; sudo pip install opencv-python; sudo pip install matplotlib;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-vsOL3QR6kqs"
   },
   "source": [
    "Get `tensorflow/models` or `cd` to parent directory of the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ykA0c-om51s1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "\n",
    "if \"models\" in pathlib.Path.cwd().parts:\n",
    "  while \"models\" in pathlib.Path.cwd().parts:\n",
    "    os.chdir('..')\n",
    "elif not pathlib.Path('models').exists():\n",
    "  !git clone --depth 1 https://github.com/tensorflow/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O219m6yWAj9l"
   },
   "source": [
    "Compile protobufs and install the object_detection package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PY41vdYYNlXc"
   },
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# cd models/research/\n",
    "# protoc object_detection/protos/*.proto --python_out=."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s62yJyQUcYbp"
   },
   "outputs": [],
   "source": [
    "# %%bash \n",
    "# cd models/research\n",
    "# sudo pip install ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LBdjK2G5ywuc"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hV4P5gyTWKMI"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "# import tensorflow.compat.v1 as tf\n",
    "import zipfile\n",
    "\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r5FNuiRPWKMN"
   },
   "source": [
    "Import the object detection module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4-IMl4b6BdGO"
   },
   "outputs": [],
   "source": [
    "from object_detection.utils import ops as utils_ops\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RYPCiag2iz_q"
   },
   "source": [
    "Patches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mF-YlMl8c_bM"
   },
   "outputs": [],
   "source": [
    "# patch tf1 into `utils.ops`\n",
    "utils_ops.tf = tf.compat.v1\n",
    "\n",
    "# Patch the location of gfile\n",
    "tf.gfile = tf.io.gfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cfn_tRFOWKMO"
   },
   "source": [
    "# Model preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X_sEBLpVWKMQ"
   },
   "source": [
    "## Variables\n",
    "\n",
    "Any model exported using the `export_inference_graph.py` tool can be loaded here simply by changing the path.\n",
    "\n",
    "By default we use an \"SSD with Mobilenet\" model here. See the [detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) for a list of other models that can be run out-of-the-box with varying speeds and accuracies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7ai8pLZZWKMS"
   },
   "source": [
    "## Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zm8xp-0eoItE"
   },
   "outputs": [],
   "source": [
    "# tf.saved_model.load=tf.compat.v1.saved_model.loader.load # not work\n",
    "\n",
    "def load_model(model_name):\n",
    "  base_url = 'http://download.tensorflow.org/models/object_detection/'\n",
    "  model_file = model_name + '.tar.gz'\n",
    "  model_dir = tf.keras.utils.get_file(\n",
    "    fname=model_name, \n",
    "    origin=base_url + model_file,\n",
    "    untar=True)\n",
    "\n",
    "  model_dir = pathlib.Path(model_dir)/\"saved_model\"\n",
    "\n",
    "  #model = tf.saved_model.load(str(model_dir))\n",
    "  model = tf.compat.v2.saved_model.load(str(model_dir), None)\n",
    "  model = model.signatures['serving_default']\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_1MVVTcLWKMW"
   },
   "source": [
    "## Loading label map\n",
    "Label maps map indices to category names, so that when our convolution network predicts `5`, we know that this corresponds to `airplane`.  Here we use internal utility functions, but anything that returns a dictionary mapping integers to appropriate string labels would be fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hDbpHkiWWKMX"
   },
   "outputs": [],
   "source": [
    "# List of the strings that is used to add correct label for each box.\n",
    "PATH_TO_LABELS = 'models/research/object_detection/data/mscoco_label_map.pbtxt'\n",
    "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oVU3U_J6IJVb"
   },
   "source": [
    "For the sake of simplicity we will test on 2 images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jG-zn5ykWKMd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('models/research/object_detection/test_images/image1.jpg'),\n",
       " PosixPath('models/research/object_detection/test_images/image2.jpg')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you want to test the code with your images, just add path to the images to the TEST_IMAGE_PATHS.\n",
    "PATH_TO_TEST_IMAGES_DIR = pathlib.Path('models/research/object_detection/test_images')\n",
    "TEST_IMAGE_PATHS = sorted(list(PATH_TO_TEST_IMAGES_DIR.glob(\"*.jpg\")))\n",
    "TEST_IMAGE_PATHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H0_1AGhrWKMc"
   },
   "source": [
    "# Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f7aOtOlebK7h"
   },
   "source": [
    "Load an object detection model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1XNT0wxybKR6"
   },
   "outputs": [],
   "source": [
    "model_name = 'ssd_mobilenet_v1_coco_2017_11_17'\n",
    "detection_model = load_model(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yN1AYfAEJIGp"
   },
   "source": [
    "Check the model's input signature, it expects a batch of 3-color images of type uint8: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CK4cnry6wsHY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'image_tensor:0' shape=(?, ?, ?, 3) dtype=uint8>]\n"
     ]
    }
   ],
   "source": [
    "print(detection_model.inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q8u3BjpMJXZF"
   },
   "source": [
    "And retuns several outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oLSZpfaYwuSk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'detection_classes': tf.float32,\n",
       " 'num_detections': tf.float32,\n",
       " 'detection_boxes': tf.float32,\n",
       " 'detection_scores': tf.float32}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detection_model.output_dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FZyKUJeuxvpT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'detection_classes': TensorShape([Dimension(None), Dimension(100)]),\n",
       " 'num_detections': TensorShape([Dimension(None)]),\n",
       " 'detection_boxes': TensorShape([Dimension(None), Dimension(100), Dimension(4)]),\n",
       " 'detection_scores': TensorShape([Dimension(None), Dimension(100)])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detection_model.output_shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JP5qZ7sXJpwG"
   },
   "source": [
    "Add a wrapper function to call the model, and cleanup the outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ajmR_exWyN76"
   },
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(model, image):\n",
    "  image = np.asarray(image)\n",
    "  # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
    "  input_tensor = tf.convert_to_tensor(image)\n",
    "  # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
    "  input_tensor = input_tensor[tf.newaxis,...]\n",
    "\n",
    "  # Run inference\n",
    "  output_dict = model(input_tensor)\n",
    "\n",
    "\n",
    "  #LSW:test\n",
    "  print('* * * Check num_detections:', output_dict.pop('num_detections'), 'out_dict', output_dict)\n",
    "  \"\"\"\n",
    "  num_detections = int(output_dict.pop(‘num_detections’)) 報錯\n",
    "  TypeError: int() argument must be a string, a bytes-like object or a number, not 'Tensor' \n",
    "\n",
    "  models/research/object_detection/test_images/image1.jpg\n",
    "  Tensor(\"StatefulPartitionedCall_4:3\", shape=(?,), dtype=float32)\n",
    "  看起來是第一張圖輸入進去，\n",
    "  \n",
    "  看來此jpupyter是在TF2跑，反而不能用tf1.x!!!!\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  # All outputs are batches tensors.\n",
    "  # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
    "  # We're only interested in the first num_detections.\n",
    "\n",
    "  num_detections = int(output_dict.pop('num_detections'))\n",
    "#   num_detections = int(1) #LSW to test with tf1.14, not works!\n",
    "  output_dict = {key:value[0, :num_detections].numpy() \n",
    "                 for key,value in output_dict.items()}\n",
    "  output_dict['num_detections'] = num_detections\n",
    "\n",
    "  # detection_classes should be ints.\n",
    "  output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "   \n",
    "  # Handle models with masks:\n",
    "  if 'detection_masks' in output_dict:\n",
    "    # Reframe the the bbox mask to the image size.\n",
    "    detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "              output_dict['detection_masks'], output_dict['detection_boxes'],\n",
    "               image.shape[0], image.shape[1])      \n",
    "    detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5,\n",
    "                                       tf.uint8)\n",
    "    output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "    \n",
    "  return output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z1wq0LVyMRR_"
   },
   "source": [
    "Run it on each test image and show the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DWh_1zz6aqxs"
   },
   "outputs": [],
   "source": [
    "def show_inference(model, image_path):\n",
    "  # the array based representation of the image will be used later in order to prepare the\n",
    "  # result image with boxes and labels on it.\n",
    "  image_np = np.array(Image.open(image_path))\n",
    "  # Actual detection.\n",
    "  output_dict = run_inference_for_single_image(model, image_np)\n",
    "  # Visualization of the results of a detection.\n",
    "  vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "      image_np,\n",
    "      output_dict['detection_boxes'],\n",
    "      output_dict['detection_classes'],\n",
    "      output_dict['detection_scores'],\n",
    "      category_index,\n",
    "      instance_masks=output_dict.get('detection_masks_reframed', None),\n",
    "      use_normalized_coordinates=True,\n",
    "      line_thickness=8)\n",
    "\n",
    "  display(Image.fromarray(image_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3a5wMHN8WKMh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/research/object_detection/test_images/image1.jpg\n",
      "* * * Check num_detections: Tensor(\"StatefulPartitionedCall:3\", shape=(?,), dtype=float32) out_dict {'detection_classes': <tf.Tensor 'StatefulPartitionedCall:1' shape=(?, 100) dtype=float32>, 'detection_boxes': <tf.Tensor 'StatefulPartitionedCall:0' shape=(?, 100, 4) dtype=float32>, 'detection_scores': <tf.Tensor 'StatefulPartitionedCall:2' shape=(?, 100) dtype=float32>}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'num_detections'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-c689a76e14bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mTEST_IMAGE_PATHS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mshow_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetection_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-e474e557b383>\u001b[0m in \u001b[0;36mshow_inference\u001b[0;34m(model, image_path)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mimage_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;31m# Actual detection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0moutput_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_inference_for_single_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0;31m# Visualization of the results of a detection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   vis_util.visualize_boxes_and_labels_on_image_array(\n",
      "\u001b[0;32m<ipython-input-13-3e2112d5f773>\u001b[0m in \u001b[0;36mrun_inference_for_single_image\u001b[0;34m(model, image)\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0;31m# We're only interested in the first num_detections.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m   \u001b[0mnum_detections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'num_detections'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;31m#   num_detections = int(1) #LSW to test with tf1.14, not works!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m   output_dict = {key:value[0, :num_detections].numpy() \n",
      "\u001b[0;31mKeyError\u001b[0m: 'num_detections'"
     ]
    }
   ],
   "source": [
    "for image_path in TEST_IMAGE_PATHS:\n",
    "  print(image_path)\n",
    "  show_inference(detection_model, image_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DsspMPX3Cssg"
   },
   "source": [
    "## Instance Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CzkVv_n2MxKC"
   },
   "outputs": [],
   "source": [
    "model_name = \"mask_rcnn_inception_resnet_v2_atrous_coco_2018_01_28\"\n",
    "masking_model = load_model(\"mask_rcnn_inception_resnet_v2_atrous_coco_2018_01_28\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0S7aZi8ZOhVV"
   },
   "source": [
    "The instance segmentation model includes a `detection_masks` output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vQ2Sj2VIOZLA"
   },
   "outputs": [],
   "source": [
    "masking_model.output_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AS57rZlnNL7W"
   },
   "outputs": [],
   "source": [
    "for image_path in TEST_IMAGE_PATHS:\n",
    "  show_inference(masking_model, image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nLlmm9JojEKm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 u3148947 TRI108216 127K Dec 24 15:43 models/research/object_detection/test_images/image1.jpg\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh models/research/object_detection/test_images/image1.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "last_runtime": {
    "build_target": "//learning/brain/python/client:colab_notebook",
    "kind": "private"
   },
   "name": "object_detection_tutorial.ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1LNYL6Zsn9Xlil2CVNOTsgDZQSBKeOjCh",
     "timestamp": 1566498233247
    },
    {
     "file_id": "/piper/depot/google3/third_party/tensorflow_models/object_detection/object_detection_tutorial.ipynb?workspaceId=markdaoust:copybara_AFABFE845DCD573AD3D43A6BAFBE77D4_0::citc",
     "timestamp": 1566488313397
    },
    {
     "file_id": "/piper/depot/google3/third_party/py/tensorflow_docs/g3doc/en/r2/tutorials/generative/object_detection_tutorial.ipynb?workspaceId=markdaoust:copybara_AFABFE845DCD573AD3D43A6BAFBE77D4_0::citc",
     "timestamp": 1566145894046
    },
    {
     "file_id": "1nBPoWynOV0auSIy40eQcBIk9C6YRSkI8",
     "timestamp": 1566145841085
    },
    {
     "file_id": "/piper/depot/google3/third_party/tensorflow_models/object_detection/object_detection_tutorial.ipynb?workspaceId=markdaoust:copybara_AFABFE845DCD573AD3D43A6BAFBE77D4_0::citc",
     "timestamp": 1556295408037
    },
    {
     "file_id": "1layerger-51XwWOwYMY_5zHaCavCeQkO",
     "timestamp": 1556214267924
    },
    {
     "file_id": "/piper/depot/google3/third_party/tensorflow_models/object_detection/object_detection_tutorial.ipynb?workspaceId=markdaoust:copybara_AFABFE845DCD573AD3D43A6BAFBE77D4_0::citc",
     "timestamp": 1556207836484
    },
    {
     "file_id": "1w6mqQiNV3liPIX70NOgitOlDF1_4sRMw",
     "timestamp": 1556154824101
    },
    {
     "file_id": "https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb",
     "timestamp": 1556150293326
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
