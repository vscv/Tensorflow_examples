{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 101 Course of transfer learning and Fine tune 2021-01-01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [How-to] 1. k-fold for cross validation\n",
    "\n",
    "#### Create a simple k-fold for train classification model.\n",
    "\n",
    "* In this short course you learned:\n",
    "\n",
    "* data pipline\n",
    "\n",
    "* transfer learning\n",
    "\n",
    "* fine tune\n",
    "\n",
    "* callbacks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo: move to note.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.style.use(\"bmh\")\n",
    "\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "\n",
    "import errno\n",
    "\n",
    "# albumentations\n",
    "from functools import partial\n",
    "# from albumentations import (\n",
    "#     Compose, RandomBrightness, JpegCompression, HueSaturationValue, RandomContrast, HorizontalFlip,\n",
    "#     Rotate\n",
    "# )\n",
    "import albumentations as A\n",
    "\n",
    "# from adabelief_tf import AdaBeliefOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n",
      "4.3.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(cv2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytictoc import TicToc\n",
    "\n",
    "t_timer = TicToc() #create instance of class\n",
    "\n",
    "t_timer.tic() #Start timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LookAheadanneal 2021-03-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image size, Batch size, toe/heel-offset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64 # 16 #32 #64 #128 #32 #64 #todo: reduce the BS maybe help to reduce the loss\n",
    "img_height = 120 #512 #224 #100\n",
    "img_width = 120 #512 #224 #100\n",
    "\n",
    "y_offset_toe = 80\n",
    "y_offset_heel = 280 #400-120=280"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf MirroredStrategy seting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "\n",
      "Number of REPLICAS: 1\n",
      "\n",
      "BATCH_SIZE: 64, MULTI_BATCH_SIZE: 64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# tf MirroredStrategy seting\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "REPLICAS = strategy.num_replicas_in_sync\n",
    "print('\\nNumber of REPLICAS: {}\\n'.format(REPLICAS))\n",
    "\n",
    "\n",
    "MULTI_BATCH_SIZE = BATCH_SIZE * strategy.num_replicas_in_sync\n",
    "print('BATCH_SIZE: {}, MULTI_BATCH_SIZE: {}'.format(BATCH_SIZE, MULTI_BATCH_SIZE))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自動調節tf.data管道\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create the training dataset W/ croped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load samples as data-farame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>images</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1120</th>\n",
       "      <td>image_6195.jpg</td>\n",
       "      <td>52</td>\n",
       "      <td>127</td>\n",
       "      <td>75</td>\n",
       "      <td>370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1121</th>\n",
       "      <td>image_6196.jpg</td>\n",
       "      <td>62</td>\n",
       "      <td>138</td>\n",
       "      <td>29</td>\n",
       "      <td>383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1122</th>\n",
       "      <td>image_6197.jpg</td>\n",
       "      <td>54</td>\n",
       "      <td>135</td>\n",
       "      <td>78</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123</th>\n",
       "      <td>image_6198.jpg</td>\n",
       "      <td>60</td>\n",
       "      <td>125</td>\n",
       "      <td>29</td>\n",
       "      <td>384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1124</th>\n",
       "      <td>image_6199.jpg</td>\n",
       "      <td>51</td>\n",
       "      <td>147</td>\n",
       "      <td>70</td>\n",
       "      <td>353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1340</th>\n",
       "      <td>image_6419.jpg</td>\n",
       "      <td>60</td>\n",
       "      <td>135</td>\n",
       "      <td>70</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1341</th>\n",
       "      <td>image_6421.jpg</td>\n",
       "      <td>53</td>\n",
       "      <td>157</td>\n",
       "      <td>76</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1342</th>\n",
       "      <td>image_6422.jpg</td>\n",
       "      <td>49</td>\n",
       "      <td>154</td>\n",
       "      <td>33</td>\n",
       "      <td>375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1343</th>\n",
       "      <td>image_6423.jpg</td>\n",
       "      <td>64</td>\n",
       "      <td>149</td>\n",
       "      <td>76</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1344</th>\n",
       "      <td>image_6424.jpg</td>\n",
       "      <td>55</td>\n",
       "      <td>147</td>\n",
       "      <td>36</td>\n",
       "      <td>385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>225 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              images  x1   y1  x2   y2\n",
       "1120  image_6195.jpg  52  127  75  370\n",
       "1121  image_6196.jpg  62  138  29  383\n",
       "1122  image_6197.jpg  54  135  78  380\n",
       "1123  image_6198.jpg  60  125  29  384\n",
       "1124  image_6199.jpg  51  147  70  353\n",
       "...              ...  ..  ...  ..  ...\n",
       "1340  image_6419.jpg  60  135  70  381\n",
       "1341  image_6421.jpg  53  157  76  376\n",
       "1342  image_6422.jpg  49  154  33  375\n",
       "1343  image_6423.jpg  64  149  76  381\n",
       "1344  image_6424.jpg  55  147  36  385\n",
       "\n",
       "[225 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# # data-org #\n",
    "# ann = 'annotation_1424_merge.csv'\n",
    "# im_p = 'train/images/'\n",
    "# CSVNAME = \"\"\n",
    "\n",
    "\n",
    "# # data-org-augm#\n",
    "# ann = 'annotation_2848_augm.csv'\n",
    "# im_p = 'train_augm/images/'\n",
    "# CSVNAME = \"\"\n",
    "\n",
    "\n",
    "# data-train # # current best dataset.1424-79.\n",
    "ann = 'annotation_1345_good.csv'\n",
    "im_p = 'train/images/'\n",
    "CSVNAME = \"K1345\"\n",
    "\n",
    "# data-augm #\n",
    "# ann = 'annotation_2690_augm.csv'\n",
    "# im_p = 'train_augm/images/'\n",
    "# CSVNAME = \"\"\n",
    "\n",
    "\n",
    "# # data-train-HPL-1123\n",
    "# ann = 'annotation_1123_HPL_Good.csv'\n",
    "# im_p = 'train/images/'\n",
    "# CSVNAME = \"\"\n",
    "\n",
    "\n",
    "df = pd.read_csv(ann)\n",
    "df[1120:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1345"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffle and reset index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index\n",
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>images</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>image_0060.png</td>\n",
       "      <td>63</td>\n",
       "      <td>109</td>\n",
       "      <td>35</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>image_6313.jpg</td>\n",
       "      <td>64</td>\n",
       "      <td>153</td>\n",
       "      <td>76</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>image_0234.png</td>\n",
       "      <td>54</td>\n",
       "      <td>117</td>\n",
       "      <td>76</td>\n",
       "      <td>369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>image_0247.png</td>\n",
       "      <td>63</td>\n",
       "      <td>102</td>\n",
       "      <td>40</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>image_0257.png</td>\n",
       "      <td>64</td>\n",
       "      <td>106</td>\n",
       "      <td>35</td>\n",
       "      <td>386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>image_0429.png</td>\n",
       "      <td>56</td>\n",
       "      <td>122</td>\n",
       "      <td>77</td>\n",
       "      <td>383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>image_0087.png</td>\n",
       "      <td>46</td>\n",
       "      <td>115</td>\n",
       "      <td>57</td>\n",
       "      <td>354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>image_0680.png</td>\n",
       "      <td>54</td>\n",
       "      <td>139</td>\n",
       "      <td>83</td>\n",
       "      <td>385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>image_0702.png</td>\n",
       "      <td>48</td>\n",
       "      <td>141</td>\n",
       "      <td>76</td>\n",
       "      <td>382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>image_6308.jpg</td>\n",
       "      <td>61</td>\n",
       "      <td>104</td>\n",
       "      <td>34</td>\n",
       "      <td>383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           images  x1   y1  x2   y2\n",
       "0  image_0060.png  63  109  35  376\n",
       "1  image_6313.jpg  64  153  76  380\n",
       "2  image_0234.png  54  117  76  369\n",
       "3  image_0247.png  63  102  40  363\n",
       "4  image_0257.png  64  106  35  386\n",
       "5  image_0429.png  56  122  77  383\n",
       "6  image_0087.png  46  115  57  354\n",
       "7  image_0680.png  54  139  83  385\n",
       "8  image_0702.png  48  141  76  382\n",
       "9  image_6308.jpg  61  104  34  383"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See, the image_####.jpg now are random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create tf.dataset (DS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataframe\n",
    "list_ds = tf.data.Dataset.from_tensor_slices((df['images'], df['x1'], df['y1'], df['x2'], df['y2']))\n",
    "# list_ds = list_ds.shuffle(image_count, reshuffle_each_iteration=True) #todo: move to ds_pre. see https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1345"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_ds)#.shape() #take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check The type specification of an element of this dataset.\n",
    "list_ds.element_spec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "take sample: b'image_0060.png' 63 109 35 376\n",
      "take sample: b'image_6313.jpg' 64 153 76 380\n",
      "take sample: b'image_0234.png' 54 117 76 369\n",
      "take sample: b'image_0247.png' 63 102 40 363\n",
      "take sample: b'image_0257.png' 64 106 35 386\n"
     ]
    }
   ],
   "source": [
    "for f,x1,y1,x2,y2 in list_ds.take(5):\n",
    "    print(f'take sample: {f} {x1} {y1} {x2} {y2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_0257.png\n"
     ]
    }
   ],
   "source": [
    "# use np decode to UTF-8\n",
    "print(f.numpy().decode('UTF-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1345"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check ds iterator for consuming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Python iterator\n",
    "\n",
    "it_list_ds = iter(list_ds) # Make sure iter ds only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'image_0060.png' 63 109\n",
      "b'image_6313.jpg' 64 153\n",
      "b'image_0234.png' 54 117\n",
      "b'image_0247.png' 63 102\n"
     ]
    }
   ],
   "source": [
    "# using iter and consuming its elements using next: every print different image name.\n",
    "\n",
    "for i in range(4):\n",
    "    image, x1, y1, x2, y2 = next(it_list_ds)\n",
    "    print(image.numpy(), x1.numpy(), y1.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'image_0060.png' 63 109 35 376\n",
      "b'image_0060.png' 63 109 35 376\n",
      "b'image_0060.png' 63 109 35 376\n",
      "b'image_0060.png' 63 109 35 376\n",
      "===== Create iterator once and pull out to above cell. =====\n",
      "b'image_0060.png' 63 109 35 376\n",
      "b'image_6313.jpg' 64 153 76 380\n",
      "b'image_0234.png' 54 117 76 369\n",
      "b'image_0247.png' 63 102 40 363\n"
     ]
    }
   ],
   "source": [
    "# image_batch, label_batch = valid_ds_pre_s.as_numpy_iterator().next()\n",
    "# pred = model.predict_on_batch(image_batch)\n",
    "\n",
    "\n",
    "for i in range(4):\n",
    "    image, x1, y1, x2, y2 = list_ds.as_numpy_iterator().next()# every time create a new iter, so need put iter out of above cell.\n",
    "    print(image, x1, y1, x2, y2)\n",
    "    \n",
    "iter_test_list = list_ds.as_numpy_iterator()\n",
    "print(\"===== Create iterator once and pull out to above cell. =====\")\n",
    "for i in range(4):\n",
    "    image, x1, y1, x2, y2 = iter_test_list.next()\n",
    "    print(image, x1, y1, x2, y2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process path to image tensor in DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "def get_label(file_path):\n",
    "    # convert the path to a list of path components\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    # The second to last is the class-directory\n",
    "    boolen = parts[-2] == class_names\n",
    "    #one_hot_num = np.array(boolen, dtype=np.int) not works should use tf.x repalced.\n",
    "    one_hot_num = tf.dtypes.cast(boolen, tf.int64)\n",
    "    one_num = tf.argmax(one_hot_num)\n",
    "    print('one_num:', one_num)\n",
    "    # Integer encode the label\n",
    "    return one_num\n",
    "\n",
    "def decode_img(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    # resize the image to the desired size\n",
    "#     return tf.image.resize(img, [img_height, img_width])# augment 已經resize過一次了 但這邊不先做會比較慢\n",
    "    return tf.cast(tf.image.resize(img, [img_height, img_width]), tf.uint8)# 避免float over at augment\n",
    "'''\n",
    "\n",
    "#\n",
    "# map list to ds, Toe part.\n",
    "#\n",
    "\n",
    "def decode_crop_png_toe(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.io.decode_png(img, channels=3)\n",
    "    # crop the toe from top-left corner [image, offset_height y1, offset_width x1, target_height, target_width]\n",
    "    y1=y_offset_toe;    x1=0;    h=img_height;    w=img_width # not the pp location\n",
    "    img = tf.image.crop_to_bounding_box(img, int(y1), int(x1), h, w)\n",
    "    #img = tf.image.crop_to_bounding_box(img, int(y1), int(x1), int(y2)-int(y1), int(x2)-int(x1))\n",
    "    # resize the image to the desired size\n",
    "    return img\n",
    "\n",
    "def process_path_toe(file_path,x1,y1,x2,y2):\n",
    "    file_path = im_p + file_path\n",
    "    #label = get_label(file_path)\n",
    "    # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)#can read the byte string paths b'image_0001.png'\n",
    "    img = decode_crop_png_toe(img)\n",
    "    return img, [x1,y1-y_offset_toe]#Original [0,120]\n",
    "    #return img, x1, y1-y_offset_toe #Original [0,120] #貌似ed不用改，蛋mse變超大\n",
    "    #return img, [x1/120,(y1-y_offset_toe)/120]#normalized [0,1] xy <dtype: 'float64'>, no help.\n",
    "    #return img, [(x1-60)/60,((y1-y_offset_toe)-60)/60]#normalized [-1,1] , no help.\n",
    "\n",
    "#\n",
    "# map list to ds, Heel part.\n",
    "#\n",
    "\n",
    "def decode_crop_png_heel(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.io.decode_png(img, channels=3)\n",
    "    # crop the toe from top-left corner [image, offset_height y1, offset_width x1, target_height, target_width]\n",
    "    y2=y_offset_heel;    x2=0;    h=img_height;    w=img_width # not the pp location\n",
    "    img = tf.image.crop_to_bounding_box(img, int(y2), int(x2), h, w)\n",
    "    #img = tf.image.crop_to_bounding_box(img, int(y1), int(x1), int(y2)-int(y1), int(x2)-int(x1))\n",
    "    # resize the image to the desired size\n",
    "    return img\n",
    "\n",
    "def process_path_heel(file_path,x1,y1,x2,y2):\n",
    "    file_path = im_p + file_path\n",
    "    #label = get_label(file_path)\n",
    "    # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)#can read the byte string paths b'image_0001.png'\n",
    "    img = decode_crop_png_heel(img)\n",
    "    return img, [x2,y2-y_offset_heel]#Original [0,120]\n",
    "    #return img, x2, y2-y_offset_heel #Original [0,120] #貌似ed不用改，蛋mse變超大\n",
    "    #return img, [x2/120,(y2-y_offset_heel)/120]#normalized [0,1] xy <dtype: 'float64'>, no help.\n",
    "    #return img, [(x2-60)/60,((y2-y_offset_heel)-60)/60]#normalized [-1,1] , no help.\n",
    "\n",
    "#\n",
    "# test how to put parameters to map\n",
    "#\n",
    "\n",
    "def t_ds_map(file_path,x1,y1,x2,y2):\n",
    "#     img = get_img('train/images/' + str(file_path))\n",
    "#     print(file_path)\n",
    "    return file_path,x1,y1,x2,y2 #img, [x1,y1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toe ds\n",
    "train_ds_map_toe = list_ds.map(process_path_toe, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "# Heel ds\n",
    "train_ds_map_heel = list_ds.map(process_path_heel, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "take sample: 63 29\n",
      "f <dtype: 'string'>\n",
      "x <dtype: 'int64'>\n"
     ]
    }
   ],
   "source": [
    "# for img, xy in train_ds_map_toe.take(1):\n",
    "#     print(f'take sample: {xy}')\n",
    "    \n",
    "# print('f', f.dtype)\n",
    "# print('xy', xy.dtype)\n",
    "\n",
    "# for img, x, y in train_ds_map_toe.take(1):\n",
    "#     print(f'take sample: {x} {y}')\n",
    "    \n",
    "# print('img', img.dtype)\n",
    "# print('x', x.dtype)\n",
    "# x\n",
    "\n",
    "for img, [x, y] in train_ds_map_toe.take(1):\n",
    "    print(f'take sample: {x} {y}')\n",
    "    \n",
    "print('f', f.dtype)\n",
    "print('x', x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int64, numpy=63>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for f,x1,y1,x2,y2 in train_ds_map.take(5):\n",
    "#     print(f'take sample: {f} {x1} {y1} {x2} {y2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [new] Split train_ds_pre with ratio of validation %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ToDo 20210201] keep orignal validation in 0.1, but augmenting train_ds in input layer or in the tf.ds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2021-02-23] New k-split ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # split Toe\n",
    "# val_size = int(tf.data.experimental.cardinality(train_ds_map_toe).numpy() * 0.2)\n",
    "# # val_size = int(tf.data.experimental.cardinality(train_ds_map_toe).numpy() * 0.1)#no help\n",
    "\n",
    "# train_ds_map_toe_s = train_ds_map_toe.skip(val_size)\n",
    "# valid_ds_map_toe_s = train_ds_map_toe.take(val_size)\n",
    "\n",
    "# print(f'whole samples = {len(train_ds_map_toe)}')\n",
    "# print(f'val_size = {val_size}')\n",
    "\n",
    "# print('ds_train = ', tf.data.experimental.cardinality(train_ds_map_toe_s).numpy())\n",
    "# print('ds_valid = ', tf.data.experimental.cardinality(valid_ds_map_toe_s).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # split Heel\n",
    "# val_size = int(tf.data.experimental.cardinality(train_ds_map_heel).numpy() * 0.2)\n",
    "# # val_size = int(tf.data.experimental.cardinality(train_ds_map_heel).numpy() * 0.1)\n",
    "\n",
    "\n",
    "# train_ds_map_heel_s = train_ds_map_heel.skip(val_size)\n",
    "# valid_ds_map_heel_s = train_ds_map_heel.take(val_size)\n",
    "\n",
    "# print(len(train_ds_map_heel))\n",
    "# print(val_size)\n",
    "# print(tf.data.experimental.cardinality(train_ds_map_heel_s).numpy())\n",
    "# print(tf.data.experimental.cardinality(valid_ds_map_heel_s).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## testing cell\n",
    "# kf = []\n",
    "# for k in range(5):\n",
    "#     kf.append(train_ds_map_heel.shard(num_shards=5, index=k))\n",
    "#     print(\"k =\", k,\"num=\", tf.data.experimental.cardinality(kf[k]).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for img, [x,y] in kf[1].take(1):\n",
    "#     print(f'take sample: {x} {y}')\n",
    "    \n",
    "# print('img', img.dtype)\n",
    "# print('x', x.dtype)\n",
    "# print('y', y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## testing cell\n",
    "\n",
    "# range_k_0 = train_ds_map_heel.window(5)\n",
    "\n",
    "# print(len(range_k_0))\n",
    "# print(tf.data.experimental.cardinality(range_k_0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## testing cell\n",
    "\n",
    "# def get_train_valid_k_split():\n",
    "#     x = tf.data.Dataset.range(1000)\n",
    "#     val_size = int(tf.data.experimental.cardinality(x).numpy() * 0.2)\n",
    "    \n",
    "#     for k in range(5):\n",
    "#         train_num = x.take(val_size + k*val_size)\n",
    "#         valid_num = x.skip(k*val_size)\n",
    "        \n",
    "#         print(\"k=\", k)\n",
    "#         print(tf.data.experimental.cardinality(train_num).numpy())\n",
    "#         print(tf.data.experimental.cardinality(valid_num).numpy())\n",
    "    \n",
    "    \n",
    "# get_train_valid_k_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_size= 2\n",
      "k = 0 k*val_size+val_size 2 k_train num= 8\n",
      "k = 1 k*val_size+val_size 4 k_train num= 8\n",
      "k = 2 k*val_size+val_size 6 k_train num= 8\n",
      "k = 3 k*val_size+val_size 8 k_train num= 8\n",
      "k = 4 k*val_size+val_size 10 k_train num= 8\n"
     ]
    }
   ],
   "source": [
    "## testing cell\n",
    "\n",
    "# \n",
    "# tf.slice\n",
    "# tf.data.experimental.choose_from_datasets\n",
    "\n",
    "\n",
    "# x = list_ds\n",
    "# val_size = int(tf.data.experimental.cardinality(list_ds).numpy() * 0.2)\n",
    "x = tf.data.Dataset.range(10)\n",
    "\n",
    "def check_KFold_ds(x, K=5):\n",
    "    \n",
    "    val_size = int(tf.data.experimental.cardinality(x).numpy() * 0.2)\n",
    "    print(\"val_size=\", val_size)\n",
    "    \n",
    "    for k in range(K):\n",
    "#         k_train = x.take(val_size + k*val_size)\n",
    "#         k_valid = x.skip(k*val_size)\n",
    "#         k_train = tf.slice(x, k*val_size, val_size) #only for pure tensors not \n",
    "#         k_valid = x.skip(k*val_size)\n",
    "\n",
    "        # may skip twicce to performe kflod\n",
    "        t_take = x.take(k*val_size)\n",
    "        t_skip = x.skip(k*val_size+val_size)\n",
    "        k_train = t_take.concatenate(t_skip)\n",
    "        \n",
    "        v_skip = x.skip(k*val_size)\n",
    "        k_valid = v_skip.take(val_size)\n",
    "\n",
    "        print(\"k =\", k,\"k*val_size+val_size\", k*val_size+val_size, \"k_train num=\", tf.data.experimental.cardinality(k_train).numpy())\n",
    "\n",
    "\n",
    "        # x = tf.data.Dataset.range(10)\n",
    "#         for n in k_train:\n",
    "#             print(n.numpy())\n",
    "#         for n in k_valid:\n",
    "#             print(n.numpy())\n",
    "        \n",
    "        # list_ds\n",
    "#         for img, x1, y1, x2, y2 in k_train:\n",
    "#             print(x1, y1)\n",
    "\n",
    "        # train_ds_map_toe\n",
    "#         for img, (x, y) in k_train:\n",
    "#             print(x.numpy(), y.numpy())\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "check_KFold_ds(x)\n",
    "# check_KFold_ds(list_ds)    \n",
    "# check_KFold_ds(train_ds_map_toe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = tf.data.Dataset.range(10)\n",
    "# val_size = int(tf.data.experimental.cardinality(x).numpy() * 0.2)\n",
    "# print(\"val_size=\", val_size)\n",
    "\n",
    "def get_KFold_ds(x, K=0):\n",
    "        \n",
    "    k = K\n",
    "    # may skip twicce to perform kflod\n",
    "    # train ds\n",
    "    t_take = x.take(k*val_size)\n",
    "    t_skip = x.skip(k*val_size+val_size)\n",
    "    k_train = t_take.concatenate(t_skip)\n",
    "    # val ds\n",
    "    v_skip = x.skip(k*val_size)\n",
    "    k_valid = v_skip.take(val_size)\n",
    "\n",
    "    return k_train, k_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1076\n",
      "269\n"
     ]
    }
   ],
   "source": [
    "val_size = int(tf.data.experimental.cardinality(train_ds_map_toe).numpy() * 0.2)\n",
    "t, v = get_KFold_ds(train_ds_map_toe, 1)\n",
    "\n",
    "print(tf.data.experimental.cardinality(t).numpy())\n",
    "print(tf.data.experimental.cardinality(v).numpy())\n",
    "\n",
    "# for n in v:\n",
    "#     print(n.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ParallelMapDataset shapes: ((120, 120, 3), (2,)), types: (tf.uint8, tf.int64)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds_map_toe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset shapes: ((120, 120, 3), (2,)), types: (tf.uint8, tf.int64)>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Albumentation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # for NO keypoint augment\n",
    "# transforms = A.Compose([\n",
    "# #             A.RandomBrightness(limit=0.1, p=0.5),\n",
    "#             A.JpegCompression(quality_lower=65, quality_upper=100, p=0.5),#A.JpegCompression(quality_lower=85, quality_upper=100, p=0.5)\n",
    "#             A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "# #             A.RandomContrast(limit=0.2, p=0.5),\n",
    "#             A.FancyPCA(alpha=0.1, always_apply=False, p=1),#A.FancyPCA(alpha=0.1, always_apply=False, p=0.5)\n",
    "#             A.Downscale(scale_min=0.7, scale_max=0.9, interpolation=0, always_apply=False, p=0.5), #0.8~0.99 may better\n",
    "#             A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, always_apply=False, p=0.5),\n",
    "#             A.CLAHE(clip_limit=(1, 8), tile_grid_size=(8, 8), always_apply=False, p=0.5), #A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), always_apply=False, p=0.5)\n",
    "# #             A.GlassBlur(sigma=0.9, max_delta=2, iterations=2, always_apply=False, mode='fast', p=0.5),\n",
    "# #             A.GaussNoise(var_limit=(10.0, 50.0), mean=0, always_apply=False, p=0.5),\n",
    "# #             A.GaussianBlur(blur_limit=(3, 7), sigma_limit=0, always_apply=False, p=0.5),\n",
    "# #             A.HorizontalFlip(),\n",
    "    \n",
    "#             # try other augm, seems to strong...\n",
    "#             A.RandomBrightnessContrast(always_apply=False, p=0.5, brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), brightness_by_max=True),\n",
    "#             A.Equalize(always_apply=False, p=0.5, mode='cv', by_channels=True),\n",
    "#             A.MultiplicativeNoise(always_apply=False, p=0.5, multiplier=(0.8, 1.5), per_channel=False, elementwise=False),\n",
    "#             A.RandomFog(always_apply=False, p=0.5, fog_coef_lower=0.2, fog_coef_upper=0.3, alpha_coef=0.25),\n",
    "\n",
    "# ])\n",
    "\n",
    "\n",
    "# def aug_fn(image, img_size):\n",
    "#     data = {\"image\":image}\n",
    "#     aug_data = transforms(**data)\n",
    "#     aug_img = aug_data[\"image\"]\n",
    "# #     aug_img = tf.cast(aug_img/255.0, tf.float32)\n",
    "#     aug_img = tf.cast(aug_img, tf.float32)\n",
    "#     aug_img = tf.image.resize(aug_img, size=[img_size, img_size])\n",
    "#     return aug_img\n",
    "\n",
    "# def process_data(image, label, img_size):\n",
    "#     aug_img = tf.numpy_function(func=aug_fn, inp=[image, img_size], Tout=tf.float32)\n",
    "#     return aug_img, label\n",
    "\n",
    "# def set_shapes(img, label, img_shape=(120,120,3)):\n",
    "#     img.set_shape(img_shape)\n",
    "# #     label.set_shape([]) # commited for go around error\n",
    "#     return img, label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # for NO keypoint augment AND for OneOf[] for better heel loss.\n",
    "# transforms_oneof = A.Compose(A.OneOf([\n",
    "#             A.RandomBrightness(limit=0.1, p=0.5),\n",
    "#             A.JpegCompression(quality_lower=85, quality_upper=100, p=0.5),\n",
    "#             A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "#             A.RandomContrast(limit=0.2, p=0.5),\n",
    "#             A.FancyPCA(alpha=0.1, always_apply=False, p=0.5),\n",
    "#             A.Downscale(scale_min=0.7, scale_max=0.9, interpolation=0, always_apply=False, p=0.5),\n",
    "#             A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, always_apply=False, p=0.5),\n",
    "#             A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), always_apply=False, p=0.5),\n",
    "#             A.GlassBlur(sigma=0.9, max_delta=2, iterations=2, always_apply=False, mode='fast', p=0.5),\n",
    "#             A.GaussNoise(var_limit=(10.0, 50.0), mean=0, always_apply=False, p=0.5),\n",
    "#             A.GaussianBlur(blur_limit=(3, 7), sigma_limit=0, always_apply=False, p=.5)\n",
    "# #             A.HorizontalFlip(),\n",
    "#             ]),p=0.5)\n",
    "\n",
    "\n",
    "# def aug_fn_oneof(image, img_size):\n",
    "#     data = {\"image\":image}\n",
    "#     aug_data = transforms_oneof(**data)\n",
    "#     aug_img = aug_data[\"image\"]\n",
    "# #     aug_img = tf.cast(aug_img/255.0, tf.float32)\n",
    "#     aug_img = tf.cast(aug_img, tf.float32)\n",
    "#     aug_img = tf.image.resize(aug_img, size=[img_size, img_size])\n",
    "#     return aug_img\n",
    "\n",
    "# def process_data_oneof(image, label, img_size):\n",
    "#     aug_img = tf.numpy_function(func=aug_fn_oneof, inp=[image, img_size], Tout=tf.float32)\n",
    "#     return aug_img, label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Testing keypoints augment\n",
    "# transforms = A.Compose([\n",
    "#             A.RandomBrightness(limit=0.1, p=0.5),\n",
    "#             A.JpegCompression(quality_lower=85, quality_upper=100, p=0.5),\n",
    "#             A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "#             A.RandomContrast(limit=0.2, p=0.5),\n",
    "#             A.FancyPCA (alpha=0.1, always_apply=False, p=1),\n",
    "#             A.Downscale (scale_min=0.7, scale_max=0.9, interpolation=0, always_apply=False, p=0.5),\n",
    "#             A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, always_apply=False, p=0.5),\n",
    "#             A.CLAHE (clip_limit=4.0, tile_grid_size=(8, 8), always_apply=False, p=0.5),\n",
    "#             A.HorizontalFlip(p=0.5),\n",
    "#             ]\n",
    "#             , \n",
    "#             keypoint_params=A.KeypointParams(format='xy'),  #currently not works for tf.ds yet.\n",
    "#             )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Testing keypoints augment\n",
    "transforms = A.Compose([\n",
    "            A.RandomBrightness(limit=0.1, p=0.5),\n",
    "            A.JpegCompression(quality_lower=85, quality_upper=100, p=0.5),\n",
    "            A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "            A.RandomContrast(limit=0.2, p=0.5),\n",
    "            A.FancyPCA(alpha=0.1, always_apply=False, p=0.5),\n",
    "            A.Downscale(scale_min=0.7, scale_max=0.9, interpolation=0, always_apply=False, p=0.5),\n",
    "            A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, always_apply=False, p=0.5),\n",
    "            A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), always_apply=False, p=0.5),\n",
    "    \n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.RandomResizedCrop(always_apply=False, height=120, width=120, scale=(0.9, 0.99), ratio=(1.0, 1.0), interpolation=0, p=0.5),#xy become double need change dtype of label. # pp will outside the image.\n",
    "            A.IAAAffine(scale=0.9, translate_percent=None, translate_px=None, rotate=0.0, shear=0.0, order=1, cval=0, mode='reflect', always_apply=False, p=0.5),\n",
    "#             A.ShiftScaleRotate(always_apply=False, p=0.5, shift_limit=(0.06, 0.06), scale_limit=(-0.1, 0.1), rotate_limit=(-5, 5), interpolation=1, border_mode=2, value=(0, 0, 0), mask_value=None),\n",
    "    #2021-02-26\n",
    "#             A.IAAPerspective(scale=(0.05, 0.1), keep_size=True, always_apply=False, p=0.5),#fallout image make train stop. NOT support keypoints!!!!!\n",
    "            ]\n",
    "            , \n",
    "            keypoint_params=A.KeypointParams(format='xy',remove_invisible=True),  #currently not works for tf.ds yet.\n",
    "            )\n",
    "\n",
    "# Testing keypoints augment\n",
    "# @tf.function\n",
    "def aug_fn(image, keypoints, img_size):\n",
    "#     print('Check keypoints aug_fun 00:', keypoints) # Check keypoints aug_fun 00: [[53 58]]\n",
    "#     data = {\"image\":image}\n",
    "    aug_data = transforms(image=image, keypoints=keypoints)\n",
    "    aug_img = aug_data[\"image\"]\n",
    "    aug_xy  = aug_data[\"keypoints\"]\n",
    "#     aug_img = tf.cast(aug_img/255.0, tf.float32)\n",
    "    aug_img = tf.cast(aug_img, tf.float32)\n",
    "    aug_img = tf.image.resize(aug_img, size=[img_size, img_size])\n",
    "    \n",
    "    aug_xy = tf.cast(aug_xy, tf.float32) #有些變形輸出是double\n",
    "#     print('Check aug_xy:', aug_xy) # Check aug_xy: [(95, 45)] #印到這邊都是對的\n",
    "    return aug_img, aug_xy \n",
    "\n",
    "# @tf.function\n",
    "def process_data(image, keypoints, img_size):\n",
    "    \n",
    "    print('Check keypoints process01:', keypoints, np.shape(keypoints), type(keypoints))\n",
    "        \n",
    "#     keypoints = tf.make_ndarray(keypoints)\n",
    "#     keypoints = np.array(keypoints)\n",
    "#     keypoints = list(keypoints)\n",
    "#     keypoints = np.asarray(keypoints, dtype=np.float32)\n",
    "#     keypoints = tf.make_ndarray(keypoints.op.get_attr('value'))\n",
    "\n",
    "#     keypoints = tf.reshape(keypoints, [1, 2])\n",
    "    keypoints = tf.reshape(keypoints, [1, 2]) # for 'convert_keypoint_to_albumentations'\n",
    "#     keypoints = np.reshape(keypoints, (1, 2))#not support tensor with np.call.\n",
    "\n",
    "    print('Check keypoints process02:', keypoints, np.shape(keypoints), type(keypoints))\n",
    "\n",
    "#     aug_img, aug_xy = tf.numpy_function(func=aug_fn, inp=[image, img_size], Tout=tf.float32)\n",
    "#     aug_img, aug_xy = tf.py_function(func=aug_fn, inp=[image, keypoints, img_size], Tout=[tf.float32, tf.int64])#for tensors.\n",
    "    aug_img, aug_xy = tf.numpy_function(func=aug_fn, inp=[image, keypoints, img_size], Tout=[tf.float32, tf.float32])\n",
    "    print('Check keypoints process03:', aug_xy)\n",
    "    \n",
    "    aug_xy = tf.reshape(aug_xy, [2,]) # for 'tf ds tarining'\n",
    "    print('Check keypoints process04:', aug_xy)\n",
    "        \n",
    "    return aug_img, aug_xy \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###  for AToe ###\n",
    "# some pp will outside of image bcs p2 is close to 400.\n",
    "\n",
    "\n",
    "\n",
    "# Testing keypoints augment\n",
    "transforms_AToe = A.Compose([\n",
    "#             A.RandomBrightness(limit=0.1, p=0.5),\n",
    "#             A.JpegCompression(quality_lower=85, quality_upper=100, p=0.5),\n",
    "#             A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "#             A.RandomContrast(limit=0.2, p=0.5),\n",
    "#             A.FancyPCA(alpha=0.1, always_apply=False, p=0.5),\n",
    "#             A.Downscale(scale_min=0.7, scale_max=0.9, interpolation=0, always_apply=False, p=0.5),\n",
    "#             A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, always_apply=False, p=0.5),\n",
    "#             A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), always_apply=False, p=0.5),\n",
    "    \n",
    "#             A.RandomBrightness(limit=0.1, p=0.5),\n",
    "            A.JpegCompression(quality_lower=65, quality_upper=100, p=0.5),#A.JpegCompression(quality_lower=85, quality_upper=100, p=0.5)\n",
    "            A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "#             A.RandomContrast(limit=0.2, p=0.5),\n",
    "            A.FancyPCA(alpha=0.1, always_apply=False, p=1),#A.FancyPCA(alpha=0.1, always_apply=False, p=0.5)\n",
    "            A.Downscale(scale_min=0.7, scale_max=0.9, interpolation=0, always_apply=False, p=0.5), #0.8~0.99 may better\n",
    "            A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, always_apply=False, p=0.5),\n",
    "            A.CLAHE(clip_limit=(1, 8), tile_grid_size=(8, 8), always_apply=False, p=0.5), #A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), always_apply=False, p=0.5)\n",
    "    \n",
    "            A.HorizontalFlip(p=0.5),\n",
    "#             A.RandomResizedCrop(always_apply=False, height=120, width=120, scale=(0.75, 0.9), ratio=(1.0, 1.0), interpolation=0, p=0.5),#xy become double need change dtype of label. # pp will outside the image.\n",
    "            A.IAAAffine (scale=0.9, translate_percent=None, translate_px=None, rotate=0.0, shear=0.0, order=1, cval=0, mode='reflect', always_apply=False, p=0.5),\n",
    "#             A.ShiftScaleRotate(always_apply=False, p=0.5, shift_limit=(0.06, 0.06), scale_limit=(-0.1, 0.1), rotate_limit=(-5, 5), interpolation=1, border_mode=2, value=(0, 0, 0), mask_value=None),\n",
    "            ]\n",
    "            , \n",
    "            keypoint_params=A.KeypointParams(format='xy',remove_invisible=True),  #currently not works for tf.ds yet.\n",
    "            )\n",
    "\n",
    "\n",
    "# Testing keypoints augment\n",
    "# @tf.function\n",
    "def aug_fn_AToe(image, keypoints, img_size):\n",
    "#     print('Check keypoints aug_fun 00:', keypoints) # Check keypoints aug_fun 00: [[53 58]]\n",
    "#     data = {\"image\":image}\n",
    "    aug_data = transforms_AToe(image=image, keypoints=keypoints)\n",
    "    aug_img = aug_data[\"image\"]\n",
    "    aug_xy  = aug_data[\"keypoints\"]\n",
    "#     aug_img = tf.cast(aug_img/255.0, tf.float32)\n",
    "    aug_img = tf.cast(aug_img, tf.float32)\n",
    "    aug_img = tf.image.resize(aug_img, size=[img_size, img_size])\n",
    "    \n",
    "    aug_xy = tf.cast(aug_xy, tf.float32) #有些變形輸出是double\n",
    "#     print('Check aug_xy:', aug_xy) # Check aug_xy: [(95, 45)] #印到這邊都是對的\n",
    "    return aug_img, aug_xy \n",
    "\n",
    "# @tf.function\n",
    "def process_data_AToe(image, keypoints, img_size):\n",
    "    \n",
    "    print('Check keypoints process01:', keypoints, np.shape(keypoints), type(keypoints))\n",
    "        \n",
    "#     keypoints = tf.make_ndarray(keypoints)\n",
    "#     keypoints = np.array(keypoints)\n",
    "#     keypoints = list(keypoints)\n",
    "#     keypoints = np.asarray(keypoints, dtype=np.float32)\n",
    "#     keypoints = tf.make_ndarray(keypoints.op.get_attr('value'))\n",
    "\n",
    "#     keypoints = tf.reshape(keypoints, [1, 2])\n",
    "    keypoints = tf.reshape(keypoints, [1, 2]) # for 'convert_keypoint_to_albumentations'\n",
    "#     keypoints = np.reshape(keypoints, (1, 2))#not support tensor with np.call.\n",
    "\n",
    "    print('Check keypoints process02:', keypoints, np.shape(keypoints), type(keypoints))\n",
    "\n",
    "#     aug_img, aug_xy = tf.numpy_function(func=aug_fn, inp=[image, img_size], Tout=tf.float32)\n",
    "#     aug_img, aug_xy = tf.py_function(func=aug_fn, inp=[image, keypoints, img_size], Tout=[tf.float32, tf.int64])#for tensors.\n",
    "    aug_img, aug_xy = tf.numpy_function(func=aug_fn_AToe, inp=[image, keypoints, img_size], Tout=[tf.float32, tf.float32])\n",
    "    print('Check keypoints process03:', aug_xy)\n",
    "    \n",
    "    aug_xy = tf.reshape(aug_xy, [2,]) # for 'tf ds tarining'\n",
    "    print('Check keypoints process04:', aug_xy)\n",
    "        \n",
    "    return aug_img, aug_xy \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def set_shapes(img, label, img_shape=(120,120,3)):\n",
    "    img.set_shape(img_shape)\n",
    "#     label.set_shape([]) # commited for go around error\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepare train_ds_prefetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_for_performance_cache_train(ds, cache=True, augment=False):\n",
    "\n",
    "    \n",
    "    \"\"\"#TODO: need to check the parse logic of ds.cache.\n",
    "    if cache:\n",
    "        print(\"Check cache-f1 to file:\", cache)\n",
    "        if isinstance(cache, str):\n",
    "            ds = ds.cache(cache)\n",
    "            print(\"Check cache-f2 to file:\", cache)\n",
    "    else:\n",
    "        ds = ds.cache()\n",
    "        print(\"Check cache in memory:\", cache)\n",
    "    \"\"\"    \n",
    "    if cache:\n",
    "        ds = ds.cache()\n",
    "        print(\"Check cache in memory:Y\", cache)\n",
    "    else:\n",
    "        print(\"Check cache in memory:N\", cache)\n",
    "        \n",
    "    if augment:\n",
    "        ds = ds.map(partial(process_data, img_size=120),num_parallel_calls=AUTOTUNE)\n",
    "        ds = ds.map(set_shapes, num_parallel_calls=AUTOTUNE)\n",
    "        print(\"Check augment :Y\", augment)\n",
    "    else:\n",
    "        print(\"Check augment :N\", augment)\n",
    "    \n",
    "    #ds = ds.repeat()#TODO:2020-12-14: test\n",
    "    #ds = ds.shuffle(buffer_size=MULTI_BATCH_SIZE*2) # (buffer_size=MULTI_BATCH_SIZE*5) 6sec,buffer_size=1000 take few sec. or buffer_size=image_count <- take too long # each take ds take 30~45 sec, TODO!!\n",
    "    ds = ds.shuffle(len(list_ds), reshuffle_each_iteration=True) #todo: move to ds_pre. see https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle\n",
    "    ds = ds.batch(MULTI_BATCH_SIZE)#MULTI_BATCH_SIZE for multi-GPUs\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE) #buffer_size=AUTOTUNE seem no speed improve\n",
    "    \n",
    "    print(\"Check ds cache[{}] and augment[{}]\".format(cache, augment))\n",
    "    \n",
    "    return ds\n",
    "\n",
    "\n",
    "def configure_for_performance_cache_train_AToe(ds, cache=True, augment=False):\n",
    "\n",
    "    \n",
    "    \"\"\"#TODO: need to check the parse logic of ds.cache.\n",
    "    if cache:\n",
    "        print(\"Check cache-f1 to file:\", cache)\n",
    "        if isinstance(cache, str):\n",
    "            ds = ds.cache(cache)\n",
    "            print(\"Check cache-f2 to file:\", cache)\n",
    "    else:\n",
    "        ds = ds.cache()\n",
    "        print(\"Check cache in memory:\", cache)\n",
    "    \"\"\"    \n",
    "    if cache:\n",
    "        ds = ds.cache()\n",
    "        print(\"Check cache in memory:Y\", cache)\n",
    "    else:\n",
    "        print(\"Check cache in memory:N\", cache)\n",
    "        \n",
    "    if augment:\n",
    "        ds = ds.map(partial(process_data_AToe, img_size=120),num_parallel_calls=AUTOTUNE)\n",
    "        ds = ds.map(set_shapes, num_parallel_calls=AUTOTUNE)\n",
    "        print(\"Check augment :Y\", augment)\n",
    "    else:\n",
    "        print(\"Check augment :N\", augment)\n",
    "    \n",
    "    #ds = ds.repeat()#TODO:2020-12-14: test\n",
    "    #ds = ds.shuffle(buffer_size=MULTI_BATCH_SIZE*2) # (buffer_size=MULTI_BATCH_SIZE*5) 6sec,buffer_size=1000 take few sec. or buffer_size=image_count <- take too long # each take ds take 30~45 sec, TODO!!\n",
    "    ds = ds.shuffle(len(list_ds), reshuffle_each_iteration=True) #todo: move to ds_pre. see https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle\n",
    "    ds = ds.batch(MULTI_BATCH_SIZE)#MULTI_BATCH_SIZE for multi-GPUs\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE) #buffer_size=AUTOTUNE seem no speed improve\n",
    "    \n",
    "    print(\"Check ds cache[{}] and augment[{}]\".format(cache, augment))\n",
    "    \n",
    "    return ds\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def configure_for_performance_cache_train_oneof(ds, cache=True, augment=False):\n",
    "\n",
    "    \n",
    "#     \"\"\"#TODO: need to check the parse logic of ds.cache.\n",
    "#     if cache:\n",
    "#         print(\"Check cache-f1 to file:\", cache)\n",
    "#         if isinstance(cache, str):\n",
    "#             ds = ds.cache(cache)\n",
    "#             print(\"Check cache-f2 to file:\", cache)\n",
    "#     else:\n",
    "#         ds = ds.cache()\n",
    "#         print(\"Check cache in memory:\", cache)\n",
    "#     \"\"\"    \n",
    "#     if cache:\n",
    "#         ds = ds.cache()\n",
    "#         print(\"Check cache in memory:Y\", cache)\n",
    "#     else:\n",
    "#         print(\"Check cache in memory:N\", cache)\n",
    "        \n",
    "#     if augment:\n",
    "#         ds = ds.map(partial(process_data_oneof, img_size=120),num_parallel_calls=AUTOTUNE)\n",
    "#         ds = ds.map(set_shapes, num_parallel_calls=AUTOTUNE)\n",
    "#         print(\"Check augment :Y\", augment)\n",
    "#     else:\n",
    "#         print(\"Check augment :N\", augment)\n",
    "    \n",
    "#     #ds = ds.repeat()#TODO:2020-12-14: test\n",
    "#     #ds = ds.shuffle(buffer_size=MULTI_BATCH_SIZE*2) # (buffer_size=MULTI_BATCH_SIZE*5) 6sec,buffer_size=1000 take few sec. or buffer_size=image_count <- take too long # each take ds take 30~45 sec, TODO!!\n",
    "#     ds = ds.shuffle(len(list_ds), reshuffle_each_iteration=True) #todo: move to ds_pre. see https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle\n",
    "#     ds = ds.batch(MULTI_BATCH_SIZE)#MULTI_BATCH_SIZE for multi-GPUs\n",
    "#     ds = ds.prefetch(buffer_size=AUTOTUNE) #buffer_size=AUTOTUNE seem no speed improve\n",
    "    \n",
    "#     print(\"Check ds cache[{}] and augment[{}]\".format(cache, augment))\n",
    "    \n",
    "#     return ds\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def configure_for_performance_cache_val(ds, cache=True, augment=False):\n",
    "\n",
    "    \n",
    "    \"\"\"#TODO: need to check the parse logic of ds.cache\n",
    "    TODO:test remove ds.shuffle from val_ds.\n",
    "    .\n",
    "    if cache:\n",
    "        print(\"Check cache-f1 to file:\", cache)\n",
    "        if isinstance(cache, str):\n",
    "            ds = ds.cache(cache)\n",
    "            print(\"Check cache-f2 to file:\", cache)\n",
    "    else:\n",
    "        ds = ds.cache()\n",
    "        print(\"Check cache in memory:\", cache)\n",
    "    \"\"\"    \n",
    "    if cache:\n",
    "        ds = ds.cache()\n",
    "        print(\"Check cache in memory:Y\", cache)\n",
    "    else:\n",
    "        print(\"Check cache in memory:N\", cache)\n",
    "        \n",
    "    if augment:\n",
    "#         ds = ds.map(data_augment, num_parallel_calls=AUTOTUNE)\n",
    "        ds = ds.map(AA, num_parallel_calls=AUTOTUNE)\n",
    "#         ds = ds.map(RA, num_parallel_calls=AUTOTUNE)\n",
    "        print(\"Check augment :Y\", augment)\n",
    "    else:\n",
    "        print(\"Check augment :N\", augment)\n",
    "    \n",
    "    #ds = ds.repeat()#TODO:2020-12-14: test\n",
    "    #ds = ds.shuffle(buffer_size=MULTI_BATCH_SIZE*2) # (buffer_size=MULTI_BATCH_SIZE*5) 6sec,buffer_size=1000 take few sec. or buffer_size=image_count <- take too long # each take ds take 30~45 sec, TODO!!\n",
    "#     ds = ds.shuffle(len(list_ds), reshuffle_each_iteration=False) #todo: move to ds_pre. see https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle\n",
    "    ds = ds.batch(MULTI_BATCH_SIZE)#MULTI_BATCH_SIZE for multi-GPUs\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE) #buffer_size=AUTOTUNE seem no speed improve\n",
    "    \n",
    "    print(\"Check ds cache[{}] and augment[{}]\".format(cache, augment))\n",
    "    \n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Prepare the ds properties (cache, augment, bs, shuffle, prefetch, etc.) for better performance.\n",
    "# \"\"\"\n",
    "# # Toe ds_pre\n",
    "# train_ds_pre_toe = configure_for_performance_cache_train(train_ds_map_toe)\n",
    "\n",
    "# # Heel ds_pre\n",
    "# train_ds_pre_heel = configure_for_performance_cache_val(train_ds_map_heel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All split ds_prefetch\n",
    "* train_ds_map_toe_s = train_ds_map_toe.skip(val_size)\n",
    "* valid_ds_map_toe_s = train_ds_map_toe.take(val_size)\n",
    "\n",
    "* train_ds_map_heel_s = train_ds_map_heel.skip(val_size)\n",
    "* valid_ds_map_heel_s = train_ds_map_heel.take(val_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Prepare the ds properties (cache, augment, bs, shuffle, prefetch, etc.) for better performance.\n",
    "# \"\"\"\n",
    "# # Toe ds_pre\n",
    "# train_ds_pre_toe_s = configure_for_performance_cache_train(train_ds_map_toe_s, augment=True)\n",
    "# valid_ds_pre_toe_s = configure_for_performance_cache_val(valid_ds_map_toe_s)\n",
    "\n",
    "# # Heel ds_pre\n",
    "# train_ds_pre_heel_s = configure_for_performance_cache_train(train_ds_map_heel_s, augment=True)\n",
    "# valid_ds_pre_heel_s = configure_for_performance_cache_val(valid_ds_map_heel_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check ds_prefetch samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create it_ds once\n",
    "# it_train_ds_pre_toe_s = iter(train_ds_pre_toe_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # for albu keypoint\n",
    "\n",
    "# # for original return aug_img, , aug_xy \n",
    "\n",
    "\n",
    "# image_batch, label_batch = next(it_train_ds_pre_toe_s)\n",
    "\n",
    "# fig = plt.figure(figsize=(8, 8))\n",
    "# print('batch * multi:', len(label_batch), ', MULTI_BATCH_SIZE=', MULTI_BATCH_SIZE)\n",
    "# for i in range(16):\n",
    "#     ax = plt.subplot(4, 4, i + 1)\n",
    "#     plt.imshow(image_batch[i].numpy().astype(\"uint8\"))\n",
    "#     plt.title(label_batch[i].numpy())\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.plot(label_batch[i].numpy()[0], label_batch[i].numpy()[1], 'r+', markersize=13, mew=2.5)\n",
    "\n",
    "#     print(f'Check lables: {label_batch[i]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create it_ds once\n",
    "# it_train_ds_pre_heel_s = iter(train_ds_pre_heel_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # for albu keypoint\n",
    "\n",
    "# # for original return aug_img, , aug_xy \n",
    "\n",
    "\n",
    "# image_batch, label_batch = next(it_train_ds_pre_heel_s)\n",
    "\n",
    "# fig = plt.figure(figsize=(8, 8))\n",
    "# print('batch * multi:', len(label_batch), ', MULTI_BATCH_SIZE=', MULTI_BATCH_SIZE)\n",
    "# for i in range(16):\n",
    "#     ax = plt.subplot(4, 4, i + 1)\n",
    "#     plt.imshow(image_batch[i].numpy().astype(\"uint8\"))\n",
    "#     plt.title(label_batch[i].numpy())\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.plot(label_batch[i].numpy()[0], label_batch[i].numpy()[1], 'r+', markersize=13, mew=2.5)\n",
    "\n",
    "#     print(f'Check lables: {label_batch[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create it_ds once\n",
    "# it_valid_ds_pre_toe_s = iter(valid_ds_pre_toe_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # for albu keypoint\n",
    "\n",
    "# # for return aug_img, aug_xy \n",
    "\n",
    "\n",
    "# image_batch, label_batch = next(it_valid_ds_pre_toe_s)\n",
    "\n",
    "# plt.figure(figsize=(8, 8))\n",
    "# # for images, labels in valid_ds_pre_toe_s.take(1):\n",
    "# print('batch * multi:', len(label_batch), ', MULTI_BATCH_SIZE=', MULTI_BATCH_SIZE)\n",
    "# for i in range(16):\n",
    "#     ax = plt.subplot(4, 4, i + 1)\n",
    "#     plt.imshow(image_batch[i].numpy().astype(\"uint8\"))\n",
    "#     plt.title(label_batch[i].numpy())\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.plot(label_batch[i].numpy()[0], label_batch[i].numpy()[1], 'r+', markersize=13, mew=2.5)\n",
    "\n",
    "#     print(f'Check lables: {label_batch[i]}')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Loss function\n",
    "\n",
    "* mae\n",
    "* euclidean distance\n",
    "* others\n",
    "\n",
    "\n",
    "        # 'x' is [[1, 1, 1]\n",
    "        #         [1, 1, 1]]\n",
    "        tf.reduce_sum(x) ==> 6\n",
    "        tf.reduce_sum(x, 0) ==> [2, 2, 2]\n",
    "        tf.reduce_sum(x, 1) ==> [3, 3]\n",
    "        the function is default for 2-D array, therefor, in our 1-D [x1,y1] to [x2,y2] the axis should be '0' or just leave it.\n",
    "        \n",
    "        tf.sqrt need \tA tf.Tensor of type bfloat16, half, float32, float64, complex64, complex128\n",
    "        so, convert it first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "should be -> tf.Tensor([56 39], shape=(2,), dtype=int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [10, 10]\n",
    "y_pred = [10, 20]\n",
    "\n",
    "# y_true = [1.00000000000000000000123, 10]\n",
    "# y_pred = [1.0, 10.000000000000000000000000001]\n",
    "\n",
    "# y_true = [1.0000123, 10]\n",
    "# y_pred = [1.0, 10.0000321]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=5>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mae\n",
    "\n",
    "loss_mae = tf.keras.losses.MAE(\n",
    "    y_true, y_pred\n",
    ")\n",
    "\n",
    "loss_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=10.0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ed\n",
    "\n",
    "# loss_ed = tf.sqrt(tf.reduce_sum(tf.square(tf.constant(y_true) - tf.constant(y_pred)), 0))\n",
    "\n",
    "# loss_ed = tf.sqrt(tf.reduce_sum(tf.square(tf.Variable(y_true) - tf.Variable(y_pred)), 0))\n",
    "\n",
    "loss_ed = tf.sqrt(tf.reduce_sum(tf.square(tf.cast(y_true, tf.float32) - tf.cast(y_pred, tf.float32)), 0))\n",
    "\n",
    "loss_ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ed_loss(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_sum(tf.square(tf.cast(y_true, tf.float32) - tf.cast(y_pred, tf.float32)), 0))\n",
    "\n",
    "# fix NaN in euclidean distance\n",
    "# tf.maximum(d, 1e-9), to keep atlease is 1e-9.\n",
    "# def ed_loss(y_true, y_pred):\n",
    "#     return tf.sqrt(tf.maximum(tf.reduce_sum(tf.square(tf.cast(y_true, tf.float32) - tf.cast(y_pred, tf.float32)), 0), 1e-9))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the euclidean distance loss\n",
    "ed_loss(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=10.0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ed_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mean Euclidean distance \n",
    "\n",
    "* here the y_true and y_pred is 2-D array. the axis use 1.\n",
    "\n",
    "\n",
    "* NOTE: LB評分的mean euclidean distance功能，應該跟model.evaluate()一樣so不需重新寫。evaluate()會自動用loss (model.metrics_names)計算後在自動平均，而模型loss我們是用ed-loss取代。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_true = [[60, 76],\n",
    "#        [58, 49 ],\n",
    "#        [63, 67 ],\n",
    "#        [58 , 57]]\n",
    "# y_pred = [[59.927303, 76.471214],\n",
    "#        [58.056904, 49.98754 ],\n",
    "#        [63.067844, 67.03861 ],\n",
    "#        [58.70202 , 57.372707]]\n",
    "\n",
    "y_true = [[60, 70],\n",
    "       [70, 80]]\n",
    "y_pred = [[61, 71],\n",
    "       [72, 82]]\n",
    "\n",
    "# y_true = [(60, 70),\n",
    "#        (70, 80)]\n",
    "# y_pred = [(61, 71),\n",
    "#        (72, 82)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[1., 1.],\n",
       "       [4., 4.]], dtype=float32)>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.square(tf.cast(y_true, tf.float32) - tf.cast(y_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([2., 8.], dtype=float32)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(tf.square(tf.cast(y_true, tf.float32) - tf.cast(y_pred, tf.float32)), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ed_metric_2d(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_sum(tf.square(tf.cast(y_true, tf.float32) - tf.cast(y_pred, tf.float32)), 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.4142135, 2.828427 ], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ed_metric_2d(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1.4142135, 2.828427 ], dtype=float32)>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ed_metric_2d(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 等於true, pred點位ed的平均，LB評分方式。(toe/heel即p1,p2要個別算ed一次再相加)\n",
    "def ed_metric_2d_mean(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.sqrt(tf.reduce_sum(tf.square(tf.cast(y_true, tf.float32) - tf.cast(y_pred, tf.float32)), 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for re-scale back xy \n",
    "# return img, [(x1-60)/60,((y1-y_offset_toe)-60)/60]#normalized [-1,1] \n",
    "# return img, [(x2-60)/60,((y2-y_offset_toe)-60)/60]#normalized [-1,1] \n",
    "\n",
    "# 等於true, pred點位ed的平均，LB評分方式。(toe/heel即p1,p2要個別算ed一次再相加)\n",
    "def edRescal(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.sqrt(tf.reduce_sum(tf.square(tf.cast((y_true*60)+60, tf.float32) - tf.cast((y_pred*60)+60, tf.float32)), 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1213202"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ed_metric_2d_mean(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=2.1213202>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ed_metric_2d_mean(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EFNE Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maybe mae better than ed loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4f2825d160>]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"dump lr\n",
    "\"\"\"\n",
    "ep_num_transf = 500\n",
    "\n",
    "\n",
    "\n",
    "def lrdump(epoch):\n",
    "    \n",
    "    #step_size = 100\n",
    "    lr_max = 0.006\n",
    "    lr_min = 0.001\n",
    "    lr_start = 0.01\n",
    "\n",
    "    lr_init_ep = 0\n",
    "    lr_ramp_ep = 100\n",
    "    lr_sus_ep  = 0\n",
    "    lr_decay   = 0.8\n",
    "\n",
    "    \n",
    "    # warm up\n",
    "    if epoch < lr_init_ep:\n",
    "        lr = (lr_max - lr_min) / lr_ramp_ep * epoch + lr_min    \n",
    "        \n",
    "    elif lr_init_ep -1 < epoch < lr_ramp_ep:\n",
    "        lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "\n",
    "    elif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "        lr = lr_max\n",
    "\n",
    "    else:\n",
    "        lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
    "\n",
    "    return lr\n",
    "\n",
    "rng = [i for i in range(ep_num_transf)]\n",
    "y = [lrdump(x) for x in rng]\n",
    "sns.set(style='darkgrid')\n",
    "fig, ax = plt.subplots(figsize=(20, 6))\n",
    "plt.plot(rng, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4f2825d4e0>]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# flat + cosine anneal\n",
    "\n",
    "ep_num = 1000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def lrfca(epoch):\n",
    "    lr_init_ep = 100\n",
    "\n",
    "    initial_learning_rate = 0.01\n",
    "    decay_steps = 100\n",
    "    lr_decayed_fn = tf.keras.experimental.CosineDecay(\n",
    "                    initial_learning_rate, decay_steps=decay_steps, alpha=0.0001)\n",
    "\n",
    "    if epoch < lr_init_ep:\n",
    "        lr = initial_learning_rate\n",
    "    else:\n",
    "        lr = lr_decayed_fn(epoch-decay_steps)\n",
    "        \n",
    "    return lr\n",
    "    \n",
    "\n",
    "rng = [i for i in range(ep_num)]\n",
    "y = [lrfca(x) for x in rng]\n",
    "sns.set(style='darkgrid')\n",
    "fig, ax = plt.subplots(figsize=(20, 6))\n",
    "plt.plot(rng, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.999999974752427e-07 ~ 0.01\n"
     ]
    }
   ],
   "source": [
    "print('{} ~ {}'.format(min(y), max(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t 0.01\n",
      "\n",
      "1\t 0.01\n",
      "\n",
      "2\t 0.01\n",
      "\n",
      "3\t 0.01\n",
      "\n",
      "4\t 0.01\n",
      "\n",
      "5\t 0.01\n",
      "\n",
      "6\t 0.01\n",
      "\n",
      "7\t 0.01\n",
      "\n",
      "8\t 0.01\n",
      "\n",
      "9\t 0.01\n",
      "\n",
      "10\t 0.01\n",
      "\n",
      "11\t 0.01\n",
      "\n",
      "12\t 0.01\n",
      "\n",
      "13\t 0.01\n",
      "\n",
      "14\t 0.01\n",
      "\n",
      "15\t 0.01\n",
      "\n",
      "16\t 0.01\n",
      "\n",
      "17\t 0.01\n",
      "\n",
      "18\t 0.01\n",
      "\n",
      "19\t 0.01\n",
      "\n",
      "20\t 0.01\n",
      "\n",
      "21\t 0.01\n",
      "\n",
      "22\t 0.01\n",
      "\n",
      "23\t 0.01\n",
      "\n",
      "24\t 0.01\n",
      "\n",
      "25\t 0.01\n",
      "\n",
      "26\t 0.01\n",
      "\n",
      "27\t 0.01\n",
      "\n",
      "28\t 0.01\n",
      "\n",
      "29\t 0.01\n",
      "\n",
      "30\t 0.01\n",
      "\n",
      "31\t 0.01\n",
      "\n",
      "32\t 0.01\n",
      "\n",
      "33\t 0.01\n",
      "\n",
      "34\t 0.01\n",
      "\n",
      "35\t 0.01\n",
      "\n",
      "36\t 0.01\n",
      "\n",
      "37\t 0.01\n",
      "\n",
      "38\t 0.01\n",
      "\n",
      "39\t 0.01\n",
      "\n",
      "40\t 0.01\n",
      "\n",
      "41\t 0.01\n",
      "\n",
      "42\t 0.01\n",
      "\n",
      "43\t 0.01\n",
      "\n",
      "44\t 0.01\n",
      "\n",
      "45\t 0.01\n",
      "\n",
      "46\t 0.01\n",
      "\n",
      "47\t 0.01\n",
      "\n",
      "48\t 0.01\n",
      "\n",
      "49\t 0.01\n",
      "\n",
      "50\t 0.01\n",
      "\n",
      "51\t 0.01\n",
      "\n",
      "52\t 0.01\n",
      "\n",
      "53\t 0.01\n",
      "\n",
      "54\t 0.01\n",
      "\n",
      "55\t 0.01\n",
      "\n",
      "56\t 0.01\n",
      "\n",
      "57\t 0.01\n",
      "\n",
      "58\t 0.01\n",
      "\n",
      "59\t 0.01\n",
      "\n",
      "60\t 0.01\n",
      "\n",
      "61\t 0.01\n",
      "\n",
      "62\t 0.01\n",
      "\n",
      "63\t 0.01\n",
      "\n",
      "64\t 0.01\n",
      "\n",
      "65\t 0.01\n",
      "\n",
      "66\t 0.01\n",
      "\n",
      "67\t 0.01\n",
      "\n",
      "68\t 0.01\n",
      "\n",
      "69\t 0.01\n",
      "\n",
      "70\t 0.01\n",
      "\n",
      "71\t 0.01\n",
      "\n",
      "72\t 0.01\n",
      "\n",
      "73\t 0.01\n",
      "\n",
      "74\t 0.01\n",
      "\n",
      "75\t 0.01\n",
      "\n",
      "76\t 0.01\n",
      "\n",
      "77\t 0.01\n",
      "\n",
      "78\t 0.01\n",
      "\n",
      "79\t 0.01\n",
      "\n",
      "80\t 0.01\n",
      "\n",
      "81\t 0.01\n",
      "\n",
      "82\t 0.01\n",
      "\n",
      "83\t 0.01\n",
      "\n",
      "84\t 0.01\n",
      "\n",
      "85\t 0.01\n",
      "\n",
      "86\t 0.01\n",
      "\n",
      "87\t 0.01\n",
      "\n",
      "88\t 0.01\n",
      "\n",
      "89\t 0.01\n",
      "\n",
      "90\t 0.01\n",
      "\n",
      "91\t 0.01\n",
      "\n",
      "92\t 0.01\n",
      "\n",
      "93\t 0.01\n",
      "\n",
      "94\t 0.01\n",
      "\n",
      "95\t 0.01\n",
      "\n",
      "96\t 0.01\n",
      "\n",
      "97\t 0.01\n",
      "\n",
      "98\t 0.01\n",
      "\n",
      "99\t 0.01\n",
      "\n",
      "100\t 0.009999999776482582\n",
      "\n",
      "101\t 0.009997531771659851\n",
      "\n",
      "102\t 0.00999013427644968\n",
      "\n",
      "103\t 0.009977811947464943\n",
      "\n",
      "104\t 0.009960577823221684\n",
      "\n",
      "105\t 0.009938446804881096\n",
      "\n",
      "106\t 0.00991144496947527\n",
      "\n",
      "107\t 0.00987959560006857\n",
      "\n",
      "108\t 0.009842930361628532\n",
      "\n",
      "109\t 0.009801487438380718\n",
      "\n",
      "110\t 0.009755306877195835\n",
      "\n",
      "111\t 0.009704433381557465\n",
      "\n",
      "112\t 0.009648917242884636\n",
      "\n",
      "113\t 0.009588814340531826\n",
      "\n",
      "114\t 0.009524183347821236\n",
      "\n",
      "115\t 0.009455086663365364\n",
      "\n",
      "116\t 0.009381595999002457\n",
      "\n",
      "117\t 0.009303780272603035\n",
      "\n",
      "118\t 0.009221717715263367\n",
      "\n",
      "119\t 0.009135489352047443\n",
      "\n",
      "120\t 0.009045179933309555\n",
      "\n",
      "121\t 0.00895087979733944\n",
      "\n",
      "122\t 0.008852681145071983\n",
      "\n",
      "123\t 0.008750680834054947\n",
      "\n",
      "124\t 0.008644978515803814\n",
      "\n",
      "125\t 0.00853568036109209\n",
      "\n",
      "126\t 0.008422893472015858\n",
      "\n",
      "127\t 0.008306728675961494\n",
      "\n",
      "128\t 0.008187300525605679\n",
      "\n",
      "129\t 0.008064729161560535\n",
      "\n",
      "130\t 0.007939131930470467\n",
      "\n",
      "131\t 0.00781063549220562\n",
      "\n",
      "132\t 0.007679365109652281\n",
      "\n",
      "133\t 0.00754545209929347\n",
      "\n",
      "134\t 0.007409026846289635\n",
      "\n",
      "135\t 0.007270225789397955\n",
      "\n",
      "136\t 0.007129183039069176\n",
      "\n",
      "137\t 0.006986041087657213\n",
      "\n",
      "138\t 0.006840938236564398\n",
      "\n",
      "139\t 0.006694020703434944\n",
      "\n",
      "140\t 0.006545430049300194\n",
      "\n",
      "141\t 0.0063953157514333725\n",
      "\n",
      "142\t 0.006243825424462557\n",
      "\n",
      "143\t 0.006091107148677111\n",
      "\n",
      "144\t 0.005937312729656696\n",
      "\n",
      "145\t 0.005782594438642263\n",
      "\n",
      "146\t 0.005627102684229612\n",
      "\n",
      "147\t 0.005470994394272566\n",
      "\n",
      "148\t 0.005314420908689499\n",
      "\n",
      "149\t 0.005157537758350372\n",
      "\n",
      "150\t 0.005000499542802572\n",
      "\n",
      "151\t 0.004843461327254772\n",
      "\n",
      "152\t 0.004686578642576933\n",
      "\n",
      "153\t 0.004530005156993866\n",
      "\n",
      "154\t 0.004373895935714245\n",
      "\n",
      "155\t 0.004218405112624168\n",
      "\n",
      "156\t 0.0040636868216097355\n",
      "\n",
      "157\t 0.0039098928682506084\n",
      "\n",
      "158\t 0.0037571745924651623\n",
      "\n",
      "159\t 0.0036056842654943466\n",
      "\n",
      "160\t 0.0034555685706436634\n",
      "\n",
      "161\t 0.0033069790806621313\n",
      "\n",
      "162\t 0.0031600608490407467\n",
      "\n",
      "163\t 0.0030149584636092186\n",
      "\n",
      "164\t 0.002871815813705325\n",
      "\n",
      "165\t 0.002730775158852339\n",
      "\n",
      "166\t 0.002591971307992935\n",
      "\n",
      "167\t 0.0024555467534810305\n",
      "\n",
      "168\t 0.0023216332774609327\n",
      "\n",
      "169\t 0.0021903631277382374\n",
      "\n",
      "170\t 0.002061867620795965\n",
      "\n",
      "171\t 0.0019362712046131492\n",
      "\n",
      "172\t 0.0018136977450922132\n",
      "\n",
      "173\t 0.0016942702932283282\n",
      "\n",
      "174\t 0.0015781065449118614\n",
      "\n",
      "175\t 0.0014653196558356285\n",
      "\n",
      "176\t 0.0013560212682932615\n",
      "\n",
      "177\t 0.0012503194157034159\n",
      "\n",
      "178\t 0.001148318755440414\n",
      "\n",
      "179\t 0.0010501198703423142\n",
      "\n",
      "180\t 0.0009558191522955894\n",
      "\n",
      "181\t 0.0008655104320496321\n",
      "\n",
      "182\t 0.0007792821852490306\n",
      "\n",
      "183\t 0.000697219860740006\n",
      "\n",
      "184\t 0.0006194051820784807\n",
      "\n",
      "185\t 0.0005459125386551023\n",
      "\n",
      "186\t 0.00047681681462563574\n",
      "\n",
      "187\t 0.0004121853271499276\n",
      "\n",
      "188\t 0.00035208213375881314\n",
      "\n",
      "189\t 0.00029656654805876315\n",
      "\n",
      "190\t 0.00024569311062805355\n",
      "\n",
      "191\t 0.00019951128342654556\n",
      "\n",
      "192\t 0.00015806815645191818\n",
      "\n",
      "193\t 0.00012140423496020958\n",
      "\n",
      "194\t 8.95549965207465e-05\n",
      "\n",
      "195\t 6.255202606553212e-05\n",
      "\n",
      "196\t 4.0422441088594496e-05\n",
      "\n",
      "197\t 2.3187696569948457e-05\n",
      "\n",
      "198\t 1.0865368494705763e-05\n",
      "\n",
      "199\t 3.467087481112685e-06\n",
      "\n",
      "200\t 9.999999974752427e-07\n",
      "\n",
      "201\t 9.999999974752427e-07\n",
      "\n",
      "202\t 9.999999974752427e-07\n",
      "\n",
      "203\t 9.999999974752427e-07\n",
      "\n",
      "204\t 9.999999974752427e-07\n",
      "\n",
      "205\t 9.999999974752427e-07\n",
      "\n",
      "206\t 9.999999974752427e-07\n",
      "\n",
      "207\t 9.999999974752427e-07\n",
      "\n",
      "208\t 9.999999974752427e-07\n",
      "\n",
      "209\t 9.999999974752427e-07\n",
      "\n",
      "210\t 9.999999974752427e-07\n",
      "\n",
      "211\t 9.999999974752427e-07\n",
      "\n",
      "212\t 9.999999974752427e-07\n",
      "\n",
      "213\t 9.999999974752427e-07\n",
      "\n",
      "214\t 9.999999974752427e-07\n",
      "\n",
      "215\t 9.999999974752427e-07\n",
      "\n",
      "216\t 9.999999974752427e-07\n",
      "\n",
      "217\t 9.999999974752427e-07\n",
      "\n",
      "218\t 9.999999974752427e-07\n",
      "\n",
      "219\t 9.999999974752427e-07\n",
      "\n",
      "220\t 9.999999974752427e-07\n",
      "\n",
      "221\t 9.999999974752427e-07\n",
      "\n",
      "222\t 9.999999974752427e-07\n",
      "\n",
      "223\t 9.999999974752427e-07\n",
      "\n",
      "224\t 9.999999974752427e-07\n",
      "\n",
      "225\t 9.999999974752427e-07\n",
      "\n",
      "226\t 9.999999974752427e-07\n",
      "\n",
      "227\t 9.999999974752427e-07\n",
      "\n",
      "228\t 9.999999974752427e-07\n",
      "\n",
      "229\t 9.999999974752427e-07\n",
      "\n",
      "230\t 9.999999974752427e-07\n",
      "\n",
      "231\t 9.999999974752427e-07\n",
      "\n",
      "232\t 9.999999974752427e-07\n",
      "\n",
      "233\t 9.999999974752427e-07\n",
      "\n",
      "234\t 9.999999974752427e-07\n",
      "\n",
      "235\t 9.999999974752427e-07\n",
      "\n",
      "236\t 9.999999974752427e-07\n",
      "\n",
      "237\t 9.999999974752427e-07\n",
      "\n",
      "238\t 9.999999974752427e-07\n",
      "\n",
      "239\t 9.999999974752427e-07\n",
      "\n",
      "240\t 9.999999974752427e-07\n",
      "\n",
      "241\t 9.999999974752427e-07\n",
      "\n",
      "242\t 9.999999974752427e-07\n",
      "\n",
      "243\t 9.999999974752427e-07\n",
      "\n",
      "244\t 9.999999974752427e-07\n",
      "\n",
      "245\t 9.999999974752427e-07\n",
      "\n",
      "246\t 9.999999974752427e-07\n",
      "\n",
      "247\t 9.999999974752427e-07\n",
      "\n",
      "248\t 9.999999974752427e-07\n",
      "\n",
      "249\t 9.999999974752427e-07\n",
      "\n",
      "250\t 9.999999974752427e-07\n",
      "\n",
      "251\t 9.999999974752427e-07\n",
      "\n",
      "252\t 9.999999974752427e-07\n",
      "\n",
      "253\t 9.999999974752427e-07\n",
      "\n",
      "254\t 9.999999974752427e-07\n",
      "\n",
      "255\t 9.999999974752427e-07\n",
      "\n",
      "256\t 9.999999974752427e-07\n",
      "\n",
      "257\t 9.999999974752427e-07\n",
      "\n",
      "258\t 9.999999974752427e-07\n",
      "\n",
      "259\t 9.999999974752427e-07\n",
      "\n",
      "260\t 9.999999974752427e-07\n",
      "\n",
      "261\t 9.999999974752427e-07\n",
      "\n",
      "262\t 9.999999974752427e-07\n",
      "\n",
      "263\t 9.999999974752427e-07\n",
      "\n",
      "264\t 9.999999974752427e-07\n",
      "\n",
      "265\t 9.999999974752427e-07\n",
      "\n",
      "266\t 9.999999974752427e-07\n",
      "\n",
      "267\t 9.999999974752427e-07\n",
      "\n",
      "268\t 9.999999974752427e-07\n",
      "\n",
      "269\t 9.999999974752427e-07\n",
      "\n",
      "270\t 9.999999974752427e-07\n",
      "\n",
      "271\t 9.999999974752427e-07\n",
      "\n",
      "272\t 9.999999974752427e-07\n",
      "\n",
      "273\t 9.999999974752427e-07\n",
      "\n",
      "274\t 9.999999974752427e-07\n",
      "\n",
      "275\t 9.999999974752427e-07\n",
      "\n",
      "276\t 9.999999974752427e-07\n",
      "\n",
      "277\t 9.999999974752427e-07\n",
      "\n",
      "278\t 9.999999974752427e-07\n",
      "\n",
      "279\t 9.999999974752427e-07\n",
      "\n",
      "280\t 9.999999974752427e-07\n",
      "\n",
      "281\t 9.999999974752427e-07\n",
      "\n",
      "282\t 9.999999974752427e-07\n",
      "\n",
      "283\t 9.999999974752427e-07\n",
      "\n",
      "284\t 9.999999974752427e-07\n",
      "\n",
      "285\t 9.999999974752427e-07\n",
      "\n",
      "286\t 9.999999974752427e-07\n",
      "\n",
      "287\t 9.999999974752427e-07\n",
      "\n",
      "288\t 9.999999974752427e-07\n",
      "\n",
      "289\t 9.999999974752427e-07\n",
      "\n",
      "290\t 9.999999974752427e-07\n",
      "\n",
      "291\t 9.999999974752427e-07\n",
      "\n",
      "292\t 9.999999974752427e-07\n",
      "\n",
      "293\t 9.999999974752427e-07\n",
      "\n",
      "294\t 9.999999974752427e-07\n",
      "\n",
      "295\t 9.999999974752427e-07\n",
      "\n",
      "296\t 9.999999974752427e-07\n",
      "\n",
      "297\t 9.999999974752427e-07\n",
      "\n",
      "298\t 9.999999974752427e-07\n",
      "\n",
      "299\t 9.999999974752427e-07\n",
      "\n",
      "300\t 9.999999974752427e-07\n",
      "\n",
      "301\t 9.999999974752427e-07\n",
      "\n",
      "302\t 9.999999974752427e-07\n",
      "\n",
      "303\t 9.999999974752427e-07\n",
      "\n",
      "304\t 9.999999974752427e-07\n",
      "\n",
      "305\t 9.999999974752427e-07\n",
      "\n",
      "306\t 9.999999974752427e-07\n",
      "\n",
      "307\t 9.999999974752427e-07\n",
      "\n",
      "308\t 9.999999974752427e-07\n",
      "\n",
      "309\t 9.999999974752427e-07\n",
      "\n",
      "310\t 9.999999974752427e-07\n",
      "\n",
      "311\t 9.999999974752427e-07\n",
      "\n",
      "312\t 9.999999974752427e-07\n",
      "\n",
      "313\t 9.999999974752427e-07\n",
      "\n",
      "314\t 9.999999974752427e-07\n",
      "\n",
      "315\t 9.999999974752427e-07\n",
      "\n",
      "316\t 9.999999974752427e-07\n",
      "\n",
      "317\t 9.999999974752427e-07\n",
      "\n",
      "318\t 9.999999974752427e-07\n",
      "\n",
      "319\t 9.999999974752427e-07\n",
      "\n",
      "320\t 9.999999974752427e-07\n",
      "\n",
      "321\t 9.999999974752427e-07\n",
      "\n",
      "322\t 9.999999974752427e-07\n",
      "\n",
      "323\t 9.999999974752427e-07\n",
      "\n",
      "324\t 9.999999974752427e-07\n",
      "\n",
      "325\t 9.999999974752427e-07\n",
      "\n",
      "326\t 9.999999974752427e-07\n",
      "\n",
      "327\t 9.999999974752427e-07\n",
      "\n",
      "328\t 9.999999974752427e-07\n",
      "\n",
      "329\t 9.999999974752427e-07\n",
      "\n",
      "330\t 9.999999974752427e-07\n",
      "\n",
      "331\t 9.999999974752427e-07\n",
      "\n",
      "332\t 9.999999974752427e-07\n",
      "\n",
      "333\t 9.999999974752427e-07\n",
      "\n",
      "334\t 9.999999974752427e-07\n",
      "\n",
      "335\t 9.999999974752427e-07\n",
      "\n",
      "336\t 9.999999974752427e-07\n",
      "\n",
      "337\t 9.999999974752427e-07\n",
      "\n",
      "338\t 9.999999974752427e-07\n",
      "\n",
      "339\t 9.999999974752427e-07\n",
      "\n",
      "340\t 9.999999974752427e-07\n",
      "\n",
      "341\t 9.999999974752427e-07\n",
      "\n",
      "342\t 9.999999974752427e-07\n",
      "\n",
      "343\t 9.999999974752427e-07\n",
      "\n",
      "344\t 9.999999974752427e-07\n",
      "\n",
      "345\t 9.999999974752427e-07\n",
      "\n",
      "346\t 9.999999974752427e-07\n",
      "\n",
      "347\t 9.999999974752427e-07\n",
      "\n",
      "348\t 9.999999974752427e-07\n",
      "\n",
      "349\t 9.999999974752427e-07\n",
      "\n",
      "350\t 9.999999974752427e-07\n",
      "\n",
      "351\t 9.999999974752427e-07\n",
      "\n",
      "352\t 9.999999974752427e-07\n",
      "\n",
      "353\t 9.999999974752427e-07\n",
      "\n",
      "354\t 9.999999974752427e-07\n",
      "\n",
      "355\t 9.999999974752427e-07\n",
      "\n",
      "356\t 9.999999974752427e-07\n",
      "\n",
      "357\t 9.999999974752427e-07\n",
      "\n",
      "358\t 9.999999974752427e-07\n",
      "\n",
      "359\t 9.999999974752427e-07\n",
      "\n",
      "360\t 9.999999974752427e-07\n",
      "\n",
      "361\t 9.999999974752427e-07\n",
      "\n",
      "362\t 9.999999974752427e-07\n",
      "\n",
      "363\t 9.999999974752427e-07\n",
      "\n",
      "364\t 9.999999974752427e-07\n",
      "\n",
      "365\t 9.999999974752427e-07\n",
      "\n",
      "366\t 9.999999974752427e-07\n",
      "\n",
      "367\t 9.999999974752427e-07\n",
      "\n",
      "368\t 9.999999974752427e-07\n",
      "\n",
      "369\t 9.999999974752427e-07\n",
      "\n",
      "370\t 9.999999974752427e-07\n",
      "\n",
      "371\t 9.999999974752427e-07\n",
      "\n",
      "372\t 9.999999974752427e-07\n",
      "\n",
      "373\t 9.999999974752427e-07\n",
      "\n",
      "374\t 9.999999974752427e-07\n",
      "\n",
      "375\t 9.999999974752427e-07\n",
      "\n",
      "376\t 9.999999974752427e-07\n",
      "\n",
      "377\t 9.999999974752427e-07\n",
      "\n",
      "378\t 9.999999974752427e-07\n",
      "\n",
      "379\t 9.999999974752427e-07\n",
      "\n",
      "380\t 9.999999974752427e-07\n",
      "\n",
      "381\t 9.999999974752427e-07\n",
      "\n",
      "382\t 9.999999974752427e-07\n",
      "\n",
      "383\t 9.999999974752427e-07\n",
      "\n",
      "384\t 9.999999974752427e-07\n",
      "\n",
      "385\t 9.999999974752427e-07\n",
      "\n",
      "386\t 9.999999974752427e-07\n",
      "\n",
      "387\t 9.999999974752427e-07\n",
      "\n",
      "388\t 9.999999974752427e-07\n",
      "\n",
      "389\t 9.999999974752427e-07\n",
      "\n",
      "390\t 9.999999974752427e-07\n",
      "\n",
      "391\t 9.999999974752427e-07\n",
      "\n",
      "392\t 9.999999974752427e-07\n",
      "\n",
      "393\t 9.999999974752427e-07\n",
      "\n",
      "394\t 9.999999974752427e-07\n",
      "\n",
      "395\t 9.999999974752427e-07\n",
      "\n",
      "396\t 9.999999974752427e-07\n",
      "\n",
      "397\t 9.999999974752427e-07\n",
      "\n",
      "398\t 9.999999974752427e-07\n",
      "\n",
      "399\t 9.999999974752427e-07\n",
      "\n",
      "400\t 9.999999974752427e-07\n",
      "\n",
      "401\t 9.999999974752427e-07\n",
      "\n",
      "402\t 9.999999974752427e-07\n",
      "\n",
      "403\t 9.999999974752427e-07\n",
      "\n",
      "404\t 9.999999974752427e-07\n",
      "\n",
      "405\t 9.999999974752427e-07\n",
      "\n",
      "406\t 9.999999974752427e-07\n",
      "\n",
      "407\t 9.999999974752427e-07\n",
      "\n",
      "408\t 9.999999974752427e-07\n",
      "\n",
      "409\t 9.999999974752427e-07\n",
      "\n",
      "410\t 9.999999974752427e-07\n",
      "\n",
      "411\t 9.999999974752427e-07\n",
      "\n",
      "412\t 9.999999974752427e-07\n",
      "\n",
      "413\t 9.999999974752427e-07\n",
      "\n",
      "414\t 9.999999974752427e-07\n",
      "\n",
      "415\t 9.999999974752427e-07\n",
      "\n",
      "416\t 9.999999974752427e-07\n",
      "\n",
      "417\t 9.999999974752427e-07\n",
      "\n",
      "418\t 9.999999974752427e-07\n",
      "\n",
      "419\t 9.999999974752427e-07\n",
      "\n",
      "420\t 9.999999974752427e-07\n",
      "\n",
      "421\t 9.999999974752427e-07\n",
      "\n",
      "422\t 9.999999974752427e-07\n",
      "\n",
      "423\t 9.999999974752427e-07\n",
      "\n",
      "424\t 9.999999974752427e-07\n",
      "\n",
      "425\t 9.999999974752427e-07\n",
      "\n",
      "426\t 9.999999974752427e-07\n",
      "\n",
      "427\t 9.999999974752427e-07\n",
      "\n",
      "428\t 9.999999974752427e-07\n",
      "\n",
      "429\t 9.999999974752427e-07\n",
      "\n",
      "430\t 9.999999974752427e-07\n",
      "\n",
      "431\t 9.999999974752427e-07\n",
      "\n",
      "432\t 9.999999974752427e-07\n",
      "\n",
      "433\t 9.999999974752427e-07\n",
      "\n",
      "434\t 9.999999974752427e-07\n",
      "\n",
      "435\t 9.999999974752427e-07\n",
      "\n",
      "436\t 9.999999974752427e-07\n",
      "\n",
      "437\t 9.999999974752427e-07\n",
      "\n",
      "438\t 9.999999974752427e-07\n",
      "\n",
      "439\t 9.999999974752427e-07\n",
      "\n",
      "440\t 9.999999974752427e-07\n",
      "\n",
      "441\t 9.999999974752427e-07\n",
      "\n",
      "442\t 9.999999974752427e-07\n",
      "\n",
      "443\t 9.999999974752427e-07\n",
      "\n",
      "444\t 9.999999974752427e-07\n",
      "\n",
      "445\t 9.999999974752427e-07\n",
      "\n",
      "446\t 9.999999974752427e-07\n",
      "\n",
      "447\t 9.999999974752427e-07\n",
      "\n",
      "448\t 9.999999974752427e-07\n",
      "\n",
      "449\t 9.999999974752427e-07\n",
      "\n",
      "450\t 9.999999974752427e-07\n",
      "\n",
      "451\t 9.999999974752427e-07\n",
      "\n",
      "452\t 9.999999974752427e-07\n",
      "\n",
      "453\t 9.999999974752427e-07\n",
      "\n",
      "454\t 9.999999974752427e-07\n",
      "\n",
      "455\t 9.999999974752427e-07\n",
      "\n",
      "456\t 9.999999974752427e-07\n",
      "\n",
      "457\t 9.999999974752427e-07\n",
      "\n",
      "458\t 9.999999974752427e-07\n",
      "\n",
      "459\t 9.999999974752427e-07\n",
      "\n",
      "460\t 9.999999974752427e-07\n",
      "\n",
      "461\t 9.999999974752427e-07\n",
      "\n",
      "462\t 9.999999974752427e-07\n",
      "\n",
      "463\t 9.999999974752427e-07\n",
      "\n",
      "464\t 9.999999974752427e-07\n",
      "\n",
      "465\t 9.999999974752427e-07\n",
      "\n",
      "466\t 9.999999974752427e-07\n",
      "\n",
      "467\t 9.999999974752427e-07\n",
      "\n",
      "468\t 9.999999974752427e-07\n",
      "\n",
      "469\t 9.999999974752427e-07\n",
      "\n",
      "470\t 9.999999974752427e-07\n",
      "\n",
      "471\t 9.999999974752427e-07\n",
      "\n",
      "472\t 9.999999974752427e-07\n",
      "\n",
      "473\t 9.999999974752427e-07\n",
      "\n",
      "474\t 9.999999974752427e-07\n",
      "\n",
      "475\t 9.999999974752427e-07\n",
      "\n",
      "476\t 9.999999974752427e-07\n",
      "\n",
      "477\t 9.999999974752427e-07\n",
      "\n",
      "478\t 9.999999974752427e-07\n",
      "\n",
      "479\t 9.999999974752427e-07\n",
      "\n",
      "480\t 9.999999974752427e-07\n",
      "\n",
      "481\t 9.999999974752427e-07\n",
      "\n",
      "482\t 9.999999974752427e-07\n",
      "\n",
      "483\t 9.999999974752427e-07\n",
      "\n",
      "484\t 9.999999974752427e-07\n",
      "\n",
      "485\t 9.999999974752427e-07\n",
      "\n",
      "486\t 9.999999974752427e-07\n",
      "\n",
      "487\t 9.999999974752427e-07\n",
      "\n",
      "488\t 9.999999974752427e-07\n",
      "\n",
      "489\t 9.999999974752427e-07\n",
      "\n",
      "490\t 9.999999974752427e-07\n",
      "\n",
      "491\t 9.999999974752427e-07\n",
      "\n",
      "492\t 9.999999974752427e-07\n",
      "\n",
      "493\t 9.999999974752427e-07\n",
      "\n",
      "494\t 9.999999974752427e-07\n",
      "\n",
      "495\t 9.999999974752427e-07\n",
      "\n",
      "496\t 9.999999974752427e-07\n",
      "\n",
      "497\t 9.999999974752427e-07\n",
      "\n",
      "498\t 9.999999974752427e-07\n",
      "\n",
      "499\t 9.999999974752427e-07\n",
      "\n",
      "500\t 9.999999974752427e-07\n",
      "\n",
      "501\t 9.999999974752427e-07\n",
      "\n",
      "502\t 9.999999974752427e-07\n",
      "\n",
      "503\t 9.999999974752427e-07\n",
      "\n",
      "504\t 9.999999974752427e-07\n",
      "\n",
      "505\t 9.999999974752427e-07\n",
      "\n",
      "506\t 9.999999974752427e-07\n",
      "\n",
      "507\t 9.999999974752427e-07\n",
      "\n",
      "508\t 9.999999974752427e-07\n",
      "\n",
      "509\t 9.999999974752427e-07\n",
      "\n",
      "510\t 9.999999974752427e-07\n",
      "\n",
      "511\t 9.999999974752427e-07\n",
      "\n",
      "512\t 9.999999974752427e-07\n",
      "\n",
      "513\t 9.999999974752427e-07\n",
      "\n",
      "514\t 9.999999974752427e-07\n",
      "\n",
      "515\t 9.999999974752427e-07\n",
      "\n",
      "516\t 9.999999974752427e-07\n",
      "\n",
      "517\t 9.999999974752427e-07\n",
      "\n",
      "518\t 9.999999974752427e-07\n",
      "\n",
      "519\t 9.999999974752427e-07\n",
      "\n",
      "520\t 9.999999974752427e-07\n",
      "\n",
      "521\t 9.999999974752427e-07\n",
      "\n",
      "522\t 9.999999974752427e-07\n",
      "\n",
      "523\t 9.999999974752427e-07\n",
      "\n",
      "524\t 9.999999974752427e-07\n",
      "\n",
      "525\t 9.999999974752427e-07\n",
      "\n",
      "526\t 9.999999974752427e-07\n",
      "\n",
      "527\t 9.999999974752427e-07\n",
      "\n",
      "528\t 9.999999974752427e-07\n",
      "\n",
      "529\t 9.999999974752427e-07\n",
      "\n",
      "530\t 9.999999974752427e-07\n",
      "\n",
      "531\t 9.999999974752427e-07\n",
      "\n",
      "532\t 9.999999974752427e-07\n",
      "\n",
      "533\t 9.999999974752427e-07\n",
      "\n",
      "534\t 9.999999974752427e-07\n",
      "\n",
      "535\t 9.999999974752427e-07\n",
      "\n",
      "536\t 9.999999974752427e-07\n",
      "\n",
      "537\t 9.999999974752427e-07\n",
      "\n",
      "538\t 9.999999974752427e-07\n",
      "\n",
      "539\t 9.999999974752427e-07\n",
      "\n",
      "540\t 9.999999974752427e-07\n",
      "\n",
      "541\t 9.999999974752427e-07\n",
      "\n",
      "542\t 9.999999974752427e-07\n",
      "\n",
      "543\t 9.999999974752427e-07\n",
      "\n",
      "544\t 9.999999974752427e-07\n",
      "\n",
      "545\t 9.999999974752427e-07\n",
      "\n",
      "546\t 9.999999974752427e-07\n",
      "\n",
      "547\t 9.999999974752427e-07\n",
      "\n",
      "548\t 9.999999974752427e-07\n",
      "\n",
      "549\t 9.999999974752427e-07\n",
      "\n",
      "550\t 9.999999974752427e-07\n",
      "\n",
      "551\t 9.999999974752427e-07\n",
      "\n",
      "552\t 9.999999974752427e-07\n",
      "\n",
      "553\t 9.999999974752427e-07\n",
      "\n",
      "554\t 9.999999974752427e-07\n",
      "\n",
      "555\t 9.999999974752427e-07\n",
      "\n",
      "556\t 9.999999974752427e-07\n",
      "\n",
      "557\t 9.999999974752427e-07\n",
      "\n",
      "558\t 9.999999974752427e-07\n",
      "\n",
      "559\t 9.999999974752427e-07\n",
      "\n",
      "560\t 9.999999974752427e-07\n",
      "\n",
      "561\t 9.999999974752427e-07\n",
      "\n",
      "562\t 9.999999974752427e-07\n",
      "\n",
      "563\t 9.999999974752427e-07\n",
      "\n",
      "564\t 9.999999974752427e-07\n",
      "\n",
      "565\t 9.999999974752427e-07\n",
      "\n",
      "566\t 9.999999974752427e-07\n",
      "\n",
      "567\t 9.999999974752427e-07\n",
      "\n",
      "568\t 9.999999974752427e-07\n",
      "\n",
      "569\t 9.999999974752427e-07\n",
      "\n",
      "570\t 9.999999974752427e-07\n",
      "\n",
      "571\t 9.999999974752427e-07\n",
      "\n",
      "572\t 9.999999974752427e-07\n",
      "\n",
      "573\t 9.999999974752427e-07\n",
      "\n",
      "574\t 9.999999974752427e-07\n",
      "\n",
      "575\t 9.999999974752427e-07\n",
      "\n",
      "576\t 9.999999974752427e-07\n",
      "\n",
      "577\t 9.999999974752427e-07\n",
      "\n",
      "578\t 9.999999974752427e-07\n",
      "\n",
      "579\t 9.999999974752427e-07\n",
      "\n",
      "580\t 9.999999974752427e-07\n",
      "\n",
      "581\t 9.999999974752427e-07\n",
      "\n",
      "582\t 9.999999974752427e-07\n",
      "\n",
      "583\t 9.999999974752427e-07\n",
      "\n",
      "584\t 9.999999974752427e-07\n",
      "\n",
      "585\t 9.999999974752427e-07\n",
      "\n",
      "586\t 9.999999974752427e-07\n",
      "\n",
      "587\t 9.999999974752427e-07\n",
      "\n",
      "588\t 9.999999974752427e-07\n",
      "\n",
      "589\t 9.999999974752427e-07\n",
      "\n",
      "590\t 9.999999974752427e-07\n",
      "\n",
      "591\t 9.999999974752427e-07\n",
      "\n",
      "592\t 9.999999974752427e-07\n",
      "\n",
      "593\t 9.999999974752427e-07\n",
      "\n",
      "594\t 9.999999974752427e-07\n",
      "\n",
      "595\t 9.999999974752427e-07\n",
      "\n",
      "596\t 9.999999974752427e-07\n",
      "\n",
      "597\t 9.999999974752427e-07\n",
      "\n",
      "598\t 9.999999974752427e-07\n",
      "\n",
      "599\t 9.999999974752427e-07\n",
      "\n",
      "600\t 9.999999974752427e-07\n",
      "\n",
      "601\t 9.999999974752427e-07\n",
      "\n",
      "602\t 9.999999974752427e-07\n",
      "\n",
      "603\t 9.999999974752427e-07\n",
      "\n",
      "604\t 9.999999974752427e-07\n",
      "\n",
      "605\t 9.999999974752427e-07\n",
      "\n",
      "606\t 9.999999974752427e-07\n",
      "\n",
      "607\t 9.999999974752427e-07\n",
      "\n",
      "608\t 9.999999974752427e-07\n",
      "\n",
      "609\t 9.999999974752427e-07\n",
      "\n",
      "610\t 9.999999974752427e-07\n",
      "\n",
      "611\t 9.999999974752427e-07\n",
      "\n",
      "612\t 9.999999974752427e-07\n",
      "\n",
      "613\t 9.999999974752427e-07\n",
      "\n",
      "614\t 9.999999974752427e-07\n",
      "\n",
      "615\t 9.999999974752427e-07\n",
      "\n",
      "616\t 9.999999974752427e-07\n",
      "\n",
      "617\t 9.999999974752427e-07\n",
      "\n",
      "618\t 9.999999974752427e-07\n",
      "\n",
      "619\t 9.999999974752427e-07\n",
      "\n",
      "620\t 9.999999974752427e-07\n",
      "\n",
      "621\t 9.999999974752427e-07\n",
      "\n",
      "622\t 9.999999974752427e-07\n",
      "\n",
      "623\t 9.999999974752427e-07\n",
      "\n",
      "624\t 9.999999974752427e-07\n",
      "\n",
      "625\t 9.999999974752427e-07\n",
      "\n",
      "626\t 9.999999974752427e-07\n",
      "\n",
      "627\t 9.999999974752427e-07\n",
      "\n",
      "628\t 9.999999974752427e-07\n",
      "\n",
      "629\t 9.999999974752427e-07\n",
      "\n",
      "630\t 9.999999974752427e-07\n",
      "\n",
      "631\t 9.999999974752427e-07\n",
      "\n",
      "632\t 9.999999974752427e-07\n",
      "\n",
      "633\t 9.999999974752427e-07\n",
      "\n",
      "634\t 9.999999974752427e-07\n",
      "\n",
      "635\t 9.999999974752427e-07\n",
      "\n",
      "636\t 9.999999974752427e-07\n",
      "\n",
      "637\t 9.999999974752427e-07\n",
      "\n",
      "638\t 9.999999974752427e-07\n",
      "\n",
      "639\t 9.999999974752427e-07\n",
      "\n",
      "640\t 9.999999974752427e-07\n",
      "\n",
      "641\t 9.999999974752427e-07\n",
      "\n",
      "642\t 9.999999974752427e-07\n",
      "\n",
      "643\t 9.999999974752427e-07\n",
      "\n",
      "644\t 9.999999974752427e-07\n",
      "\n",
      "645\t 9.999999974752427e-07\n",
      "\n",
      "646\t 9.999999974752427e-07\n",
      "\n",
      "647\t 9.999999974752427e-07\n",
      "\n",
      "648\t 9.999999974752427e-07\n",
      "\n",
      "649\t 9.999999974752427e-07\n",
      "\n",
      "650\t 9.999999974752427e-07\n",
      "\n",
      "651\t 9.999999974752427e-07\n",
      "\n",
      "652\t 9.999999974752427e-07\n",
      "\n",
      "653\t 9.999999974752427e-07\n",
      "\n",
      "654\t 9.999999974752427e-07\n",
      "\n",
      "655\t 9.999999974752427e-07\n",
      "\n",
      "656\t 9.999999974752427e-07\n",
      "\n",
      "657\t 9.999999974752427e-07\n",
      "\n",
      "658\t 9.999999974752427e-07\n",
      "\n",
      "659\t 9.999999974752427e-07\n",
      "\n",
      "660\t 9.999999974752427e-07\n",
      "\n",
      "661\t 9.999999974752427e-07\n",
      "\n",
      "662\t 9.999999974752427e-07\n",
      "\n",
      "663\t 9.999999974752427e-07\n",
      "\n",
      "664\t 9.999999974752427e-07\n",
      "\n",
      "665\t 9.999999974752427e-07\n",
      "\n",
      "666\t 9.999999974752427e-07\n",
      "\n",
      "667\t 9.999999974752427e-07\n",
      "\n",
      "668\t 9.999999974752427e-07\n",
      "\n",
      "669\t 9.999999974752427e-07\n",
      "\n",
      "670\t 9.999999974752427e-07\n",
      "\n",
      "671\t 9.999999974752427e-07\n",
      "\n",
      "672\t 9.999999974752427e-07\n",
      "\n",
      "673\t 9.999999974752427e-07\n",
      "\n",
      "674\t 9.999999974752427e-07\n",
      "\n",
      "675\t 9.999999974752427e-07\n",
      "\n",
      "676\t 9.999999974752427e-07\n",
      "\n",
      "677\t 9.999999974752427e-07\n",
      "\n",
      "678\t 9.999999974752427e-07\n",
      "\n",
      "679\t 9.999999974752427e-07\n",
      "\n",
      "680\t 9.999999974752427e-07\n",
      "\n",
      "681\t 9.999999974752427e-07\n",
      "\n",
      "682\t 9.999999974752427e-07\n",
      "\n",
      "683\t 9.999999974752427e-07\n",
      "\n",
      "684\t 9.999999974752427e-07\n",
      "\n",
      "685\t 9.999999974752427e-07\n",
      "\n",
      "686\t 9.999999974752427e-07\n",
      "\n",
      "687\t 9.999999974752427e-07\n",
      "\n",
      "688\t 9.999999974752427e-07\n",
      "\n",
      "689\t 9.999999974752427e-07\n",
      "\n",
      "690\t 9.999999974752427e-07\n",
      "\n",
      "691\t 9.999999974752427e-07\n",
      "\n",
      "692\t 9.999999974752427e-07\n",
      "\n",
      "693\t 9.999999974752427e-07\n",
      "\n",
      "694\t 9.999999974752427e-07\n",
      "\n",
      "695\t 9.999999974752427e-07\n",
      "\n",
      "696\t 9.999999974752427e-07\n",
      "\n",
      "697\t 9.999999974752427e-07\n",
      "\n",
      "698\t 9.999999974752427e-07\n",
      "\n",
      "699\t 9.999999974752427e-07\n",
      "\n",
      "700\t 9.999999974752427e-07\n",
      "\n",
      "701\t 9.999999974752427e-07\n",
      "\n",
      "702\t 9.999999974752427e-07\n",
      "\n",
      "703\t 9.999999974752427e-07\n",
      "\n",
      "704\t 9.999999974752427e-07\n",
      "\n",
      "705\t 9.999999974752427e-07\n",
      "\n",
      "706\t 9.999999974752427e-07\n",
      "\n",
      "707\t 9.999999974752427e-07\n",
      "\n",
      "708\t 9.999999974752427e-07\n",
      "\n",
      "709\t 9.999999974752427e-07\n",
      "\n",
      "710\t 9.999999974752427e-07\n",
      "\n",
      "711\t 9.999999974752427e-07\n",
      "\n",
      "712\t 9.999999974752427e-07\n",
      "\n",
      "713\t 9.999999974752427e-07\n",
      "\n",
      "714\t 9.999999974752427e-07\n",
      "\n",
      "715\t 9.999999974752427e-07\n",
      "\n",
      "716\t 9.999999974752427e-07\n",
      "\n",
      "717\t 9.999999974752427e-07\n",
      "\n",
      "718\t 9.999999974752427e-07\n",
      "\n",
      "719\t 9.999999974752427e-07\n",
      "\n",
      "720\t 9.999999974752427e-07\n",
      "\n",
      "721\t 9.999999974752427e-07\n",
      "\n",
      "722\t 9.999999974752427e-07\n",
      "\n",
      "723\t 9.999999974752427e-07\n",
      "\n",
      "724\t 9.999999974752427e-07\n",
      "\n",
      "725\t 9.999999974752427e-07\n",
      "\n",
      "726\t 9.999999974752427e-07\n",
      "\n",
      "727\t 9.999999974752427e-07\n",
      "\n",
      "728\t 9.999999974752427e-07\n",
      "\n",
      "729\t 9.999999974752427e-07\n",
      "\n",
      "730\t 9.999999974752427e-07\n",
      "\n",
      "731\t 9.999999974752427e-07\n",
      "\n",
      "732\t 9.999999974752427e-07\n",
      "\n",
      "733\t 9.999999974752427e-07\n",
      "\n",
      "734\t 9.999999974752427e-07\n",
      "\n",
      "735\t 9.999999974752427e-07\n",
      "\n",
      "736\t 9.999999974752427e-07\n",
      "\n",
      "737\t 9.999999974752427e-07\n",
      "\n",
      "738\t 9.999999974752427e-07\n",
      "\n",
      "739\t 9.999999974752427e-07\n",
      "\n",
      "740\t 9.999999974752427e-07\n",
      "\n",
      "741\t 9.999999974752427e-07\n",
      "\n",
      "742\t 9.999999974752427e-07\n",
      "\n",
      "743\t 9.999999974752427e-07\n",
      "\n",
      "744\t 9.999999974752427e-07\n",
      "\n",
      "745\t 9.999999974752427e-07\n",
      "\n",
      "746\t 9.999999974752427e-07\n",
      "\n",
      "747\t 9.999999974752427e-07\n",
      "\n",
      "748\t 9.999999974752427e-07\n",
      "\n",
      "749\t 9.999999974752427e-07\n",
      "\n",
      "750\t 9.999999974752427e-07\n",
      "\n",
      "751\t 9.999999974752427e-07\n",
      "\n",
      "752\t 9.999999974752427e-07\n",
      "\n",
      "753\t 9.999999974752427e-07\n",
      "\n",
      "754\t 9.999999974752427e-07\n",
      "\n",
      "755\t 9.999999974752427e-07\n",
      "\n",
      "756\t 9.999999974752427e-07\n",
      "\n",
      "757\t 9.999999974752427e-07\n",
      "\n",
      "758\t 9.999999974752427e-07\n",
      "\n",
      "759\t 9.999999974752427e-07\n",
      "\n",
      "760\t 9.999999974752427e-07\n",
      "\n",
      "761\t 9.999999974752427e-07\n",
      "\n",
      "762\t 9.999999974752427e-07\n",
      "\n",
      "763\t 9.999999974752427e-07\n",
      "\n",
      "764\t 9.999999974752427e-07\n",
      "\n",
      "765\t 9.999999974752427e-07\n",
      "\n",
      "766\t 9.999999974752427e-07\n",
      "\n",
      "767\t 9.999999974752427e-07\n",
      "\n",
      "768\t 9.999999974752427e-07\n",
      "\n",
      "769\t 9.999999974752427e-07\n",
      "\n",
      "770\t 9.999999974752427e-07\n",
      "\n",
      "771\t 9.999999974752427e-07\n",
      "\n",
      "772\t 9.999999974752427e-07\n",
      "\n",
      "773\t 9.999999974752427e-07\n",
      "\n",
      "774\t 9.999999974752427e-07\n",
      "\n",
      "775\t 9.999999974752427e-07\n",
      "\n",
      "776\t 9.999999974752427e-07\n",
      "\n",
      "777\t 9.999999974752427e-07\n",
      "\n",
      "778\t 9.999999974752427e-07\n",
      "\n",
      "779\t 9.999999974752427e-07\n",
      "\n",
      "780\t 9.999999974752427e-07\n",
      "\n",
      "781\t 9.999999974752427e-07\n",
      "\n",
      "782\t 9.999999974752427e-07\n",
      "\n",
      "783\t 9.999999974752427e-07\n",
      "\n",
      "784\t 9.999999974752427e-07\n",
      "\n",
      "785\t 9.999999974752427e-07\n",
      "\n",
      "786\t 9.999999974752427e-07\n",
      "\n",
      "787\t 9.999999974752427e-07\n",
      "\n",
      "788\t 9.999999974752427e-07\n",
      "\n",
      "789\t 9.999999974752427e-07\n",
      "\n",
      "790\t 9.999999974752427e-07\n",
      "\n",
      "791\t 9.999999974752427e-07\n",
      "\n",
      "792\t 9.999999974752427e-07\n",
      "\n",
      "793\t 9.999999974752427e-07\n",
      "\n",
      "794\t 9.999999974752427e-07\n",
      "\n",
      "795\t 9.999999974752427e-07\n",
      "\n",
      "796\t 9.999999974752427e-07\n",
      "\n",
      "797\t 9.999999974752427e-07\n",
      "\n",
      "798\t 9.999999974752427e-07\n",
      "\n",
      "799\t 9.999999974752427e-07\n",
      "\n",
      "800\t 9.999999974752427e-07\n",
      "\n",
      "801\t 9.999999974752427e-07\n",
      "\n",
      "802\t 9.999999974752427e-07\n",
      "\n",
      "803\t 9.999999974752427e-07\n",
      "\n",
      "804\t 9.999999974752427e-07\n",
      "\n",
      "805\t 9.999999974752427e-07\n",
      "\n",
      "806\t 9.999999974752427e-07\n",
      "\n",
      "807\t 9.999999974752427e-07\n",
      "\n",
      "808\t 9.999999974752427e-07\n",
      "\n",
      "809\t 9.999999974752427e-07\n",
      "\n",
      "810\t 9.999999974752427e-07\n",
      "\n",
      "811\t 9.999999974752427e-07\n",
      "\n",
      "812\t 9.999999974752427e-07\n",
      "\n",
      "813\t 9.999999974752427e-07\n",
      "\n",
      "814\t 9.999999974752427e-07\n",
      "\n",
      "815\t 9.999999974752427e-07\n",
      "\n",
      "816\t 9.999999974752427e-07\n",
      "\n",
      "817\t 9.999999974752427e-07\n",
      "\n",
      "818\t 9.999999974752427e-07\n",
      "\n",
      "819\t 9.999999974752427e-07\n",
      "\n",
      "820\t 9.999999974752427e-07\n",
      "\n",
      "821\t 9.999999974752427e-07\n",
      "\n",
      "822\t 9.999999974752427e-07\n",
      "\n",
      "823\t 9.999999974752427e-07\n",
      "\n",
      "824\t 9.999999974752427e-07\n",
      "\n",
      "825\t 9.999999974752427e-07\n",
      "\n",
      "826\t 9.999999974752427e-07\n",
      "\n",
      "827\t 9.999999974752427e-07\n",
      "\n",
      "828\t 9.999999974752427e-07\n",
      "\n",
      "829\t 9.999999974752427e-07\n",
      "\n",
      "830\t 9.999999974752427e-07\n",
      "\n",
      "831\t 9.999999974752427e-07\n",
      "\n",
      "832\t 9.999999974752427e-07\n",
      "\n",
      "833\t 9.999999974752427e-07\n",
      "\n",
      "834\t 9.999999974752427e-07\n",
      "\n",
      "835\t 9.999999974752427e-07\n",
      "\n",
      "836\t 9.999999974752427e-07\n",
      "\n",
      "837\t 9.999999974752427e-07\n",
      "\n",
      "838\t 9.999999974752427e-07\n",
      "\n",
      "839\t 9.999999974752427e-07\n",
      "\n",
      "840\t 9.999999974752427e-07\n",
      "\n",
      "841\t 9.999999974752427e-07\n",
      "\n",
      "842\t 9.999999974752427e-07\n",
      "\n",
      "843\t 9.999999974752427e-07\n",
      "\n",
      "844\t 9.999999974752427e-07\n",
      "\n",
      "845\t 9.999999974752427e-07\n",
      "\n",
      "846\t 9.999999974752427e-07\n",
      "\n",
      "847\t 9.999999974752427e-07\n",
      "\n",
      "848\t 9.999999974752427e-07\n",
      "\n",
      "849\t 9.999999974752427e-07\n",
      "\n",
      "850\t 9.999999974752427e-07\n",
      "\n",
      "851\t 9.999999974752427e-07\n",
      "\n",
      "852\t 9.999999974752427e-07\n",
      "\n",
      "853\t 9.999999974752427e-07\n",
      "\n",
      "854\t 9.999999974752427e-07\n",
      "\n",
      "855\t 9.999999974752427e-07\n",
      "\n",
      "856\t 9.999999974752427e-07\n",
      "\n",
      "857\t 9.999999974752427e-07\n",
      "\n",
      "858\t 9.999999974752427e-07\n",
      "\n",
      "859\t 9.999999974752427e-07\n",
      "\n",
      "860\t 9.999999974752427e-07\n",
      "\n",
      "861\t 9.999999974752427e-07\n",
      "\n",
      "862\t 9.999999974752427e-07\n",
      "\n",
      "863\t 9.999999974752427e-07\n",
      "\n",
      "864\t 9.999999974752427e-07\n",
      "\n",
      "865\t 9.999999974752427e-07\n",
      "\n",
      "866\t 9.999999974752427e-07\n",
      "\n",
      "867\t 9.999999974752427e-07\n",
      "\n",
      "868\t 9.999999974752427e-07\n",
      "\n",
      "869\t 9.999999974752427e-07\n",
      "\n",
      "870\t 9.999999974752427e-07\n",
      "\n",
      "871\t 9.999999974752427e-07\n",
      "\n",
      "872\t 9.999999974752427e-07\n",
      "\n",
      "873\t 9.999999974752427e-07\n",
      "\n",
      "874\t 9.999999974752427e-07\n",
      "\n",
      "875\t 9.999999974752427e-07\n",
      "\n",
      "876\t 9.999999974752427e-07\n",
      "\n",
      "877\t 9.999999974752427e-07\n",
      "\n",
      "878\t 9.999999974752427e-07\n",
      "\n",
      "879\t 9.999999974752427e-07\n",
      "\n",
      "880\t 9.999999974752427e-07\n",
      "\n",
      "881\t 9.999999974752427e-07\n",
      "\n",
      "882\t 9.999999974752427e-07\n",
      "\n",
      "883\t 9.999999974752427e-07\n",
      "\n",
      "884\t 9.999999974752427e-07\n",
      "\n",
      "885\t 9.999999974752427e-07\n",
      "\n",
      "886\t 9.999999974752427e-07\n",
      "\n",
      "887\t 9.999999974752427e-07\n",
      "\n",
      "888\t 9.999999974752427e-07\n",
      "\n",
      "889\t 9.999999974752427e-07\n",
      "\n",
      "890\t 9.999999974752427e-07\n",
      "\n",
      "891\t 9.999999974752427e-07\n",
      "\n",
      "892\t 9.999999974752427e-07\n",
      "\n",
      "893\t 9.999999974752427e-07\n",
      "\n",
      "894\t 9.999999974752427e-07\n",
      "\n",
      "895\t 9.999999974752427e-07\n",
      "\n",
      "896\t 9.999999974752427e-07\n",
      "\n",
      "897\t 9.999999974752427e-07\n",
      "\n",
      "898\t 9.999999974752427e-07\n",
      "\n",
      "899\t 9.999999974752427e-07\n",
      "\n",
      "900\t 9.999999974752427e-07\n",
      "\n",
      "901\t 9.999999974752427e-07\n",
      "\n",
      "902\t 9.999999974752427e-07\n",
      "\n",
      "903\t 9.999999974752427e-07\n",
      "\n",
      "904\t 9.999999974752427e-07\n",
      "\n",
      "905\t 9.999999974752427e-07\n",
      "\n",
      "906\t 9.999999974752427e-07\n",
      "\n",
      "907\t 9.999999974752427e-07\n",
      "\n",
      "908\t 9.999999974752427e-07\n",
      "\n",
      "909\t 9.999999974752427e-07\n",
      "\n",
      "910\t 9.999999974752427e-07\n",
      "\n",
      "911\t 9.999999974752427e-07\n",
      "\n",
      "912\t 9.999999974752427e-07\n",
      "\n",
      "913\t 9.999999974752427e-07\n",
      "\n",
      "914\t 9.999999974752427e-07\n",
      "\n",
      "915\t 9.999999974752427e-07\n",
      "\n",
      "916\t 9.999999974752427e-07\n",
      "\n",
      "917\t 9.999999974752427e-07\n",
      "\n",
      "918\t 9.999999974752427e-07\n",
      "\n",
      "919\t 9.999999974752427e-07\n",
      "\n",
      "920\t 9.999999974752427e-07\n",
      "\n",
      "921\t 9.999999974752427e-07\n",
      "\n",
      "922\t 9.999999974752427e-07\n",
      "\n",
      "923\t 9.999999974752427e-07\n",
      "\n",
      "924\t 9.999999974752427e-07\n",
      "\n",
      "925\t 9.999999974752427e-07\n",
      "\n",
      "926\t 9.999999974752427e-07\n",
      "\n",
      "927\t 9.999999974752427e-07\n",
      "\n",
      "928\t 9.999999974752427e-07\n",
      "\n",
      "929\t 9.999999974752427e-07\n",
      "\n",
      "930\t 9.999999974752427e-07\n",
      "\n",
      "931\t 9.999999974752427e-07\n",
      "\n",
      "932\t 9.999999974752427e-07\n",
      "\n",
      "933\t 9.999999974752427e-07\n",
      "\n",
      "934\t 9.999999974752427e-07\n",
      "\n",
      "935\t 9.999999974752427e-07\n",
      "\n",
      "936\t 9.999999974752427e-07\n",
      "\n",
      "937\t 9.999999974752427e-07\n",
      "\n",
      "938\t 9.999999974752427e-07\n",
      "\n",
      "939\t 9.999999974752427e-07\n",
      "\n",
      "940\t 9.999999974752427e-07\n",
      "\n",
      "941\t 9.999999974752427e-07\n",
      "\n",
      "942\t 9.999999974752427e-07\n",
      "\n",
      "943\t 9.999999974752427e-07\n",
      "\n",
      "944\t 9.999999974752427e-07\n",
      "\n",
      "945\t 9.999999974752427e-07\n",
      "\n",
      "946\t 9.999999974752427e-07\n",
      "\n",
      "947\t 9.999999974752427e-07\n",
      "\n",
      "948\t 9.999999974752427e-07\n",
      "\n",
      "949\t 9.999999974752427e-07\n",
      "\n",
      "950\t 9.999999974752427e-07\n",
      "\n",
      "951\t 9.999999974752427e-07\n",
      "\n",
      "952\t 9.999999974752427e-07\n",
      "\n",
      "953\t 9.999999974752427e-07\n",
      "\n",
      "954\t 9.999999974752427e-07\n",
      "\n",
      "955\t 9.999999974752427e-07\n",
      "\n",
      "956\t 9.999999974752427e-07\n",
      "\n",
      "957\t 9.999999974752427e-07\n",
      "\n",
      "958\t 9.999999974752427e-07\n",
      "\n",
      "959\t 9.999999974752427e-07\n",
      "\n",
      "960\t 9.999999974752427e-07\n",
      "\n",
      "961\t 9.999999974752427e-07\n",
      "\n",
      "962\t 9.999999974752427e-07\n",
      "\n",
      "963\t 9.999999974752427e-07\n",
      "\n",
      "964\t 9.999999974752427e-07\n",
      "\n",
      "965\t 9.999999974752427e-07\n",
      "\n",
      "966\t 9.999999974752427e-07\n",
      "\n",
      "967\t 9.999999974752427e-07\n",
      "\n",
      "968\t 9.999999974752427e-07\n",
      "\n",
      "969\t 9.999999974752427e-07\n",
      "\n",
      "970\t 9.999999974752427e-07\n",
      "\n",
      "971\t 9.999999974752427e-07\n",
      "\n",
      "972\t 9.999999974752427e-07\n",
      "\n",
      "973\t 9.999999974752427e-07\n",
      "\n",
      "974\t 9.999999974752427e-07\n",
      "\n",
      "975\t 9.999999974752427e-07\n",
      "\n",
      "976\t 9.999999974752427e-07\n",
      "\n",
      "977\t 9.999999974752427e-07\n",
      "\n",
      "978\t 9.999999974752427e-07\n",
      "\n",
      "979\t 9.999999974752427e-07\n",
      "\n",
      "980\t 9.999999974752427e-07\n",
      "\n",
      "981\t 9.999999974752427e-07\n",
      "\n",
      "982\t 9.999999974752427e-07\n",
      "\n",
      "983\t 9.999999974752427e-07\n",
      "\n",
      "984\t 9.999999974752427e-07\n",
      "\n",
      "985\t 9.999999974752427e-07\n",
      "\n",
      "986\t 9.999999974752427e-07\n",
      "\n",
      "987\t 9.999999974752427e-07\n",
      "\n",
      "988\t 9.999999974752427e-07\n",
      "\n",
      "989\t 9.999999974752427e-07\n",
      "\n",
      "990\t 9.999999974752427e-07\n",
      "\n",
      "991\t 9.999999974752427e-07\n",
      "\n",
      "992\t 9.999999974752427e-07\n",
      "\n",
      "993\t 9.999999974752427e-07\n",
      "\n",
      "994\t 9.999999974752427e-07\n",
      "\n",
      "995\t 9.999999974752427e-07\n",
      "\n",
      "996\t 9.999999974752427e-07\n",
      "\n",
      "997\t 9.999999974752427e-07\n",
      "\n",
      "998\t 9.999999974752427e-07\n",
      "\n",
      "999\t 9.999999974752427e-07\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for e, lr in zip(rng,y):\n",
    "    print('{}\\t {}\\n'.format(e, lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4f281461d0>]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# clr3\n",
    "# warm up 10% of epoch: it can reduce fall in local min in inital steps.\n",
    "\n",
    "\n",
    "ep_num = 1000\n",
    "\n",
    "\n",
    "\n",
    "def clr3(epoch):\n",
    "    \n",
    "    \n",
    "    step_size = 25 # currently best for foot pp\n",
    "    max_lr = 0.01 # currently best for foot pp\n",
    "    base_lr = 1e-6 # 1e-6 1e-7\n",
    "\n",
    "    # warm up\n",
    "    lr_init_ep = 0\n",
    "    lr_ramp_ep = 100\n",
    "    lr_sus_ep  = 0\n",
    "    lr_decay   = 0.5\n",
    "\n",
    "    iterations = epoch\n",
    "    cycle = np.floor(1+iterations/(2*step_size))\n",
    "    x = np.abs(iterations/step_size - 2*cycle + 1)\n",
    "    lr = base_lr + (max_lr-base_lr)*np.maximum(0, (1-x))\n",
    "    \n",
    "    #todo: boost the lr at initial setps.\n",
    "#     initial_lr = lambda epoch: lr if epoch > step_size else max_lr\n",
    "#     initial_lr = lambda epoch: lr if epoch > step_size else boost_lr\n",
    "#     lr = initial_lr(epoch)\n",
    "    #todo: boost the lr at fist step_size.\n",
    "    \n",
    "    # warm up\n",
    "    if epoch < lr_ramp_ep:\n",
    "        lr = (max_lr - base_lr) / lr_ramp_ep * epoch + base_lr\n",
    "    \n",
    "    decay = ((epoch+1)/ep_num)\n",
    "    base_part = 1.001 #1.1\n",
    "#     print(decay)\n",
    "    return lr * (base_part-decay) * lr_decay # supressed the lr!\n",
    "\n",
    "\n",
    "rng = [i for i in range(ep_num)]\n",
    "y = [clr3(x) for x in rng]\n",
    "sns.set(style='darkgrid')\n",
    "fig, ax = plt.subplots(figsize=(20, 6))\n",
    "plt.plot(rng, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4999999999999966e-08 ~ 0.004459954505\n"
     ]
    }
   ],
   "source": [
    "print('{} ~ {}'.format(min(y), max(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4999999999999966e-08 ~ 0.004459954505 1e-2~1e-6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# \"\"\"\n",
    "# cosine_decay_restarts是cosine_decay的cycle版本。\n",
    "# first_decay_steps是指第一次完全下降的step數，\n",
    "# t_mul是指每一次循環的步數都將乘以t_mul倍，\n",
    "# m_mul指每一次循環重新開始時的初始lr是上一次循環初始值的m_mul倍。\n",
    "# alpha\n",
    "# \"\"\"\n",
    "\n",
    "# from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "\n",
    "\n",
    "# ep_num = 1000\n",
    "\n",
    "\n",
    "\n",
    "# def CosineDecayCLRWarmUp(epoch):\n",
    "    \n",
    "#     #step_size = 25 # currently best for foot pp\n",
    "#     max_lr = 1e-2 # currently best for foot pp\n",
    "#     base_lr = 1e-8# 1e-6 1e-7\n",
    "\n",
    "#     # warm up\n",
    "#     lr_init_ep = 0\n",
    "#     lr_ramp_ep = 100\n",
    "#     lr_sus_ep  = 0\n",
    "#     lr_decay   = 0.8\n",
    "\n",
    "\n",
    "#     initial_learning_rate = 1e-2\n",
    "#     first_decay_steps = 100\n",
    "\n",
    "\n",
    "#     lr_decayed_fn = (\n",
    "#       tf.keras.experimental.CosineDecayRestarts(\n",
    "#           initial_learning_rate,\n",
    "#           first_decay_steps,\n",
    "#           t_mul=1.0,\n",
    "#           m_mul=0.8,\n",
    "#           alpha = 0.000001,\n",
    "#           name=\"CCosineDecayRestarts\"))\n",
    "    \n",
    "#     # warm up\n",
    "#     if epoch < lr_ramp_ep:\n",
    "#         lr = (max_lr - base_lr) / lr_ramp_ep * epoch + base_lr    \n",
    "#     else:\n",
    "#         lr = lr_decayed_fn(epoch)\n",
    "#     return lr\n",
    "\n",
    "\n",
    "\n",
    "# rng = [i for i in range(ep_num)]\n",
    "# y = [CosineDecayCLRWarmUp(x) for x in rng]\n",
    "# sns.set(style='darkgrid')\n",
    "# fig, ax = plt.subplots(figsize=(20, 6))\n",
    "# # plt.ylim(.0000000000000001, .01)# for too large loss\n",
    "# ax.yaxis.set_major_formatter(FormatStrFormatter('%.12f'))# for too small loss\n",
    "# plt.plot(rng, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# \"\"\"\n",
    "# cosine_decay_restarts是cosine_decay的cycle版本。\n",
    "# first_decay_steps是指第一次完全下降的step數，\n",
    "# t_mul是指每一次循環的步數都將乘以t_mul倍，\n",
    "# m_mul指每一次循環重新開始時的初始lr是上一次循環初始值的m_mul倍。\n",
    "# alpha\n",
    "# \"\"\"\n",
    "\n",
    "# from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "\n",
    "\n",
    "# ep_num = 1000\n",
    "\n",
    "\n",
    "\n",
    "# def CosineDecayCLRWarmUpLSW(epoch):\n",
    "    \n",
    "#     #step_size = 25 # currently best for foot pp\n",
    "#     max_lr = 1e-3 # currently best for foot pp\n",
    "#     base_lr = 1e-6# 1e-6 1e-7\n",
    "\n",
    "#     # warm up\n",
    "#     lr_init_ep = 0\n",
    "#     lr_ramp_ep = 20\n",
    "#     lr_sus_ep  = 0\n",
    "#     lr_decay   = 0.8\n",
    "\n",
    "\n",
    "#     initial_learning_rate = 1e-3\n",
    "#     first_decay_steps = 50\n",
    "\n",
    "\n",
    "#     lr_decayed_fn = (\n",
    "#       tf.keras.experimental.CosineDecayRestarts(\n",
    "#           initial_learning_rate,\n",
    "#           first_decay_steps,\n",
    "#           t_mul=1.0,\n",
    "#           m_mul=0.8,\n",
    "#           alpha = 0.000001,\n",
    "#           name=\"CCosineDecayRestarts\"))\n",
    "    \n",
    "#     # warm up\n",
    "#     if epoch < lr_ramp_ep:\n",
    "#         lr = (max_lr - base_lr) / lr_ramp_ep * epoch + base_lr    \n",
    "#     else:\n",
    "#         lr = lr_decayed_fn(epoch-lr_ramp_ep)\n",
    "#     return lr\n",
    "\n",
    "\n",
    "\n",
    "# rng = [i for i in range(ep_num)]\n",
    "# y = [CosineDecayCLRWarmUpLSW(x) for x in rng]\n",
    "# sns.set(style='darkgrid')\n",
    "# fig, ax = plt.subplots(figsize=(20, 6))\n",
    "# # plt.ylim(.0000000000000001, .01)# for too large loss\n",
    "# ax.yaxis.set_major_formatter(FormatStrFormatter('%.12f'))# for too small loss\n",
    "# plt.plot(rng, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# \"\"\"\n",
    "# cosine_decay_restarts是cosine_decay的cycle版本。\n",
    "# first_decay_steps是指第一次完全下降的step數，\n",
    "# t_mul是指每一次循環的步數都將乘以t_mul倍，\n",
    "# m_mul指每一次循環重新開始時的初始lr是上一次循環初始值的m_mul倍。\n",
    "# alpha\n",
    "# \"\"\"\n",
    "\n",
    "# from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "\n",
    "\n",
    "# ep_num = 1000\n",
    "\n",
    "\n",
    "\n",
    "# def CosineDecayCLRWarmUpLSW_2(epoch):\n",
    "    \n",
    "#     #step_size = 25 # currently best for foot pp\n",
    "#     max_lr = 1e-2 # currently best for foot pp\n",
    "#     base_lr = 1e-6# 1e-6 1e-7\n",
    "\n",
    "#     # warm up\n",
    "#     lr_init_ep = 0\n",
    "#     lr_ramp_ep = 20\n",
    "#     lr_sus_ep  = 0\n",
    "#     #lr_decay   = 0.8\n",
    "\n",
    "\n",
    "#     initial_learning_rate = 1e-2\n",
    "#     first_decay_steps = 50\n",
    "\n",
    "\n",
    "#     lr_decayed_fn = (\n",
    "#       tf.keras.experimental.CosineDecayRestarts(\n",
    "#           initial_learning_rate,\n",
    "#           first_decay_steps,\n",
    "#           t_mul=1,\n",
    "#           m_mul=1,\n",
    "#           alpha = 0.000001,\n",
    "#           name=\"CCosineDecayRestarts\"))\n",
    "    \n",
    "#     # warm up\n",
    "#     if epoch < lr_ramp_ep:\n",
    "#         lr = (max_lr - base_lr) / lr_ramp_ep * epoch + base_lr    \n",
    "#     else:\n",
    "#         lr = lr_decayed_fn(epoch-lr_ramp_ep)\n",
    "#     return lr\n",
    "\n",
    "\n",
    "\n",
    "# rng = [i for i in range(ep_num)]\n",
    "# y = [CosineDecayCLRWarmUpLSW_2(x) for x in rng]\n",
    "# sns.set(style='darkgrid')\n",
    "# fig, ax = plt.subplots(figsize=(20, 6))\n",
    "# # plt.ylim(.0000000000000001, .01)# for too large loss\n",
    "# ax.yaxis.set_major_formatter(FormatStrFormatter('%.12f'))# for too small loss\n",
    "# plt.plot(rng, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4999999999999966e-08 ~ 0.004459954505\n"
     ]
    }
   ],
   "source": [
    "print('{} ~ {}'.format(min(y), max(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.8774175103430935e-08 ~ 0.0010000000474974513 1e-3 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_reduceonplateau = tf.keras.callbacks.ReduceLROnPlateau(monitor=monitor, factor=0.1, patience=5, verbose=1, mode='auto', min_delta=0.0001, cooldown=0, min_lr=1e-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback for printing the LR at the end of each epoch.\n",
    "class PrintLRtoe(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "#     def on_epoch_end(self, epoch, logs=None):\n",
    "#         print('\\n[{}] Learning rate for epoch {} is {}'.format(\n",
    "#             datetime.now().strftime(\"%Y%m%d-%H%M-%S\"), \n",
    "#             epoch + 1,\n",
    "#             self.model.optimizer.lr.numpy()))\n",
    "        print('\\n[{}] Learning rate for epoch {} is {}'.format(\n",
    "        datetime.now().strftime(\"%Y%m%d-%H%M-%S\"), \n",
    "        epoch + 1,\n",
    "        model_toe.optimizer._decayed_lr(tf.float32).numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback for printing the LR at the end of each epoch.\n",
    "class PrintLRheel(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "#         print('\\nLearning rate for epoch {} is {}'.format(epoch + 1,\n",
    "#                                               model_heel.optimizer.lr.numpy()))\n",
    "        print('\\n[{}] Learning rate for epoch {} is {}'.format(\n",
    "        datetime.now().strftime(\"%Y%m%d-%H%M-%S\"), \n",
    "        epoch + 1,\n",
    "        model_heel.optimizer._decayed_lr(tf.float32).numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output dir and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_log_dir(log_dir_name):\n",
    "    try:\n",
    "        os.makedirs(log_dir_name)\n",
    "    except OSError as e:\n",
    "        print(\"This log dir exist.\")\n",
    "        if e.errno != errno.EEXIST:\n",
    "            raise ValueError(\"we got problem.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor = 'val_loss' #'val_loss' 'val_accuracy' if use ed_loss it still the loss here.\n",
    "\n",
    "log_dir_name = datetime.now().strftime(\"%Y%m%d-%H%M%S\") \n",
    "\n",
    "# mk_log_dir(datetime.now().strftime(\"%Y%m%d-%H%M%S\") )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use once at the time\n",
    "mk_log_dir(log_dir_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'EfficientNetB0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_best_model_name\n",
    "\n",
    "# best_model_name = './' + model_name + '_bs-' + str(BATCH_SIZE) + '_s-' + str(img_height) + '_' + \"ep-{epoch:02d}-vloss-{val_loss:.2f}\" +'_best-weight.h5'\n",
    "# best_model_name = '{model_name}-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
    "#best_model_name = './' + model_name + '_bs' + str(BATCH_SIZE) + '_w' + str(img_width) + '_' + monitor + '_best.h5'\n",
    "# best_model_name = './Leaf_' + model_name + '_bs' + str(BATCH_SIZE) + '_w' + str(img_width) + '_' + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + '_best_' + monitor + '.h5'\n",
    "\n",
    "# best_model_name = './cop' + model_name + '_bs' + str(BATCH_SIZE) + '_w' + str(img_width) + '_' + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + '_best_' + monitor + '.h5'\n",
    "\n",
    "def get_best_model_name(th, K):\n",
    "    return './' + log_dir_name + '/' + th + '_K' + K + '_' + model_name + '_bs' + str(BATCH_SIZE) + '_w' + str(img_width) + '_best_' + monitor + '.h5'\n",
    "\n",
    "# th = 'toe'\n",
    "# # th = 'heel'\n",
    "\n",
    "# # print(get_best_model_name(th,K))\n",
    "\n",
    "# best_model_name = get_best_model_name(th, K)\n",
    "\n",
    "\n",
    "# best_model_save = tf.keras.callbacks.ModelCheckpoint(filepath=best_model_name, \n",
    "#                              save_best_only = True, \n",
    "#                              save_weights_only = False,\n",
    "#                              monitor = monitor, \n",
    "#                              mode = 'auto', verbose = 1)\n",
    "# print('best_model_name:', best_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = log_dir_name + \"/logs/toe/\"\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks = [\n",
    "# #     tensorboard_callback,\n",
    "#     best_model_save,\n",
    "#     tf.keras.callbacks.EarlyStopping(monitor=monitor, patience=20), #patience=step_size or ep_num\n",
    "# #     lr_reduceonplateau,\n",
    "#     tf.keras.callbacks.LearningRateScheduler(lrdump),#lrdump, decay or lrfn or lrfn2. clr\n",
    "#     PrintLRtoe()\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LookAheadAnnean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "radam = tfa.optimizers.RectifiedAdam(lr=1e-2,\n",
    "                                     total_steps=1000,\n",
    "                                     warmup_proportion=0.1,\n",
    "                                     min_lr=1e-5,\n",
    "                                    )\n",
    "ranger = tfa.optimizers.Lookahead(radam, sync_period=6, slow_step_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transfer learning from pre-trained weights\n",
    "def build_efn_model(outputnum, top_dropout_rate, drop_connect_rate):\n",
    "    base_model = tf.keras.applications.EfficientNetB0(include_top=False, weights=\"imagenet\", input_shape=(120,120,3),drop_connect_rate=drop_connect_rate) #{'imagenet', None}\n",
    "\n",
    "    # Freeze the pretrained weights\n",
    "    base_model.trainable = False\n",
    "    print(\"base_model.trainable : \", base_model.trainable)\n",
    "\n",
    "    # Rebuild top\n",
    "    gap2d = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "    BNL = tf.keras.layers.BatchNormalization()(gap2d) #tood: remove#\n",
    "    dropout = tf.keras.layers.Dropout(top_dropout_rate)(BNL)#tood: remove# J add dropout, for flood 0.2 is ok. for leaf 0.4 is better.\n",
    "    outputs = tf.keras.layers.Dense(outputnum)(dropout)# remove activation for regression output (to default, the linear), , activation = 'relu' no help\n",
    "\n",
    "    # Compile new model\n",
    "    model = tf.keras.Model(base_model.input, outputs, name=model_name)\n",
    "\n",
    "\n",
    "#     # unfreeze the top #fine_tune_at# layers while leaving BatchNorm layers frozen\n",
    "#     fine_tune_at = 20 #10 #241 #20\n",
    "#     print('[Note] Now create model fine tuneing at Top-{} layers!'.format(fine_tune_at))\n",
    "#     for layer in model_toe.layers[-fine_tune_at:]:\n",
    "#         if not isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "#             layer.trainable = True\n",
    "\n",
    "#     model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),#RMSprop , Adam, SGD Adadelta(learning_rate=0.001), if set lr_callback the learning_rate=0.001 will not effeced.\n",
    "    model.compile(optimizer = ranger,#RMSprop , Adam, SGD Adadelta(learning_rate=0.001), if set lr_callback the learning_rate=0.001 will not effeced.\n",
    "                    loss=ed_metric_2d_mean)\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Supervised pre-training 減少每次fold都要重新train的時間\n",
    "# 只先改toe\"\"\"\n",
    "\n",
    "# # Transfer learning from pre-trained weights\n",
    "# def load_pretrained_efn_model():\n",
    "#     pre_model_toe_name = \"20210224-200728/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\"\n",
    "#     model = tf.keras.models.load_model(pre_model_toe_name,compile=False)\n",
    "\n",
    "#     # Freeze the pretrained weights\n",
    "#     model.trainable = False\n",
    "# #     print(\"base_model.trainable : \", base_model.trainable)\n",
    "\n",
    "# #     # Rebuild top\n",
    "# #     gap2d = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "# #     BNL = tf.keras.layers.BatchNormalization()(gap2d) #tood: remove#\n",
    "# #     dropout = tf.keras.layers.Dropout(top_dropout_rate)(BNL)#tood: remove# J add dropout, for flood 0.2 is ok. for leaf 0.4 is better.\n",
    "# #     outputs = tf.keras.layers.Dense(outputnum)(dropout)# remove activation for regression output (to default, the linear), , activation = 'relu' no help\n",
    "\n",
    "# #     # Compile new model\n",
    "# #     model = tf.keras.Model(base_model.input, outputs, name=model_name)\n",
    "\n",
    "\n",
    "# #     # unfreeze the top #fine_tune_at# layers while leaving BatchNorm layers frozen\n",
    "#     fine_tune_at = 4 #10 #241 #20\n",
    "#     print('[Note] Now create model fine tuneing at Top-{} layers!'.format(fine_tune_at))\n",
    "#     for layer in model.layers[-fine_tune_at:]:\n",
    "#         if not isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "#             print('layer trainable +1', layer.name)\n",
    "#             layer.trainable = True\n",
    "\n",
    "#     model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),#RMSprop , Adam, SGD Adadelta(learning_rate=0.001), if set lr_callback the learning_rate=0.001 will not effeced.\n",
    "#                     loss=ed_metric_2d_mean)\n",
    "\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unfreeze_model(model):\n",
    "#\n",
    "#'block7a_expand_conv'20 'block6c_expand_conv'50 'block6a_expand_conv'79 'block5b_expand_conv'109 'block4a_expand_conv' 166  block3a_expand_conv 195\n",
    "#\n",
    "    model.trainable = True\n",
    "    set_trainable = False\n",
    "    for layer in model.layers:\n",
    "        if layer.name == 'block3a_expand_conv': \n",
    "            set_trainable = True\n",
    "        if set_trainable:\n",
    "            layer.trainable = True\n",
    "        else:\n",
    "            layer.trainable = False\n",
    "\n",
    "#     model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),#RMSprop , Adam, SGD Adadelta(learning_rate=0.001), if set lr_callback the learning_rate=0.001 will not effeced.\n",
    "    model.compile(optimizer = ranger,#RMSprop , Adam, SGD Adadelta(learning_rate=0.001), if set lr_callback the learning_rate=0.001 will not effeced.\n",
    "                    loss=ed_metric_2d_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# top_dropout_rate = 0.8 #less dp rate, say 0.1, train_loss will lower than val_loss\n",
    "# drop_connect_rate = 0.9 #for efnet This parameter serves as a toggle for extra regularization in finetuning, but does not affect loaded weights.\n",
    "# outputnum = 2\n",
    "# with strategy.scope():\n",
    "#     model_toe = build_efn_model(outputnum, top_dropout_rate, drop_connect_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(model_toe.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tt = 0\n",
    "# nt = 0\n",
    "# for layer in model_toe.layers:\n",
    "#     if layer.trainable:\n",
    "#         tt +=1\n",
    "#         print(f'{layer.name}')\n",
    "#     else:\n",
    "#         nt +=1\n",
    "# print(f'tt: {tt}, nt:{nt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_model_trainOrNot_layers(model, printlayers=False):\n",
    "    tt = 0\n",
    "    nt = 0\n",
    "    for layer in model.layers:\n",
    "        if layer.trainable:\n",
    "            tt +=1\n",
    "            if printlayers:\n",
    "                print(f'{layer.name}')\n",
    "        else:\n",
    "            nt +=1\n",
    "    print('\\n*********************************** Start fine tune ***********************************')\n",
    "    print(f'tt: {tt}, nt:{nt}, total layers:{tt+nt}')\n",
    "    print('*********************************** Start fine tune ***********************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_model_trainOrNot_layers(model_toe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model_toe.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # fit the model on all data\n",
    "# history_toe = model_toe.fit(train_ds_pre_toe_s, \n",
    "#                       verbose=1, \n",
    "#                       epochs=ep_num_transf, \n",
    "#                       validation_data=valid_ds_pre_toe_s, \n",
    "#                       callbacks=callbacks)#, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold Training\n",
    "\n",
    "2021-02-23 v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toe K-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " K =  0 \n",
      "\n",
      "Check cache in memory:Y True\n",
      "Check keypoints process01: Tensor(\"args_1:0\", shape=(2,), dtype=int64) (2,) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Check keypoints process02: Tensor(\"Reshape:0\", shape=(1, 2), dtype=int64) (1, 2) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Check keypoints process03: Tensor(\"PyFunc:1\", dtype=float32, device=/job:localhost/replica:0/task:0)\n",
      "Check keypoints process04: Tensor(\"Reshape_1:0\", shape=(2,), dtype=float32)\n",
      "Check augment :Y True\n",
      "Check ds cache[True] and augment[True]\n",
      "Check cache in memory:Y True\n",
      "Check augment :N False\n",
      "Check ds cache[True] and augment[False]\n",
      "best_model_name: ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "base_model.trainable :  False\n",
      "Epoch 1/500\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Iterator.get_next_as_optional()` instead.\n",
      "17/17 [==============================] - ETA: 0s - loss: 81.9319\n",
      "Epoch 00001: val_loss improved from inf to 79.93254, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 15s 897ms/step - loss: 81.9319 - val_loss: 79.9325\n",
      "Epoch 2/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 81.5246\n",
      "Epoch 00002: val_loss improved from 79.93254 to 79.86186, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 47ms/step - loss: 81.5415 - val_loss: 79.8619\n",
      "Epoch 3/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 81.3240\n",
      "Epoch 00003: val_loss improved from 79.86186 to 79.65905, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 11s 651ms/step - loss: 81.3240 - val_loss: 79.6590\n",
      "Epoch 4/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 81.4072\n",
      "Epoch 00004: val_loss improved from 79.65905 to 79.34032, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 48ms/step - loss: 81.3994 - val_loss: 79.3403\n",
      "Epoch 5/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 80.5392\n",
      "Epoch 00005: val_loss improved from 79.34032 to 78.75267, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 47ms/step - loss: 80.5995 - val_loss: 78.7527\n",
      "Epoch 6/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 80.1195\n",
      "Epoch 00006: val_loss improved from 78.75267 to 77.84232, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 47ms/step - loss: 80.0523 - val_loss: 77.8423\n",
      "Epoch 7/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 79.0366\n",
      "Epoch 00007: val_loss improved from 77.84232 to 75.97169, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 47ms/step - loss: 78.9857 - val_loss: 75.9717\n",
      "Epoch 8/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 77.2420\n",
      "Epoch 00008: val_loss improved from 75.97169 to 73.65489, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 48ms/step - loss: 77.0426 - val_loss: 73.6549\n",
      "Epoch 9/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 74.1145\n",
      "Epoch 00009: val_loss improved from 73.65489 to 70.04859, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 49ms/step - loss: 73.9701 - val_loss: 70.0486\n",
      "Epoch 10/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 69.5057\n",
      "Epoch 00010: val_loss improved from 70.04859 to 64.68195, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 48ms/step - loss: 69.3201 - val_loss: 64.6820\n",
      "Epoch 11/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 62.7148\n",
      "Epoch 00011: val_loss improved from 64.68195 to 56.80709, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 48ms/step - loss: 62.5890 - val_loss: 56.8071\n",
      "Epoch 12/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 52.9738\n",
      "Epoch 00012: val_loss improved from 56.80709 to 45.95154, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 47ms/step - loss: 52.5196 - val_loss: 45.9515\n",
      "Epoch 13/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 39.3388\n",
      "Epoch 00013: val_loss improved from 45.95154 to 26.37387, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 48ms/step - loss: 38.7743 - val_loss: 26.3739\n",
      "Epoch 14/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 24.9553\n",
      "Epoch 00014: val_loss improved from 26.37387 to 15.44598, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 68ms/step - loss: 24.7556 - val_loss: 15.4460\n",
      "Epoch 15/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 19.1150\n",
      "Epoch 00015: val_loss improved from 15.44598 to 13.32219, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 48ms/step - loss: 19.0119 - val_loss: 13.3222\n",
      "Epoch 16/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 18.8078\n",
      "Epoch 00016: val_loss improved from 13.32219 to 13.05455, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 48ms/step - loss: 18.7268 - val_loss: 13.0546\n",
      "Epoch 17/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 18.2426\n",
      "Epoch 00017: val_loss improved from 13.05455 to 12.80770, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 48ms/step - loss: 18.4434 - val_loss: 12.8077\n",
      "Epoch 18/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 17.9707\n",
      "Epoch 00018: val_loss improved from 12.80770 to 12.64440, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 47ms/step - loss: 17.9813 - val_loss: 12.6444\n",
      "Epoch 19/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 18.5050\n",
      "Epoch 00019: val_loss improved from 12.64440 to 12.45712, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 47ms/step - loss: 18.5176 - val_loss: 12.4571\n",
      "Epoch 20/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 17.2644\n",
      "Epoch 00020: val_loss improved from 12.45712 to 12.27038, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 47ms/step - loss: 17.2049 - val_loss: 12.2704\n",
      "Epoch 21/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 17.4473\n",
      "Epoch 00021: val_loss improved from 12.27038 to 12.07117, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 49ms/step - loss: 17.5528 - val_loss: 12.0712\n",
      "Epoch 22/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 17.2534\n",
      "Epoch 00022: val_loss improved from 12.07117 to 11.92345, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 48ms/step - loss: 17.2326 - val_loss: 11.9234\n",
      "Epoch 23/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 16.9156\n",
      "Epoch 00023: val_loss improved from 11.92345 to 11.76765, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 47ms/step - loss: 16.8600 - val_loss: 11.7676\n",
      "Epoch 24/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 16.4570\n",
      "Epoch 00024: val_loss improved from 11.76765 to 11.71602, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 48ms/step - loss: 16.5278 - val_loss: 11.7160\n",
      "Epoch 25/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 16.7485\n",
      "Epoch 00025: val_loss improved from 11.71602 to 11.60137, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 47ms/step - loss: 16.8695 - val_loss: 11.6014\n",
      "Epoch 26/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 17.0341\n",
      "Epoch 00026: val_loss improved from 11.60137 to 11.46592, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 46ms/step - loss: 17.0509 - val_loss: 11.4659\n",
      "Epoch 27/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 16.0775\n",
      "Epoch 00027: val_loss improved from 11.46592 to 11.36707, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 48ms/step - loss: 16.1562 - val_loss: 11.3671\n",
      "Epoch 28/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 16.4064\n",
      "Epoch 00028: val_loss improved from 11.36707 to 11.29907, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 47ms/step - loss: 16.4258 - val_loss: 11.2991\n",
      "Epoch 29/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 16.0254\n",
      "Epoch 00029: val_loss improved from 11.29907 to 11.24895, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 50ms/step - loss: 15.9476 - val_loss: 11.2490\n",
      "Epoch 30/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 16.3234\n",
      "Epoch 00030: val_loss improved from 11.24895 to 11.20112, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 47ms/step - loss: 16.2821 - val_loss: 11.2011\n",
      "Epoch 31/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 16.3636\n",
      "Epoch 00031: val_loss improved from 11.20112 to 11.12724, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 49ms/step - loss: 16.2910 - val_loss: 11.1272\n",
      "Epoch 32/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 16.0724\n",
      "Epoch 00032: val_loss improved from 11.12724 to 11.09243, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 47ms/step - loss: 16.1150 - val_loss: 11.0924\n",
      "Epoch 33/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 16.0598\n",
      "Epoch 00033: val_loss improved from 11.09243 to 11.07533, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 49ms/step - loss: 16.0668 - val_loss: 11.0753\n",
      "Epoch 34/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 15.8347\n",
      "Epoch 00034: val_loss improved from 11.07533 to 11.03431, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 48ms/step - loss: 15.8706 - val_loss: 11.0343\n",
      "Epoch 35/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 16.1140\n",
      "Epoch 00035: val_loss improved from 11.03431 to 10.99318, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 49ms/step - loss: 16.1056 - val_loss: 10.9932\n",
      "Epoch 36/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 15.4551\n",
      "Epoch 00036: val_loss improved from 10.99318 to 10.91244, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 47ms/step - loss: 15.5163 - val_loss: 10.9124\n",
      "Epoch 37/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 16.0375\n",
      "Epoch 00037: val_loss improved from 10.91244 to 10.87056, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 48ms/step - loss: 15.9730 - val_loss: 10.8706\n",
      "Epoch 38/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 15.6670\n",
      "Epoch 00038: val_loss improved from 10.87056 to 10.81368, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 47ms/step - loss: 15.6437 - val_loss: 10.8137\n",
      "Epoch 39/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 15.9538\n",
      "Epoch 00039: val_loss improved from 10.81368 to 10.77926, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 47ms/step - loss: 15.9466 - val_loss: 10.7793\n",
      "Epoch 40/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 15.6830\n",
      "Epoch 00040: val_loss improved from 10.77926 to 10.77237, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 47ms/step - loss: 15.7110 - val_loss: 10.7724\n",
      "Epoch 41/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 15.5774\n",
      "Epoch 00041: val_loss did not improve from 10.77237\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 15.6101 - val_loss: 10.7744\n",
      "Epoch 42/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 15.5209\n",
      "Epoch 00042: val_loss improved from 10.77237 to 10.74601, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 47ms/step - loss: 15.5902 - val_loss: 10.7460\n",
      "Epoch 43/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 15.8692\n",
      "Epoch 00043: val_loss did not improve from 10.74601\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 15.9155 - val_loss: 10.8116\n",
      "Epoch 44/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 15.8026\n",
      "Epoch 00044: val_loss did not improve from 10.74601\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 15.8271 - val_loss: 10.8176\n",
      "Epoch 45/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 15.3144\n",
      "Epoch 00045: val_loss did not improve from 10.74601\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 15.2590 - val_loss: 10.7994\n",
      "Epoch 46/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 15.5586\n",
      "Epoch 00046: val_loss improved from 10.74601 to 10.72332, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 48ms/step - loss: 15.5748 - val_loss: 10.7233\n",
      "Epoch 47/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 15.6038\n",
      "Epoch 00047: val_loss improved from 10.72332 to 10.67345, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 48ms/step - loss: 15.6489 - val_loss: 10.6735\n",
      "Epoch 48/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 15.1599\n",
      "Epoch 00048: val_loss improved from 10.67345 to 10.65440, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 47ms/step - loss: 15.1659 - val_loss: 10.6544\n",
      "Epoch 49/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 15.5037\n",
      "Epoch 00049: val_loss did not improve from 10.65440\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 15.4542 - val_loss: 10.6556\n",
      "Epoch 50/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 15.3739\n",
      "Epoch 00050: val_loss improved from 10.65440 to 10.63249, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 46ms/step - loss: 15.3158 - val_loss: 10.6325\n",
      "Epoch 51/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 15.3791\n",
      "Epoch 00051: val_loss improved from 10.63249 to 10.62589, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 48ms/step - loss: 15.3444 - val_loss: 10.6259\n",
      "Epoch 52/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 15.3116\n",
      "Epoch 00052: val_loss improved from 10.62589 to 10.61580, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 47ms/step - loss: 15.3394 - val_loss: 10.6158\n",
      "Epoch 53/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 15.7351\n",
      "Epoch 00053: val_loss improved from 10.61580 to 10.59853, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 46ms/step - loss: 15.6447 - val_loss: 10.5985\n",
      "Epoch 54/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 15.6538\n",
      "Epoch 00054: val_loss improved from 10.59853 to 10.58981, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 48ms/step - loss: 15.6225 - val_loss: 10.5898\n",
      "Epoch 55/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 15.0093\n",
      "Epoch 00055: val_loss improved from 10.58981 to 10.58466, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 47ms/step - loss: 15.0582 - val_loss: 10.5847\n",
      "Epoch 56/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 15.4775\n",
      "Epoch 00056: val_loss improved from 10.58466 to 10.58423, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 47ms/step - loss: 15.5491 - val_loss: 10.5842\n",
      "Epoch 57/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 15.2158\n",
      "Epoch 00057: val_loss did not improve from 10.58423\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 15.1955 - val_loss: 10.5915\n",
      "Epoch 58/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 15.1584\n",
      "Epoch 00058: val_loss did not improve from 10.58423\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 15.2421 - val_loss: 10.5851\n",
      "Epoch 59/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 15.3482\n",
      "Epoch 00059: val_loss improved from 10.58423 to 10.58227, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 47ms/step - loss: 15.2901 - val_loss: 10.5823\n",
      "Epoch 60/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 15.2579\n",
      "Epoch 00060: val_loss did not improve from 10.58227\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 15.1922 - val_loss: 10.5824\n",
      "Epoch 61/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 15.1282\n",
      "Epoch 00061: val_loss did not improve from 10.58227\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 15.0887 - val_loss: 10.5854\n",
      "Epoch 62/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 15.5141\n",
      "Epoch 00062: val_loss improved from 10.58227 to 10.58125, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 48ms/step - loss: 15.5426 - val_loss: 10.5812\n",
      "Epoch 63/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 15.6081\n",
      "Epoch 00063: val_loss improved from 10.58125 to 10.57920, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 48ms/step - loss: 15.6267 - val_loss: 10.5792\n",
      "Epoch 64/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 15.6870\n",
      "Epoch 00064: val_loss improved from 10.57920 to 10.57749, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 47ms/step - loss: 15.6270 - val_loss: 10.5775\n",
      "Epoch 65/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 15.5880\n",
      "Epoch 00065: val_loss improved from 10.57749 to 10.57578, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 47ms/step - loss: 15.7001 - val_loss: 10.5758\n",
      "Epoch 66/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 15.6233\n",
      "Epoch 00066: val_loss did not improve from 10.57578\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 15.5171 - val_loss: 10.5758\n",
      "Epoch 67/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 15.1700\n",
      "Epoch 00067: val_loss did not improve from 10.57578\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 15.2139 - val_loss: 10.5773\n",
      "Epoch 68/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 15.6504\n",
      "Epoch 00068: val_loss did not improve from 10.57578\n",
      "17/17 [==============================] - 1s 88ms/step - loss: 15.5820 - val_loss: 10.5813\n",
      "Epoch 69/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 15.3030\n",
      "Epoch 00069: val_loss did not improve from 10.57578\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 15.1116 - val_loss: 10.5765\n",
      "Epoch 70/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 14.9270\n",
      "Epoch 00070: val_loss improved from 10.57578 to 10.57254, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 47ms/step - loss: 14.9681 - val_loss: 10.5725\n",
      "Epoch 71/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 15.1920\n",
      "Epoch 00071: val_loss did not improve from 10.57254\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 15.2038 - val_loss: 10.5731\n",
      "Epoch 72/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 15.2282\n",
      "Epoch 00072: val_loss did not improve from 10.57254\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 15.3212 - val_loss: 10.5728\n",
      "Epoch 73/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 15.5910\n",
      "Epoch 00073: val_loss did not improve from 10.57254\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 15.6508 - val_loss: 10.5748\n",
      "Epoch 74/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 15.2442\n",
      "Epoch 00074: val_loss did not improve from 10.57254\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 15.4437 - val_loss: 10.5757\n",
      "Epoch 75/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 15.1422\n",
      "Epoch 00075: val_loss did not improve from 10.57254\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 15.1853 - val_loss: 10.5760\n",
      "\n",
      "*********************************** Start fine tune ***********************************\n",
      "tt: 195, nt:46, total layers:241\n",
      "*********************************** Start fine tune ***********************************\n",
      "Epoch 1/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 16.3329\n",
      "Epoch 00001: val_loss did not improve from 10.57254\n",
      "17/17 [==============================] - 72s 4s/step - loss: 16.3329 - val_loss: 15.3292\n",
      "Epoch 2/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 15.7297\n",
      "Epoch 00002: val_loss did not improve from 10.57254\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 15.7297 - val_loss: 15.1731\n",
      "Epoch 3/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 15.5190\n",
      "Epoch 00003: val_loss did not improve from 10.57254\n",
      "17/17 [==============================] - 33s 2s/step - loss: 15.5190 - val_loss: 15.0504\n",
      "Epoch 4/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 15.7164\n",
      "Epoch 00004: val_loss did not improve from 10.57254\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 15.7164 - val_loss: 14.9699\n",
      "Epoch 5/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 15.6488\n",
      "Epoch 00005: val_loss did not improve from 10.57254\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 15.6488 - val_loss: 14.8996\n",
      "Epoch 6/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 15.4142\n",
      "Epoch 00006: val_loss did not improve from 10.57254\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 15.4142 - val_loss: 14.8324\n",
      "Epoch 7/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 15.1102\n",
      "Epoch 00007: val_loss did not improve from 10.57254\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 15.1102 - val_loss: 14.7900\n",
      "Epoch 8/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 15.4581\n",
      "Epoch 00008: val_loss did not improve from 10.57254\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 15.4581 - val_loss: 14.7490\n",
      "Epoch 9/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 15.2917\n",
      "Epoch 00009: val_loss did not improve from 10.57254\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 15.2917 - val_loss: 14.7268\n",
      "Epoch 10/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 15.1980\n",
      "Epoch 00010: val_loss did not improve from 10.57254\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 15.1980 - val_loss: 14.7010\n",
      "Epoch 11/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.9251\n",
      "Epoch 00011: val_loss did not improve from 10.57254\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 14.9251 - val_loss: 14.6822\n",
      "Epoch 12/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 15.3173\n",
      "Epoch 00012: val_loss did not improve from 10.57254\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 15.3173 - val_loss: 14.6733\n",
      "Epoch 13/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.9406\n",
      "Epoch 00013: val_loss did not improve from 10.57254\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 14.9406 - val_loss: 14.6581\n",
      "Epoch 14/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.6456\n",
      "Epoch 00014: val_loss did not improve from 10.57254\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 14.6456 - val_loss: 14.6295\n",
      "Epoch 15/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 15.1749\n",
      "Epoch 00015: val_loss did not improve from 10.57254\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 15.1749 - val_loss: 14.6128\n",
      "Epoch 16/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.9010\n",
      "Epoch 00016: val_loss did not improve from 10.57254\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 14.9010 - val_loss: 14.5955\n",
      "Epoch 17/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 15.2714\n",
      "Epoch 00017: val_loss did not improve from 10.57254\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 15.2714 - val_loss: 14.5726\n",
      "Epoch 18/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 15.0847\n",
      "Epoch 00018: val_loss did not improve from 10.57254\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 15.0847 - val_loss: 14.5306\n",
      "Epoch 19/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.4776\n",
      "Epoch 00019: val_loss did not improve from 10.57254\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 14.4776 - val_loss: 14.5014\n",
      "Epoch 20/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.8245\n",
      "Epoch 00020: val_loss did not improve from 10.57254\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 14.8245 - val_loss: 14.4589\n",
      "Epoch 21/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.7865\n",
      "Epoch 00021: val_loss did not improve from 10.57254\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 14.7865 - val_loss: 14.3873\n",
      "Epoch 22/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.5551\n",
      "Epoch 00022: val_loss did not improve from 10.57254\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 14.5551 - val_loss: 14.3066\n",
      "Epoch 23/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.3821\n",
      "Epoch 00023: val_loss did not improve from 10.57254\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 14.3821 - val_loss: 14.2247\n",
      "Epoch 24/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.2499\n",
      "Epoch 00024: val_loss did not improve from 10.57254\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 14.2499 - val_loss: 14.1375\n",
      "Epoch 25/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.7119\n",
      "Epoch 00025: val_loss did not improve from 10.57254\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 14.7119 - val_loss: 14.0231\n",
      "Epoch 26/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.2174\n",
      "Epoch 00026: val_loss did not improve from 10.57254\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 14.2174 - val_loss: 13.8903\n",
      "Epoch 27/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.6092\n",
      "Epoch 00027: val_loss did not improve from 10.57254\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 14.6092 - val_loss: 13.7423\n",
      "Epoch 28/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.4530\n",
      "Epoch 00028: val_loss did not improve from 10.57254\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 14.4530 - val_loss: 13.5509\n",
      "Epoch 29/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.6183\n",
      "Epoch 00029: val_loss did not improve from 10.57254\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 14.6183 - val_loss: 13.3243\n",
      "Epoch 30/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.3938\n",
      "Epoch 00030: val_loss did not improve from 10.57254\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 14.3938 - val_loss: 13.0206\n",
      "Epoch 31/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.6355\n",
      "Epoch 00031: val_loss did not improve from 10.57254\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 14.6355 - val_loss: 12.6925\n",
      "Epoch 32/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.8916\n",
      "Epoch 00032: val_loss did not improve from 10.57254\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 14.8916 - val_loss: 12.3023\n",
      "Epoch 33/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.5717\n",
      "Epoch 00033: val_loss did not improve from 10.57254\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 14.5717 - val_loss: 11.9034\n",
      "Epoch 34/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.4194\n",
      "Epoch 00034: val_loss did not improve from 10.57254\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 14.4194 - val_loss: 11.4883\n",
      "Epoch 35/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.2708\n",
      "Epoch 00035: val_loss did not improve from 10.57254\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 14.2708 - val_loss: 11.0613\n",
      "Epoch 36/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.3848\n",
      "Epoch 00036: val_loss did not improve from 10.57254\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 14.3848 - val_loss: 10.6568\n",
      "Epoch 37/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.2626\n",
      "Epoch 00037: val_loss improved from 10.57254 to 10.31719, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 154ms/step - loss: 14.2626 - val_loss: 10.3172\n",
      "Epoch 38/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.6519\n",
      "Epoch 00038: val_loss improved from 10.31719 to 10.01165, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 151ms/step - loss: 14.6519 - val_loss: 10.0116\n",
      "Epoch 39/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.4023\n",
      "Epoch 00039: val_loss improved from 10.01165 to 9.76060, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 151ms/step - loss: 14.4023 - val_loss: 9.7606\n",
      "Epoch 40/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.8030\n",
      "Epoch 00040: val_loss improved from 9.76060 to 9.55077, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 153ms/step - loss: 14.8030 - val_loss: 9.5508\n",
      "Epoch 41/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.1606\n",
      "Epoch 00041: val_loss improved from 9.55077 to 9.42509, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 152ms/step - loss: 14.1606 - val_loss: 9.4251\n",
      "Epoch 42/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.2708\n",
      "Epoch 00042: val_loss improved from 9.42509 to 9.32094, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 149ms/step - loss: 14.2708 - val_loss: 9.3209\n",
      "Epoch 43/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.1968\n",
      "Epoch 00043: val_loss improved from 9.32094 to 9.24220, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 151ms/step - loss: 14.1968 - val_loss: 9.2422\n",
      "Epoch 44/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.1830\n",
      "Epoch 00044: val_loss improved from 9.24220 to 9.20254, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 150ms/step - loss: 14.1830 - val_loss: 9.2025\n",
      "Epoch 45/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.3130\n",
      "Epoch 00045: val_loss improved from 9.20254 to 9.16914, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 152ms/step - loss: 14.3130 - val_loss: 9.1691\n",
      "Epoch 46/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.9485\n",
      "Epoch 00046: val_loss improved from 9.16914 to 9.13519, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 150ms/step - loss: 13.9485 - val_loss: 9.1352\n",
      "Epoch 47/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.0953\n",
      "Epoch 00047: val_loss improved from 9.13519 to 9.10621, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 150ms/step - loss: 14.0953 - val_loss: 9.1062\n",
      "Epoch 48/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.4851\n",
      "Epoch 00048: val_loss improved from 9.10621 to 9.07743, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 152ms/step - loss: 14.4851 - val_loss: 9.0774\n",
      "Epoch 49/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.3835\n",
      "Epoch 00049: val_loss improved from 9.07743 to 9.03902, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 150ms/step - loss: 14.3835 - val_loss: 9.0390\n",
      "Epoch 50/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.2859\n",
      "Epoch 00050: val_loss improved from 9.03902 to 9.00954, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 150ms/step - loss: 14.2859 - val_loss: 9.0095\n",
      "Epoch 51/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.1295\n",
      "Epoch 00051: val_loss improved from 9.00954 to 8.99015, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 152ms/step - loss: 14.1295 - val_loss: 8.9901\n",
      "Epoch 52/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.2334\n",
      "Epoch 00052: val_loss improved from 8.99015 to 8.98403, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 152ms/step - loss: 14.2334 - val_loss: 8.9840\n",
      "Epoch 53/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.6579\n",
      "Epoch 00053: val_loss did not improve from 8.98403\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 13.6579 - val_loss: 8.9872\n",
      "Epoch 54/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.1694\n",
      "Epoch 00054: val_loss improved from 8.98403 to 8.97672, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 155ms/step - loss: 14.1694 - val_loss: 8.9767\n",
      "Epoch 55/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.7648\n",
      "Epoch 00055: val_loss improved from 8.97672 to 8.94777, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 154ms/step - loss: 13.7648 - val_loss: 8.9478\n",
      "Epoch 56/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.7392\n",
      "Epoch 00056: val_loss improved from 8.94777 to 8.91935, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 148ms/step - loss: 13.7392 - val_loss: 8.9194\n",
      "Epoch 57/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.1436\n",
      "Epoch 00057: val_loss did not improve from 8.91935\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 14.1436 - val_loss: 8.9236\n",
      "Epoch 58/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.8429\n",
      "Epoch 00058: val_loss improved from 8.91935 to 8.88774, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 150ms/step - loss: 13.8429 - val_loss: 8.8877\n",
      "Epoch 59/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.3204\n",
      "Epoch 00059: val_loss improved from 8.88774 to 8.85694, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 151ms/step - loss: 14.3204 - val_loss: 8.8569\n",
      "Epoch 60/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.9051\n",
      "Epoch 00060: val_loss improved from 8.85694 to 8.84505, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 151ms/step - loss: 13.9051 - val_loss: 8.8451\n",
      "Epoch 61/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.7549\n",
      "Epoch 00061: val_loss improved from 8.84505 to 8.82343, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 155ms/step - loss: 13.7549 - val_loss: 8.8234\n",
      "Epoch 62/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.6878\n",
      "Epoch 00062: val_loss improved from 8.82343 to 8.80065, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 149ms/step - loss: 13.6878 - val_loss: 8.8007\n",
      "Epoch 63/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.0110\n",
      "Epoch 00063: val_loss improved from 8.80065 to 8.77604, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 153ms/step - loss: 14.0110 - val_loss: 8.7760\n",
      "Epoch 64/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.9704\n",
      "Epoch 00064: val_loss improved from 8.77604 to 8.73684, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 154ms/step - loss: 13.9704 - val_loss: 8.7368\n",
      "Epoch 65/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.7073\n",
      "Epoch 00065: val_loss improved from 8.73684 to 8.70850, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 168ms/step - loss: 13.7073 - val_loss: 8.7085\n",
      "Epoch 66/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.9753\n",
      "Epoch 00066: val_loss improved from 8.70850 to 8.69989, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 148ms/step - loss: 13.9753 - val_loss: 8.6999\n",
      "Epoch 67/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.8064\n",
      "Epoch 00067: val_loss did not improve from 8.69989\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 13.8064 - val_loss: 8.7032\n",
      "Epoch 68/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.8485\n",
      "Epoch 00068: val_loss did not improve from 8.69989\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 13.8485 - val_loss: 8.7037\n",
      "Epoch 69/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.5050\n",
      "Epoch 00069: val_loss improved from 8.69989 to 8.69537, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 153ms/step - loss: 13.5050 - val_loss: 8.6954\n",
      "Epoch 70/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.6717\n",
      "Epoch 00070: val_loss did not improve from 8.69537\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 13.6717 - val_loss: 8.6994\n",
      "Epoch 71/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.1451\n",
      "Epoch 00071: val_loss did not improve from 8.69537\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 14.1451 - val_loss: 8.6961\n",
      "Epoch 72/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.7289\n",
      "Epoch 00072: val_loss improved from 8.69537 to 8.69049, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 150ms/step - loss: 13.7289 - val_loss: 8.6905\n",
      "Epoch 73/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.2126\n",
      "Epoch 00073: val_loss improved from 8.69049 to 8.67008, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 152ms/step - loss: 14.2126 - val_loss: 8.6701\n",
      "Epoch 74/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.7283\n",
      "Epoch 00074: val_loss improved from 8.67008 to 8.64805, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 149ms/step - loss: 13.7283 - val_loss: 8.6480\n",
      "Epoch 75/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.6955\n",
      "Epoch 00075: val_loss improved from 8.64805 to 8.61126, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 151ms/step - loss: 13.6955 - val_loss: 8.6113\n",
      "Epoch 76/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.9272\n",
      "Epoch 00076: val_loss improved from 8.61126 to 8.56880, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 151ms/step - loss: 13.9272 - val_loss: 8.5688\n",
      "Epoch 77/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.6982\n",
      "Epoch 00077: val_loss did not improve from 8.56880\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 13.6982 - val_loss: 8.5824\n",
      "Epoch 78/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.9143\n",
      "Epoch 00078: val_loss did not improve from 8.56880\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 13.9143 - val_loss: 8.6091\n",
      "Epoch 79/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.5550\n",
      "Epoch 00079: val_loss did not improve from 8.56880\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 13.5550 - val_loss: 8.6146\n",
      "Epoch 80/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.6362\n",
      "Epoch 00080: val_loss did not improve from 8.56880\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 13.6362 - val_loss: 8.5836\n",
      "Epoch 81/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.6143\n",
      "Epoch 00081: val_loss improved from 8.56880 to 8.55916, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 152ms/step - loss: 13.6143 - val_loss: 8.5592\n",
      "Epoch 82/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.6252\n",
      "Epoch 00082: val_loss did not improve from 8.55916\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 13.6252 - val_loss: 8.5622\n",
      "Epoch 83/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.9006\n",
      "Epoch 00083: val_loss improved from 8.55916 to 8.53646, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 153ms/step - loss: 13.9006 - val_loss: 8.5365\n",
      "Epoch 84/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.4089\n",
      "Epoch 00084: val_loss improved from 8.53646 to 8.51620, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 151ms/step - loss: 13.4089 - val_loss: 8.5162\n",
      "Epoch 85/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.9861\n",
      "Epoch 00085: val_loss improved from 8.51620 to 8.50119, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 148ms/step - loss: 13.9861 - val_loss: 8.5012\n",
      "Epoch 86/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.9764\n",
      "Epoch 00086: val_loss improved from 8.50119 to 8.47066, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 152ms/step - loss: 13.9764 - val_loss: 8.4707\n",
      "Epoch 87/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.6988\n",
      "Epoch 00087: val_loss did not improve from 8.47066\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 13.6988 - val_loss: 8.4762\n",
      "Epoch 88/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.8056\n",
      "Epoch 00088: val_loss did not improve from 8.47066\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 13.8056 - val_loss: 8.4987\n",
      "Epoch 89/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.5322\n",
      "Epoch 00089: val_loss did not improve from 8.47066\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 13.5322 - val_loss: 8.4762\n",
      "Epoch 90/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.6523\n",
      "Epoch 00090: val_loss did not improve from 8.47066\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 13.6523 - val_loss: 8.4760\n",
      "Epoch 91/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.4732\n",
      "Epoch 00091: val_loss improved from 8.47066 to 8.45517, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 153ms/step - loss: 13.4732 - val_loss: 8.4552\n",
      "Epoch 92/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.2585\n",
      "Epoch 00092: val_loss improved from 8.45517 to 8.44339, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 152ms/step - loss: 13.2585 - val_loss: 8.4434\n",
      "Epoch 93/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.5939\n",
      "Epoch 00093: val_loss improved from 8.44339 to 8.44045, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 152ms/step - loss: 13.5939 - val_loss: 8.4404\n",
      "Epoch 94/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.5523\n",
      "Epoch 00094: val_loss improved from 8.44045 to 8.40505, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 151ms/step - loss: 13.5523 - val_loss: 8.4050\n",
      "Epoch 95/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.3969\n",
      "Epoch 00095: val_loss improved from 8.40505 to 8.39895, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 152ms/step - loss: 13.3969 - val_loss: 8.3990\n",
      "Epoch 96/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.5970\n",
      "Epoch 00096: val_loss did not improve from 8.39895\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 13.5970 - val_loss: 8.4117\n",
      "Epoch 97/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.6513\n",
      "Epoch 00097: val_loss improved from 8.39895 to 8.39888, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 155ms/step - loss: 13.6513 - val_loss: 8.3989\n",
      "Epoch 98/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.7761\n",
      "Epoch 00098: val_loss improved from 8.39888 to 8.36840, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 152ms/step - loss: 13.7761 - val_loss: 8.3684\n",
      "Epoch 99/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.6195\n",
      "Epoch 00099: val_loss improved from 8.36840 to 8.35129, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 150ms/step - loss: 13.6195 - val_loss: 8.3513\n",
      "Epoch 100/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.9251\n",
      "Epoch 00100: val_loss improved from 8.35129 to 8.31990, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 150ms/step - loss: 13.9251 - val_loss: 8.3199\n",
      "Epoch 101/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.9237\n",
      "Epoch 00101: val_loss improved from 8.31990 to 8.30430, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 153ms/step - loss: 12.9237 - val_loss: 8.3043\n",
      "Epoch 102/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.4035\n",
      "Epoch 00102: val_loss improved from 8.30430 to 8.29914, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 151ms/step - loss: 13.4035 - val_loss: 8.2991\n",
      "Epoch 103/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.5383\n",
      "Epoch 00103: val_loss improved from 8.29914 to 8.29717, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 151ms/step - loss: 13.5383 - val_loss: 8.2972\n",
      "Epoch 104/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.4365\n",
      "Epoch 00104: val_loss did not improve from 8.29717\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 13.4365 - val_loss: 8.3068\n",
      "Epoch 105/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.6055\n",
      "Epoch 00105: val_loss did not improve from 8.29717\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 13.6055 - val_loss: 8.3205\n",
      "Epoch 106/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.3001\n",
      "Epoch 00106: val_loss did not improve from 8.29717\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 13.3001 - val_loss: 8.3043\n",
      "Epoch 107/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.3542\n",
      "Epoch 00107: val_loss improved from 8.29717 to 8.26686, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 150ms/step - loss: 13.3542 - val_loss: 8.2669\n",
      "Epoch 108/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.5995\n",
      "Epoch 00108: val_loss did not improve from 8.26686\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 13.5995 - val_loss: 8.2711\n",
      "Epoch 109/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.8157\n",
      "Epoch 00109: val_loss improved from 8.26686 to 8.24636, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 151ms/step - loss: 13.8157 - val_loss: 8.2464\n",
      "Epoch 110/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.2662\n",
      "Epoch 00110: val_loss improved from 8.24636 to 8.23878, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 154ms/step - loss: 13.2662 - val_loss: 8.2388\n",
      "Epoch 111/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.7396\n",
      "Epoch 00111: val_loss improved from 8.23878 to 8.23382, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 151ms/step - loss: 13.7396 - val_loss: 8.2338\n",
      "Epoch 112/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.3719\n",
      "Epoch 00112: val_loss did not improve from 8.23382\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 13.3719 - val_loss: 8.2344\n",
      "Epoch 113/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.5455\n",
      "Epoch 00113: val_loss improved from 8.23382 to 8.20716, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 150ms/step - loss: 13.5455 - val_loss: 8.2072\n",
      "Epoch 114/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.9556\n",
      "Epoch 00114: val_loss did not improve from 8.20716\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 12.9556 - val_loss: 8.2075\n",
      "Epoch 115/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.1667\n",
      "Epoch 00115: val_loss improved from 8.20716 to 8.18514, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 151ms/step - loss: 13.1667 - val_loss: 8.1851\n",
      "Epoch 116/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.2239\n",
      "Epoch 00116: val_loss improved from 8.18514 to 8.16140, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 154ms/step - loss: 13.2239 - val_loss: 8.1614\n",
      "Epoch 117/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.6328\n",
      "Epoch 00117: val_loss did not improve from 8.16140\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 13.6328 - val_loss: 8.1622\n",
      "Epoch 118/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.3047\n",
      "Epoch 00118: val_loss improved from 8.16140 to 8.15461, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 151ms/step - loss: 13.3047 - val_loss: 8.1546\n",
      "Epoch 119/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.4763\n",
      "Epoch 00119: val_loss improved from 8.15461 to 8.11734, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 153ms/step - loss: 13.4763 - val_loss: 8.1173\n",
      "Epoch 120/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.2592\n",
      "Epoch 00120: val_loss did not improve from 8.11734\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 13.2592 - val_loss: 8.1237\n",
      "Epoch 121/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.5965\n",
      "Epoch 00121: val_loss did not improve from 8.11734\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 13.5965 - val_loss: 8.1561\n",
      "Epoch 122/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.0441\n",
      "Epoch 00122: val_loss did not improve from 8.11734\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 13.0441 - val_loss: 8.1718\n",
      "Epoch 123/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.3494\n",
      "Epoch 00123: val_loss did not improve from 8.11734\n",
      "17/17 [==============================] - 2s 116ms/step - loss: 13.3494 - val_loss: 8.1708\n",
      "Epoch 124/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.4375\n",
      "Epoch 00124: val_loss did not improve from 8.11734\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 13.4375 - val_loss: 8.2014\n",
      "Epoch 125/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.7853\n",
      "Epoch 00125: val_loss did not improve from 8.11734\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 13.7853 - val_loss: 8.1977\n",
      "Epoch 126/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.1349\n",
      "Epoch 00126: val_loss did not improve from 8.11734\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 13.1349 - val_loss: 8.1872\n",
      "Epoch 127/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.3602\n",
      "Epoch 00127: val_loss did not improve from 8.11734\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 13.3602 - val_loss: 8.1786\n",
      "Epoch 128/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.3670\n",
      "Epoch 00128: val_loss did not improve from 8.11734\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 13.3670 - val_loss: 8.1982\n",
      "Epoch 129/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.2665\n",
      "Epoch 00129: val_loss did not improve from 8.11734\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 13.2665 - val_loss: 8.1955\n",
      "Epoch 130/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.2637\n",
      "Epoch 00130: val_loss did not improve from 8.11734\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 13.2637 - val_loss: 8.1594\n",
      "Epoch 131/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.4185\n",
      "Epoch 00131: val_loss did not improve from 8.11734\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 13.4185 - val_loss: 8.1426\n",
      "Epoch 132/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.2993\n",
      "Epoch 00132: val_loss did not improve from 8.11734\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 13.2993 - val_loss: 8.1265\n",
      "Epoch 133/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.2456\n",
      "Epoch 00133: val_loss improved from 8.11734 to 8.09211, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 152ms/step - loss: 13.2456 - val_loss: 8.0921\n",
      "Epoch 134/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.3126\n",
      "Epoch 00134: val_loss improved from 8.09211 to 8.04863, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 155ms/step - loss: 13.3126 - val_loss: 8.0486\n",
      "Epoch 135/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.3431\n",
      "Epoch 00135: val_loss improved from 8.04863 to 8.04672, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 153ms/step - loss: 13.3431 - val_loss: 8.0467\n",
      "Epoch 136/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.9251\n",
      "Epoch 00136: val_loss improved from 8.04672 to 8.04151, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 154ms/step - loss: 12.9251 - val_loss: 8.0415\n",
      "Epoch 137/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.1138\n",
      "Epoch 00137: val_loss improved from 8.04151 to 8.03397, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 154ms/step - loss: 13.1138 - val_loss: 8.0340\n",
      "Epoch 138/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.9362\n",
      "Epoch 00138: val_loss improved from 8.03397 to 8.00359, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 149ms/step - loss: 12.9362 - val_loss: 8.0036\n",
      "Epoch 139/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.2517\n",
      "Epoch 00139: val_loss improved from 8.00359 to 7.97978, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 148ms/step - loss: 13.2517 - val_loss: 7.9798\n",
      "Epoch 140/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.4414\n",
      "Epoch 00140: val_loss did not improve from 7.97978\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 13.4414 - val_loss: 7.9803\n",
      "Epoch 141/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.0975\n",
      "Epoch 00141: val_loss did not improve from 7.97978\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 13.0975 - val_loss: 7.9891\n",
      "Epoch 142/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.1607\n",
      "Epoch 00142: val_loss improved from 7.97978 to 7.95171, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 152ms/step - loss: 13.1607 - val_loss: 7.9517\n",
      "Epoch 143/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.1824\n",
      "Epoch 00143: val_loss improved from 7.95171 to 7.94713, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 150ms/step - loss: 13.1824 - val_loss: 7.9471\n",
      "Epoch 144/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.5222\n",
      "Epoch 00144: val_loss improved from 7.94713 to 7.94300, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 151ms/step - loss: 13.5222 - val_loss: 7.9430\n",
      "Epoch 145/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.4555\n",
      "Epoch 00145: val_loss did not improve from 7.94300\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 13.4555 - val_loss: 7.9566\n",
      "Epoch 146/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.0339\n",
      "Epoch 00146: val_loss did not improve from 7.94300\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 13.0339 - val_loss: 8.0107\n",
      "Epoch 147/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.0774\n",
      "Epoch 00147: val_loss did not improve from 7.94300\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 13.0774 - val_loss: 8.0163\n",
      "Epoch 148/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.2422\n",
      "Epoch 00148: val_loss did not improve from 7.94300\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 13.2422 - val_loss: 8.0053\n",
      "Epoch 149/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.0368\n",
      "Epoch 00149: val_loss did not improve from 7.94300\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 13.0368 - val_loss: 8.0153\n",
      "Epoch 150/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.2830\n",
      "Epoch 00150: val_loss did not improve from 7.94300\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 13.2830 - val_loss: 8.0077\n",
      "Epoch 151/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.3322\n",
      "Epoch 00151: val_loss did not improve from 7.94300\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 13.3322 - val_loss: 7.9819\n",
      "Epoch 152/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.0523\n",
      "Epoch 00152: val_loss did not improve from 7.94300\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 13.0523 - val_loss: 7.9683\n",
      "Epoch 153/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.1423\n",
      "Epoch 00153: val_loss improved from 7.94300 to 7.93195, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 154ms/step - loss: 13.1423 - val_loss: 7.9319\n",
      "Epoch 154/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.0503\n",
      "Epoch 00154: val_loss improved from 7.93195 to 7.91623, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 153ms/step - loss: 13.0503 - val_loss: 7.9162\n",
      "Epoch 155/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.9542\n",
      "Epoch 00155: val_loss improved from 7.91623 to 7.90267, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 151ms/step - loss: 12.9542 - val_loss: 7.9027\n",
      "Epoch 156/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.1661\n",
      "Epoch 00156: val_loss did not improve from 7.90267\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 13.1661 - val_loss: 7.9046\n",
      "Epoch 157/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.5194\n",
      "Epoch 00157: val_loss did not improve from 7.90267\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 13.5194 - val_loss: 7.9101\n",
      "Epoch 158/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.0227\n",
      "Epoch 00158: val_loss did not improve from 7.90267\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 13.0227 - val_loss: 7.9099\n",
      "Epoch 159/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.9776\n",
      "Epoch 00159: val_loss improved from 7.90267 to 7.89833, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 151ms/step - loss: 12.9776 - val_loss: 7.8983\n",
      "Epoch 160/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.9663\n",
      "Epoch 00160: val_loss did not improve from 7.89833\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 12.9663 - val_loss: 7.9162\n",
      "Epoch 161/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.3100\n",
      "Epoch 00161: val_loss did not improve from 7.89833\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 13.3100 - val_loss: 7.9203\n",
      "Epoch 162/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.6810\n",
      "Epoch 00162: val_loss improved from 7.89833 to 7.87074, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 153ms/step - loss: 12.6810 - val_loss: 7.8707\n",
      "Epoch 163/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.8860\n",
      "Epoch 00163: val_loss did not improve from 7.87074\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 12.8860 - val_loss: 7.9033\n",
      "Epoch 164/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.3661\n",
      "Epoch 00164: val_loss did not improve from 7.87074\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 13.3661 - val_loss: 7.9092\n",
      "Epoch 165/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.2447\n",
      "Epoch 00165: val_loss did not improve from 7.87074\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 13.2447 - val_loss: 7.8897\n",
      "Epoch 166/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.9716\n",
      "Epoch 00166: val_loss improved from 7.87074 to 7.86788, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 152ms/step - loss: 12.9716 - val_loss: 7.8679\n",
      "Epoch 167/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.1018\n",
      "Epoch 00167: val_loss improved from 7.86788 to 7.86486, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 150ms/step - loss: 13.1018 - val_loss: 7.8649\n",
      "Epoch 168/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.1629\n",
      "Epoch 00168: val_loss did not improve from 7.86486\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 13.1629 - val_loss: 7.8870\n",
      "Epoch 169/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.1179\n",
      "Epoch 00169: val_loss did not improve from 7.86486\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 13.1179 - val_loss: 7.8850\n",
      "Epoch 170/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.8845\n",
      "Epoch 00170: val_loss did not improve from 7.86486\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 12.8845 - val_loss: 7.8783\n",
      "Epoch 171/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.5089\n",
      "Epoch 00171: val_loss did not improve from 7.86486\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 13.5089 - val_loss: 7.8768\n",
      "Epoch 172/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.9823\n",
      "Epoch 00172: val_loss improved from 7.86486 to 7.85849, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 151ms/step - loss: 12.9823 - val_loss: 7.8585\n",
      "Epoch 173/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.7228\n",
      "Epoch 00173: val_loss improved from 7.85849 to 7.84927, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 149ms/step - loss: 12.7228 - val_loss: 7.8493\n",
      "Epoch 174/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.9805\n",
      "Epoch 00174: val_loss improved from 7.84927 to 7.83147, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 151ms/step - loss: 12.9805 - val_loss: 7.8315\n",
      "Epoch 175/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.3100\n",
      "Epoch 00175: val_loss improved from 7.83147 to 7.78782, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 154ms/step - loss: 13.3100 - val_loss: 7.7878\n",
      "Epoch 176/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.1520\n",
      "Epoch 00176: val_loss improved from 7.78782 to 7.77480, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 152ms/step - loss: 13.1520 - val_loss: 7.7748\n",
      "Epoch 177/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.7266\n",
      "Epoch 00177: val_loss improved from 7.77480 to 7.74322, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 150ms/step - loss: 12.7266 - val_loss: 7.7432\n",
      "Epoch 178/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.7329\n",
      "Epoch 00178: val_loss improved from 7.74322 to 7.72130, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 151ms/step - loss: 12.7329 - val_loss: 7.7213\n",
      "Epoch 179/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.9347\n",
      "Epoch 00179: val_loss improved from 7.72130 to 7.71887, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 151ms/step - loss: 12.9347 - val_loss: 7.7189\n",
      "Epoch 180/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.8657\n",
      "Epoch 00180: val_loss improved from 7.71887 to 7.71548, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 150ms/step - loss: 12.8657 - val_loss: 7.7155\n",
      "Epoch 181/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.7882\n",
      "Epoch 00181: val_loss did not improve from 7.71548\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 12.7882 - val_loss: 7.7622\n",
      "Epoch 182/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.0981\n",
      "Epoch 00182: val_loss did not improve from 7.71548\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 13.0981 - val_loss: 7.7670\n",
      "Epoch 183/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.8666\n",
      "Epoch 00183: val_loss did not improve from 7.71548\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 12.8666 - val_loss: 7.7618\n",
      "Epoch 184/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.9911\n",
      "Epoch 00184: val_loss did not improve from 7.71548\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 12.9911 - val_loss: 7.7546\n",
      "Epoch 185/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.5714\n",
      "Epoch 00185: val_loss did not improve from 7.71548\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 12.5714 - val_loss: 7.7397\n",
      "Epoch 186/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.0412\n",
      "Epoch 00186: val_loss did not improve from 7.71548\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 13.0412 - val_loss: 7.7351\n",
      "Epoch 187/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.7567\n",
      "Epoch 00187: val_loss improved from 7.71548 to 7.71292, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 152ms/step - loss: 12.7567 - val_loss: 7.7129\n",
      "Epoch 188/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.6642\n",
      "Epoch 00188: val_loss improved from 7.71292 to 7.68553, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 151ms/step - loss: 12.6642 - val_loss: 7.6855\n",
      "Epoch 189/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.8602\n",
      "Epoch 00189: val_loss improved from 7.68553 to 7.66638, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 155ms/step - loss: 12.8602 - val_loss: 7.6664\n",
      "Epoch 190/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.4574\n",
      "Epoch 00190: val_loss did not improve from 7.66638\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 13.4574 - val_loss: 7.6734\n",
      "Epoch 191/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.7500\n",
      "Epoch 00191: val_loss did not improve from 7.66638\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 12.7500 - val_loss: 7.6989\n",
      "Epoch 192/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.0539\n",
      "Epoch 00192: val_loss did not improve from 7.66638\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 13.0539 - val_loss: 7.6969\n",
      "Epoch 193/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.8937\n",
      "Epoch 00193: val_loss did not improve from 7.66638\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 12.8937 - val_loss: 7.6859\n",
      "Epoch 194/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.8489\n",
      "Epoch 00194: val_loss did not improve from 7.66638\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 12.8489 - val_loss: 7.6733\n",
      "Epoch 195/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.8961\n",
      "Epoch 00195: val_loss improved from 7.66638 to 7.65026, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 151ms/step - loss: 12.8961 - val_loss: 7.6503\n",
      "Epoch 196/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.7918\n",
      "Epoch 00196: val_loss improved from 7.65026 to 7.61320, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 151ms/step - loss: 12.7918 - val_loss: 7.6132\n",
      "Epoch 197/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.8543\n",
      "Epoch 00197: val_loss improved from 7.61320 to 7.61076, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 152ms/step - loss: 12.8543 - val_loss: 7.6108\n",
      "Epoch 198/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.8479\n",
      "Epoch 00198: val_loss improved from 7.61076 to 7.59395, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 152ms/step - loss: 12.8479 - val_loss: 7.5939\n",
      "Epoch 199/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.0239\n",
      "Epoch 00199: val_loss improved from 7.59395 to 7.58633, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 151ms/step - loss: 13.0239 - val_loss: 7.5863\n",
      "Epoch 200/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.6305\n",
      "Epoch 00200: val_loss did not improve from 7.58633\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 12.6305 - val_loss: 7.5892\n",
      "Epoch 201/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.9397\n",
      "Epoch 00201: val_loss improved from 7.58633 to 7.56243, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 159ms/step - loss: 12.9397 - val_loss: 7.5624\n",
      "Epoch 202/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.9192\n",
      "Epoch 00202: val_loss improved from 7.56243 to 7.56006, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 149ms/step - loss: 12.9192 - val_loss: 7.5601\n",
      "Epoch 203/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.2116\n",
      "Epoch 00203: val_loss did not improve from 7.56006\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 13.2116 - val_loss: 7.5739\n",
      "Epoch 204/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.9873\n",
      "Epoch 00204: val_loss did not improve from 7.56006\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 12.9873 - val_loss: 7.5875\n",
      "Epoch 205/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.7443\n",
      "Epoch 00205: val_loss did not improve from 7.56006\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 12.7443 - val_loss: 7.5653\n",
      "Epoch 206/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.5477\n",
      "Epoch 00206: val_loss did not improve from 7.56006\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 12.5477 - val_loss: 7.5601\n",
      "Epoch 207/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.0702\n",
      "Epoch 00207: val_loss improved from 7.56006 to 7.55388, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 151ms/step - loss: 13.0702 - val_loss: 7.5539\n",
      "Epoch 208/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.4728\n",
      "Epoch 00208: val_loss improved from 7.55388 to 7.53902, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 149ms/step - loss: 12.4728 - val_loss: 7.5390\n",
      "Epoch 209/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.7839\n",
      "Epoch 00209: val_loss improved from 7.53902 to 7.53606, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 180ms/step - loss: 12.7839 - val_loss: 7.5361\n",
      "Epoch 210/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.7580\n",
      "Epoch 00210: val_loss improved from 7.53606 to 7.53364, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 151ms/step - loss: 12.7580 - val_loss: 7.5336\n",
      "Epoch 211/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.1922\n",
      "Epoch 00211: val_loss improved from 7.53364 to 7.52713, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 150ms/step - loss: 13.1922 - val_loss: 7.5271\n",
      "Epoch 212/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.0483\n",
      "Epoch 00212: val_loss improved from 7.52713 to 7.52554, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 154ms/step - loss: 13.0483 - val_loss: 7.5255\n",
      "Epoch 213/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.8437\n",
      "Epoch 00213: val_loss did not improve from 7.52554\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 12.8437 - val_loss: 7.5313\n",
      "Epoch 214/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.7259\n",
      "Epoch 00214: val_loss did not improve from 7.52554\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 12.7259 - val_loss: 7.5285\n",
      "Epoch 215/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.6426\n",
      "Epoch 00215: val_loss did not improve from 7.52554\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 12.6426 - val_loss: 7.5408\n",
      "Epoch 216/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.7329\n",
      "Epoch 00216: val_loss did not improve from 7.52554\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 12.7329 - val_loss: 7.5571\n",
      "Epoch 217/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.0877\n",
      "Epoch 00217: val_loss did not improve from 7.52554\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 13.0877 - val_loss: 7.5653\n",
      "Epoch 218/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.8509\n",
      "Epoch 00218: val_loss did not improve from 7.52554\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 12.8509 - val_loss: 7.5573\n",
      "Epoch 219/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.0303\n",
      "Epoch 00219: val_loss did not improve from 7.52554\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 13.0303 - val_loss: 7.5459\n",
      "Epoch 220/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.7853\n",
      "Epoch 00220: val_loss did not improve from 7.52554\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 12.7853 - val_loss: 7.5602\n",
      "Epoch 221/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.7851\n",
      "Epoch 00221: val_loss did not improve from 7.52554\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 12.7851 - val_loss: 7.5422\n",
      "Epoch 222/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.6493\n",
      "Epoch 00222: val_loss improved from 7.52554 to 7.52524, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 150ms/step - loss: 12.6493 - val_loss: 7.5252\n",
      "Epoch 223/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.9519\n",
      "Epoch 00223: val_loss improved from 7.52524 to 7.52512, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 152ms/step - loss: 12.9519 - val_loss: 7.5251\n",
      "Epoch 224/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.7141\n",
      "Epoch 00224: val_loss improved from 7.52512 to 7.52098, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 152ms/step - loss: 12.7141 - val_loss: 7.5210\n",
      "Epoch 225/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.7055\n",
      "Epoch 00225: val_loss improved from 7.52098 to 7.51667, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 150ms/step - loss: 12.7055 - val_loss: 7.5167\n",
      "Epoch 226/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.6793\n",
      "Epoch 00226: val_loss did not improve from 7.51667\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 12.6793 - val_loss: 7.5203\n",
      "Epoch 227/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.4651\n",
      "Epoch 00227: val_loss did not improve from 7.51667\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 12.4651 - val_loss: 7.5306\n",
      "Epoch 228/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.8788\n",
      "Epoch 00228: val_loss improved from 7.51667 to 7.51524, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 153ms/step - loss: 12.8788 - val_loss: 7.5152\n",
      "Epoch 229/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.8160\n",
      "Epoch 00229: val_loss improved from 7.51524 to 7.51025, saving model to ./20210301-223059/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 153ms/step - loss: 12.8160 - val_loss: 7.5102\n",
      "Epoch 230/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.8563\n",
      "Epoch 00230: val_loss did not improve from 7.51025\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 12.8563 - val_loss: 7.5272\n",
      "Epoch 231/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.6634\n",
      "Epoch 00231: val_loss did not improve from 7.51025\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 12.6634 - val_loss: 7.5283\n",
      "Epoch 232/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.5710\n",
      "Epoch 00232: val_loss did not improve from 7.51025\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 12.5710 - val_loss: 7.5142\n",
      "Epoch 233/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.0191\n",
      "Epoch 00233: val_loss did not improve from 7.51025\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 13.0191 - val_loss: 7.5178\n",
      "Epoch 234/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.7132\n",
      "Epoch 00234: val_loss did not improve from 7.51025\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 12.7132 - val_loss: 7.5438\n",
      "Epoch 235/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.5589\n",
      "Epoch 00235: val_loss did not improve from 7.51025\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 12.5589 - val_loss: 7.5736\n",
      "Epoch 236/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.9598\n",
      "Epoch 00236: val_loss did not improve from 7.51025\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 12.9598 - val_loss: 7.5828\n",
      "Epoch 237/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.4613\n",
      "Epoch 00237: val_loss did not improve from 7.51025\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 12.4613 - val_loss: 7.6248\n",
      "Epoch 238/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.8435\n",
      "Epoch 00238: val_loss did not improve from 7.51025\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 12.8435 - val_loss: 7.6552\n",
      "Epoch 239/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.8762\n",
      "Epoch 00239: val_loss did not improve from 7.51025\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 12.8762 - val_loss: 7.6182\n",
      "Epoch 240/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.5903\n",
      "Epoch 00240: val_loss did not improve from 7.51025\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 12.5903 - val_loss: 7.6222\n",
      "Epoch 241/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.6590\n",
      "Epoch 00241: val_loss did not improve from 7.51025\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 12.6590 - val_loss: 7.6104\n",
      "Epoch 242/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.7344\n",
      "Epoch 00242: val_loss did not improve from 7.51025\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 12.7344 - val_loss: 7.5829\n",
      "Epoch 243/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.3317\n",
      "Epoch 00243: val_loss did not improve from 7.51025\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 12.3317 - val_loss: 7.5516\n",
      "Epoch 244/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.6578\n",
      "Epoch 00244: val_loss did not improve from 7.51025\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 12.6578 - val_loss: 7.5487\n",
      "Epoch 245/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.4514\n",
      "Epoch 00245: val_loss did not improve from 7.51025\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 12.4514 - val_loss: 7.5667\n",
      "Epoch 246/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.9727\n",
      "Epoch 00246: val_loss did not improve from 7.51025\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 12.9727 - val_loss: 7.5470\n",
      "Epoch 247/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.5624\n",
      "Epoch 00247: val_loss did not improve from 7.51025\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 12.5624 - val_loss: 7.5475\n",
      "Epoch 248/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.1594\n",
      "Epoch 00248: val_loss did not improve from 7.51025\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 13.1594 - val_loss: 7.5445\n",
      "Epoch 249/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.1455\n",
      "Epoch 00249: val_loss did not improve from 7.51025\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 12.1455 - val_loss: 7.5450\n",
      "CPU times: user 43min 37s, sys: 6min 57s, total: 50min 35s\n",
      "Wall time: 29min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# big K = 5 (fold 0 ~ 4) \n",
    "KFlodNum = 1\n",
    "\n",
    "\n",
    "\n",
    "history_toe = []\n",
    "history_toe_finetune = []\n",
    "\n",
    "#above until 'train_ds_map_toe now' to 'train_ds_map_toe_s', 'valid_ds_map_toe_s'\n",
    "for k in range(KFlodNum):\n",
    "    \n",
    "    \n",
    "    # Split data to train/valid with K-Fold #\n",
    "    print(\"\\n \\n K = \", k, \"\\n\")\n",
    "    # Toe split\n",
    "    train_ds_map_toe_s, valid_ds_map_toe_s = get_KFold_ds(train_ds_map_toe, K=k)\n",
    "    \n",
    "    # Toe ds_pre\n",
    "    train_ds_pre_toe_s = configure_for_performance_cache_train(train_ds_map_toe_s, augment=True)\n",
    "    valid_ds_pre_toe_s = configure_for_performance_cache_val(valid_ds_map_toe_s)\n",
    "    \n",
    "    \n",
    "#     # heel split\n",
    "#     train_ds_map_heel_s, valid_ds_map_heel_s = get_KFold_ds(train_ds_map_heel, K=k)\n",
    "#     # Heel ds_pre\n",
    "#     train_ds_pre_heel_s = configure_for_performance_cache_train(train_ds_map_heel_s, augment=True)\n",
    "#     valid_ds_pre_heel_s = configure_for_performance_cache_val(valid_ds_map_heel_s)\n",
    "    \n",
    "    \n",
    "    # Train K-Model with transfer learnling #\n",
    "    \n",
    "    # Toe model, TL\n",
    "    th = 'toe'\n",
    "    # th = 'heel'\n",
    "    best_model_name = get_best_model_name(th, K=str(k))\n",
    "    best_model_save = tf.keras.callbacks.ModelCheckpoint(filepath=best_model_name, \n",
    "                                 save_best_only = True, \n",
    "                                 save_weights_only = False,\n",
    "                                 monitor = monitor, \n",
    "                                 mode = 'auto', verbose = 1)\n",
    "    callbacks_toe_tl = [\n",
    "                    #     tensorboard_callback,\n",
    "                        best_model_save,\n",
    "                        tf.keras.callbacks.EarlyStopping(monitor=monitor, patience=5), #patience=step_size or ep_num\n",
    "                    #     lr_reduceonplateau,\n",
    "                        tf.keras.callbacks.LearningRateScheduler(lrfca),#lrdump, decay or lrfn or lrfn2. clr\n",
    "#                         PrintLRtoe()\n",
    "                        ]\n",
    "    callbacks_toe_fn = [\n",
    "                    #     tensorboard_callback,\n",
    "                        best_model_save,\n",
    "                        tf.keras.callbacks.EarlyStopping(monitor=monitor, patience=20), #patience=step_size or ep_num\n",
    "                    #     lr_reduceonplateau,\n",
    "                        tf.keras.callbacks.LearningRateScheduler(lrfca),#lrdump, decay or lrfn or lrfn2. clr, CosineDecayCLRWarmUp, CosineDecayCLRWarmUpLSW\n",
    "#                         PrintLRtoe()\n",
    "                    ]\n",
    "    print('best_model_name:', best_model_name)\n",
    "\n",
    "\n",
    "    top_dropout_rate = 0.8 #less dp rate, say 0.1, train_loss will lower than val_loss\n",
    "    drop_connect_rate = 0.9 #for efnet This parameter serves as a toggle for extra regularization in finetuning, but does not affect loaded weights.\n",
    "    outputnum = 2\n",
    "    with strategy.scope():\n",
    "        model_toe = build_efn_model(outputnum, top_dropout_rate, drop_connect_rate)\n",
    "#         model_toe = load_pretrained_efn_model() # from 20210224-200728 ed5.3\n",
    "#         count_model_trainOrNot_layers(model_toe)\n",
    "        \n",
    "    # fit the model on all data\n",
    "    hist = model_toe.fit(train_ds_pre_toe_s, \n",
    "                          verbose=1, \n",
    "                          epochs=ep_num_transf, \n",
    "                          validation_data=valid_ds_pre_toe_s, \n",
    "                          callbacks=callbacks_toe_tl)#, validation_split=0.1)\n",
    "    history_toe.append(hist)\n",
    "    \n",
    "      \n",
    "    # Train K-Model with fine tune #\n",
    "    \n",
    "    # Toe model, FT\n",
    "    unfreeze_model(model_toe)\n",
    "    count_model_trainOrNot_layers(model_toe)\n",
    "    # fit the model on all data\n",
    "    hist = model_toe.fit(train_ds_pre_toe_s, \n",
    "                          verbose=1, \n",
    "                          epochs=ep_num, \n",
    "                          validation_data=valid_ds_pre_toe_s, \n",
    "                          callbacks=callbacks_toe_fn)#, validation_split=0.1)\n",
    "    history_toe_finetune.append(hist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2021-03-01 2300\n",
    "Epoch 249/1000\n",
    "17/17 [==============================] - ETA: 0s - loss: 12.1455\n",
    "Epoch 00249: val_loss did not improve from 7.51025\n",
    "17/17 [==============================] - 2s 111ms/step - loss: 12.1455 - val_loss: 7.5450\n",
    "CPU times: user 43min 37s, sys: 6min 57s, total: 50min 35s\n",
    "Wall time: 29min 32s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[7.51024866104126]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ED sum\n",
    "def get_valloss(his_v_l):  \n",
    "    return np.min(his_v_l), np.argmin(his_v_l)\n",
    "\n",
    "t_vl = []\n",
    "# h_vl = []\n",
    "for k in range(KFlodNum):\n",
    "    print(f'K:{k}')\n",
    "    t_v, _ = get_valloss(history_toe_finetune[k].history['val_loss'])\n",
    "#     h_v, _ = get_valloss(history_heel_finetune[k].history['val_loss'])\n",
    "    \n",
    "    t_vl.append(t_v)\n",
    "#     h_vl.append(h_v)\n",
    "\n",
    "# t_vl = np.mean(t_vl, axis=0)\n",
    "# h_vl = np.mean(h_vl, axis=0)\n",
    "# print(f'{round(t_vl,5)} + {round(h_vl,5)} = {round(t_vl + h_vl,5)}')\n",
    "\n",
    "t_vl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(f'{log_dir_name}/toe_FNED.txt', t_vl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the loos the model trained.\n",
    "\n",
    "def get_valloss(his_v_l):  \n",
    "    return np.min(his_v_l), np.argmin(his_v_l)\n",
    "\n",
    "t_vl = []\n",
    "# h_vl = []\n",
    "for k in range(KFlodNum):\n",
    "    print(f'K:{k}')\n",
    "    t_v, _ = get_valloss(history_toe[k].history['val_loss'])\n",
    "#     h_v, _ = get_valloss(history_heel_finetune[k].history['val_loss'])\n",
    "    \n",
    "    t_vl.append(t_v)\n",
    "    \n",
    "# for different scales (different Y-axes)\n",
    "# fig, ax1 = plt.subplots()\n",
    "fig, ax1 = plt.subplots(figsize=(25, 10))\n",
    "\n",
    "# nice to have this colorful tip.\n",
    "color = 'tab:red'\n",
    "\n",
    "ax1.set_title('[ toe_finetune ] \\n ED loss')\n",
    "\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.set_ylabel('ed loss', color=color)\n",
    "\n",
    "for k in range(KFlodNum):\n",
    "\n",
    "    plt.plot(history_toe[k].history['loss'])\n",
    "    plt.plot(history_toe[k].history['val_loss'])\n",
    "\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "# ax1.legend(['loss', 'val_loss'], loc='upper center') # legend may ocvered by next ax ploting. moved to end.\n",
    "ax1.legend(['k0 loss', 'k0 val_loss', 'k1 loss', 'k1 val_loss', 'k2 loss', 'k2 val_loss', \n",
    "            'k3 loss', 'k3 val_loss', 'k4 loss', 'k4 val_loss'], loc='upper left') \n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('learning rate', color=color)\n",
    "ax2.plot(history_toe[0].history['lr'], color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.legend(['lr'], loc='upper right') \n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# # save plot : comment plo.show in jupyter notebook.\n",
    "# def get_valloss(his_v_l):   \n",
    "#     return np.min(his_v_l), np.argmin(his_v_l)\n",
    "\n",
    "# vl, ep = get_valloss(history_toe_finetune.history['val_loss'])\n",
    "\n",
    "\n",
    "t_vl = np.mean(t_vl, axis=0)\n",
    "plt.savefig(f'{log_dir_name}/{log_dir_name}_toe_tl_Ksum-clr_ed{round(t_vl,4)}.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum plot losses toe-tl\n",
    "\n",
    "plt.figure(figsize=(25, 10))\n",
    "\n",
    "for k in range(KFlodNum):\n",
    "\n",
    "    plt.plot(history_toe[k].history['loss'])\n",
    "    plt.plot(history_toe[k].history['val_loss'])\n",
    "\n",
    "    \n",
    "plt.title('K-model ed loss toe-TL')\n",
    "plt.ylabel('ed loss'), plt.ylim(5, 80)# for too large loss\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['k0 loss', 'k0 val_loss', 'k1 loss', 'k1 val_loss', 'k2 loss', 'k2 val_loss', \n",
    "            'k3 loss', 'k3 val_loss', 'k4 loss', 'k4 val_loss'], loc='upper left') \n",
    "# plt.show()\n",
    "\n",
    "# save plot : comment plo.show in jupyter notebook.\n",
    "plt.savefig(f'{log_dir_name}/{log_dir_name}_toe_Ksum_TL.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Single plot loss toe-tl\n",
    "\n",
    "for k in range(KFlodNum):\n",
    "    plt.figure(figsize=(25, 10))\n",
    "    plt.plot(history_toe[k].history['loss'])\n",
    "    plt.plot(history_toe[k].history['val_loss'])\n",
    "    plt.title('K-model ed loss toe-TL')\n",
    "    plt.ylabel('ed loss'), plt.ylim(5, 20)# for too large loss\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['loss', 'val_loss'], loc='upper left') \n",
    "    # plt.show()\n",
    "\n",
    "    # save plot : comment plo.show in jupyter notebook.\n",
    "    def get_valloss(his_v_l):   \n",
    "        return np.min(his_v_l), np.argmin(his_v_l)\n",
    "\n",
    "    vl, ep = get_valloss(history_toe[k].history['val_loss'])\n",
    "    plt.savefig(f'{log_dir_name}/{log_dir_name}_toe_K{k}_TL_clr_ed{round(vl,4)}@{ep}.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the loos the model trained.\n",
    "\n",
    "def get_valloss(his_v_l):  \n",
    "    return np.min(his_v_l), np.argmin(his_v_l)\n",
    "\n",
    "t_vl = []\n",
    "# h_vl = []\n",
    "for k in range(KFlodNum):\n",
    "    print(f'K:{k}')\n",
    "    t_v, _ = get_valloss(history_toe_finetune[k].history['val_loss'])\n",
    "#     h_v, _ = get_valloss(history_heel_finetune[k].history['val_loss'])\n",
    "    \n",
    "    t_vl.append(t_v)\n",
    "    \n",
    "# for different scales (different Y-axes)\n",
    "# fig, ax1 = plt.subplots()\n",
    "fig, ax1 = plt.subplots(figsize=(25, 10))\n",
    "\n",
    "# nice to have this colorful tip.\n",
    "color = 'tab:red'\n",
    "\n",
    "ax1.set_title('[ toe_finetune ] \\n ED loss')\n",
    "\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.set_ylabel('ed loss', color=color)\n",
    "\n",
    "for k in range(KFlodNum):\n",
    "\n",
    "    plt.plot(history_toe_finetune[k].history['loss'])\n",
    "    plt.plot(history_toe_finetune[k].history['val_loss'])\n",
    "\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "# ax1.legend(['loss', 'val_loss'], loc='upper center') # legend may ocvered by next ax ploting. moved to end.\n",
    "ax1.legend(['k0 loss', 'k0 val_loss', 'k1 loss', 'k1 val_loss', 'k2 loss', 'k2 val_loss', \n",
    "            'k3 loss', 'k3 val_loss', 'k4 loss', 'k4 val_loss'], loc='upper left') \n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('learning rate', color=color)\n",
    "ax2.plot(history_toe_finetune[0].history['lr'], color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.legend(['lr'], loc='upper right') \n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# # save plot : comment plo.show in jupyter notebook.\n",
    "# def get_valloss(his_v_l):   \n",
    "#     return np.min(his_v_l), np.argmin(his_v_l)\n",
    "\n",
    "# vl, ep = get_valloss(history_toe_finetune.history['val_loss'])\n",
    "\n",
    "\n",
    "t_vl = np.mean(t_vl, axis=0)\n",
    "plt.savefig(f'{log_dir_name}/{log_dir_name}_toe_ft_Ksum-clr_ed{round(t_vl,4)}.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum plot losses toe-ft\n",
    "\n",
    "plt.figure(figsize=(25, 10))\n",
    "\n",
    "for k in range(KFlodNum):\n",
    "\n",
    "    plt.plot(history_toe_finetune[k].history['loss'])\n",
    "    plt.plot(history_toe_finetune[k].history['val_loss'])\n",
    "\n",
    "    \n",
    "plt.title('K-model ed loss toe-FT')\n",
    "plt.ylabel('ed loss'), plt.ylim(4, 20)# for too large loss\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['k0 loss', 'k0 val_loss', 'k1 loss', 'k1 val_loss', 'k2 loss', 'k2 val_loss', \n",
    "            'k3 loss', 'k3 val_loss', 'k4 loss', 'k4 val_loss'], loc='upper left') \n",
    "# plt.show()\n",
    "\n",
    "# save plot : comment plo.show in jupyter notebook.\n",
    "plt.savefig(f'{log_dir_name}/{log_dir_name}_toe_Ksum_FT.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Single plot loss toe-FT\n",
    "\n",
    "for k in range(KFlodNum):\n",
    "    plt.figure(figsize=(25, 10))\n",
    "    plt.plot(history_toe_finetune[k].history['loss'])\n",
    "    plt.plot(history_toe_finetune[k].history['val_loss'])\n",
    "    plt.title('K-model ed loss toe-FT')\n",
    "    plt.ylabel('ed loss'), plt.ylim(4, 20)# for too large loss\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['loss', 'val_loss'], loc='upper left') \n",
    "    # plt.show()\n",
    "\n",
    "    # save plot : comment plo.show in jupyter notebook.\n",
    "    def get_valloss(his_v_l):   \n",
    "        return np.min(his_v_l), np.argmin(his_v_l)\n",
    "\n",
    "    vl, ep = get_valloss(history_toe_finetune[k].history['val_loss'])\n",
    "    plt.savefig(f'{log_dir_name}/{log_dir_name}_toe_K{k}_FT_clr_ed{round(vl,4)}@{ep}.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heel K-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# big K = 5 (fold 0 ~ 4) \n",
    "# KFlodNum = 5 # follow Toe's K.\n",
    "\n",
    "\n",
    "\n",
    "history_heel = []\n",
    "history_heel_finetune = []\n",
    "\n",
    "#above until 'train_ds_map_toe now' to 'train_ds_map_toe_s', 'valid_ds_map_toe_s'\n",
    "for k in range(KFlodNum):\n",
    "    \n",
    "    \n",
    "    # Split data to train/valid with K-Fold #\n",
    "    print(\"K=\", k)\n",
    "#     # Toe split\n",
    "#     train_ds_map_toe_s, valid_ds_map_toe_s = get_KFold_ds(train_ds_map_toe, K=k)\n",
    "    \n",
    "#     # Toe ds_pre\n",
    "#     train_ds_pre_toe_s = configure_for_performance_cache_train(train_ds_map_toe_s, augment=True)\n",
    "#     valid_ds_pre_toe_s = configure_for_performance_cache_val(valid_ds_map_toe_s)\n",
    "    \n",
    "    \n",
    "    # heel split\n",
    "    train_ds_map_heel_s, valid_ds_map_heel_s = get_KFold_ds(train_ds_map_heel, K=k)\n",
    "    # Heel ds_pre\n",
    "    train_ds_pre_heel_s = configure_for_performance_cache_train_AToe(train_ds_map_heel_s, augment=True)\n",
    "    valid_ds_pre_heel_s = configure_for_performance_cache_val(valid_ds_map_heel_s)\n",
    "    \n",
    "    \n",
    "    # Train K-Model with transfer learnling #\n",
    "    \n",
    "    # Toe model, TL\n",
    "    #th = 'toe'\n",
    "    th = 'heel'\n",
    "    best_model_name = get_best_model_name(th, K=str(k))\n",
    "    best_model_save = tf.keras.callbacks.ModelCheckpoint(filepath=best_model_name, \n",
    "                                 save_best_only = True, \n",
    "                                 save_weights_only = False,\n",
    "                                 monitor = monitor, \n",
    "                                 mode = 'auto', verbose = 1)\n",
    "    callbacks_heel_tl = [\n",
    "                    #     tensorboard_callback,\n",
    "                        best_model_save,\n",
    "                        tf.keras.callbacks.EarlyStopping(monitor=monitor, patience=20), #patience=step_size or ep_num\n",
    "                    #     lr_reduceonplateau,\n",
    "                        tf.keras.callbacks.LearningRateScheduler(lrdump),#lrdump, decay or lrfn or lrfn2. clr\n",
    "                        PrintLRtoe()\n",
    "                        ]\n",
    "    callbacks_heel_fn = [\n",
    "                    #     tensorboard_callback,\n",
    "                        best_model_save,\n",
    "                        tf.keras.callbacks.EarlyStopping(monitor=monitor, patience=100), #patience=step_size or ep_num\n",
    "                    #     lr_reduceonplateau,\n",
    "                        tf.keras.callbacks.LearningRateScheduler(clr3),#lrdump, decay or lrfn or lrfn2. clr, CosineDecayCLRWarmUp, CosineDecayCLRWarmUpLSW\n",
    "                        PrintLRheel()\n",
    "                    ]\n",
    "    print('best_model_name:', best_model_name)\n",
    "\n",
    "\n",
    "    top_dropout_rate = 0.4 #less dp rate, say 0.1, train_loss will lower than val_loss\n",
    "    drop_connect_rate = 0.4 #for efnet This parameter serves as a toggle for extra regularization in finetuning, but does not affect loaded weights.\n",
    "    outputnum = 2\n",
    "    with strategy.scope():\n",
    "        model_heel = build_efn_model(outputnum, top_dropout_rate, drop_connect_rate)\n",
    "    # fit the model on all data\n",
    "    hist = model_heel.fit(train_ds_pre_heel_s, \n",
    "                          verbose=1, \n",
    "                          epochs=ep_num_transf, \n",
    "                          validation_data=valid_ds_pre_heel_s, \n",
    "                          callbacks=callbacks_heel_tl)#, validation_split=0.1)\n",
    "    history_heel.append(hist)\n",
    "    \n",
    "      \n",
    "    # Train K-Model with fine tune #\n",
    "    \n",
    "    # Toe model, FT\n",
    "    unfreeze_model(model_heel)\n",
    "    count_model_trainOrNot_layers(model_heel)\n",
    "    # fit the model on all data\n",
    "    hist = model_heel.fit(train_ds_pre_heel_s, \n",
    "                          verbose=1, \n",
    "                          epochs=ep_num, \n",
    "                          validation_data=valid_ds_pre_heel_s, \n",
    "                          callbacks=callbacks_heel_fn)#, validation_split=0.1)\n",
    "    history_heel_finetune.append(hist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ED sum\n",
    "def get_valloss(his_v_l):  \n",
    "    return np.min(his_v_l), np.argmin(his_v_l)\n",
    "\n",
    "# t_vl = []\n",
    "h_vl = []\n",
    "for k in range(KFlodNum):\n",
    "    print(f'K:{k}')\n",
    "#     t_v, _ = get_valloss(history_toe_finetune[k].history['val_loss'])\n",
    "    h_v, _ = get_valloss(history_heel_finetune[k].history['val_loss'])\n",
    "    \n",
    "#     t_vl.append(t_v)\n",
    "    h_vl.append(h_v)\n",
    "\n",
    "# t_vl = np.mean(t_vl, axis=0)\n",
    "# h_vl = np.mean(h_vl, axis=0)\n",
    "# print(f'{round(t_vl,5)} + {round(h_vl,5)} = {round(t_vl + h_vl,5)}')\n",
    "\n",
    "# t_vl\n",
    "h_vl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(f'{log_dir_name}/heel_FNED.txt', h_vl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum plot losses heel-tl\n",
    "\n",
    "plt.figure(figsize=(25, 10))\n",
    "\n",
    "for k in range(KFlodNum):\n",
    "\n",
    "    plt.plot(history_heel[k].history['loss'])\n",
    "    plt.plot(history_heel[k].history['val_loss'])\n",
    "\n",
    "    \n",
    "plt.title('K-model ed loss heel-TL')\n",
    "plt.ylabel('ed loss'), plt.ylim(5, 50)# for too large loss\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['k0 loss', 'k0 val_loss', 'k1 loss', 'k1 val_loss', 'k2 loss', 'k2 val_loss', \n",
    "            'k3 loss', 'k3 val_loss', 'k4 loss', 'k4 val_loss'], loc='upper left') \n",
    "# plt.show()\n",
    "\n",
    "# save plot : comment plo.show in jupyter notebook.\n",
    "plt.savefig(f'{log_dir_name}/{log_dir_name}_heel_Ksum_TL.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Single plot loss heel-tl\n",
    "\n",
    "for k in range(KFlodNum):\n",
    "    plt.figure(figsize=(25, 10))\n",
    "    plt.plot(history_heel[k].history['loss'])\n",
    "    plt.plot(history_heel[k].history['val_loss'])\n",
    "    plt.title('K-model ed loss heel-TL')\n",
    "    plt.ylabel('ed loss'), plt.ylim(5, 80)# for too large loss\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['loss', 'val_loss'], loc='upper left') \n",
    "    # plt.show()\n",
    "\n",
    "    # save plot : comment plo.show in jupyter notebook.\n",
    "    def get_valloss(his_v_l):   \n",
    "        return np.min(his_v_l), np.argmin(his_v_l)\n",
    "\n",
    "    vl, ep = get_valloss(history_heel[k].history['val_loss'])\n",
    "    plt.savefig(f'{log_dir_name}/{log_dir_name}_heel_K{k}_TL_clr_ed{round(vl,4)}@{ep}.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the loos the model trained.\n",
    "\n",
    "def get_valloss(his_v_l):  \n",
    "    return np.min(his_v_l), np.argmin(his_v_l)\n",
    "\n",
    "# t_vl = []\n",
    "h_vl = []\n",
    "for k in range(KFlodNum):\n",
    "    print(f'K:{k}')\n",
    "#     t_v, _ = get_valloss(history_toe_finetune[k].history['val_loss'])\n",
    "    h_v, _ = get_valloss(history_heel_finetune[k].history['val_loss'])\n",
    "    \n",
    "    h_vl.append(h_v)\n",
    "    \n",
    "# for different scales (different Y-axes)\n",
    "# fig, ax1 = plt.subplots()\n",
    "fig, ax1 = plt.subplots(figsize=(25, 10))\n",
    "\n",
    "# nice to have this colorful tip.\n",
    "color = 'tab:red'\n",
    "\n",
    "ax1.set_title('[ heel_finetune ] \\n ED loss')\n",
    "\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.set_ylabel('ed loss', color=color)\n",
    "\n",
    "for k in range(KFlodNum):\n",
    "\n",
    "    plt.plot(history_heel_finetune[k].history['loss'])\n",
    "    plt.plot(history_heel_finetune[k].history['val_loss'])\n",
    "\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "# ax1.legend(['loss', 'val_loss'], loc='upper center') # legend may ocvered by next ax ploting. moved to end.\n",
    "ax1.legend(['k0 loss', 'k0 val_loss', 'k1 loss', 'k1 val_loss', 'k2 loss', 'k2 val_loss', \n",
    "            'k3 loss', 'k3 val_loss', 'k4 loss', 'k4 val_loss'], loc='upper right') \n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('learning rate', color=color)\n",
    "ax2.plot(history_heel_finetune[0].history['lr'], color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.legend(['lr'], loc='upper right') \n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# # save plot : comment plo.show in jupyter notebook.\n",
    "# def get_valloss(his_v_l):   \n",
    "#     return np.min(his_v_l), np.argmin(his_v_l)\n",
    "\n",
    "# vl, ep = get_valloss(history_toe_finetune.history['val_loss'])\n",
    "\n",
    "\n",
    "# t_vl = np.mean(t_vl, axis=0)\n",
    "h_vl = np.mean(h_vl, axis=0)\n",
    "plt.savefig(f'{log_dir_name}/{log_dir_name}_heel_ft_Ksum-clr_ed{round(h_vl,4)}.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum plot losses heel-ft\n",
    "\n",
    "plt.figure(figsize=(25, 10))\n",
    "\n",
    "for k in range(KFlodNum):\n",
    "\n",
    "    plt.plot(history_heel_finetune[k].history['loss'])\n",
    "    plt.plot(history_heel_finetune[k].history['val_loss'])\n",
    "\n",
    "    \n",
    "plt.title('K-model ed loss heel-FT')\n",
    "plt.ylabel('ed loss'), plt.ylim(2, 20)# for too large loss\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['k0 loss', 'k0 val_loss', 'k1 loss', 'k1 val_loss', 'k2 loss', 'k2 val_loss', \n",
    "            'k3 loss', 'k3 val_loss', 'k4 loss', 'k4 val_loss'], loc='upper left') \n",
    "# plt.show()\n",
    "\n",
    "# save plot : comment plo.show in jupyter notebook.\n",
    "plt.savefig(f'{log_dir_name}/{log_dir_name}_heel_Ksum_FT.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Single plot loss heel-FT\n",
    "\n",
    "for k in range(KFlodNum):\n",
    "    plt.figure(figsize=(25, 10))\n",
    "    plt.plot(history_heel_finetune[k].history['loss'])\n",
    "    plt.plot(history_heel_finetune[k].history['val_loss'])\n",
    "    plt.title('K-model ed loss heel-FT')\n",
    "    plt.ylabel('ed loss'), plt.ylim(2, 20)# for too large loss\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['loss', 'val_loss'], loc='upper left') \n",
    "    # plt.show()\n",
    "\n",
    "    # save plot : comment plo.show in jupyter notebook.\n",
    "    def get_valloss(his_v_l):   \n",
    "        return np.min(his_v_l), np.argmin(his_v_l)\n",
    "\n",
    "    vl, ep = get_valloss(history_heel_finetune[k].history['val_loss'])\n",
    "    plt.savefig(f'{log_dir_name}/{log_dir_name}_heel_K{k}_FT_clr_ed{round(vl,4)}@{ep}.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ploting your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model_toe.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # show after modl.fit\n",
    "# model_toe.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check metrics the model have.\n",
    "# history_toe.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.utils.plot_model(model_toe, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from tensorflow.keras.utils import plot_model\n",
    "# plot_model(model_toe, to_file='model_toe_conv_layer_blocks.png', show_shapes=True)\n",
    "# from IPython.display import Image\n",
    "# Image(filename='model_toe_conv_layer_blocks.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## show pp pred\n",
    "\n",
    "* we can switch toe/hell by comment it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EFN Toe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it_valid_ds_pre_toe_s = iter(valid_ds_pre_toe_s)\n",
    "# # it_valid_ds_pre_heel_s = iter(valid_ds_pre_heel_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # image_batch, label_batch = next(valid_ds_pre_toe_s)\n",
    "\n",
    "# image_batch, label_batch = next(it_valid_ds_pre_toe_s)\n",
    "# # image_batch, label_batch = next(it_valid_ds_pre_heel_s)\n",
    "\n",
    "\n",
    "# pred = model_toe.predict_on_batch(image_batch) #predictions\n",
    "# # pred = model.predict_on_batch(image_batch) #Simple 2D CNN model predictions\n",
    "\n",
    "# plt.figure(figsize=(20, 20))\n",
    "# for i in range(64):\n",
    "#     ax = plt.subplot(8, 8, i + 1)\n",
    "#     plt.imshow(image_batch[i])\n",
    "#     plt.title(label_batch[i].numpy())\n",
    "#     plt.axis(\"off\")\n",
    "    \n",
    "#     # note: y_offset_toe for ds image\n",
    "    \n",
    "#     #ground truth\n",
    "#     plt.plot(label_batch[i].numpy()[0], label_batch[i].numpy()[1], 'r+', markersize=15, mew=2)\n",
    "\n",
    "#     #pred\n",
    "#     plt.plot(pred[i][0], pred[i][1], 'k+', markersize=15, mew=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #test_ds一次做完即可不用分batch\n",
    "# neg = label_batch - pred\n",
    "# neg[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.abs(neg)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neg_abs = np.abs(neg)\n",
    "# neg_abs.mean(axis=0)#所有x 所有y個別平均  neg.mean(axis=0)#所有x 所有y個別平均"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ED 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # y_pred = neg_abs.mean(axis=0)\n",
    "# ed_metric_2d([0,0], [neg_abs.mean(axis=0)]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EFN Heel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # it_valid_ds_pre_toe_s = iter(valid_ds_pre_toe_s)\n",
    "# it_valid_ds_pre_heel_s = iter(valid_ds_pre_heel_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # image_batch, label_batch = next(valid_ds_pre_toe_s)\n",
    "\n",
    "# # image_batch, label_batch = next(it_valid_ds_pre_toe_s)\n",
    "# image_batch, label_batch = next(it_valid_ds_pre_heel_s)\n",
    "\n",
    "\n",
    "# pred = model_heel.predict_on_batch(image_batch) #predictions\n",
    "# # pred = model.predict_on_batch(image_batch) #Simple 2D CNN model predictions\n",
    "\n",
    "# plt.figure(figsize=(20, 20))\n",
    "# for i in range(64):\n",
    "#     ax = plt.subplot(8, 8, i + 1)\n",
    "#     plt.imshow(image_batch[i])\n",
    "#     plt.title(label_batch[i].numpy())\n",
    "#     plt.axis(\"off\")\n",
    "    \n",
    "#     # note: y_offset_toe for ds image\n",
    "    \n",
    "#     #ground truth\n",
    "#     plt.plot(label_batch[i].numpy()[0], label_batch[i].numpy()[1], 'r+', markersize=15, mew=2)\n",
    "\n",
    "#     #pred\n",
    "#     plt.plot(pred[i][0], pred[i][1], 'k+', markersize=15, mew=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_batch[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #test_ds一次做完即可不用分batch\n",
    "# neg = label_batch - pred\n",
    "# neg[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.abs(neg)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neg_abs = np.abs(neg)\n",
    "# neg_abs.mean(axis=0)#所有x 所有y個別平均  neg.mean(axis=0)#所有x 所有y個別平均"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ED 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # y_pred = neg_abs.mean(axis=0)\n",
    "# ed_metric_2d([0,0], [neg_abs.mean(axis=0)]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merg Toe/Heel model and predict the Test data at once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TEST DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = 'test_submission.csv'\n",
    "df_ts = pd.read_csv(ts)\n",
    "df_ts.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ts['images']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataframe\n",
    "list_ds_test = tf.data.Dataset.from_tensor_slices(df_ts['images'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_ds_test)#.shape() #take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check The type specification of an element of this dataset.\n",
    "list_ds_test.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in list_ds_test.take(5):\n",
    "    print(f'take test sample: {f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST DS: Process TEST path to image tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST-DS: re-used from train/val-ds\n",
    "\n",
    "im_test = 'test_images/'\n",
    "\n",
    "'''\n",
    "def get_label(file_path):\n",
    "    # convert the path to a list of path components\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    # The second to last is the class-directory\n",
    "    boolen = parts[-2] == class_names\n",
    "    #one_hot_num = np.array(boolen, dtype=np.int) not works should use tf.x repalced.\n",
    "    one_hot_num = tf.dtypes.cast(boolen, tf.int64)\n",
    "    one_num = tf.argmax(one_hot_num)\n",
    "    print('one_num:', one_num)\n",
    "    # Integer encode the label\n",
    "    return one_num\n",
    "\n",
    "def decode_img(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    # resize the image to the desired size\n",
    "#     return tf.image.resize(img, [img_height, img_width])# augment 已經resize過一次了 但這邊不先做會比較慢\n",
    "    return tf.cast(tf.image.resize(img, [img_height, img_width]), tf.uint8)# 避免float over at augment\n",
    "'''\n",
    "\n",
    "#\n",
    "# map list to ds, Toe part.\n",
    "#\n",
    "\n",
    "def decode_crop_png_toe_test(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.io.decode_png(img, channels=3)\n",
    "    # crop the toe from top-left corner [image, offset_height y1, offset_width x1, target_height, target_width]\n",
    "    y1=y_offset_toe;    x1=0;    h=img_height;    w=img_width # not the pp location\n",
    "    img = tf.image.crop_to_bounding_box(img, int(y1), int(x1), h, w)\n",
    "    #img = tf.image.crop_to_bounding_box(img, int(y1), int(x1), int(y2)-int(y1), int(x2)-int(x1))\n",
    "    # resize the image to the desired size\n",
    "    return img\n",
    "\n",
    "def process_path_toe_test(file_name):\n",
    "    file_path = im_test + file_name\n",
    "    #label = get_label(file_path)\n",
    "    # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)#can read the byte string paths b'image_0001.png'\n",
    "    img = decode_crop_png_toe_test(img)\n",
    "    return img, file_name\n",
    "\n",
    "#\n",
    "# map list to ds, Heel part.\n",
    "#\n",
    "\n",
    "def decode_crop_png_heel_test(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.io.decode_png(img, channels=3)\n",
    "    # crop the toe from top-left corner [image, offset_height y1, offset_width x1, target_height, target_width]\n",
    "    y2=y_offset_heel;    x2=0;    h=img_height;    w=img_width # not the pp location\n",
    "    img = tf.image.crop_to_bounding_box(img, int(y2), int(x2), h, w)\n",
    "    #img = tf.image.crop_to_bounding_box(img, int(y1), int(x1), int(y2)-int(y1), int(x2)-int(x1))\n",
    "    # resize the image to the desired size\n",
    "    return img\n",
    "\n",
    "def process_path_heel_test(file_name):\n",
    "    file_path = im_test + file_name\n",
    "    #label = get_label(file_path)\n",
    "    # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)#can read the byte string paths b'image_0001.png'\n",
    "    img = decode_crop_png_heel_test(img)\n",
    "    return img, file_name\n",
    "\n",
    "\n",
    "#\n",
    "# test how to put parameters to map\n",
    "#\n",
    "\n",
    "def t_ds_map(file_path,x1,y1,x2,y2):\n",
    "#     img = get_img('train/images/' + str(file_path))\n",
    "#     print(file_path)\n",
    "    return file_path,x1,y1,x2,y2 #img, [x1,y1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Toe ds\n",
    "test_ds_map_toe = list_ds_test.map(process_path_toe_test, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "# TEST Heel ds\n",
    "test_ds_map_heel = list_ds_test.map(process_path_heel_test, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, file_name in test_ds_map_toe.take(5):\n",
    "    print(f'take sample: {img.shape} {file_name}')\n",
    "    \n",
    "# print('f', f.dtype)\n",
    "# print('xy', xy.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare TEST_ds_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_for_performance_cache_test(ds, cache=True):\n",
    "\n",
    "    if cache:\n",
    "        ds = ds.cache()\n",
    "        print(\"Check cache in memory:Y\", cache)\n",
    "    else:\n",
    "        print(\"Check cache in memory:N\", cache)\n",
    "        \n",
    "#     if augment:\n",
    "# #         ds = ds.map(data_augment, num_parallel_calls=AUTOTUNE)\n",
    "#         ds = ds.map(AA, num_parallel_calls=AUTOTUNE)\n",
    "# #         ds = ds.map(RA, num_parallel_calls=AUTOTUNE)\n",
    "#         print(\"Check augment :Y\", augment)\n",
    "#     else:\n",
    "#         print(\"Check augment :N\", augment)\n",
    "    \n",
    "#     #ds = ds.repeat()#TODO:2020-12-14: test\n",
    "#     ds = ds.shuffle(buffer_size=MULTI_BATCH_SIZE*2) # (buffer_size=MULTI_BATCH_SIZE*5) 6sec,buffer_size=1000 take few sec. or buffer_size=image_count <- take too long # each take ds take 30~45 sec, TODO!!\n",
    "#     ds = ds.shuffle(len(list_ds), reshuffle_each_iteration=False) #todo: move to ds_pre. see https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle\n",
    "    ds = ds.batch(1000)# 1k for foot test images #MULTI_BATCH_SIZE for multi-GPUs\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE) #buffer_size=AUTOTUNE seem no speed improve\n",
    "    \n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prepare the ds properties (cache, augment, bs, shuffle, prefetch, etc.) for better performance.\n",
    "\"\"\"\n",
    "# TEST Toe ds_pre\n",
    "test_ds_pre_toe = configure_for_performance_cache_test(test_ds_map_toe)\n",
    "\n",
    "# TEST Heel ds_pre\n",
    "test_ds_pre_heel = configure_for_performance_cache_test(test_ds_map_heel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Best-K-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if K-models are in last time frame\n",
    "# best_model_name = get_best_model_name(th, K=str(k))\n",
    "\n",
    "predictions_toe = []\n",
    "predictions_heel = []\n",
    "\n",
    "for k in range(KFlodNum):\n",
    "    print(f'K:{k}')\n",
    "    best_model_toe_name = get_best_model_name('toe', K=str(k))\n",
    "    best_model_heel_name = get_best_model_name('heel', K=str(k))\n",
    "\n",
    "# # if models are in last time frame\n",
    "# best_model_toe_name = get_best_model_name('toe')\n",
    "# best_model_heel_name = get_best_model_name('heel')\n",
    "\n",
    "# # if toe/heel are in different time frame\n",
    "# best_model_toe_name = '20210118-212454/toe_EfficientNetB0_bs64_w120_best_val_loss.h5'#6.3318 @e393\n",
    "# best_model_heel_name = '20210122-084854/heel_EfficientNetB0_bs64_w120_best_val_loss.h5'#3.27979@152\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(best_model_toe_name)\n",
    "    print(best_model_heel_name)\n",
    "    # log_dir_name + '/' + 'leaf-2020-12-01-EfficientNetB7_top-layer50_lr_lrfn_val-acc.8352_wh512_e37.h5'\n",
    "\n",
    "    best_model_toe = tf.keras.models.load_model(best_model_toe_name,compile=False)\n",
    "    best_model_heel = tf.keras.models.load_model(best_model_heel_name,compile=False)\n",
    "    \n",
    "    best_model_toe.compile(optimizer = tf.keras.optimizers.Adam(),#RMSprop , Adam\n",
    "                loss=ed_metric_2d_mean)#, ed_loss ed_metric_2d ed_metric_2d_mean            \n",
    "                #metrics=['mae', 'accuracy'])\n",
    "    best_model_heel.compile(optimizer = tf.keras.optimizers.Adam(),#RMSprop , Adam\n",
    "                loss=ed_metric_2d_mean)#, ed_loss ed_metric_2d ed_metric_2d_mean            \n",
    "                #metrics=['mae', 'accuracy'])\n",
    "        \n",
    "        \n",
    "    pred_toe = best_model_toe.predict(test_ds_pre_toe)\n",
    "    pred_toe[:,1] = pred_toe[:,1] + y_offset_toe\n",
    "    predictions_toe.append(pred_toe)\n",
    "    \n",
    "    pred_heel = best_model_heel.predict(test_ds_pre_heel)\n",
    "    pred_heel[:,1] = pred_heel[:,1] + y_offset_heel\n",
    "    predictions_heel.append(pred_heel)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictions_toe[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure we got the k-pred as k models.\n",
    "for i, _ in enumerate(predictions_toe):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(predictions_toe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_toe[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_toe[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_heel[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_heel[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean the k-predictions\n",
    "k_predictions_toe = np.mean(predictions_toe, axis=0)\n",
    "k_predictions_toe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(k_predictions_toe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean the k-predictions\n",
    "k_predictions_heel = np.mean(predictions_heel, axis=0)\n",
    "k_predictions_heel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge toe/hell pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_th = np.append(k_predictions_toe, k_predictions_heel, axis=1)#左右接\n",
    "predictions_th.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_name = np.expand_dims(df_ts['images'], axis=1)\n",
    "images_name.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_merge = np.append(images_name, predictions_th, axis=1)#左右接\n",
    "predictions_merge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.DataFrame(predictions_merge)\n",
    "df_submission.columns = ['images','x1','y1','x2','y2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submi_name = CSVNAME + '.' + log_dir_name +'.csv'\n",
    "\n",
    "df_submission.to_csv(submi_name, index=False)\n",
    "print('Save {} as submission CSV.'.format(submi_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ED sum\n",
    "def get_valloss(his_v_l):  \n",
    "    return np.min(his_v_l), np.argmin(his_v_l)\n",
    "\n",
    "t_vl = []\n",
    "h_vl = []\n",
    "for k in range(KFlodNum):\n",
    "    print(f'K:{k}')\n",
    "    t_v, _ = get_valloss(history_toe_finetune[k].history['val_loss'])\n",
    "    h_v, _ = get_valloss(history_heel_finetune[k].history['val_loss'])\n",
    "    \n",
    "    t_vl.append(t_v)\n",
    "    h_vl.append(h_v)\n",
    "\n",
    "t_vl = np.mean(t_vl, axis=0)\n",
    "h_vl = np.mean(h_vl, axis=0)\n",
    "print(f'{round(t_vl,5)} + {round(h_vl,5)} = {round(t_vl + h_vl,5)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K134520210224-114845.csv\n",
    "# 5.63922 + 3.34466 = 8.98389 LB:8.4890610"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_vl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_vl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_timer.toc() #Time elapsed since t.tic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compile the model separately afterwards. to load model with custom loss function\n",
    "\n",
    "* https://github.com/tensorflow/tensorflow/issues/32348"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model_toe.compile(optimizer = tf.keras.optimizers.Adam(),#RMSprop , Adam\n",
    "#                 loss=ed_metric_2d_mean)#, ed_loss ed_metric_2d ed_metric_2d_mean            \n",
    "#                 #metrics=['mae', 'accuracy'])\n",
    "# best_model_heel.compile(optimizer = tf.keras.optimizers.Adam(),#RMSprop , Adam\n",
    "#                 loss=ed_metric_2d_mean)#, ed_loss ed_metric_2d ed_metric_2d_mean            \n",
    "#                 #metrics=['mae', 'accuracy'])\n",
    "\n",
    "# best_model_toe.compile(optimizer = tf.keras.optimizers.Adam(),#RMSprop , Adam\n",
    "#                 loss=tf.keras.losses.MeanSquaredError())#, ed_loss ed_metric_2d ed_metric_2d_mean            \n",
    "#                 #metrics=['mae', 'accuracy'])\n",
    "# best_model_heel.compile(optimizer = tf.keras.optimizers.Adam(),#RMSprop , Adam\n",
    "#                 loss=tf.keras.losses.MeanSquaredError())#, ed_loss ed_metric_2d ed_metric_2d_mean            \n",
    "#                 #metrics=['mae', 'accuracy'])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # inference all test_ds once\n",
    "# predictions_toe = best_model_toe.predict(test_ds_pre_toe)\n",
    "# predictions_toe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offset Toe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_toe[:,1] = predictions_toe[:,1] + y_offset_toe\n",
    "\n",
    "# # for [0,1]\n",
    "# # predictions_toe[:,0] = predictions_toe[:,0]*120\n",
    "# # predictions_toe[:,1] = predictions_toe[:,1]*120 + y_offset_toe\n",
    "\n",
    "# # # for [-1,1]\n",
    "# # # for re-scale back xy \n",
    "# # # return img, [(x1-60)/60,((y1-y_offset_toe)-60)/60]#normalized [-1,1] \n",
    "# # # return img, [(x2-60)/60,((y2-y_offset_heel)-60)/60]#normalized [-1,1] \n",
    "# # predictions_toe[:,0] = (predictions_toe[:,0]*60)+60\n",
    "# # predictions_toe[:,1] = (predictions_toe[:,1]*60)+60 + y_offset_toe\n",
    "\n",
    "# predictions_toe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # inference all test_ds once\n",
    "# predictions_heel = best_model_heel.predict(test_ds_pre_heel)\n",
    "# predictions_heel.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offset Heel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_heel[:,1] = predictions_heel[:,1] + y_offset_heel\n",
    "\n",
    "# # for [0,1]\n",
    "# # predictions_heel[:,0] = predictions_heel[:,0]*120\n",
    "# # predictions_heel[:,1] = predictions_heel[:,1]*120 + y_offset_heel\n",
    "\n",
    "# # # for [-1,1]\n",
    "# # predictions_heel[:,0] = (predictions_heel[:,0]*60)+60\n",
    "# # predictions_heel[:,1] = (predictions_heel[:,1]*60)+60 + y_offset_heel\n",
    "\n",
    "# predictions_heel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge toe/hell pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_th = np.append(predictions_toe, predictions_heel, axis=1)#左右接\n",
    "# predictions_th.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images_name = np.expand_dims(df_ts['images'], axis=1)\n",
    "# images_name.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_merge = np.append(images_name, predictions_th, axis=1)#左右接\n",
    "# predictions_merge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_submission = pd.DataFrame(predictions_merge)\n",
    "# df_submission.columns = ['images','x1','y1','x2','y2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submi_name = '0000_ft_' + log_dir_name +'.csv'\n",
    "# # submi_name = 'Bth_clr3_2690_XYnorm[0-1]_' + log_dir_name +'.csv'\n",
    "# df_submission.to_csv(submi_name, index=False)\n",
    "# print('Save {} as submission CSV.'.format(submi_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bth_clr3_2690_ed_findtune_20210202-141718.csv\n",
    "\n",
    "#toe.9.9/heel.4.4 109 trainable LB:9.3411759 比heel保持top-20略高0.04 (9.3084957)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #ED sum\n",
    "# def get_valloss(his_v_l):  \n",
    "#     return np.min(his_v_l), np.argmin(his_v_l)\n",
    "\n",
    "# t_vl, _ = get_valloss(history_toe_finetune.history['val_loss'])\n",
    "# h_vl, _ = get_valloss(history_heel_finetune.history['val_loss'])\n",
    "\n",
    "# print(f'{round(t_vl,5)} + {round(h_vl,5)} = {round(t_vl + h_vl,5)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# best_model_name = './cop_' + model_name + '_bs' + str(BATCH_SIZE) + '_w' + str(img_width) + '_e' + str(ep_num) + '_' + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + '_.h5'\n",
    "# # model.save(best_model_name)\n",
    "# print(\"Save model: \", best_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "multi output model:\n",
    "https://navoshta.com/end-to-end-deep-learning/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
