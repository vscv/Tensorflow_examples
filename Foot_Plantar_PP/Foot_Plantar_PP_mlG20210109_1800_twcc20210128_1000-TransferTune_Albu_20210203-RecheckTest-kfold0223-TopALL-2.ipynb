{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 101 Course of transfer learning and Fine tune 2021-01-01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [How-to] 1. k-fold for cross validation\n",
    "\n",
    "#### Create a simple k-fold for train classification model.\n",
    "\n",
    "* In this short course you learned:\n",
    "\n",
    "* data pipline\n",
    "\n",
    "* transfer learning\n",
    "\n",
    "* fine tune\n",
    "\n",
    "* callbacks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo: move to note.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.style.use(\"bmh\")\n",
    "\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "\n",
    "import errno\n",
    "\n",
    "# albumentations\n",
    "from functools import partial\n",
    "# from albumentations import (\n",
    "#     Compose, RandomBrightness, JpegCompression, HueSaturationValue, RandomContrast, HorizontalFlip,\n",
    "#     Rotate\n",
    "# )\n",
    "import albumentations as A\n",
    "\n",
    "# from adabelief_tf import AdaBeliefOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n",
      "4.3.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(cv2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytictoc import TicToc\n",
    "\n",
    "t_timer = TicToc() #create instance of class\n",
    "\n",
    "t_timer.tic() #Start timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image size, Batch size, toe/heel-offset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64 # 16 #32 #64 #128 #32 #64 #todo: reduce the BS maybe help to reduce the loss\n",
    "img_height = 120 #512 #224 #100\n",
    "img_width = 120 #512 #224 #100\n",
    "\n",
    "y_offset_toe = 80\n",
    "y_offset_heel = 280 #400-120=280"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf MirroredStrategy seting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "\n",
      "Number of REPLICAS: 1\n",
      "\n",
      "BATCH_SIZE: 64, MULTI_BATCH_SIZE: 64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# tf MirroredStrategy seting\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "REPLICAS = strategy.num_replicas_in_sync\n",
    "print('\\nNumber of REPLICAS: {}\\n'.format(REPLICAS))\n",
    "\n",
    "\n",
    "MULTI_BATCH_SIZE = BATCH_SIZE * strategy.num_replicas_in_sync\n",
    "print('BATCH_SIZE: {}, MULTI_BATCH_SIZE: {}'.format(BATCH_SIZE, MULTI_BATCH_SIZE))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自動調節tf.data管道\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create the training dataset W/ croped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load samples as data-farame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>images</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1120</th>\n",
       "      <td>image_6195.jpg</td>\n",
       "      <td>52</td>\n",
       "      <td>127</td>\n",
       "      <td>75</td>\n",
       "      <td>370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1121</th>\n",
       "      <td>image_6196.jpg</td>\n",
       "      <td>62</td>\n",
       "      <td>138</td>\n",
       "      <td>29</td>\n",
       "      <td>383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1122</th>\n",
       "      <td>image_6197.jpg</td>\n",
       "      <td>54</td>\n",
       "      <td>135</td>\n",
       "      <td>78</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123</th>\n",
       "      <td>image_6198.jpg</td>\n",
       "      <td>60</td>\n",
       "      <td>125</td>\n",
       "      <td>29</td>\n",
       "      <td>384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1124</th>\n",
       "      <td>image_6199.jpg</td>\n",
       "      <td>51</td>\n",
       "      <td>147</td>\n",
       "      <td>70</td>\n",
       "      <td>353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1340</th>\n",
       "      <td>image_6419.jpg</td>\n",
       "      <td>60</td>\n",
       "      <td>135</td>\n",
       "      <td>70</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1341</th>\n",
       "      <td>image_6421.jpg</td>\n",
       "      <td>53</td>\n",
       "      <td>157</td>\n",
       "      <td>76</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1342</th>\n",
       "      <td>image_6422.jpg</td>\n",
       "      <td>49</td>\n",
       "      <td>154</td>\n",
       "      <td>33</td>\n",
       "      <td>375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1343</th>\n",
       "      <td>image_6423.jpg</td>\n",
       "      <td>64</td>\n",
       "      <td>149</td>\n",
       "      <td>76</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1344</th>\n",
       "      <td>image_6424.jpg</td>\n",
       "      <td>55</td>\n",
       "      <td>147</td>\n",
       "      <td>36</td>\n",
       "      <td>385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>225 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              images  x1   y1  x2   y2\n",
       "1120  image_6195.jpg  52  127  75  370\n",
       "1121  image_6196.jpg  62  138  29  383\n",
       "1122  image_6197.jpg  54  135  78  380\n",
       "1123  image_6198.jpg  60  125  29  384\n",
       "1124  image_6199.jpg  51  147  70  353\n",
       "...              ...  ..  ...  ..  ...\n",
       "1340  image_6419.jpg  60  135  70  381\n",
       "1341  image_6421.jpg  53  157  76  376\n",
       "1342  image_6422.jpg  49  154  33  375\n",
       "1343  image_6423.jpg  64  149  76  381\n",
       "1344  image_6424.jpg  55  147  36  385\n",
       "\n",
       "[225 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# # data-org #\n",
    "# ann = 'annotation_1424_merge.csv'\n",
    "# im_p = 'train/images/'\n",
    "# CSVNAME = \"\"\n",
    "\n",
    "\n",
    "# # data-org-augm#\n",
    "# ann = 'annotation_2848_augm.csv'\n",
    "# im_p = 'train_augm/images/'\n",
    "# CSVNAME = \"\"\n",
    "\n",
    "\n",
    "# data-train # # current best dataset.1424-79.\n",
    "ann = 'annotation_1345_good.csv'\n",
    "im_p = 'train/images/'\n",
    "CSVNAME = \"K1345\"\n",
    "\n",
    "# data-augm #\n",
    "# ann = 'annotation_2690_augm.csv'\n",
    "# im_p = 'train_augm/images/'\n",
    "# CSVNAME = \"\"\n",
    "\n",
    "\n",
    "# # data-train-HPL-1123\n",
    "# ann = 'annotation_1123_HPL_Good.csv'\n",
    "# im_p = 'train/images/'\n",
    "# CSVNAME = \"\"\n",
    "\n",
    "\n",
    "df = pd.read_csv(ann)\n",
    "df[1120:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1345"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffle and reset index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index\n",
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>images</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>image_0772.png</td>\n",
       "      <td>55</td>\n",
       "      <td>137</td>\n",
       "      <td>70</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>image_6092.png</td>\n",
       "      <td>57</td>\n",
       "      <td>143</td>\n",
       "      <td>35</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>image_6228.jpg</td>\n",
       "      <td>60</td>\n",
       "      <td>147</td>\n",
       "      <td>81</td>\n",
       "      <td>383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>image_0394.JPG</td>\n",
       "      <td>68</td>\n",
       "      <td>120</td>\n",
       "      <td>30</td>\n",
       "      <td>384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>image_6116.png</td>\n",
       "      <td>64</td>\n",
       "      <td>138</td>\n",
       "      <td>40</td>\n",
       "      <td>366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>image_0688.png</td>\n",
       "      <td>63</td>\n",
       "      <td>129</td>\n",
       "      <td>84</td>\n",
       "      <td>384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>image_6160.jpg</td>\n",
       "      <td>57</td>\n",
       "      <td>145</td>\n",
       "      <td>36</td>\n",
       "      <td>385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>image_0086.png</td>\n",
       "      <td>61</td>\n",
       "      <td>140</td>\n",
       "      <td>26</td>\n",
       "      <td>366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>image_0079.png</td>\n",
       "      <td>52</td>\n",
       "      <td>141</td>\n",
       "      <td>86</td>\n",
       "      <td>383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>image_0341.png</td>\n",
       "      <td>56</td>\n",
       "      <td>124</td>\n",
       "      <td>84</td>\n",
       "      <td>383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           images  x1   y1  x2   y2\n",
       "0  image_0772.png  55  137  70  381\n",
       "1  image_6092.png  57  143  35  379\n",
       "2  image_6228.jpg  60  147  81  383\n",
       "3  image_0394.JPG  68  120  30  384\n",
       "4  image_6116.png  64  138  40  366\n",
       "5  image_0688.png  63  129  84  384\n",
       "6  image_6160.jpg  57  145  36  385\n",
       "7  image_0086.png  61  140  26  366\n",
       "8  image_0079.png  52  141  86  383\n",
       "9  image_0341.png  56  124  84  383"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See, the image_####.jpg now are random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create tf.dataset (DS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataframe\n",
    "list_ds = tf.data.Dataset.from_tensor_slices((df['images'], df['x1'], df['y1'], df['x2'], df['y2']))\n",
    "# list_ds = list_ds.shuffle(image_count, reshuffle_each_iteration=True) #todo: move to ds_pre. see https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1345"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_ds)#.shape() #take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check The type specification of an element of this dataset.\n",
    "list_ds.element_spec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "take sample: b'image_0772.png' 55 137 70 381\n",
      "take sample: b'image_6092.png' 57 143 35 379\n",
      "take sample: b'image_6228.jpg' 60 147 81 383\n",
      "take sample: b'image_0394.JPG' 68 120 30 384\n",
      "take sample: b'image_6116.png' 64 138 40 366\n"
     ]
    }
   ],
   "source": [
    "for f,x1,y1,x2,y2 in list_ds.take(5):\n",
    "    print(f'take sample: {f} {x1} {y1} {x2} {y2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_6116.png\n"
     ]
    }
   ],
   "source": [
    "# use np decode to UTF-8\n",
    "print(f.numpy().decode('UTF-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1345"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check ds iterator for consuming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Python iterator\n",
    "\n",
    "it_list_ds = iter(list_ds) # Make sure iter ds only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'image_0772.png' 55 137\n",
      "b'image_6092.png' 57 143\n",
      "b'image_6228.jpg' 60 147\n",
      "b'image_0394.JPG' 68 120\n"
     ]
    }
   ],
   "source": [
    "# using iter and consuming its elements using next: every print different image name.\n",
    "\n",
    "for i in range(4):\n",
    "    image, x1, y1, x2, y2 = next(it_list_ds)\n",
    "    print(image.numpy(), x1.numpy(), y1.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'image_0772.png' 55 137 70 381\n",
      "b'image_0772.png' 55 137 70 381\n",
      "b'image_0772.png' 55 137 70 381\n",
      "b'image_0772.png' 55 137 70 381\n",
      "===== Create iterator once and pull out to above cell. =====\n",
      "b'image_0772.png' 55 137 70 381\n",
      "b'image_6092.png' 57 143 35 379\n",
      "b'image_6228.jpg' 60 147 81 383\n",
      "b'image_0394.JPG' 68 120 30 384\n"
     ]
    }
   ],
   "source": [
    "# image_batch, label_batch = valid_ds_pre_s.as_numpy_iterator().next()\n",
    "# pred = model.predict_on_batch(image_batch)\n",
    "\n",
    "\n",
    "for i in range(4):\n",
    "    image, x1, y1, x2, y2 = list_ds.as_numpy_iterator().next()# every time create a new iter, so need put iter out of above cell.\n",
    "    print(image, x1, y1, x2, y2)\n",
    "    \n",
    "iter_test_list = list_ds.as_numpy_iterator()\n",
    "print(\"===== Create iterator once and pull out to above cell. =====\")\n",
    "for i in range(4):\n",
    "    image, x1, y1, x2, y2 = iter_test_list.next()\n",
    "    print(image, x1, y1, x2, y2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process path to image tensor in DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "def get_label(file_path):\n",
    "    # convert the path to a list of path components\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    # The second to last is the class-directory\n",
    "    boolen = parts[-2] == class_names\n",
    "    #one_hot_num = np.array(boolen, dtype=np.int) not works should use tf.x repalced.\n",
    "    one_hot_num = tf.dtypes.cast(boolen, tf.int64)\n",
    "    one_num = tf.argmax(one_hot_num)\n",
    "    print('one_num:', one_num)\n",
    "    # Integer encode the label\n",
    "    return one_num\n",
    "\n",
    "def decode_img(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    # resize the image to the desired size\n",
    "#     return tf.image.resize(img, [img_height, img_width])# augment 已經resize過一次了 但這邊不先做會比較慢\n",
    "    return tf.cast(tf.image.resize(img, [img_height, img_width]), tf.uint8)# 避免float over at augment\n",
    "'''\n",
    "\n",
    "#\n",
    "# map list to ds, Toe part.\n",
    "#\n",
    "\n",
    "def decode_crop_png_toe(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.io.decode_png(img, channels=3)\n",
    "    # crop the toe from top-left corner [image, offset_height y1, offset_width x1, target_height, target_width]\n",
    "    y1=y_offset_toe;    x1=0;    h=img_height;    w=img_width # not the pp location\n",
    "    img = tf.image.crop_to_bounding_box(img, int(y1), int(x1), h, w)\n",
    "    #img = tf.image.crop_to_bounding_box(img, int(y1), int(x1), int(y2)-int(y1), int(x2)-int(x1))\n",
    "    # resize the image to the desired size\n",
    "    return img\n",
    "\n",
    "def process_path_toe(file_path,x1,y1,x2,y2):\n",
    "    file_path = im_p + file_path\n",
    "    #label = get_label(file_path)\n",
    "    # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)#can read the byte string paths b'image_0001.png'\n",
    "    img = decode_crop_png_toe(img)\n",
    "    return img, [x1,y1-y_offset_toe]#Original [0,120]\n",
    "    #return img, x1, y1-y_offset_toe #Original [0,120] #貌似ed不用改，蛋mse變超大\n",
    "    #return img, [x1/120,(y1-y_offset_toe)/120]#normalized [0,1] xy <dtype: 'float64'>, no help.\n",
    "    #return img, [(x1-60)/60,((y1-y_offset_toe)-60)/60]#normalized [-1,1] , no help.\n",
    "\n",
    "#\n",
    "# map list to ds, Heel part.\n",
    "#\n",
    "\n",
    "def decode_crop_png_heel(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.io.decode_png(img, channels=3)\n",
    "    # crop the toe from top-left corner [image, offset_height y1, offset_width x1, target_height, target_width]\n",
    "    y2=y_offset_heel;    x2=0;    h=img_height;    w=img_width # not the pp location\n",
    "    img = tf.image.crop_to_bounding_box(img, int(y2), int(x2), h, w)\n",
    "    #img = tf.image.crop_to_bounding_box(img, int(y1), int(x1), int(y2)-int(y1), int(x2)-int(x1))\n",
    "    # resize the image to the desired size\n",
    "    return img\n",
    "\n",
    "def process_path_heel(file_path,x1,y1,x2,y2):\n",
    "    file_path = im_p + file_path\n",
    "    #label = get_label(file_path)\n",
    "    # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)#can read the byte string paths b'image_0001.png'\n",
    "    img = decode_crop_png_heel(img)\n",
    "    return img, [x2,y2-y_offset_heel]#Original [0,120]\n",
    "    #return img, x2, y2-y_offset_heel #Original [0,120] #貌似ed不用改，蛋mse變超大\n",
    "    #return img, [x2/120,(y2-y_offset_heel)/120]#normalized [0,1] xy <dtype: 'float64'>, no help.\n",
    "    #return img, [(x2-60)/60,((y2-y_offset_heel)-60)/60]#normalized [-1,1] , no help.\n",
    "\n",
    "#\n",
    "# test how to put parameters to map\n",
    "#\n",
    "\n",
    "def t_ds_map(file_path,x1,y1,x2,y2):\n",
    "#     img = get_img('train/images/' + str(file_path))\n",
    "#     print(file_path)\n",
    "    return file_path,x1,y1,x2,y2 #img, [x1,y1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toe ds\n",
    "train_ds_map_toe = list_ds.map(process_path_toe, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "# Heel ds\n",
    "train_ds_map_heel = list_ds.map(process_path_heel, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "take sample: 55 57\n",
      "f <dtype: 'string'>\n",
      "x <dtype: 'int64'>\n"
     ]
    }
   ],
   "source": [
    "# for img, xy in train_ds_map_toe.take(1):\n",
    "#     print(f'take sample: {xy}')\n",
    "    \n",
    "# print('f', f.dtype)\n",
    "# print('xy', xy.dtype)\n",
    "\n",
    "# for img, x, y in train_ds_map_toe.take(1):\n",
    "#     print(f'take sample: {x} {y}')\n",
    "    \n",
    "# print('img', img.dtype)\n",
    "# print('x', x.dtype)\n",
    "# x\n",
    "\n",
    "for img, [x, y] in train_ds_map_toe.take(1):\n",
    "    print(f'take sample: {x} {y}')\n",
    "    \n",
    "print('f', f.dtype)\n",
    "print('x', x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int64, numpy=55>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for f,x1,y1,x2,y2 in train_ds_map.take(5):\n",
    "#     print(f'take sample: {f} {x1} {y1} {x2} {y2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [new] Split train_ds_pre with ratio of validation %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ToDo 20210201] keep orignal validation in 0.1, but augmenting train_ds in input layer or in the tf.ds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2021-02-23] New k-split ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # split Toe\n",
    "# val_size = int(tf.data.experimental.cardinality(train_ds_map_toe).numpy() * 0.2)\n",
    "# # val_size = int(tf.data.experimental.cardinality(train_ds_map_toe).numpy() * 0.1)#no help\n",
    "\n",
    "# train_ds_map_toe_s = train_ds_map_toe.skip(val_size)\n",
    "# valid_ds_map_toe_s = train_ds_map_toe.take(val_size)\n",
    "\n",
    "# print(f'whole samples = {len(train_ds_map_toe)}')\n",
    "# print(f'val_size = {val_size}')\n",
    "\n",
    "# print('ds_train = ', tf.data.experimental.cardinality(train_ds_map_toe_s).numpy())\n",
    "# print('ds_valid = ', tf.data.experimental.cardinality(valid_ds_map_toe_s).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # split Heel\n",
    "# val_size = int(tf.data.experimental.cardinality(train_ds_map_heel).numpy() * 0.2)\n",
    "# # val_size = int(tf.data.experimental.cardinality(train_ds_map_heel).numpy() * 0.1)\n",
    "\n",
    "\n",
    "# train_ds_map_heel_s = train_ds_map_heel.skip(val_size)\n",
    "# valid_ds_map_heel_s = train_ds_map_heel.take(val_size)\n",
    "\n",
    "# print(len(train_ds_map_heel))\n",
    "# print(val_size)\n",
    "# print(tf.data.experimental.cardinality(train_ds_map_heel_s).numpy())\n",
    "# print(tf.data.experimental.cardinality(valid_ds_map_heel_s).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## testing cell\n",
    "# kf = []\n",
    "# for k in range(5):\n",
    "#     kf.append(train_ds_map_heel.shard(num_shards=5, index=k))\n",
    "#     print(\"k =\", k,\"num=\", tf.data.experimental.cardinality(kf[k]).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for img, [x,y] in kf[1].take(1):\n",
    "#     print(f'take sample: {x} {y}')\n",
    "    \n",
    "# print('img', img.dtype)\n",
    "# print('x', x.dtype)\n",
    "# print('y', y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## testing cell\n",
    "\n",
    "# range_k_0 = train_ds_map_heel.window(5)\n",
    "\n",
    "# print(len(range_k_0))\n",
    "# print(tf.data.experimental.cardinality(range_k_0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## testing cell\n",
    "\n",
    "# def get_train_valid_k_split():\n",
    "#     x = tf.data.Dataset.range(1000)\n",
    "#     val_size = int(tf.data.experimental.cardinality(x).numpy() * 0.2)\n",
    "    \n",
    "#     for k in range(5):\n",
    "#         train_num = x.take(val_size + k*val_size)\n",
    "#         valid_num = x.skip(k*val_size)\n",
    "        \n",
    "#         print(\"k=\", k)\n",
    "#         print(tf.data.experimental.cardinality(train_num).numpy())\n",
    "#         print(tf.data.experimental.cardinality(valid_num).numpy())\n",
    "    \n",
    "    \n",
    "# get_train_valid_k_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_size= 2\n",
      "k = 0 k*val_size+val_size 2 k_train num= 8\n",
      "k = 1 k*val_size+val_size 4 k_train num= 8\n",
      "k = 2 k*val_size+val_size 6 k_train num= 8\n",
      "k = 3 k*val_size+val_size 8 k_train num= 8\n",
      "k = 4 k*val_size+val_size 10 k_train num= 8\n"
     ]
    }
   ],
   "source": [
    "## testing cell\n",
    "\n",
    "# \n",
    "# tf.slice\n",
    "# tf.data.experimental.choose_from_datasets\n",
    "\n",
    "\n",
    "# x = list_ds\n",
    "# val_size = int(tf.data.experimental.cardinality(list_ds).numpy() * 0.2)\n",
    "x = tf.data.Dataset.range(10)\n",
    "\n",
    "def check_KFold_ds(x, K=5):\n",
    "    \n",
    "    val_size = int(tf.data.experimental.cardinality(x).numpy() * 0.2)\n",
    "    print(\"val_size=\", val_size)\n",
    "    \n",
    "    for k in range(K):\n",
    "#         k_train = x.take(val_size + k*val_size)\n",
    "#         k_valid = x.skip(k*val_size)\n",
    "#         k_train = tf.slice(x, k*val_size, val_size) #only for pure tensors not \n",
    "#         k_valid = x.skip(k*val_size)\n",
    "\n",
    "        # may skip twicce to performe kflod\n",
    "        t_take = x.take(k*val_size)\n",
    "        t_skip = x.skip(k*val_size+val_size)\n",
    "        k_train = t_take.concatenate(t_skip)\n",
    "        \n",
    "        v_skip = x.skip(k*val_size)\n",
    "        k_valid = v_skip.take(val_size)\n",
    "\n",
    "        print(\"k =\", k,\"k*val_size+val_size\", k*val_size+val_size, \"k_train num=\", tf.data.experimental.cardinality(k_train).numpy())\n",
    "\n",
    "\n",
    "        # x = tf.data.Dataset.range(10)\n",
    "#         for n in k_train:\n",
    "#             print(n.numpy())\n",
    "#         for n in k_valid:\n",
    "#             print(n.numpy())\n",
    "        \n",
    "        # list_ds\n",
    "#         for img, x1, y1, x2, y2 in k_train:\n",
    "#             print(x1, y1)\n",
    "\n",
    "        # train_ds_map_toe\n",
    "#         for img, (x, y) in k_train:\n",
    "#             print(x.numpy(), y.numpy())\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "check_KFold_ds(x)\n",
    "# check_KFold_ds(list_ds)    \n",
    "# check_KFold_ds(train_ds_map_toe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = tf.data.Dataset.range(10)\n",
    "# val_size = int(tf.data.experimental.cardinality(x).numpy() * 0.2)\n",
    "# print(\"val_size=\", val_size)\n",
    "\n",
    "def get_KFold_ds(x, K=0):\n",
    "        \n",
    "    k = K\n",
    "    # may skip twicce to perform kflod\n",
    "    # train ds\n",
    "    t_take = x.take(k*val_size)\n",
    "    t_skip = x.skip(k*val_size+val_size)\n",
    "    k_train = t_take.concatenate(t_skip)\n",
    "    # val ds\n",
    "    v_skip = x.skip(k*val_size)\n",
    "    k_valid = v_skip.take(val_size)\n",
    "\n",
    "    return k_train, k_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1076\n",
      "269\n"
     ]
    }
   ],
   "source": [
    "val_size = int(tf.data.experimental.cardinality(train_ds_map_toe).numpy() * 0.2)\n",
    "t, v = get_KFold_ds(train_ds_map_toe, 1)\n",
    "\n",
    "print(tf.data.experimental.cardinality(t).numpy())\n",
    "print(tf.data.experimental.cardinality(v).numpy())\n",
    "\n",
    "# for n in v:\n",
    "#     print(n.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ParallelMapDataset shapes: ((120, 120, 3), (2,)), types: (tf.uint8, tf.int64)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds_map_toe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset shapes: ((120, 120, 3), (2,)), types: (tf.uint8, tf.int64)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Albumentation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # for NO keypoint augment\n",
    "# transforms = A.Compose([\n",
    "# #             A.RandomBrightness(limit=0.1, p=0.5),\n",
    "#             A.JpegCompression(quality_lower=65, quality_upper=100, p=0.5),#A.JpegCompression(quality_lower=85, quality_upper=100, p=0.5)\n",
    "#             A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "# #             A.RandomContrast(limit=0.2, p=0.5),\n",
    "#             A.FancyPCA(alpha=0.1, always_apply=False, p=1),#A.FancyPCA(alpha=0.1, always_apply=False, p=0.5)\n",
    "#             A.Downscale(scale_min=0.7, scale_max=0.9, interpolation=0, always_apply=False, p=0.5), #0.8~0.99 may better\n",
    "#             A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, always_apply=False, p=0.5),\n",
    "#             A.CLAHE(clip_limit=(1, 8), tile_grid_size=(8, 8), always_apply=False, p=0.5), #A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), always_apply=False, p=0.5)\n",
    "# #             A.GlassBlur(sigma=0.9, max_delta=2, iterations=2, always_apply=False, mode='fast', p=0.5),\n",
    "# #             A.GaussNoise(var_limit=(10.0, 50.0), mean=0, always_apply=False, p=0.5),\n",
    "# #             A.GaussianBlur(blur_limit=(3, 7), sigma_limit=0, always_apply=False, p=0.5),\n",
    "# #             A.HorizontalFlip(),\n",
    "    \n",
    "#             # try other augm, seems to strong...\n",
    "#             A.RandomBrightnessContrast(always_apply=False, p=0.5, brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), brightness_by_max=True),\n",
    "#             A.Equalize(always_apply=False, p=0.5, mode='cv', by_channels=True),\n",
    "#             A.MultiplicativeNoise(always_apply=False, p=0.5, multiplier=(0.8, 1.5), per_channel=False, elementwise=False),\n",
    "#             A.RandomFog(always_apply=False, p=0.5, fog_coef_lower=0.2, fog_coef_upper=0.3, alpha_coef=0.25),\n",
    "\n",
    "# ])\n",
    "\n",
    "\n",
    "# def aug_fn(image, img_size):\n",
    "#     data = {\"image\":image}\n",
    "#     aug_data = transforms(**data)\n",
    "#     aug_img = aug_data[\"image\"]\n",
    "# #     aug_img = tf.cast(aug_img/255.0, tf.float32)\n",
    "#     aug_img = tf.cast(aug_img, tf.float32)\n",
    "#     aug_img = tf.image.resize(aug_img, size=[img_size, img_size])\n",
    "#     return aug_img\n",
    "\n",
    "# def process_data(image, label, img_size):\n",
    "#     aug_img = tf.numpy_function(func=aug_fn, inp=[image, img_size], Tout=tf.float32)\n",
    "#     return aug_img, label\n",
    "\n",
    "# def set_shapes(img, label, img_shape=(120,120,3)):\n",
    "#     img.set_shape(img_shape)\n",
    "# #     label.set_shape([]) # commited for go around error\n",
    "#     return img, label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # for NO keypoint augment AND for OneOf[] for better heel loss.\n",
    "# transforms_oneof = A.Compose(A.OneOf([\n",
    "#             A.RandomBrightness(limit=0.1, p=0.5),\n",
    "#             A.JpegCompression(quality_lower=85, quality_upper=100, p=0.5),\n",
    "#             A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "#             A.RandomContrast(limit=0.2, p=0.5),\n",
    "#             A.FancyPCA(alpha=0.1, always_apply=False, p=0.5),\n",
    "#             A.Downscale(scale_min=0.7, scale_max=0.9, interpolation=0, always_apply=False, p=0.5),\n",
    "#             A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, always_apply=False, p=0.5),\n",
    "#             A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), always_apply=False, p=0.5),\n",
    "#             A.GlassBlur(sigma=0.9, max_delta=2, iterations=2, always_apply=False, mode='fast', p=0.5),\n",
    "#             A.GaussNoise(var_limit=(10.0, 50.0), mean=0, always_apply=False, p=0.5),\n",
    "#             A.GaussianBlur(blur_limit=(3, 7), sigma_limit=0, always_apply=False, p=.5)\n",
    "# #             A.HorizontalFlip(),\n",
    "#             ]),p=0.5)\n",
    "\n",
    "\n",
    "# def aug_fn_oneof(image, img_size):\n",
    "#     data = {\"image\":image}\n",
    "#     aug_data = transforms_oneof(**data)\n",
    "#     aug_img = aug_data[\"image\"]\n",
    "# #     aug_img = tf.cast(aug_img/255.0, tf.float32)\n",
    "#     aug_img = tf.cast(aug_img, tf.float32)\n",
    "#     aug_img = tf.image.resize(aug_img, size=[img_size, img_size])\n",
    "#     return aug_img\n",
    "\n",
    "# def process_data_oneof(image, label, img_size):\n",
    "#     aug_img = tf.numpy_function(func=aug_fn_oneof, inp=[image, img_size], Tout=tf.float32)\n",
    "#     return aug_img, label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Testing keypoints augment\n",
    "# transforms = A.Compose([\n",
    "#             A.RandomBrightness(limit=0.1, p=0.5),\n",
    "#             A.JpegCompression(quality_lower=85, quality_upper=100, p=0.5),\n",
    "#             A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "#             A.RandomContrast(limit=0.2, p=0.5),\n",
    "#             A.FancyPCA (alpha=0.1, always_apply=False, p=1),\n",
    "#             A.Downscale (scale_min=0.7, scale_max=0.9, interpolation=0, always_apply=False, p=0.5),\n",
    "#             A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, always_apply=False, p=0.5),\n",
    "#             A.CLAHE (clip_limit=4.0, tile_grid_size=(8, 8), always_apply=False, p=0.5),\n",
    "#             A.HorizontalFlip(p=0.5),\n",
    "#             ]\n",
    "#             , \n",
    "#             keypoint_params=A.KeypointParams(format='xy'),  #currently not works for tf.ds yet.\n",
    "#             )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Testing keypoints augment\n",
    "transforms = A.Compose([\n",
    "            A.RandomBrightness(limit=0.1, p=0.5),\n",
    "            A.JpegCompression(quality_lower=85, quality_upper=100, p=0.5),\n",
    "            A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "            A.RandomContrast(limit=0.2, p=0.5),\n",
    "            A.FancyPCA(alpha=0.1, always_apply=False, p=0.5),\n",
    "            A.Downscale(scale_min=0.7, scale_max=0.9, interpolation=0, always_apply=False, p=0.5),\n",
    "            A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, always_apply=False, p=0.5),\n",
    "            A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), always_apply=False, p=0.5),\n",
    "    \n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.RandomResizedCrop(always_apply=False, height=120, width=120, scale=(0.9, 0.99), ratio=(1.0, 1.0), interpolation=0, p=0.5),#xy become double need change dtype of label. # pp will outside the image.\n",
    "            A.IAAAffine(scale=0.9, translate_percent=None, translate_px=None, rotate=0.0, shear=0.0, order=1, cval=0, mode='reflect', always_apply=False, p=0.5),\n",
    "#             A.ShiftScaleRotate(always_apply=False, p=0.5, shift_limit=(0.06, 0.06), scale_limit=(-0.1, 0.1), rotate_limit=(-5, 5), interpolation=1, border_mode=2, value=(0, 0, 0), mask_value=None),\n",
    "    #2021-02-26\n",
    "#             A.IAAPerspective(scale=(0.05, 0.1), keep_size=True, always_apply=False, p=0.5),#fallout image make train stop. NOT support keypoints!!!!!\n",
    "            ]\n",
    "            , \n",
    "            keypoint_params=A.KeypointParams(format='xy',remove_invisible=True),  #currently not works for tf.ds yet.\n",
    "            )\n",
    "\n",
    "# Testing keypoints augment\n",
    "# @tf.function\n",
    "def aug_fn(image, keypoints, img_size):\n",
    "#     print('Check keypoints aug_fun 00:', keypoints) # Check keypoints aug_fun 00: [[53 58]]\n",
    "#     data = {\"image\":image}\n",
    "    aug_data = transforms(image=image, keypoints=keypoints)\n",
    "    aug_img = aug_data[\"image\"]\n",
    "    aug_xy  = aug_data[\"keypoints\"]\n",
    "#     aug_img = tf.cast(aug_img/255.0, tf.float32)\n",
    "    aug_img = tf.cast(aug_img, tf.float32)\n",
    "    aug_img = tf.image.resize(aug_img, size=[img_size, img_size])\n",
    "    \n",
    "    aug_xy = tf.cast(aug_xy, tf.float32) #有些變形輸出是double\n",
    "#     print('Check aug_xy:', aug_xy) # Check aug_xy: [(95, 45)] #印到這邊都是對的\n",
    "    return aug_img, aug_xy \n",
    "\n",
    "# @tf.function\n",
    "def process_data(image, keypoints, img_size):\n",
    "    \n",
    "    print('Check keypoints process01:', keypoints, np.shape(keypoints), type(keypoints))\n",
    "        \n",
    "#     keypoints = tf.make_ndarray(keypoints)\n",
    "#     keypoints = np.array(keypoints)\n",
    "#     keypoints = list(keypoints)\n",
    "#     keypoints = np.asarray(keypoints, dtype=np.float32)\n",
    "#     keypoints = tf.make_ndarray(keypoints.op.get_attr('value'))\n",
    "\n",
    "#     keypoints = tf.reshape(keypoints, [1, 2])\n",
    "    keypoints = tf.reshape(keypoints, [1, 2]) # for 'convert_keypoint_to_albumentations'\n",
    "#     keypoints = np.reshape(keypoints, (1, 2))#not support tensor with np.call.\n",
    "\n",
    "    print('Check keypoints process02:', keypoints, np.shape(keypoints), type(keypoints))\n",
    "\n",
    "#     aug_img, aug_xy = tf.numpy_function(func=aug_fn, inp=[image, img_size], Tout=tf.float32)\n",
    "#     aug_img, aug_xy = tf.py_function(func=aug_fn, inp=[image, keypoints, img_size], Tout=[tf.float32, tf.int64])#for tensors.\n",
    "    aug_img, aug_xy = tf.numpy_function(func=aug_fn, inp=[image, keypoints, img_size], Tout=[tf.float32, tf.float32])\n",
    "    print('Check keypoints process03:', aug_xy)\n",
    "    \n",
    "    aug_xy = tf.reshape(aug_xy, [2,]) # for 'tf ds tarining'\n",
    "    print('Check keypoints process04:', aug_xy)\n",
    "        \n",
    "    return aug_img, aug_xy \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###  for AToe ###\n",
    "# some pp will outside of image bcs p2 is close to 400.\n",
    "\n",
    "\n",
    "\n",
    "# Testing keypoints augment\n",
    "transforms_AToe = A.Compose([\n",
    "#             A.RandomBrightness(limit=0.1, p=0.5),\n",
    "#             A.JpegCompression(quality_lower=85, quality_upper=100, p=0.5),\n",
    "#             A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "#             A.RandomContrast(limit=0.2, p=0.5),\n",
    "#             A.FancyPCA(alpha=0.1, always_apply=False, p=0.5),\n",
    "#             A.Downscale(scale_min=0.7, scale_max=0.9, interpolation=0, always_apply=False, p=0.5),\n",
    "#             A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, always_apply=False, p=0.5),\n",
    "#             A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), always_apply=False, p=0.5),\n",
    "    \n",
    "#             A.RandomBrightness(limit=0.1, p=0.5),\n",
    "            A.JpegCompression(quality_lower=65, quality_upper=100, p=0.5),#A.JpegCompression(quality_lower=85, quality_upper=100, p=0.5)\n",
    "            A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "#             A.RandomContrast(limit=0.2, p=0.5),\n",
    "            A.FancyPCA(alpha=0.1, always_apply=False, p=1),#A.FancyPCA(alpha=0.1, always_apply=False, p=0.5)\n",
    "            A.Downscale(scale_min=0.7, scale_max=0.9, interpolation=0, always_apply=False, p=0.5), #0.8~0.99 may better\n",
    "            A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, always_apply=False, p=0.5),\n",
    "            A.CLAHE(clip_limit=(1, 8), tile_grid_size=(8, 8), always_apply=False, p=0.5), #A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), always_apply=False, p=0.5)\n",
    "    \n",
    "            A.HorizontalFlip(p=0.5),\n",
    "#             A.RandomResizedCrop(always_apply=False, height=120, width=120, scale=(0.75, 0.9), ratio=(1.0, 1.0), interpolation=0, p=0.5),#xy become double need change dtype of label. # pp will outside the image.\n",
    "            A.IAAAffine (scale=0.9, translate_percent=None, translate_px=None, rotate=0.0, shear=0.0, order=1, cval=0, mode='reflect', always_apply=False, p=0.5),\n",
    "#             A.ShiftScaleRotate(always_apply=False, p=0.5, shift_limit=(0.06, 0.06), scale_limit=(-0.1, 0.1), rotate_limit=(-5, 5), interpolation=1, border_mode=2, value=(0, 0, 0), mask_value=None),\n",
    "            ]\n",
    "            , \n",
    "            keypoint_params=A.KeypointParams(format='xy',remove_invisible=True),  #currently not works for tf.ds yet.\n",
    "            )\n",
    "\n",
    "\n",
    "# Testing keypoints augment\n",
    "# @tf.function\n",
    "def aug_fn_AToe(image, keypoints, img_size):\n",
    "#     print('Check keypoints aug_fun 00:', keypoints) # Check keypoints aug_fun 00: [[53 58]]\n",
    "#     data = {\"image\":image}\n",
    "    aug_data = transforms_AToe(image=image, keypoints=keypoints)\n",
    "    aug_img = aug_data[\"image\"]\n",
    "    aug_xy  = aug_data[\"keypoints\"]\n",
    "#     aug_img = tf.cast(aug_img/255.0, tf.float32)\n",
    "    aug_img = tf.cast(aug_img, tf.float32)\n",
    "    aug_img = tf.image.resize(aug_img, size=[img_size, img_size])\n",
    "    \n",
    "    aug_xy = tf.cast(aug_xy, tf.float32) #有些變形輸出是double\n",
    "#     print('Check aug_xy:', aug_xy) # Check aug_xy: [(95, 45)] #印到這邊都是對的\n",
    "    return aug_img, aug_xy \n",
    "\n",
    "# @tf.function\n",
    "def process_data_AToe(image, keypoints, img_size):\n",
    "    \n",
    "    print('Check keypoints process01:', keypoints, np.shape(keypoints), type(keypoints))\n",
    "        \n",
    "#     keypoints = tf.make_ndarray(keypoints)\n",
    "#     keypoints = np.array(keypoints)\n",
    "#     keypoints = list(keypoints)\n",
    "#     keypoints = np.asarray(keypoints, dtype=np.float32)\n",
    "#     keypoints = tf.make_ndarray(keypoints.op.get_attr('value'))\n",
    "\n",
    "#     keypoints = tf.reshape(keypoints, [1, 2])\n",
    "    keypoints = tf.reshape(keypoints, [1, 2]) # for 'convert_keypoint_to_albumentations'\n",
    "#     keypoints = np.reshape(keypoints, (1, 2))#not support tensor with np.call.\n",
    "\n",
    "    print('Check keypoints process02:', keypoints, np.shape(keypoints), type(keypoints))\n",
    "\n",
    "#     aug_img, aug_xy = tf.numpy_function(func=aug_fn, inp=[image, img_size], Tout=tf.float32)\n",
    "#     aug_img, aug_xy = tf.py_function(func=aug_fn, inp=[image, keypoints, img_size], Tout=[tf.float32, tf.int64])#for tensors.\n",
    "    aug_img, aug_xy = tf.numpy_function(func=aug_fn_AToe, inp=[image, keypoints, img_size], Tout=[tf.float32, tf.float32])\n",
    "    print('Check keypoints process03:', aug_xy)\n",
    "    \n",
    "    aug_xy = tf.reshape(aug_xy, [2,]) # for 'tf ds tarining'\n",
    "    print('Check keypoints process04:', aug_xy)\n",
    "        \n",
    "    return aug_img, aug_xy \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def set_shapes(img, label, img_shape=(120,120,3)):\n",
    "    img.set_shape(img_shape)\n",
    "#     label.set_shape([]) # commited for go around error\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepare train_ds_prefetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_for_performance_cache_train(ds, cache=True, augment=False):\n",
    "\n",
    "    \n",
    "    \"\"\"#TODO: need to check the parse logic of ds.cache.\n",
    "    if cache:\n",
    "        print(\"Check cache-f1 to file:\", cache)\n",
    "        if isinstance(cache, str):\n",
    "            ds = ds.cache(cache)\n",
    "            print(\"Check cache-f2 to file:\", cache)\n",
    "    else:\n",
    "        ds = ds.cache()\n",
    "        print(\"Check cache in memory:\", cache)\n",
    "    \"\"\"    \n",
    "    if cache:\n",
    "        ds = ds.cache()\n",
    "        print(\"Check cache in memory:Y\", cache)\n",
    "    else:\n",
    "        print(\"Check cache in memory:N\", cache)\n",
    "        \n",
    "    if augment:\n",
    "        ds = ds.map(partial(process_data, img_size=120),num_parallel_calls=AUTOTUNE)\n",
    "        ds = ds.map(set_shapes, num_parallel_calls=AUTOTUNE)\n",
    "        print(\"Check augment :Y\", augment)\n",
    "    else:\n",
    "        print(\"Check augment :N\", augment)\n",
    "    \n",
    "    #ds = ds.repeat()#TODO:2020-12-14: test\n",
    "    #ds = ds.shuffle(buffer_size=MULTI_BATCH_SIZE*2) # (buffer_size=MULTI_BATCH_SIZE*5) 6sec,buffer_size=1000 take few sec. or buffer_size=image_count <- take too long # each take ds take 30~45 sec, TODO!!\n",
    "    ds = ds.shuffle(len(list_ds), reshuffle_each_iteration=True) #todo: move to ds_pre. see https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle\n",
    "    ds = ds.batch(MULTI_BATCH_SIZE)#MULTI_BATCH_SIZE for multi-GPUs\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE) #buffer_size=AUTOTUNE seem no speed improve\n",
    "    \n",
    "    print(\"Check ds cache[{}] and augment[{}]\".format(cache, augment))\n",
    "    \n",
    "    return ds\n",
    "\n",
    "\n",
    "def configure_for_performance_cache_train_AToe(ds, cache=True, augment=False):\n",
    "\n",
    "    \n",
    "    \"\"\"#TODO: need to check the parse logic of ds.cache.\n",
    "    if cache:\n",
    "        print(\"Check cache-f1 to file:\", cache)\n",
    "        if isinstance(cache, str):\n",
    "            ds = ds.cache(cache)\n",
    "            print(\"Check cache-f2 to file:\", cache)\n",
    "    else:\n",
    "        ds = ds.cache()\n",
    "        print(\"Check cache in memory:\", cache)\n",
    "    \"\"\"    \n",
    "    if cache:\n",
    "        ds = ds.cache()\n",
    "        print(\"Check cache in memory:Y\", cache)\n",
    "    else:\n",
    "        print(\"Check cache in memory:N\", cache)\n",
    "        \n",
    "    if augment:\n",
    "        ds = ds.map(partial(process_data_AToe, img_size=120),num_parallel_calls=AUTOTUNE)\n",
    "        ds = ds.map(set_shapes, num_parallel_calls=AUTOTUNE)\n",
    "        print(\"Check augment :Y\", augment)\n",
    "    else:\n",
    "        print(\"Check augment :N\", augment)\n",
    "    \n",
    "    #ds = ds.repeat()#TODO:2020-12-14: test\n",
    "    #ds = ds.shuffle(buffer_size=MULTI_BATCH_SIZE*2) # (buffer_size=MULTI_BATCH_SIZE*5) 6sec,buffer_size=1000 take few sec. or buffer_size=image_count <- take too long # each take ds take 30~45 sec, TODO!!\n",
    "    ds = ds.shuffle(len(list_ds), reshuffle_each_iteration=True) #todo: move to ds_pre. see https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle\n",
    "    ds = ds.batch(MULTI_BATCH_SIZE)#MULTI_BATCH_SIZE for multi-GPUs\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE) #buffer_size=AUTOTUNE seem no speed improve\n",
    "    \n",
    "    print(\"Check ds cache[{}] and augment[{}]\".format(cache, augment))\n",
    "    \n",
    "    return ds\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def configure_for_performance_cache_train_oneof(ds, cache=True, augment=False):\n",
    "\n",
    "    \n",
    "#     \"\"\"#TODO: need to check the parse logic of ds.cache.\n",
    "#     if cache:\n",
    "#         print(\"Check cache-f1 to file:\", cache)\n",
    "#         if isinstance(cache, str):\n",
    "#             ds = ds.cache(cache)\n",
    "#             print(\"Check cache-f2 to file:\", cache)\n",
    "#     else:\n",
    "#         ds = ds.cache()\n",
    "#         print(\"Check cache in memory:\", cache)\n",
    "#     \"\"\"    \n",
    "#     if cache:\n",
    "#         ds = ds.cache()\n",
    "#         print(\"Check cache in memory:Y\", cache)\n",
    "#     else:\n",
    "#         print(\"Check cache in memory:N\", cache)\n",
    "        \n",
    "#     if augment:\n",
    "#         ds = ds.map(partial(process_data_oneof, img_size=120),num_parallel_calls=AUTOTUNE)\n",
    "#         ds = ds.map(set_shapes, num_parallel_calls=AUTOTUNE)\n",
    "#         print(\"Check augment :Y\", augment)\n",
    "#     else:\n",
    "#         print(\"Check augment :N\", augment)\n",
    "    \n",
    "#     #ds = ds.repeat()#TODO:2020-12-14: test\n",
    "#     #ds = ds.shuffle(buffer_size=MULTI_BATCH_SIZE*2) # (buffer_size=MULTI_BATCH_SIZE*5) 6sec,buffer_size=1000 take few sec. or buffer_size=image_count <- take too long # each take ds take 30~45 sec, TODO!!\n",
    "#     ds = ds.shuffle(len(list_ds), reshuffle_each_iteration=True) #todo: move to ds_pre. see https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle\n",
    "#     ds = ds.batch(MULTI_BATCH_SIZE)#MULTI_BATCH_SIZE for multi-GPUs\n",
    "#     ds = ds.prefetch(buffer_size=AUTOTUNE) #buffer_size=AUTOTUNE seem no speed improve\n",
    "    \n",
    "#     print(\"Check ds cache[{}] and augment[{}]\".format(cache, augment))\n",
    "    \n",
    "#     return ds\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def configure_for_performance_cache_val(ds, cache=True, augment=False):\n",
    "\n",
    "    \n",
    "    \"\"\"#TODO: need to check the parse logic of ds.cache\n",
    "    TODO:test remove ds.shuffle from val_ds.\n",
    "    .\n",
    "    if cache:\n",
    "        print(\"Check cache-f1 to file:\", cache)\n",
    "        if isinstance(cache, str):\n",
    "            ds = ds.cache(cache)\n",
    "            print(\"Check cache-f2 to file:\", cache)\n",
    "    else:\n",
    "        ds = ds.cache()\n",
    "        print(\"Check cache in memory:\", cache)\n",
    "    \"\"\"    \n",
    "    if cache:\n",
    "        ds = ds.cache()\n",
    "        print(\"Check cache in memory:Y\", cache)\n",
    "    else:\n",
    "        print(\"Check cache in memory:N\", cache)\n",
    "        \n",
    "    if augment:\n",
    "#         ds = ds.map(data_augment, num_parallel_calls=AUTOTUNE)\n",
    "        ds = ds.map(AA, num_parallel_calls=AUTOTUNE)\n",
    "#         ds = ds.map(RA, num_parallel_calls=AUTOTUNE)\n",
    "        print(\"Check augment :Y\", augment)\n",
    "    else:\n",
    "        print(\"Check augment :N\", augment)\n",
    "    \n",
    "    #ds = ds.repeat()#TODO:2020-12-14: test\n",
    "    #ds = ds.shuffle(buffer_size=MULTI_BATCH_SIZE*2) # (buffer_size=MULTI_BATCH_SIZE*5) 6sec,buffer_size=1000 take few sec. or buffer_size=image_count <- take too long # each take ds take 30~45 sec, TODO!!\n",
    "#     ds = ds.shuffle(len(list_ds), reshuffle_each_iteration=False) #todo: move to ds_pre. see https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle\n",
    "    ds = ds.batch(MULTI_BATCH_SIZE)#MULTI_BATCH_SIZE for multi-GPUs\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE) #buffer_size=AUTOTUNE seem no speed improve\n",
    "    \n",
    "    print(\"Check ds cache[{}] and augment[{}]\".format(cache, augment))\n",
    "    \n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Prepare the ds properties (cache, augment, bs, shuffle, prefetch, etc.) for better performance.\n",
    "# \"\"\"\n",
    "# # Toe ds_pre\n",
    "# train_ds_pre_toe = configure_for_performance_cache_train(train_ds_map_toe)\n",
    "\n",
    "# # Heel ds_pre\n",
    "# train_ds_pre_heel = configure_for_performance_cache_val(train_ds_map_heel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All split ds_prefetch\n",
    "* train_ds_map_toe_s = train_ds_map_toe.skip(val_size)\n",
    "* valid_ds_map_toe_s = train_ds_map_toe.take(val_size)\n",
    "\n",
    "* train_ds_map_heel_s = train_ds_map_heel.skip(val_size)\n",
    "* valid_ds_map_heel_s = train_ds_map_heel.take(val_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Prepare the ds properties (cache, augment, bs, shuffle, prefetch, etc.) for better performance.\n",
    "# \"\"\"\n",
    "# # Toe ds_pre\n",
    "# train_ds_pre_toe_s = configure_for_performance_cache_train(train_ds_map_toe_s, augment=True)\n",
    "# valid_ds_pre_toe_s = configure_for_performance_cache_val(valid_ds_map_toe_s)\n",
    "\n",
    "# # Heel ds_pre\n",
    "# train_ds_pre_heel_s = configure_for_performance_cache_train(train_ds_map_heel_s, augment=True)\n",
    "# valid_ds_pre_heel_s = configure_for_performance_cache_val(valid_ds_map_heel_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check ds_prefetch samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create it_ds once\n",
    "# it_train_ds_pre_toe_s = iter(train_ds_pre_toe_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # for albu keypoint\n",
    "\n",
    "# # for original return aug_img, , aug_xy \n",
    "\n",
    "\n",
    "# image_batch, label_batch = next(it_train_ds_pre_toe_s)\n",
    "\n",
    "# fig = plt.figure(figsize=(8, 8))\n",
    "# print('batch * multi:', len(label_batch), ', MULTI_BATCH_SIZE=', MULTI_BATCH_SIZE)\n",
    "# for i in range(16):\n",
    "#     ax = plt.subplot(4, 4, i + 1)\n",
    "#     plt.imshow(image_batch[i].numpy().astype(\"uint8\"))\n",
    "#     plt.title(label_batch[i].numpy())\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.plot(label_batch[i].numpy()[0], label_batch[i].numpy()[1], 'r+', markersize=13, mew=2.5)\n",
    "\n",
    "#     print(f'Check lables: {label_batch[i]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create it_ds once\n",
    "# it_train_ds_pre_heel_s = iter(train_ds_pre_heel_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # for albu keypoint\n",
    "\n",
    "# # for original return aug_img, , aug_xy \n",
    "\n",
    "\n",
    "# image_batch, label_batch = next(it_train_ds_pre_heel_s)\n",
    "\n",
    "# fig = plt.figure(figsize=(8, 8))\n",
    "# print('batch * multi:', len(label_batch), ', MULTI_BATCH_SIZE=', MULTI_BATCH_SIZE)\n",
    "# for i in range(16):\n",
    "#     ax = plt.subplot(4, 4, i + 1)\n",
    "#     plt.imshow(image_batch[i].numpy().astype(\"uint8\"))\n",
    "#     plt.title(label_batch[i].numpy())\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.plot(label_batch[i].numpy()[0], label_batch[i].numpy()[1], 'r+', markersize=13, mew=2.5)\n",
    "\n",
    "#     print(f'Check lables: {label_batch[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create it_ds once\n",
    "# it_valid_ds_pre_toe_s = iter(valid_ds_pre_toe_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # for albu keypoint\n",
    "\n",
    "# # for return aug_img, aug_xy \n",
    "\n",
    "\n",
    "# image_batch, label_batch = next(it_valid_ds_pre_toe_s)\n",
    "\n",
    "# plt.figure(figsize=(8, 8))\n",
    "# # for images, labels in valid_ds_pre_toe_s.take(1):\n",
    "# print('batch * multi:', len(label_batch), ', MULTI_BATCH_SIZE=', MULTI_BATCH_SIZE)\n",
    "# for i in range(16):\n",
    "#     ax = plt.subplot(4, 4, i + 1)\n",
    "#     plt.imshow(image_batch[i].numpy().astype(\"uint8\"))\n",
    "#     plt.title(label_batch[i].numpy())\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.plot(label_batch[i].numpy()[0], label_batch[i].numpy()[1], 'r+', markersize=13, mew=2.5)\n",
    "\n",
    "#     print(f'Check lables: {label_batch[i]}')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Loss function\n",
    "\n",
    "* mae\n",
    "* euclidean distance\n",
    "* others\n",
    "\n",
    "\n",
    "        # 'x' is [[1, 1, 1]\n",
    "        #         [1, 1, 1]]\n",
    "        tf.reduce_sum(x) ==> 6\n",
    "        tf.reduce_sum(x, 0) ==> [2, 2, 2]\n",
    "        tf.reduce_sum(x, 1) ==> [3, 3]\n",
    "        the function is default for 2-D array, therefor, in our 1-D [x1,y1] to [x2,y2] the axis should be '0' or just leave it.\n",
    "        \n",
    "        tf.sqrt need \tA tf.Tensor of type bfloat16, half, float32, float64, complex64, complex128\n",
    "        so, convert it first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "should be -> tf.Tensor([56 39], shape=(2,), dtype=int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [10, 10]\n",
    "y_pred = [10, 20]\n",
    "\n",
    "# y_true = [1.00000000000000000000123, 10]\n",
    "# y_pred = [1.0, 10.000000000000000000000000001]\n",
    "\n",
    "# y_true = [1.0000123, 10]\n",
    "# y_pred = [1.0, 10.0000321]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=5>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mae\n",
    "\n",
    "loss_mae = tf.keras.losses.MAE(\n",
    "    y_true, y_pred\n",
    ")\n",
    "\n",
    "loss_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=10.0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ed\n",
    "\n",
    "# loss_ed = tf.sqrt(tf.reduce_sum(tf.square(tf.constant(y_true) - tf.constant(y_pred)), 0))\n",
    "\n",
    "# loss_ed = tf.sqrt(tf.reduce_sum(tf.square(tf.Variable(y_true) - tf.Variable(y_pred)), 0))\n",
    "\n",
    "loss_ed = tf.sqrt(tf.reduce_sum(tf.square(tf.cast(y_true, tf.float32) - tf.cast(y_pred, tf.float32)), 0))\n",
    "\n",
    "loss_ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ed_loss(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_sum(tf.square(tf.cast(y_true, tf.float32) - tf.cast(y_pred, tf.float32)), 0))\n",
    "\n",
    "# fix NaN in euclidean distance\n",
    "# tf.maximum(d, 1e-9), to keep atlease is 1e-9.\n",
    "# def ed_loss(y_true, y_pred):\n",
    "#     return tf.sqrt(tf.maximum(tf.reduce_sum(tf.square(tf.cast(y_true, tf.float32) - tf.cast(y_pred, tf.float32)), 0), 1e-9))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the euclidean distance loss\n",
    "ed_loss(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=10.0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ed_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mean Euclidean distance \n",
    "\n",
    "* here the y_true and y_pred is 2-D array. the axis use 1.\n",
    "\n",
    "\n",
    "* NOTE: LB評分的mean euclidean distance功能，應該跟model.evaluate()一樣so不需重新寫。evaluate()會自動用loss (model.metrics_names)計算後在自動平均，而模型loss我們是用ed-loss取代。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_true = [[60, 76],\n",
    "#        [58, 49 ],\n",
    "#        [63, 67 ],\n",
    "#        [58 , 57]]\n",
    "# y_pred = [[59.927303, 76.471214],\n",
    "#        [58.056904, 49.98754 ],\n",
    "#        [63.067844, 67.03861 ],\n",
    "#        [58.70202 , 57.372707]]\n",
    "\n",
    "y_true = [[60, 70],\n",
    "       [70, 80]]\n",
    "y_pred = [[61, 71],\n",
    "       [72, 82]]\n",
    "\n",
    "# y_true = [(60, 70),\n",
    "#        (70, 80)]\n",
    "# y_pred = [(61, 71),\n",
    "#        (72, 82)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[1., 1.],\n",
       "       [4., 4.]], dtype=float32)>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.square(tf.cast(y_true, tf.float32) - tf.cast(y_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([2., 8.], dtype=float32)>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(tf.square(tf.cast(y_true, tf.float32) - tf.cast(y_pred, tf.float32)), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ed_metric_2d(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_sum(tf.square(tf.cast(y_true, tf.float32) - tf.cast(y_pred, tf.float32)), 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.4142135, 2.828427 ], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ed_metric_2d(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1.4142135, 2.828427 ], dtype=float32)>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ed_metric_2d(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 等於true, pred點位ed的平均，LB評分方式。(toe/heel即p1,p2要個別算ed一次再相加)\n",
    "def ed_metric_2d_mean(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.sqrt(tf.reduce_sum(tf.square(tf.cast(y_true, tf.float32) - tf.cast(y_pred, tf.float32)), 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for re-scale back xy \n",
    "# return img, [(x1-60)/60,((y1-y_offset_toe)-60)/60]#normalized [-1,1] \n",
    "# return img, [(x2-60)/60,((y2-y_offset_toe)-60)/60]#normalized [-1,1] \n",
    "\n",
    "# 等於true, pred點位ed的平均，LB評分方式。(toe/heel即p1,p2要個別算ed一次再相加)\n",
    "def edRescal(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.sqrt(tf.reduce_sum(tf.square(tf.cast((y_true*60)+60, tf.float32) - tf.cast((y_pred*60)+60, tf.float32)), 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1213202"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ed_metric_2d_mean(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=2.1213202>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ed_metric_2d_mean(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EFNE Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maybe mae better than ed loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe96040c400>]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"dump lr\n",
    "\"\"\"\n",
    "ep_num_transf = 500\n",
    "\n",
    "\n",
    "\n",
    "def lrdump(epoch):\n",
    "    \n",
    "    #step_size = 100\n",
    "    lr_max = 0.006\n",
    "    lr_min = 0.001\n",
    "    lr_start = 0.01\n",
    "\n",
    "    lr_init_ep = 0\n",
    "    lr_ramp_ep = 100\n",
    "    lr_sus_ep  = 0\n",
    "    lr_decay   = 0.8\n",
    "\n",
    "    \n",
    "    # warm up\n",
    "    if epoch < lr_init_ep:\n",
    "        lr = (lr_max - lr_min) / lr_ramp_ep * epoch + lr_min    \n",
    "        \n",
    "    elif lr_init_ep -1 < epoch < lr_ramp_ep:\n",
    "        lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "\n",
    "    elif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "        lr = lr_max\n",
    "\n",
    "    else:\n",
    "        lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
    "\n",
    "    return lr\n",
    "\n",
    "rng = [i for i in range(ep_num_transf)]\n",
    "y = [lrdump(x) for x in rng]\n",
    "sns.set(style='darkgrid')\n",
    "fig, ax = plt.subplots(figsize=(20, 6))\n",
    "plt.plot(rng, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 ~ 0.01\n"
     ]
    }
   ],
   "source": [
    "print('{} ~ {}'.format(min(y), max(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t 0.01\n",
      "\n",
      "1\t 0.00996\n",
      "\n",
      "2\t 0.00992\n",
      "\n",
      "3\t 0.00988\n",
      "\n",
      "4\t 0.00984\n",
      "\n",
      "5\t 0.0098\n",
      "\n",
      "6\t 0.00976\n",
      "\n",
      "7\t 0.00972\n",
      "\n",
      "8\t 0.00968\n",
      "\n",
      "9\t 0.009640000000000001\n",
      "\n",
      "10\t 0.009600000000000001\n",
      "\n",
      "11\t 0.00956\n",
      "\n",
      "12\t 0.00952\n",
      "\n",
      "13\t 0.00948\n",
      "\n",
      "14\t 0.00944\n",
      "\n",
      "15\t 0.0094\n",
      "\n",
      "16\t 0.00936\n",
      "\n",
      "17\t 0.00932\n",
      "\n",
      "18\t 0.00928\n",
      "\n",
      "19\t 0.00924\n",
      "\n",
      "20\t 0.0092\n",
      "\n",
      "21\t 0.00916\n",
      "\n",
      "22\t 0.00912\n",
      "\n",
      "23\t 0.00908\n",
      "\n",
      "24\t 0.00904\n",
      "\n",
      "25\t 0.009000000000000001\n",
      "\n",
      "26\t 0.00896\n",
      "\n",
      "27\t 0.00892\n",
      "\n",
      "28\t 0.00888\n",
      "\n",
      "29\t 0.00884\n",
      "\n",
      "30\t 0.0088\n",
      "\n",
      "31\t 0.00876\n",
      "\n",
      "32\t 0.00872\n",
      "\n",
      "33\t 0.00868\n",
      "\n",
      "34\t 0.00864\n",
      "\n",
      "35\t 0.0086\n",
      "\n",
      "36\t 0.00856\n",
      "\n",
      "37\t 0.00852\n",
      "\n",
      "38\t 0.00848\n",
      "\n",
      "39\t 0.00844\n",
      "\n",
      "40\t 0.0084\n",
      "\n",
      "41\t 0.00836\n",
      "\n",
      "42\t 0.008320000000000001\n",
      "\n",
      "43\t 0.00828\n",
      "\n",
      "44\t 0.00824\n",
      "\n",
      "45\t 0.0082\n",
      "\n",
      "46\t 0.00816\n",
      "\n",
      "47\t 0.00812\n",
      "\n",
      "48\t 0.00808\n",
      "\n",
      "49\t 0.00804\n",
      "\n",
      "50\t 0.008\n",
      "\n",
      "51\t 0.00796\n",
      "\n",
      "52\t 0.00792\n",
      "\n",
      "53\t 0.00788\n",
      "\n",
      "54\t 0.00784\n",
      "\n",
      "55\t 0.0078\n",
      "\n",
      "56\t 0.0077599999999999995\n",
      "\n",
      "57\t 0.007719999999999999\n",
      "\n",
      "58\t 0.00768\n",
      "\n",
      "59\t 0.00764\n",
      "\n",
      "60\t 0.0076\n",
      "\n",
      "61\t 0.00756\n",
      "\n",
      "62\t 0.007520000000000001\n",
      "\n",
      "63\t 0.0074800000000000005\n",
      "\n",
      "64\t 0.00744\n",
      "\n",
      "65\t 0.0074\n",
      "\n",
      "66\t 0.00736\n",
      "\n",
      "67\t 0.00732\n",
      "\n",
      "68\t 0.00728\n",
      "\n",
      "69\t 0.00724\n",
      "\n",
      "70\t 0.0072\n",
      "\n",
      "71\t 0.00716\n",
      "\n",
      "72\t 0.00712\n",
      "\n",
      "73\t 0.0070799999999999995\n",
      "\n",
      "74\t 0.007039999999999999\n",
      "\n",
      "75\t 0.007\n",
      "\n",
      "76\t 0.00696\n",
      "\n",
      "77\t 0.00692\n",
      "\n",
      "78\t 0.00688\n",
      "\n",
      "79\t 0.006840000000000001\n",
      "\n",
      "80\t 0.0068000000000000005\n",
      "\n",
      "81\t 0.00676\n",
      "\n",
      "82\t 0.00672\n",
      "\n",
      "83\t 0.00668\n",
      "\n",
      "84\t 0.00664\n",
      "\n",
      "85\t 0.0066\n",
      "\n",
      "86\t 0.00656\n",
      "\n",
      "87\t 0.00652\n",
      "\n",
      "88\t 0.00648\n",
      "\n",
      "89\t 0.0064399999999999995\n",
      "\n",
      "90\t 0.0063999999999999994\n",
      "\n",
      "91\t 0.006359999999999999\n",
      "\n",
      "92\t 0.00632\n",
      "\n",
      "93\t 0.00628\n",
      "\n",
      "94\t 0.00624\n",
      "\n",
      "95\t 0.0062\n",
      "\n",
      "96\t 0.00616\n",
      "\n",
      "97\t 0.0061200000000000004\n",
      "\n",
      "98\t 0.0060799999999999995\n",
      "\n",
      "99\t 0.00604\n",
      "\n",
      "100\t 0.006\n",
      "\n",
      "101\t 0.005\n",
      "\n",
      "102\t 0.004200000000000001\n",
      "\n",
      "103\t 0.0035600000000000007\n",
      "\n",
      "104\t 0.0030480000000000004\n",
      "\n",
      "105\t 0.0026384000000000004\n",
      "\n",
      "106\t 0.0023107200000000005\n",
      "\n",
      "107\t 0.0020485760000000007\n",
      "\n",
      "108\t 0.0018388608000000006\n",
      "\n",
      "109\t 0.0016710886400000003\n",
      "\n",
      "110\t 0.0015368709120000003\n",
      "\n",
      "111\t 0.0014294967296000004\n",
      "\n",
      "112\t 0.0013435973836800003\n",
      "\n",
      "113\t 0.0012748779069440002\n",
      "\n",
      "114\t 0.0012199023255552001\n",
      "\n",
      "115\t 0.0011759218604441603\n",
      "\n",
      "116\t 0.0011407374883553281\n",
      "\n",
      "117\t 0.0011125899906842625\n",
      "\n",
      "118\t 0.0010900719925474101\n",
      "\n",
      "119\t 0.001072057594037928\n",
      "\n",
      "120\t 0.0010576460752303425\n",
      "\n",
      "121\t 0.001046116860184274\n",
      "\n",
      "122\t 0.001036893488147419\n",
      "\n",
      "123\t 0.0010295147905179354\n",
      "\n",
      "124\t 0.0010236118324143482\n",
      "\n",
      "125\t 0.0010188894659314785\n",
      "\n",
      "126\t 0.0010151115727451828\n",
      "\n",
      "127\t 0.0010120892581961464\n",
      "\n",
      "128\t 0.001009671406556917\n",
      "\n",
      "129\t 0.0010077371252455336\n",
      "\n",
      "130\t 0.0010061897001964269\n",
      "\n",
      "131\t 0.0010049517601571415\n",
      "\n",
      "132\t 0.0010039614081257132\n",
      "\n",
      "133\t 0.0010031691265005706\n",
      "\n",
      "134\t 0.0010025353012004564\n",
      "\n",
      "135\t 0.0010020282409603652\n",
      "\n",
      "136\t 0.001001622592768292\n",
      "\n",
      "137\t 0.0010012980742146336\n",
      "\n",
      "138\t 0.001001038459371707\n",
      "\n",
      "139\t 0.0010008307674973657\n",
      "\n",
      "140\t 0.0010006646139978925\n",
      "\n",
      "141\t 0.001000531691198314\n",
      "\n",
      "142\t 0.0010004253529586511\n",
      "\n",
      "143\t 0.001000340282366921\n",
      "\n",
      "144\t 0.0010002722258935367\n",
      "\n",
      "145\t 0.0010002177807148294\n",
      "\n",
      "146\t 0.0010001742245718634\n",
      "\n",
      "147\t 0.001000139379657491\n",
      "\n",
      "148\t 0.0010001115037259927\n",
      "\n",
      "149\t 0.001000089202980794\n",
      "\n",
      "150\t 0.0010000713623846353\n",
      "\n",
      "151\t 0.0010000570899077082\n",
      "\n",
      "152\t 0.0010000456719261666\n",
      "\n",
      "153\t 0.0010000365375409334\n",
      "\n",
      "154\t 0.0010000292300327467\n",
      "\n",
      "155\t 0.0010000233840261974\n",
      "\n",
      "156\t 0.0010000187072209579\n",
      "\n",
      "157\t 0.0010000149657767663\n",
      "\n",
      "158\t 0.001000011972621413\n",
      "\n",
      "159\t 0.0010000095780971303\n",
      "\n",
      "160\t 0.0010000076624777044\n",
      "\n",
      "161\t 0.0010000061299821634\n",
      "\n",
      "162\t 0.0010000049039857308\n",
      "\n",
      "163\t 0.0010000039231885846\n",
      "\n",
      "164\t 0.0010000031385508678\n",
      "\n",
      "165\t 0.0010000025108406942\n",
      "\n",
      "166\t 0.0010000020086725553\n",
      "\n",
      "167\t 0.0010000016069380442\n",
      "\n",
      "168\t 0.0010000012855504354\n",
      "\n",
      "169\t 0.0010000010284403483\n",
      "\n",
      "170\t 0.0010000008227522787\n",
      "\n",
      "171\t 0.001000000658201823\n",
      "\n",
      "172\t 0.0010000005265614583\n",
      "\n",
      "173\t 0.0010000004212491666\n",
      "\n",
      "174\t 0.0010000003369993334\n",
      "\n",
      "175\t 0.0010000002695994667\n",
      "\n",
      "176\t 0.0010000002156795734\n",
      "\n",
      "177\t 0.0010000001725436586\n",
      "\n",
      "178\t 0.001000000138034927\n",
      "\n",
      "179\t 0.0010000001104279416\n",
      "\n",
      "180\t 0.0010000000883423532\n",
      "\n",
      "181\t 0.0010000000706738826\n",
      "\n",
      "182\t 0.001000000056539106\n",
      "\n",
      "183\t 0.0010000000452312849\n",
      "\n",
      "184\t 0.0010000000361850279\n",
      "\n",
      "185\t 0.0010000000289480224\n",
      "\n",
      "186\t 0.001000000023158418\n",
      "\n",
      "187\t 0.0010000000185267342\n",
      "\n",
      "188\t 0.0010000000148213874\n",
      "\n",
      "189\t 0.00100000001185711\n",
      "\n",
      "190\t 0.0010000000094856879\n",
      "\n",
      "191\t 0.0010000000075885505\n",
      "\n",
      "192\t 0.0010000000060708402\n",
      "\n",
      "193\t 0.0010000000048566722\n",
      "\n",
      "194\t 0.0010000000038853378\n",
      "\n",
      "195\t 0.0010000000031082702\n",
      "\n",
      "196\t 0.0010000000024866162\n",
      "\n",
      "197\t 0.0010000000019892929\n",
      "\n",
      "198\t 0.0010000000015914345\n",
      "\n",
      "199\t 0.0010000000012731474\n",
      "\n",
      "200\t 0.001000000001018518\n",
      "\n",
      "201\t 0.0010000000008148144\n",
      "\n",
      "202\t 0.0010000000006518516\n",
      "\n",
      "203\t 0.0010000000005214813\n",
      "\n",
      "204\t 0.001000000000417185\n",
      "\n",
      "205\t 0.001000000000333748\n",
      "\n",
      "206\t 0.0010000000002669985\n",
      "\n",
      "207\t 0.0010000000002135987\n",
      "\n",
      "208\t 0.001000000000170879\n",
      "\n",
      "209\t 0.0010000000001367032\n",
      "\n",
      "210\t 0.0010000000001093626\n",
      "\n",
      "211\t 0.0010000000000874901\n",
      "\n",
      "212\t 0.001000000000069992\n",
      "\n",
      "213\t 0.0010000000000559936\n",
      "\n",
      "214\t 0.001000000000044795\n",
      "\n",
      "215\t 0.001000000000035836\n",
      "\n",
      "216\t 0.0010000000000286687\n",
      "\n",
      "217\t 0.001000000000022935\n",
      "\n",
      "218\t 0.001000000000018348\n",
      "\n",
      "219\t 0.0010000000000146784\n",
      "\n",
      "220\t 0.0010000000000117428\n",
      "\n",
      "221\t 0.0010000000000093942\n",
      "\n",
      "222\t 0.0010000000000075153\n",
      "\n",
      "223\t 0.0010000000000060124\n",
      "\n",
      "224\t 0.0010000000000048098\n",
      "\n",
      "225\t 0.0010000000000038479\n",
      "\n",
      "226\t 0.0010000000000030783\n",
      "\n",
      "227\t 0.0010000000000024627\n",
      "\n",
      "228\t 0.00100000000000197\n",
      "\n",
      "229\t 0.001000000000001576\n",
      "\n",
      "230\t 0.001000000000001261\n",
      "\n",
      "231\t 0.0010000000000010088\n",
      "\n",
      "232\t 0.0010000000000008069\n",
      "\n",
      "233\t 0.0010000000000006456\n",
      "\n",
      "234\t 0.0010000000000005165\n",
      "\n",
      "235\t 0.001000000000000413\n",
      "\n",
      "236\t 0.0010000000000003305\n",
      "\n",
      "237\t 0.0010000000000002643\n",
      "\n",
      "238\t 0.0010000000000002117\n",
      "\n",
      "239\t 0.0010000000000001692\n",
      "\n",
      "240\t 0.0010000000000001353\n",
      "\n",
      "241\t 0.0010000000000001082\n",
      "\n",
      "242\t 0.0010000000000000868\n",
      "\n",
      "243\t 0.0010000000000000694\n",
      "\n",
      "244\t 0.0010000000000000555\n",
      "\n",
      "245\t 0.0010000000000000445\n",
      "\n",
      "246\t 0.0010000000000000356\n",
      "\n",
      "247\t 0.0010000000000000284\n",
      "\n",
      "248\t 0.0010000000000000228\n",
      "\n",
      "249\t 0.0010000000000000182\n",
      "\n",
      "250\t 0.0010000000000000145\n",
      "\n",
      "251\t 0.0010000000000000117\n",
      "\n",
      "252\t 0.0010000000000000093\n",
      "\n",
      "253\t 0.0010000000000000074\n",
      "\n",
      "254\t 0.0010000000000000059\n",
      "\n",
      "255\t 0.0010000000000000048\n",
      "\n",
      "256\t 0.001000000000000004\n",
      "\n",
      "257\t 0.001000000000000003\n",
      "\n",
      "258\t 0.0010000000000000024\n",
      "\n",
      "259\t 0.001000000000000002\n",
      "\n",
      "260\t 0.0010000000000000015\n",
      "\n",
      "261\t 0.0010000000000000013\n",
      "\n",
      "262\t 0.001000000000000001\n",
      "\n",
      "263\t 0.0010000000000000009\n",
      "\n",
      "264\t 0.0010000000000000007\n",
      "\n",
      "265\t 0.0010000000000000005\n",
      "\n",
      "266\t 0.0010000000000000005\n",
      "\n",
      "267\t 0.0010000000000000005\n",
      "\n",
      "268\t 0.0010000000000000002\n",
      "\n",
      "269\t 0.0010000000000000002\n",
      "\n",
      "270\t 0.0010000000000000002\n",
      "\n",
      "271\t 0.0010000000000000002\n",
      "\n",
      "272\t 0.001\n",
      "\n",
      "273\t 0.001\n",
      "\n",
      "274\t 0.001\n",
      "\n",
      "275\t 0.001\n",
      "\n",
      "276\t 0.001\n",
      "\n",
      "277\t 0.001\n",
      "\n",
      "278\t 0.001\n",
      "\n",
      "279\t 0.001\n",
      "\n",
      "280\t 0.001\n",
      "\n",
      "281\t 0.001\n",
      "\n",
      "282\t 0.001\n",
      "\n",
      "283\t 0.001\n",
      "\n",
      "284\t 0.001\n",
      "\n",
      "285\t 0.001\n",
      "\n",
      "286\t 0.001\n",
      "\n",
      "287\t 0.001\n",
      "\n",
      "288\t 0.001\n",
      "\n",
      "289\t 0.001\n",
      "\n",
      "290\t 0.001\n",
      "\n",
      "291\t 0.001\n",
      "\n",
      "292\t 0.001\n",
      "\n",
      "293\t 0.001\n",
      "\n",
      "294\t 0.001\n",
      "\n",
      "295\t 0.001\n",
      "\n",
      "296\t 0.001\n",
      "\n",
      "297\t 0.001\n",
      "\n",
      "298\t 0.001\n",
      "\n",
      "299\t 0.001\n",
      "\n",
      "300\t 0.001\n",
      "\n",
      "301\t 0.001\n",
      "\n",
      "302\t 0.001\n",
      "\n",
      "303\t 0.001\n",
      "\n",
      "304\t 0.001\n",
      "\n",
      "305\t 0.001\n",
      "\n",
      "306\t 0.001\n",
      "\n",
      "307\t 0.001\n",
      "\n",
      "308\t 0.001\n",
      "\n",
      "309\t 0.001\n",
      "\n",
      "310\t 0.001\n",
      "\n",
      "311\t 0.001\n",
      "\n",
      "312\t 0.001\n",
      "\n",
      "313\t 0.001\n",
      "\n",
      "314\t 0.001\n",
      "\n",
      "315\t 0.001\n",
      "\n",
      "316\t 0.001\n",
      "\n",
      "317\t 0.001\n",
      "\n",
      "318\t 0.001\n",
      "\n",
      "319\t 0.001\n",
      "\n",
      "320\t 0.001\n",
      "\n",
      "321\t 0.001\n",
      "\n",
      "322\t 0.001\n",
      "\n",
      "323\t 0.001\n",
      "\n",
      "324\t 0.001\n",
      "\n",
      "325\t 0.001\n",
      "\n",
      "326\t 0.001\n",
      "\n",
      "327\t 0.001\n",
      "\n",
      "328\t 0.001\n",
      "\n",
      "329\t 0.001\n",
      "\n",
      "330\t 0.001\n",
      "\n",
      "331\t 0.001\n",
      "\n",
      "332\t 0.001\n",
      "\n",
      "333\t 0.001\n",
      "\n",
      "334\t 0.001\n",
      "\n",
      "335\t 0.001\n",
      "\n",
      "336\t 0.001\n",
      "\n",
      "337\t 0.001\n",
      "\n",
      "338\t 0.001\n",
      "\n",
      "339\t 0.001\n",
      "\n",
      "340\t 0.001\n",
      "\n",
      "341\t 0.001\n",
      "\n",
      "342\t 0.001\n",
      "\n",
      "343\t 0.001\n",
      "\n",
      "344\t 0.001\n",
      "\n",
      "345\t 0.001\n",
      "\n",
      "346\t 0.001\n",
      "\n",
      "347\t 0.001\n",
      "\n",
      "348\t 0.001\n",
      "\n",
      "349\t 0.001\n",
      "\n",
      "350\t 0.001\n",
      "\n",
      "351\t 0.001\n",
      "\n",
      "352\t 0.001\n",
      "\n",
      "353\t 0.001\n",
      "\n",
      "354\t 0.001\n",
      "\n",
      "355\t 0.001\n",
      "\n",
      "356\t 0.001\n",
      "\n",
      "357\t 0.001\n",
      "\n",
      "358\t 0.001\n",
      "\n",
      "359\t 0.001\n",
      "\n",
      "360\t 0.001\n",
      "\n",
      "361\t 0.001\n",
      "\n",
      "362\t 0.001\n",
      "\n",
      "363\t 0.001\n",
      "\n",
      "364\t 0.001\n",
      "\n",
      "365\t 0.001\n",
      "\n",
      "366\t 0.001\n",
      "\n",
      "367\t 0.001\n",
      "\n",
      "368\t 0.001\n",
      "\n",
      "369\t 0.001\n",
      "\n",
      "370\t 0.001\n",
      "\n",
      "371\t 0.001\n",
      "\n",
      "372\t 0.001\n",
      "\n",
      "373\t 0.001\n",
      "\n",
      "374\t 0.001\n",
      "\n",
      "375\t 0.001\n",
      "\n",
      "376\t 0.001\n",
      "\n",
      "377\t 0.001\n",
      "\n",
      "378\t 0.001\n",
      "\n",
      "379\t 0.001\n",
      "\n",
      "380\t 0.001\n",
      "\n",
      "381\t 0.001\n",
      "\n",
      "382\t 0.001\n",
      "\n",
      "383\t 0.001\n",
      "\n",
      "384\t 0.001\n",
      "\n",
      "385\t 0.001\n",
      "\n",
      "386\t 0.001\n",
      "\n",
      "387\t 0.001\n",
      "\n",
      "388\t 0.001\n",
      "\n",
      "389\t 0.001\n",
      "\n",
      "390\t 0.001\n",
      "\n",
      "391\t 0.001\n",
      "\n",
      "392\t 0.001\n",
      "\n",
      "393\t 0.001\n",
      "\n",
      "394\t 0.001\n",
      "\n",
      "395\t 0.001\n",
      "\n",
      "396\t 0.001\n",
      "\n",
      "397\t 0.001\n",
      "\n",
      "398\t 0.001\n",
      "\n",
      "399\t 0.001\n",
      "\n",
      "400\t 0.001\n",
      "\n",
      "401\t 0.001\n",
      "\n",
      "402\t 0.001\n",
      "\n",
      "403\t 0.001\n",
      "\n",
      "404\t 0.001\n",
      "\n",
      "405\t 0.001\n",
      "\n",
      "406\t 0.001\n",
      "\n",
      "407\t 0.001\n",
      "\n",
      "408\t 0.001\n",
      "\n",
      "409\t 0.001\n",
      "\n",
      "410\t 0.001\n",
      "\n",
      "411\t 0.001\n",
      "\n",
      "412\t 0.001\n",
      "\n",
      "413\t 0.001\n",
      "\n",
      "414\t 0.001\n",
      "\n",
      "415\t 0.001\n",
      "\n",
      "416\t 0.001\n",
      "\n",
      "417\t 0.001\n",
      "\n",
      "418\t 0.001\n",
      "\n",
      "419\t 0.001\n",
      "\n",
      "420\t 0.001\n",
      "\n",
      "421\t 0.001\n",
      "\n",
      "422\t 0.001\n",
      "\n",
      "423\t 0.001\n",
      "\n",
      "424\t 0.001\n",
      "\n",
      "425\t 0.001\n",
      "\n",
      "426\t 0.001\n",
      "\n",
      "427\t 0.001\n",
      "\n",
      "428\t 0.001\n",
      "\n",
      "429\t 0.001\n",
      "\n",
      "430\t 0.001\n",
      "\n",
      "431\t 0.001\n",
      "\n",
      "432\t 0.001\n",
      "\n",
      "433\t 0.001\n",
      "\n",
      "434\t 0.001\n",
      "\n",
      "435\t 0.001\n",
      "\n",
      "436\t 0.001\n",
      "\n",
      "437\t 0.001\n",
      "\n",
      "438\t 0.001\n",
      "\n",
      "439\t 0.001\n",
      "\n",
      "440\t 0.001\n",
      "\n",
      "441\t 0.001\n",
      "\n",
      "442\t 0.001\n",
      "\n",
      "443\t 0.001\n",
      "\n",
      "444\t 0.001\n",
      "\n",
      "445\t 0.001\n",
      "\n",
      "446\t 0.001\n",
      "\n",
      "447\t 0.001\n",
      "\n",
      "448\t 0.001\n",
      "\n",
      "449\t 0.001\n",
      "\n",
      "450\t 0.001\n",
      "\n",
      "451\t 0.001\n",
      "\n",
      "452\t 0.001\n",
      "\n",
      "453\t 0.001\n",
      "\n",
      "454\t 0.001\n",
      "\n",
      "455\t 0.001\n",
      "\n",
      "456\t 0.001\n",
      "\n",
      "457\t 0.001\n",
      "\n",
      "458\t 0.001\n",
      "\n",
      "459\t 0.001\n",
      "\n",
      "460\t 0.001\n",
      "\n",
      "461\t 0.001\n",
      "\n",
      "462\t 0.001\n",
      "\n",
      "463\t 0.001\n",
      "\n",
      "464\t 0.001\n",
      "\n",
      "465\t 0.001\n",
      "\n",
      "466\t 0.001\n",
      "\n",
      "467\t 0.001\n",
      "\n",
      "468\t 0.001\n",
      "\n",
      "469\t 0.001\n",
      "\n",
      "470\t 0.001\n",
      "\n",
      "471\t 0.001\n",
      "\n",
      "472\t 0.001\n",
      "\n",
      "473\t 0.001\n",
      "\n",
      "474\t 0.001\n",
      "\n",
      "475\t 0.001\n",
      "\n",
      "476\t 0.001\n",
      "\n",
      "477\t 0.001\n",
      "\n",
      "478\t 0.001\n",
      "\n",
      "479\t 0.001\n",
      "\n",
      "480\t 0.001\n",
      "\n",
      "481\t 0.001\n",
      "\n",
      "482\t 0.001\n",
      "\n",
      "483\t 0.001\n",
      "\n",
      "484\t 0.001\n",
      "\n",
      "485\t 0.001\n",
      "\n",
      "486\t 0.001\n",
      "\n",
      "487\t 0.001\n",
      "\n",
      "488\t 0.001\n",
      "\n",
      "489\t 0.001\n",
      "\n",
      "490\t 0.001\n",
      "\n",
      "491\t 0.001\n",
      "\n",
      "492\t 0.001\n",
      "\n",
      "493\t 0.001\n",
      "\n",
      "494\t 0.001\n",
      "\n",
      "495\t 0.001\n",
      "\n",
      "496\t 0.001\n",
      "\n",
      "497\t 0.001\n",
      "\n",
      "498\t 0.001\n",
      "\n",
      "499\t 0.001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for e, lr in zip(rng,y):\n",
    "    print('{}\\t {}\\n'.format(e, lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe96040cac8>]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# clr3\n",
    "# warm up 10% of epoch: it can reduce fall in local min in inital steps.\n",
    "\n",
    "\n",
    "ep_num = 1000\n",
    "\n",
    "\n",
    "\n",
    "def clr3(epoch):\n",
    "    \n",
    "    \n",
    "    step_size = 25 # currently best for foot pp\n",
    "    max_lr = 0.01 # currently best for foot pp\n",
    "    base_lr = 1e-6 # 1e-6 1e-7\n",
    "\n",
    "    # warm up\n",
    "    lr_init_ep = 0\n",
    "    lr_ramp_ep = 100\n",
    "    lr_sus_ep  = 0\n",
    "    lr_decay   = 0.5\n",
    "\n",
    "    iterations = epoch\n",
    "    cycle = np.floor(1+iterations/(2*step_size))\n",
    "    x = np.abs(iterations/step_size - 2*cycle + 1)\n",
    "    lr = base_lr + (max_lr-base_lr)*np.maximum(0, (1-x))\n",
    "    \n",
    "    #todo: boost the lr at initial setps.\n",
    "#     initial_lr = lambda epoch: lr if epoch > step_size else max_lr\n",
    "#     initial_lr = lambda epoch: lr if epoch > step_size else boost_lr\n",
    "#     lr = initial_lr(epoch)\n",
    "    #todo: boost the lr at fist step_size.\n",
    "    \n",
    "    # warm up\n",
    "    if epoch < lr_ramp_ep:\n",
    "        lr = (max_lr - base_lr) / lr_ramp_ep * epoch + base_lr\n",
    "    \n",
    "    decay = ((epoch+1)/ep_num)\n",
    "    base_part = 1.001 #1.1\n",
    "#     print(decay)\n",
    "    return lr * (base_part-decay) * lr_decay # supressed the lr!\n",
    "\n",
    "\n",
    "rng = [i for i in range(ep_num)]\n",
    "y = [clr3(x) for x in rng]\n",
    "sns.set(style='darkgrid')\n",
    "fig, ax = plt.subplots(figsize=(20, 6))\n",
    "plt.plot(rng, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4999999999999966e-08 ~ 0.004459954505\n"
     ]
    }
   ],
   "source": [
    "print('{} ~ {}'.format(min(y), max(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4999999999999966e-08 ~ 0.004459954505 1e-2~1e-6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# \"\"\"\n",
    "# cosine_decay_restarts是cosine_decay的cycle版本。\n",
    "# first_decay_steps是指第一次完全下降的step數，\n",
    "# t_mul是指每一次循環的步數都將乘以t_mul倍，\n",
    "# m_mul指每一次循環重新開始時的初始lr是上一次循環初始值的m_mul倍。\n",
    "# alpha\n",
    "# \"\"\"\n",
    "\n",
    "# from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "\n",
    "\n",
    "# ep_num = 1000\n",
    "\n",
    "\n",
    "\n",
    "# def CosineDecayCLRWarmUp(epoch):\n",
    "    \n",
    "#     #step_size = 25 # currently best for foot pp\n",
    "#     max_lr = 1e-2 # currently best for foot pp\n",
    "#     base_lr = 1e-8# 1e-6 1e-7\n",
    "\n",
    "#     # warm up\n",
    "#     lr_init_ep = 0\n",
    "#     lr_ramp_ep = 100\n",
    "#     lr_sus_ep  = 0\n",
    "#     lr_decay   = 0.8\n",
    "\n",
    "\n",
    "#     initial_learning_rate = 1e-2\n",
    "#     first_decay_steps = 100\n",
    "\n",
    "\n",
    "#     lr_decayed_fn = (\n",
    "#       tf.keras.experimental.CosineDecayRestarts(\n",
    "#           initial_learning_rate,\n",
    "#           first_decay_steps,\n",
    "#           t_mul=1.0,\n",
    "#           m_mul=0.8,\n",
    "#           alpha = 0.000001,\n",
    "#           name=\"CCosineDecayRestarts\"))\n",
    "    \n",
    "#     # warm up\n",
    "#     if epoch < lr_ramp_ep:\n",
    "#         lr = (max_lr - base_lr) / lr_ramp_ep * epoch + base_lr    \n",
    "#     else:\n",
    "#         lr = lr_decayed_fn(epoch)\n",
    "#     return lr\n",
    "\n",
    "\n",
    "\n",
    "# rng = [i for i in range(ep_num)]\n",
    "# y = [CosineDecayCLRWarmUp(x) for x in rng]\n",
    "# sns.set(style='darkgrid')\n",
    "# fig, ax = plt.subplots(figsize=(20, 6))\n",
    "# # plt.ylim(.0000000000000001, .01)# for too large loss\n",
    "# ax.yaxis.set_major_formatter(FormatStrFormatter('%.12f'))# for too small loss\n",
    "# plt.plot(rng, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# \"\"\"\n",
    "# cosine_decay_restarts是cosine_decay的cycle版本。\n",
    "# first_decay_steps是指第一次完全下降的step數，\n",
    "# t_mul是指每一次循環的步數都將乘以t_mul倍，\n",
    "# m_mul指每一次循環重新開始時的初始lr是上一次循環初始值的m_mul倍。\n",
    "# alpha\n",
    "# \"\"\"\n",
    "\n",
    "# from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "\n",
    "\n",
    "# ep_num = 1000\n",
    "\n",
    "\n",
    "\n",
    "# def CosineDecayCLRWarmUpLSW(epoch):\n",
    "    \n",
    "#     #step_size = 25 # currently best for foot pp\n",
    "#     max_lr = 1e-3 # currently best for foot pp\n",
    "#     base_lr = 1e-6# 1e-6 1e-7\n",
    "\n",
    "#     # warm up\n",
    "#     lr_init_ep = 0\n",
    "#     lr_ramp_ep = 20\n",
    "#     lr_sus_ep  = 0\n",
    "#     lr_decay   = 0.8\n",
    "\n",
    "\n",
    "#     initial_learning_rate = 1e-3\n",
    "#     first_decay_steps = 50\n",
    "\n",
    "\n",
    "#     lr_decayed_fn = (\n",
    "#       tf.keras.experimental.CosineDecayRestarts(\n",
    "#           initial_learning_rate,\n",
    "#           first_decay_steps,\n",
    "#           t_mul=1.0,\n",
    "#           m_mul=0.8,\n",
    "#           alpha = 0.000001,\n",
    "#           name=\"CCosineDecayRestarts\"))\n",
    "    \n",
    "#     # warm up\n",
    "#     if epoch < lr_ramp_ep:\n",
    "#         lr = (max_lr - base_lr) / lr_ramp_ep * epoch + base_lr    \n",
    "#     else:\n",
    "#         lr = lr_decayed_fn(epoch-lr_ramp_ep)\n",
    "#     return lr\n",
    "\n",
    "\n",
    "\n",
    "# rng = [i for i in range(ep_num)]\n",
    "# y = [CosineDecayCLRWarmUpLSW(x) for x in rng]\n",
    "# sns.set(style='darkgrid')\n",
    "# fig, ax = plt.subplots(figsize=(20, 6))\n",
    "# # plt.ylim(.0000000000000001, .01)# for too large loss\n",
    "# ax.yaxis.set_major_formatter(FormatStrFormatter('%.12f'))# for too small loss\n",
    "# plt.plot(rng, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 619 ms, sys: 54 ms, total: 673 ms\n",
      "Wall time: 579 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe9600e4550>]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "cosine_decay_restarts是cosine_decay的cycle版本。\n",
    "first_decay_steps是指第一次完全下降的step數，\n",
    "t_mul是指每一次循環的步數都將乘以t_mul倍，\n",
    "m_mul指每一次循環重新開始時的初始lr是上一次循環初始值的m_mul倍。\n",
    "alpha\n",
    "\"\"\"\n",
    "\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "\n",
    "\n",
    "ep_num = 1000\n",
    "\n",
    "\n",
    "\n",
    "def CosineDecayCLRWarmUpLSW_2(epoch):\n",
    "    \n",
    "    #step_size = 25 # currently best for foot pp\n",
    "    max_lr = 1e-2 # currently best for foot pp\n",
    "    base_lr = 1e-6# 1e-6 1e-7\n",
    "\n",
    "    # warm up\n",
    "    lr_init_ep = 0\n",
    "    lr_ramp_ep = 20\n",
    "    lr_sus_ep  = 0\n",
    "    #lr_decay   = 0.8\n",
    "\n",
    "\n",
    "    initial_learning_rate = 1e-2\n",
    "    first_decay_steps = 50\n",
    "\n",
    "\n",
    "    lr_decayed_fn = (\n",
    "      tf.keras.experimental.CosineDecayRestarts(\n",
    "          initial_learning_rate,\n",
    "          first_decay_steps,\n",
    "          t_mul=1,\n",
    "          m_mul=1,\n",
    "          alpha = 0.000001,\n",
    "          name=\"CCosineDecayRestarts\"))\n",
    "    \n",
    "    # warm up\n",
    "    if epoch < lr_ramp_ep:\n",
    "        lr = (max_lr - base_lr) / lr_ramp_ep * epoch + base_lr    \n",
    "    else:\n",
    "        lr = lr_decayed_fn(epoch-lr_ramp_ep)\n",
    "    return lr\n",
    "\n",
    "\n",
    "\n",
    "rng = [i for i in range(ep_num)]\n",
    "y = [CosineDecayCLRWarmUpLSW_2(x) for x in rng]\n",
    "sns.set(style='darkgrid')\n",
    "fig, ax = plt.subplots(figsize=(20, 6))\n",
    "# plt.ylim(.0000000000000001, .01)# for too large loss\n",
    "ax.yaxis.set_major_formatter(FormatStrFormatter('%.12f'))# for too small loss\n",
    "plt.plot(rng, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-06 ~ 0.009999999776482582\n"
     ]
    }
   ],
   "source": [
    "print('{} ~ {}'.format(min(y), max(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.8774175103430935e-08 ~ 0.0010000000474974513 1e-3 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_reduceonplateau = tf.keras.callbacks.ReduceLROnPlateau(monitor=monitor, factor=0.1, patience=5, verbose=1, mode='auto', min_delta=0.0001, cooldown=0, min_lr=1e-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback for printing the LR at the end of each epoch.\n",
    "class PrintLRtoe(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "#     def on_epoch_end(self, epoch, logs=None):\n",
    "#         print('\\n[{}] Learning rate for epoch {} is {}'.format(\n",
    "#             datetime.now().strftime(\"%Y%m%d-%H%M-%S\"), \n",
    "#             epoch + 1,\n",
    "#             self.model.optimizer.lr.numpy()))\n",
    "        print('\\n[{}] Learning rate for epoch {} is {}'.format(\n",
    "        datetime.now().strftime(\"%Y%m%d-%H%M-%S\"), \n",
    "        epoch + 1,\n",
    "        model_toe.optimizer._decayed_lr(tf.float32).numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback for printing the LR at the end of each epoch.\n",
    "class PrintLRheel(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "#         print('\\nLearning rate for epoch {} is {}'.format(epoch + 1,\n",
    "#                                               model_heel.optimizer.lr.numpy()))\n",
    "        print('\\n[{}] Learning rate for epoch {} is {}'.format(\n",
    "        datetime.now().strftime(\"%Y%m%d-%H%M-%S\"), \n",
    "        epoch + 1,\n",
    "        model_heel.optimizer._decayed_lr(tf.float32).numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output dir and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_log_dir(log_dir_name):\n",
    "    try:\n",
    "        os.makedirs(log_dir_name)\n",
    "    except OSError as e:\n",
    "        print(\"This log dir exist.\")\n",
    "        if e.errno != errno.EEXIST:\n",
    "            raise ValueError(\"we got problem.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor = 'val_loss' #'val_loss' 'val_accuracy' if use ed_loss it still the loss here.\n",
    "\n",
    "log_dir_name = datetime.now().strftime(\"%Y%m%d-%H%M%S\") \n",
    "\n",
    "# mk_log_dir(datetime.now().strftime(\"%Y%m%d-%H%M%S\") )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use once at the time\n",
    "mk_log_dir(log_dir_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'EfficientNetB0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_best_model_name\n",
    "\n",
    "# best_model_name = './' + model_name + '_bs-' + str(BATCH_SIZE) + '_s-' + str(img_height) + '_' + \"ep-{epoch:02d}-vloss-{val_loss:.2f}\" +'_best-weight.h5'\n",
    "# best_model_name = '{model_name}-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
    "#best_model_name = './' + model_name + '_bs' + str(BATCH_SIZE) + '_w' + str(img_width) + '_' + monitor + '_best.h5'\n",
    "# best_model_name = './Leaf_' + model_name + '_bs' + str(BATCH_SIZE) + '_w' + str(img_width) + '_' + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + '_best_' + monitor + '.h5'\n",
    "\n",
    "# best_model_name = './cop' + model_name + '_bs' + str(BATCH_SIZE) + '_w' + str(img_width) + '_' + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + '_best_' + monitor + '.h5'\n",
    "\n",
    "def get_best_model_name(th, K):\n",
    "    return './' + log_dir_name + '/' + th + '_K' + K + '_' + model_name + '_bs' + str(BATCH_SIZE) + '_w' + str(img_width) + '_best_' + monitor + '.h5'\n",
    "\n",
    "# th = 'toe'\n",
    "# # th = 'heel'\n",
    "\n",
    "# # print(get_best_model_name(th,K))\n",
    "\n",
    "# best_model_name = get_best_model_name(th, K)\n",
    "\n",
    "\n",
    "# best_model_save = tf.keras.callbacks.ModelCheckpoint(filepath=best_model_name, \n",
    "#                              save_best_only = True, \n",
    "#                              save_weights_only = False,\n",
    "#                              monitor = monitor, \n",
    "#                              mode = 'auto', verbose = 1)\n",
    "# print('best_model_name:', best_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = log_dir_name + \"/logs/toe/\"\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks = [\n",
    "# #     tensorboard_callback,\n",
    "#     best_model_save,\n",
    "#     tf.keras.callbacks.EarlyStopping(monitor=monitor, patience=20), #patience=step_size or ep_num\n",
    "# #     lr_reduceonplateau,\n",
    "#     tf.keras.callbacks.LearningRateScheduler(lrdump),#lrdump, decay or lrfn or lrfn2. clr\n",
    "#     PrintLRtoe()\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transfer learning from pre-trained weights\n",
    "def build_efn_model(outputnum, top_dropout_rate, drop_connect_rate):\n",
    "    base_model = tf.keras.applications.EfficientNetB0(include_top=False, weights=\"imagenet\", input_shape=(120,120,3),drop_connect_rate=drop_connect_rate) #{'imagenet', None}\n",
    "\n",
    "    # Freeze the pretrained weights\n",
    "    base_model.trainable = False\n",
    "    print(\"base_model.trainable : \", base_model.trainable)\n",
    "\n",
    "    # Rebuild top\n",
    "    gap2d = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "    BNL = tf.keras.layers.BatchNormalization()(gap2d) #tood: remove#\n",
    "    dropout = tf.keras.layers.Dropout(top_dropout_rate)(BNL)#tood: remove# J add dropout, for flood 0.2 is ok. for leaf 0.4 is better.\n",
    "    outputs = tf.keras.layers.Dense(outputnum)(dropout)# remove activation for regression output (to default, the linear), , activation = 'relu' no help\n",
    "\n",
    "    # Compile new model\n",
    "    model = tf.keras.Model(base_model.input, outputs, name=model_name)\n",
    "\n",
    "\n",
    "#     # unfreeze the top #fine_tune_at# layers while leaving BatchNorm layers frozen\n",
    "#     fine_tune_at = 20 #10 #241 #20\n",
    "#     print('[Note] Now create model fine tuneing at Top-{} layers!'.format(fine_tune_at))\n",
    "#     for layer in model_toe.layers[-fine_tune_at:]:\n",
    "#         if not isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "#             layer.trainable = True\n",
    "\n",
    "    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),#RMSprop , Adam, SGD Adadelta(learning_rate=0.001), if set lr_callback the learning_rate=0.001 will not effeced.\n",
    "                    loss=ed_metric_2d_mean)\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Supervised pre-training 減少每次fold都要重新train的時間\n",
    "只先改toe\"\"\"\n",
    "\n",
    "# Transfer learning from pre-trained weights\n",
    "def load_pretrained_efn_model():\n",
    "    pre_model_toe_name = \"20210224-200728/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\"\n",
    "    model = tf.keras.models.load_model(pre_model_toe_name,compile=False)\n",
    "\n",
    "    # Freeze the pretrained weights\n",
    "    model.trainable = False\n",
    "#     print(\"base_model.trainable : \", base_model.trainable)\n",
    "\n",
    "#     # Rebuild top\n",
    "#     gap2d = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "#     BNL = tf.keras.layers.BatchNormalization()(gap2d) #tood: remove#\n",
    "#     dropout = tf.keras.layers.Dropout(top_dropout_rate)(BNL)#tood: remove# J add dropout, for flood 0.2 is ok. for leaf 0.4 is better.\n",
    "#     outputs = tf.keras.layers.Dense(outputnum)(dropout)# remove activation for regression output (to default, the linear), , activation = 'relu' no help\n",
    "\n",
    "#     # Compile new model\n",
    "#     model = tf.keras.Model(base_model.input, outputs, name=model_name)\n",
    "\n",
    "\n",
    "#     # unfreeze the top #fine_tune_at# layers while leaving BatchNorm layers frozen\n",
    "    fine_tune_at = 4 #10 #241 #20\n",
    "    print('[Note] Now create model fine tuneing at Top-{} layers!'.format(fine_tune_at))\n",
    "    for layer in model.layers[-fine_tune_at:]:\n",
    "        if not isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "            print('layer trainable +1', layer.name)\n",
    "            layer.trainable = True\n",
    "\n",
    "    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),#RMSprop , Adam, SGD Adadelta(learning_rate=0.001), if set lr_callback the learning_rate=0.001 will not effeced.\n",
    "                    loss=ed_metric_2d_mean)\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unfreeze_model(model):\n",
    "#\n",
    "# EX: tt: 224, nt:17, total layers:241\n",
    "#'block7a_expand_conv'20 'block6c_expand_conv'50 'block6a_expand_conv'79 'block5b_expand_conv'109 'block4a_expand_conv' 166  'block3a_expand_conv' 195 'block2a_expand_conv' 224\n",
    "#\n",
    "    model.trainable = True\n",
    "#     set_trainable = False\n",
    "#     for layer in model.layers:\n",
    "#         if layer.name == 'block2a_expand_conv': \n",
    "#             set_trainable = True\n",
    "#         if set_trainable:\n",
    "#             layer.trainable = True\n",
    "#         else:\n",
    "#             layer.trainable = False\n",
    "\n",
    "    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),#RMSprop , Adam, SGD Adadelta(learning_rate=0.001), if set lr_callback the learning_rate=0.001 will not effeced.\n",
    "                    loss=ed_metric_2d_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# top_dropout_rate = 0.8 #less dp rate, say 0.1, train_loss will lower than val_loss\n",
    "# drop_connect_rate = 0.9 #for efnet This parameter serves as a toggle for extra regularization in finetuning, but does not affect loaded weights.\n",
    "# outputnum = 2\n",
    "# with strategy.scope():\n",
    "#     model_toe = build_efn_model(outputnum, top_dropout_rate, drop_connect_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(model_toe.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tt = 0\n",
    "# nt = 0\n",
    "# for layer in model_toe.layers:\n",
    "#     if layer.trainable:\n",
    "#         tt +=1\n",
    "#         print(f'{layer.name}')\n",
    "#     else:\n",
    "#         nt +=1\n",
    "# print(f'tt: {tt}, nt:{nt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_model_trainOrNot_layers(model, printlayers=False):\n",
    "    tt = 0\n",
    "    nt = 0\n",
    "    for layer in model.layers:\n",
    "        if layer.trainable:\n",
    "            tt +=1\n",
    "            if printlayers:\n",
    "                print(f'{layer.name}')\n",
    "        else:\n",
    "            nt +=1\n",
    "    print('\\n*********************************** Start fine tune ***********************************')\n",
    "    print(f'tt: {tt}, nt:{nt}, total layers:{tt+nt}')\n",
    "    print('*********************************** Start fine tune ***********************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_model_trainOrNot_layers(model_toe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model_toe.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # fit the model on all data\n",
    "# history_toe = model_toe.fit(train_ds_pre_toe_s, \n",
    "#                       verbose=1, \n",
    "#                       epochs=ep_num_transf, \n",
    "#                       validation_data=valid_ds_pre_toe_s, \n",
    "#                       callbacks=callbacks)#, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_toe = build_efn_model(2, 0.2, 0.2)\n",
    "# unfreeze_model(model_toe)\n",
    "# count_model_trainOrNot_layers(model_toe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold Training\n",
    "\n",
    "2021-02-23 v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toe K-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " K =  0 \n",
      "\n",
      "Check cache in memory:Y True\n",
      "Check keypoints process01: Tensor(\"args_1:0\", shape=(2,), dtype=int64) (2,) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Check keypoints process02: Tensor(\"Reshape:0\", shape=(1, 2), dtype=int64) (1, 2) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Check keypoints process03: Tensor(\"PyFunc:1\", dtype=float32, device=/job:localhost/replica:0/task:0)\n",
      "Check keypoints process04: Tensor(\"Reshape_1:0\", shape=(2,), dtype=float32)\n",
      "Check augment :Y True\n",
      "Check ds cache[True] and augment[True]\n",
      "Check cache in memory:Y True\n",
      "Check augment :N False\n",
      "Check ds cache[True] and augment[False]\n",
      "best_model_name: ./20210227-225757/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "base_model.trainable :  False\n",
      "\n",
      "[20210227-2258-00] Learning rate for epoch 1 is 0.009999999776482582\n",
      "Epoch 1/500\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Iterator.get_next_as_optional()` instead.\n",
      "17/17 [==============================] - ETA: 0s - loss: 76.1742\n",
      "Epoch 00001: val_loss improved from inf to 60.92091, saving model to ./20210227-225757/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 14s 807ms/step - loss: 76.1742 - val_loss: 60.9209\n",
      "\n",
      "[20210227-2258-29] Learning rate for epoch 2 is 0.009960000403225422\n",
      "Epoch 2/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 41.3525\n",
      "Epoch 00002: val_loss improved from 60.92091 to 17.70047, saving model to ./20210227-225757/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 47ms/step - loss: 41.3525 - val_loss: 17.7005\n",
      "\n",
      "[20210227-2258-33] Learning rate for epoch 3 is 0.009920000098645687\n",
      "Epoch 3/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 22.4412\n",
      "Epoch 00003: val_loss improved from 17.70047 to 12.60229, saving model to ./20210227-225757/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 11s 651ms/step - loss: 22.4412 - val_loss: 12.6023\n",
      "\n",
      "[20210227-2258-47] Learning rate for epoch 4 is 0.009879999794065952\n",
      "Epoch 4/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 19.2888\n",
      "Epoch 00004: val_loss did not improve from 12.60229\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 19.2888 - val_loss: 14.4355\n",
      "\n",
      "[20210227-2258-50] Learning rate for epoch 5 is 0.009840000420808792\n",
      "Epoch 5/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 18.3077\n",
      "Epoch 00005: val_loss improved from 12.60229 to 11.95026, saving model to ./20210227-225757/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 51ms/step - loss: 18.3077 - val_loss: 11.9503\n",
      "\n",
      "[20210227-2258-54] Learning rate for epoch 6 is 0.009800000116229057\n",
      "Epoch 6/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 17.7976\n",
      "Epoch 00006: val_loss did not improve from 11.95026\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 17.7976 - val_loss: 12.2461\n",
      "\n",
      "[20210227-2258-58] Learning rate for epoch 7 is 0.009759999811649323\n",
      "Epoch 7/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 17.4242\n",
      "Epoch 00007: val_loss did not improve from 11.95026\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 17.4242 - val_loss: 12.9503\n",
      "\n",
      "[20210227-2259-02] Learning rate for epoch 8 is 0.009720000438392162\n",
      "Epoch 8/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 16.5244\n",
      "Epoch 00008: val_loss improved from 11.95026 to 11.66922, saving model to ./20210227-225757/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 47ms/step - loss: 16.4994 - val_loss: 11.6692\n",
      "\n",
      "[20210227-2259-06] Learning rate for epoch 9 is 0.009680000133812428\n",
      "Epoch 9/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 16.9555\n",
      "Epoch 00009: val_loss did not improve from 11.66922\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 16.9390 - val_loss: 12.7869\n",
      "\n",
      "[20210227-2259-09] Learning rate for epoch 10 is 0.009639999829232693\n",
      "Epoch 10/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 15.9704\n",
      "Epoch 00010: val_loss did not improve from 11.66922\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 16.1163 - val_loss: 12.1673\n",
      "\n",
      "[20210227-2259-13] Learning rate for epoch 11 is 0.009600000455975533\n",
      "Epoch 11/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 16.0372\n",
      "Epoch 00011: val_loss improved from 11.66922 to 11.46020, saving model to ./20210227-225757/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 44ms/step - loss: 16.0581 - val_loss: 11.4602\n",
      "\n",
      "[20210227-2259-17] Learning rate for epoch 12 is 0.009560000151395798\n",
      "Epoch 12/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 15.6366\n",
      "Epoch 00012: val_loss improved from 11.46020 to 11.34254, saving model to ./20210227-225757/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 43ms/step - loss: 15.5772 - val_loss: 11.3425\n",
      "\n",
      "[20210227-2259-21] Learning rate for epoch 13 is 0.009519999846816063\n",
      "Epoch 13/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 15.6010\n",
      "Epoch 00013: val_loss improved from 11.34254 to 11.23238, saving model to ./20210227-225757/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 47ms/step - loss: 15.5841 - val_loss: 11.2324\n",
      "\n",
      "[20210227-2259-25] Learning rate for epoch 14 is 0.009479999542236328\n",
      "Epoch 14/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 14.9893\n",
      "Epoch 00014: val_loss did not improve from 11.23238\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 15.0340 - val_loss: 11.4636\n",
      "\n",
      "[20210227-2259-29] Learning rate for epoch 15 is 0.009440000168979168\n",
      "Epoch 15/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 15.4285\n",
      "Epoch 00015: val_loss improved from 11.23238 to 10.94747, saving model to ./20210227-225757/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 45ms/step - loss: 15.4285 - val_loss: 10.9475\n",
      "\n",
      "[20210227-2259-33] Learning rate for epoch 16 is 0.009399999864399433\n",
      "Epoch 16/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 15.6131\n",
      "Epoch 00016: val_loss did not improve from 10.94747\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 15.4688 - val_loss: 11.5243\n",
      "\n",
      "[20210227-2259-37] Learning rate for epoch 17 is 0.009359999559819698\n",
      "Epoch 17/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 15.2271\n",
      "Epoch 00017: val_loss did not improve from 10.94747\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 15.2271 - val_loss: 11.8799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[20210227-2259-40] Learning rate for epoch 18 is 0.009320000186562538\n",
      "Epoch 18/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 15.0524\n",
      "Epoch 00018: val_loss did not improve from 10.94747\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 15.1160 - val_loss: 11.6475\n",
      "\n",
      "[20210227-2259-44] Learning rate for epoch 19 is 0.009279999881982803\n",
      "Epoch 19/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 14.2694\n",
      "Epoch 00019: val_loss did not improve from 10.94747\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 14.3486 - val_loss: 11.2270\n",
      "\n",
      "[20210227-2259-48] Learning rate for epoch 20 is 0.009239999577403069\n",
      "Epoch 20/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 14.6998\n",
      "Epoch 00020: val_loss did not improve from 10.94747\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 14.6900 - val_loss: 11.3075\n",
      "\n",
      "[20210227-2259-51] Learning rate for epoch 21 is 0.009200000204145908\n",
      "Epoch 21/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 14.8265\n",
      "Epoch 00021: val_loss improved from 10.94747 to 10.63519, saving model to ./20210227-225757/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 45ms/step - loss: 14.8673 - val_loss: 10.6352\n",
      "\n",
      "[20210227-2259-55] Learning rate for epoch 22 is 0.009159999899566174\n",
      "Epoch 22/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.7203\n",
      "Epoch 00022: val_loss did not improve from 10.63519\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 14.7203 - val_loss: 10.9104\n",
      "\n",
      "[20210227-2259-59] Learning rate for epoch 23 is 0.009119999594986439\n",
      "Epoch 23/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 14.4900\n",
      "Epoch 00023: val_loss did not improve from 10.63519\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 14.5320 - val_loss: 10.8530\n",
      "\n",
      "[20210227-2300-02] Learning rate for epoch 24 is 0.009080000221729279\n",
      "Epoch 24/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 14.6484\n",
      "Epoch 00024: val_loss did not improve from 10.63519\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 14.4598 - val_loss: 10.8378\n",
      "\n",
      "[20210227-2300-06] Learning rate for epoch 25 is 0.009039999917149544\n",
      "Epoch 25/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 14.4349\n",
      "Epoch 00025: val_loss did not improve from 10.63519\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 14.4404 - val_loss: 11.3673\n",
      "\n",
      "[20210227-2300-09] Learning rate for epoch 26 is 0.008999999612569809\n",
      "Epoch 26/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 14.4456\n",
      "Epoch 00026: val_loss improved from 10.63519 to 10.62617, saving model to ./20210227-225757/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 45ms/step - loss: 14.3348 - val_loss: 10.6262\n",
      "\n",
      "[20210227-2300-13] Learning rate for epoch 27 is 0.008960000239312649\n",
      "Epoch 27/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 15.0511\n",
      "Epoch 00027: val_loss did not improve from 10.62617\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 15.0511 - val_loss: 10.6710\n",
      "\n",
      "[20210227-2300-17] Learning rate for epoch 28 is 0.008919999934732914\n",
      "Epoch 28/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.1959\n",
      "Epoch 00028: val_loss did not improve from 10.62617\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 14.1959 - val_loss: 11.8084\n",
      "\n",
      "[20210227-2300-20] Learning rate for epoch 29 is 0.00887999963015318\n",
      "Epoch 29/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 14.3993\n",
      "Epoch 00029: val_loss did not improve from 10.62617\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 14.3414 - val_loss: 11.6019\n",
      "\n",
      "[20210227-2300-24] Learning rate for epoch 30 is 0.008840000256896019\n",
      "Epoch 30/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 14.6933\n",
      "Epoch 00030: val_loss did not improve from 10.62617\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 14.6129 - val_loss: 11.4383\n",
      "\n",
      "[20210227-2300-28] Learning rate for epoch 31 is 0.008799999952316284\n",
      "Epoch 31/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 14.2934\n",
      "Epoch 00031: val_loss did not improve from 10.62617\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 14.2042 - val_loss: 10.8525\n",
      "\n",
      "[20210227-2300-32] Learning rate for epoch 32 is 0.00875999964773655\n",
      "Epoch 32/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 13.9495\n",
      "Epoch 00032: val_loss improved from 10.62617 to 10.50006, saving model to ./20210227-225757/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 43ms/step - loss: 13.9840 - val_loss: 10.5001\n",
      "\n",
      "[20210227-2300-35] Learning rate for epoch 33 is 0.00872000027447939\n",
      "Epoch 33/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.3239\n",
      "Epoch 00033: val_loss did not improve from 10.50006\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 14.3239 - val_loss: 10.5741\n",
      "\n",
      "[20210227-2300-39] Learning rate for epoch 34 is 0.008679999969899654\n",
      "Epoch 34/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 14.1216\n",
      "Epoch 00034: val_loss did not improve from 10.50006\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 14.0477 - val_loss: 10.5919\n",
      "\n",
      "[20210227-2300-43] Learning rate for epoch 35 is 0.00863999966531992\n",
      "Epoch 35/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 13.8716\n",
      "Epoch 00035: val_loss did not improve from 10.50006\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 14.0445 - val_loss: 10.5569\n",
      "\n",
      "[20210227-2300-46] Learning rate for epoch 36 is 0.00860000029206276\n",
      "Epoch 36/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.1205\n",
      "Epoch 00036: val_loss did not improve from 10.50006\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 14.1205 - val_loss: 11.0478\n",
      "\n",
      "[20210227-2300-50] Learning rate for epoch 37 is 0.008559999987483025\n",
      "Epoch 37/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 14.1382\n",
      "Epoch 00037: val_loss did not improve from 10.50006\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 14.0796 - val_loss: 10.5238\n",
      "\n",
      "[20210227-2300-54] Learning rate for epoch 38 is 0.00851999968290329\n",
      "Epoch 38/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 13.7905\n",
      "Epoch 00038: val_loss did not improve from 10.50006\n",
      "17/17 [==============================] - 0s 28ms/step - loss: 13.8071 - val_loss: 10.7066\n",
      "\n",
      "[20210227-2300-58] Learning rate for epoch 39 is 0.00848000030964613\n",
      "Epoch 39/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 14.4093\n",
      "Epoch 00039: val_loss did not improve from 10.50006\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 14.3686 - val_loss: 10.5138\n",
      "\n",
      "[20210227-2301-01] Learning rate for epoch 40 is 0.008440000005066395\n",
      "Epoch 40/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 14.1861\n",
      "Epoch 00040: val_loss improved from 10.50006 to 10.41888, saving model to ./20210227-225757/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 45ms/step - loss: 14.0237 - val_loss: 10.4189\n",
      "\n",
      "[20210227-2301-05] Learning rate for epoch 41 is 0.00839999970048666\n",
      "Epoch 41/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.7936\n",
      "Epoch 00041: val_loss did not improve from 10.41888\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 13.7936 - val_loss: 10.6275\n",
      "\n",
      "[20210227-2301-09] Learning rate for epoch 42 is 0.0083600003272295\n",
      "Epoch 42/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.9233\n",
      "Epoch 00042: val_loss improved from 10.41888 to 10.36694, saving model to ./20210227-225757/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 46ms/step - loss: 13.9233 - val_loss: 10.3669\n",
      "\n",
      "[20210227-2301-13] Learning rate for epoch 43 is 0.008320000022649765\n",
      "Epoch 43/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 14.0139\n",
      "Epoch 00043: val_loss improved from 10.36694 to 10.32531, saving model to ./20210227-225757/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 47ms/step - loss: 14.1004 - val_loss: 10.3253\n",
      "\n",
      "[20210227-2301-17] Learning rate for epoch 44 is 0.00827999971807003\n",
      "Epoch 44/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/17 [===========================>..] - ETA: 0s - loss: 13.4911\n",
      "Epoch 00044: val_loss did not improve from 10.32531\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 13.5321 - val_loss: 10.5381\n",
      "\n",
      "[20210227-2301-21] Learning rate for epoch 45 is 0.00824000034481287\n",
      "Epoch 45/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 13.8270\n",
      "Epoch 00045: val_loss did not improve from 10.32531\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 13.8655 - val_loss: 10.3303\n",
      "\n",
      "[20210227-2301-24] Learning rate for epoch 46 is 0.008200000040233135\n",
      "Epoch 46/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 13.8595\n",
      "Epoch 00046: val_loss did not improve from 10.32531\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 13.8963 - val_loss: 10.9253\n",
      "\n",
      "[20210227-2301-28] Learning rate for epoch 47 is 0.0081599997356534\n",
      "Epoch 47/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 14.0701\n",
      "Epoch 00047: val_loss did not improve from 10.32531\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 14.0744 - val_loss: 10.4415\n",
      "\n",
      "[20210227-2301-31] Learning rate for epoch 48 is 0.00812000036239624\n",
      "Epoch 48/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 13.9369\n",
      "Epoch 00048: val_loss did not improve from 10.32531\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 13.8122 - val_loss: 10.6336\n",
      "\n",
      "[20210227-2301-35] Learning rate for epoch 49 is 0.008080000057816505\n",
      "Epoch 49/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 14.0250\n",
      "Epoch 00049: val_loss did not improve from 10.32531\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 14.0156 - val_loss: 10.3947\n",
      "\n",
      "[20210227-2301-38] Learning rate for epoch 50 is 0.00803999975323677\n",
      "Epoch 50/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 13.9702\n",
      "Epoch 00050: val_loss did not improve from 10.32531\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 13.9787 - val_loss: 10.3826\n",
      "\n",
      "[20210227-2301-42] Learning rate for epoch 51 is 0.00800000037997961\n",
      "Epoch 51/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.9366\n",
      "Epoch 00051: val_loss improved from 10.32531 to 10.31295, saving model to ./20210227-225757/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 68ms/step - loss: 13.9366 - val_loss: 10.3130\n",
      "\n",
      "[20210227-2301-46] Learning rate for epoch 52 is 0.007960000075399876\n",
      "Epoch 52/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 13.8420\n",
      "Epoch 00052: val_loss did not improve from 10.31295\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 13.8430 - val_loss: 10.8505\n",
      "\n",
      "[20210227-2301-49] Learning rate for epoch 53 is 0.00791999977082014\n",
      "Epoch 53/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.7708\n",
      "Epoch 00053: val_loss did not improve from 10.31295\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 13.7708 - val_loss: 10.5896\n",
      "\n",
      "[20210227-2301-53] Learning rate for epoch 54 is 0.00788000039756298\n",
      "Epoch 54/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.5892\n",
      "Epoch 00054: val_loss did not improve from 10.31295\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 13.5892 - val_loss: 10.9078\n",
      "\n",
      "[20210227-2301-56] Learning rate for epoch 55 is 0.007840000092983246\n",
      "Epoch 55/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.4310\n",
      "Epoch 00055: val_loss did not improve from 10.31295\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 13.4310 - val_loss: 10.3310\n",
      "\n",
      "[20210227-2302-00] Learning rate for epoch 56 is 0.007799999788403511\n",
      "Epoch 56/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 13.7373\n",
      "Epoch 00056: val_loss did not improve from 10.31295\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 13.7579 - val_loss: 10.3405\n",
      "\n",
      "[20210227-2302-04] Learning rate for epoch 57 is 0.0077599999494850636\n",
      "Epoch 57/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.7246\n",
      "Epoch 00057: val_loss did not improve from 10.31295\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 13.7246 - val_loss: 10.7927\n",
      "\n",
      "[20210227-2302-07] Learning rate for epoch 58 is 0.007720000110566616\n",
      "Epoch 58/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 13.3058\n",
      "Epoch 00058: val_loss did not improve from 10.31295\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 13.2880 - val_loss: 11.0098\n",
      "\n",
      "[20210227-2302-11] Learning rate for epoch 59 is 0.007679999805986881\n",
      "Epoch 59/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.8467\n",
      "Epoch 00059: val_loss did not improve from 10.31295\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 13.8467 - val_loss: 10.4066\n",
      "\n",
      "[20210227-2302-14] Learning rate for epoch 60 is 0.007639999967068434\n",
      "Epoch 60/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 13.4451\n",
      "Epoch 00060: val_loss did not improve from 10.31295\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 13.4121 - val_loss: 10.7319\n",
      "\n",
      "[20210227-2302-18] Learning rate for epoch 61 is 0.007600000128149986\n",
      "Epoch 61/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.5781\n",
      "Epoch 00061: val_loss did not improve from 10.31295\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 13.5781 - val_loss: 11.2574\n",
      "\n",
      "[20210227-2302-21] Learning rate for epoch 62 is 0.0075599998235702515\n",
      "Epoch 62/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.8579\n",
      "Epoch 00062: val_loss did not improve from 10.31295\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 13.8579 - val_loss: 10.5923\n",
      "\n",
      "[20210227-2302-25] Learning rate for epoch 63 is 0.007519999984651804\n",
      "Epoch 63/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.4986\n",
      "Epoch 00063: val_loss did not improve from 10.31295\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 13.4986 - val_loss: 10.4229\n",
      "\n",
      "[20210227-2302-29] Learning rate for epoch 64 is 0.0074800001457333565\n",
      "Epoch 64/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 13.2505\n",
      "Epoch 00064: val_loss did not improve from 10.31295\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 13.3479 - val_loss: 10.7898\n",
      "\n",
      "[20210227-2302-33] Learning rate for epoch 65 is 0.007439999841153622\n",
      "Epoch 65/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 13.6553\n",
      "Epoch 00065: val_loss did not improve from 10.31295\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 13.5863 - val_loss: 10.5296\n",
      "\n",
      "[20210227-2302-36] Learning rate for epoch 66 is 0.007400000002235174\n",
      "Epoch 66/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.6305\n",
      "Epoch 00066: val_loss did not improve from 10.31295\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 13.6305 - val_loss: 10.7924\n",
      "\n",
      "[20210227-2302-40] Learning rate for epoch 67 is 0.007360000163316727\n",
      "Epoch 67/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.6849\n",
      "Epoch 00067: val_loss did not improve from 10.31295\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 13.6849 - val_loss: 10.3978\n",
      "\n",
      "[20210227-2302-44] Learning rate for epoch 68 is 0.007319999858736992\n",
      "Epoch 68/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 13.5170\n",
      "Epoch 00068: val_loss did not improve from 10.31295\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 13.5611 - val_loss: 11.4123\n",
      "\n",
      "[20210227-2302-48] Learning rate for epoch 69 is 0.007280000019818544\n",
      "Epoch 69/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 13.5698\n",
      "Epoch 00069: val_loss did not improve from 10.31295\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 13.5122 - val_loss: 11.3991\n",
      "\n",
      "[20210227-2302-51] Learning rate for epoch 70 is 0.007240000180900097\n",
      "Epoch 70/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 13.6755\n",
      "Epoch 00070: val_loss did not improve from 10.31295\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 13.6677 - val_loss: 11.2006\n",
      "\n",
      "[20210227-2302-54] Learning rate for epoch 71 is 0.007199999876320362\n",
      "Epoch 71/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 13.3648\n",
      "Epoch 00071: val_loss did not improve from 10.31295\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 13.3162 - val_loss: 10.4092\n",
      "\n",
      "*********************************** Start fine tune ***********************************\n",
      "tt: 241, nt:0, total layers:241\n",
      "*********************************** Start fine tune ***********************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[20210227-2302-58] Learning rate for epoch 1 is 4.999999987376214e-07\n",
      "Epoch 1/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 15.1790\n",
      "Epoch 00001: val_loss did not improve from 10.31295\n",
      "17/17 [==============================] - 44s 3s/step - loss: 15.1790 - val_loss: 10.3766\n",
      "\n",
      "[20210227-2304-01] Learning rate for epoch 2 is 5.044450517743826e-05\n",
      "Epoch 2/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.7611\n",
      "Epoch 00002: val_loss did not improve from 10.31295\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 14.7611 - val_loss: 10.3822\n",
      "\n",
      "[20210227-2304-06] Learning rate for epoch 3 is 0.00010028902033809572\n",
      "Epoch 3/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.5746\n",
      "Epoch 00003: val_loss did not improve from 10.31295\n",
      "17/17 [==============================] - 38s 2s/step - loss: 14.5746 - val_loss: 10.5799\n",
      "\n",
      "[20210227-2304-48] Learning rate for epoch 4 is 0.00015003354928921908\n",
      "Epoch 4/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.1270\n",
      "Epoch 00004: val_loss did not improve from 10.31295\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 14.1270 - val_loss: 11.0057\n",
      "\n",
      "[20210227-2304-53] Learning rate for epoch 5 is 0.0001996780774788931\n",
      "Epoch 5/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.6010\n",
      "Epoch 00005: val_loss did not improve from 10.31295\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 13.6010 - val_loss: 12.0507\n",
      "\n",
      "[20210227-2304-58] Learning rate for epoch 6 is 0.000249222619459033\n",
      "Epoch 6/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.8679\n",
      "Epoch 00006: val_loss did not improve from 10.31295\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 12.8679 - val_loss: 10.6040\n",
      "\n",
      "[20210227-2305-03] Learning rate for epoch 7 is 0.0002986671752296388\n",
      "Epoch 7/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.1430\n",
      "Epoch 00007: val_loss improved from 10.31295 to 9.54217, saving model to ./20210227-225757/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 2s 144ms/step - loss: 12.1430 - val_loss: 9.5422\n",
      "\n",
      "[20210227-2305-08] Learning rate for epoch 8 is 0.00034801175934262574\n",
      "Epoch 8/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 11.9312\n",
      "Epoch 00008: val_loss improved from 9.54217 to 8.56391, saving model to ./20210227-225757/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 152ms/step - loss: 11.9312 - val_loss: 8.5639\n",
      "\n",
      "[20210227-2305-15] Learning rate for epoch 9 is 0.00039725631359033287\n",
      "Epoch 9/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 11.4541\n",
      "Epoch 00009: val_loss improved from 8.56391 to 8.48858, saving model to ./20210227-225757/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 2s 144ms/step - loss: 11.4541 - val_loss: 8.4886\n",
      "\n",
      "[20210227-2305-20] Learning rate for epoch 10 is 0.0004464008961804211\n",
      "Epoch 10/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 11.6538\n",
      "Epoch 00010: val_loss did not improve from 8.48858\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 11.6538 - val_loss: 8.7129\n",
      "\n",
      "[20210227-2305-26] Learning rate for epoch 11 is 0.0004954455071128905\n",
      "Epoch 11/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 11.4158\n",
      "Epoch 00011: val_loss improved from 8.48858 to 8.30709, saving model to ./20210227-225757/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 2s 142ms/step - loss: 11.4158 - val_loss: 8.3071\n",
      "\n",
      "[20210227-2305-31] Learning rate for epoch 12 is 0.0005443901172839105\n",
      "Epoch 12/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 11.2998\n",
      "Epoch 00012: val_loss did not improve from 8.30709\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 11.2998 - val_loss: 8.4304\n",
      "\n",
      "[20210227-2305-36] Learning rate for epoch 13 is 0.0005932347266934812\n",
      "Epoch 13/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 11.4423\n",
      "Epoch 00013: val_loss improved from 8.30709 to 8.08011, saving model to ./20210227-225757/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 149ms/step - loss: 11.4423 - val_loss: 8.0801\n",
      "\n",
      "[20210227-2305-42] Learning rate for epoch 14 is 0.0006419793353416026\n",
      "Epoch 14/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 11.0265\n",
      "Epoch 00014: val_loss did not improve from 8.08011\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 11.0265 - val_loss: 8.1010\n",
      "\n",
      "[20210227-2305-48] Learning rate for epoch 15 is 0.0006906240014359355\n",
      "Epoch 15/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 10.8062\n",
      "Epoch 00015: val_loss did not improve from 8.08011\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 10.8062 - val_loss: 8.3989\n",
      "\n",
      "[20210227-2305-53] Learning rate for epoch 16 is 0.0007391686085611582\n",
      "Epoch 16/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 10.8220\n",
      "Epoch 00016: val_loss did not improve from 8.08011\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 10.8220 - val_loss: 8.6133\n",
      "\n",
      "[20210227-2305-59] Learning rate for epoch 17 is 0.0007876132731325924\n",
      "Epoch 17/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 10.7069\n",
      "Epoch 00017: val_loss did not improve from 8.08011\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 10.7069 - val_loss: 8.2463\n",
      "\n",
      "[20210227-2306-04] Learning rate for epoch 18 is 0.0008359579369425774\n",
      "Epoch 18/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 10.5419\n",
      "Epoch 00018: val_loss did not improve from 8.08011\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 10.5419 - val_loss: 8.7344\n",
      "\n",
      "[20210227-2306-09] Learning rate for epoch 19 is 0.000884202599991113\n",
      "Epoch 19/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 10.2708\n",
      "Epoch 00019: val_loss did not improve from 8.08011\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 10.2708 - val_loss: 8.9625\n",
      "\n",
      "[20210227-2306-14] Learning rate for epoch 20 is 0.0009323473204858601\n",
      "Epoch 20/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 10.1482\n",
      "Epoch 00020: val_loss did not improve from 8.08011\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 10.1482 - val_loss: 8.5804\n",
      "\n",
      "[20210227-2306-19] Learning rate for epoch 21 is 0.000980391982011497\n",
      "Epoch 21/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 10.0918\n",
      "Epoch 00021: val_loss did not improve from 8.08011\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 10.0918 - val_loss: 9.0368\n",
      "\n",
      "[20210227-2306-24] Learning rate for epoch 22 is 0.0010283367009833455\n",
      "Epoch 22/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 10.0616\n",
      "Epoch 00022: val_loss did not improve from 8.08011\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 10.0616 - val_loss: 9.6484\n",
      "\n",
      "[20210227-2306-29] Learning rate for epoch 23 is 0.0010761814191937447\n",
      "Epoch 23/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 10.0894\n",
      "Epoch 00023: val_loss did not improve from 8.08011\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 10.0894 - val_loss: 9.1484\n",
      "\n",
      "[20210227-2306-34] Learning rate for epoch 24 is 0.0011239261366426945\n",
      "Epoch 24/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 9.9168\n",
      "Epoch 00024: val_loss did not improve from 8.08011\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 9.9168 - val_loss: 10.6037\n",
      "\n",
      "[20210227-2306-40] Learning rate for epoch 25 is 0.001171570853330195\n",
      "Epoch 25/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 9.5933\n",
      "Epoch 00025: val_loss did not improve from 8.08011\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 9.5933 - val_loss: 10.1461\n",
      "\n",
      "[20210227-2306-46] Learning rate for epoch 26 is 0.001219115569256246\n",
      "Epoch 26/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 9.4517\n",
      "Epoch 00026: val_loss did not improve from 8.08011\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 9.4517 - val_loss: 9.1012\n",
      "\n",
      "[20210227-2306-51] Learning rate for epoch 27 is 0.0012665604008361697\n",
      "Epoch 27/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 9.5260\n",
      "Epoch 00027: val_loss did not improve from 8.08011\n",
      "17/17 [==============================] - 2s 100ms/step - loss: 9.5260 - val_loss: 9.5173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[20210227-2306-56] Learning rate for epoch 28 is 0.0013139051152393222\n",
      "Epoch 28/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 9.4692\n",
      "Epoch 00028: val_loss did not improve from 8.08011\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 9.4692 - val_loss: 9.1436\n",
      "\n",
      "[20210227-2307-01] Learning rate for epoch 29 is 0.0013611499452963471\n",
      "Epoch 29/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 9.5005\n",
      "Epoch 00029: val_loss did not improve from 8.08011\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 9.5005 - val_loss: 10.8539\n",
      "\n",
      "[20210227-2307-06] Learning rate for epoch 30 is 0.001408294658176601\n",
      "Epoch 30/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 9.4553\n",
      "Epoch 00030: val_loss did not improve from 8.08011\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 9.4553 - val_loss: 10.1746\n",
      "\n",
      "[20210227-2307-12] Learning rate for epoch 31 is 0.0014553394867107272\n",
      "Epoch 31/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 9.2966\n",
      "Epoch 00031: val_loss did not improve from 8.08011\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 9.2966 - val_loss: 10.7577\n",
      "\n",
      "[20210227-2307-17] Learning rate for epoch 32 is 0.0015022843144834042\n",
      "Epoch 32/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 9.5092\n",
      "Epoch 00032: val_loss did not improve from 8.08011\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 9.5092 - val_loss: 9.8157\n",
      "\n",
      "[20210227-2307-22] Learning rate for epoch 33 is 0.0015491291414946318\n",
      "Epoch 33/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.9245\n",
      "Epoch 00033: val_loss did not improve from 8.08011\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 8.9245 - val_loss: 10.6923\n",
      "\n",
      "[20210227-2307-27] Learning rate for epoch 34 is 0.00159587396774441\n",
      "Epoch 34/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 9.0846\n",
      "Epoch 00034: val_loss did not improve from 8.08011\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 9.0846 - val_loss: 10.1210\n",
      "\n",
      "[20210227-2307-33] Learning rate for epoch 35 is 0.001642518793232739\n",
      "Epoch 35/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.8990\n",
      "Epoch 00035: val_loss did not improve from 8.08011\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 8.8990 - val_loss: 10.1650\n",
      "\n",
      "[20210227-2307-38] Learning rate for epoch 36 is 0.0016890636179596186\n",
      "Epoch 36/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.6331\n",
      "Epoch 00036: val_loss did not improve from 8.08011\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 8.6331 - val_loss: 8.9961\n",
      "\n",
      "[20210227-2307-43] Learning rate for epoch 37 is 0.0017355084419250488\n",
      "Epoch 37/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.7625\n",
      "Epoch 00037: val_loss did not improve from 8.08011\n",
      "17/17 [==============================] - 2s 103ms/step - loss: 8.7625 - val_loss: 8.4744\n",
      "\n",
      "[20210227-2307-47] Learning rate for epoch 38 is 0.0017818533815443516\n",
      "Epoch 38/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.8373\n",
      "Epoch 00038: val_loss improved from 8.08011 to 7.58822, saving model to ./20210227-225757/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 2s 142ms/step - loss: 8.8373 - val_loss: 7.5882\n",
      "\n",
      "[20210227-2307-53] Learning rate for epoch 39 is 0.0018280982039868832\n",
      "Epoch 39/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.5490\n",
      "Epoch 00039: val_loss did not improve from 7.58822\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 8.5490 - val_loss: 8.2215\n",
      "\n",
      "[20210227-2307-58] Learning rate for epoch 40 is 0.0018742431420832872\n",
      "Epoch 40/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.6615\n",
      "Epoch 00040: val_loss did not improve from 7.58822\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 8.6615 - val_loss: 7.7912\n",
      "\n",
      "[20210227-2308-03] Learning rate for epoch 41 is 0.0019202879630029202\n",
      "Epoch 41/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.2904\n",
      "Epoch 00041: val_loss did not improve from 7.58822\n",
      "17/17 [==============================] - 2s 105ms/step - loss: 8.2904 - val_loss: 9.2080\n",
      "\n",
      "[20210227-2308-08] Learning rate for epoch 42 is 0.0019662328995764256\n",
      "Epoch 42/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.6340\n",
      "Epoch 00042: val_loss did not improve from 7.58822\n",
      "17/17 [==============================] - 2s 104ms/step - loss: 8.6340 - val_loss: 8.7757\n",
      "\n",
      "[20210227-2308-13] Learning rate for epoch 43 is 0.00201207771897316\n",
      "Epoch 43/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.7679\n",
      "Epoch 00043: val_loss did not improve from 7.58822\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 8.7679 - val_loss: 8.9317\n",
      "\n",
      "[20210227-2308-18] Learning rate for epoch 44 is 0.0020578226540237665\n",
      "Epoch 44/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.4216\n",
      "Epoch 00044: val_loss did not improve from 7.58822\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 8.4216 - val_loss: 7.6759\n",
      "\n",
      "[20210227-2308-23] Learning rate for epoch 45 is 0.0021034677047282457\n",
      "Epoch 45/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.2872\n",
      "Epoch 00045: val_loss did not improve from 7.58822\n",
      "17/17 [==============================] - 2s 103ms/step - loss: 8.2872 - val_loss: 8.7444\n",
      "\n",
      "[20210227-2308-28] Learning rate for epoch 46 is 0.002149012638255954\n",
      "Epoch 46/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.4442\n",
      "Epoch 00046: val_loss improved from 7.58822 to 7.50482, saving model to ./20210227-225757/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 148ms/step - loss: 8.4442 - val_loss: 7.5048\n",
      "\n",
      "[20210227-2308-34] Learning rate for epoch 47 is 0.0021944576874375343\n",
      "Epoch 47/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.2439\n",
      "Epoch 00047: val_loss improved from 7.50482 to 7.34792, saving model to ./20210227-225757/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 2s 135ms/step - loss: 8.2439 - val_loss: 7.3479\n",
      "\n",
      "[20210227-2308-39] Learning rate for epoch 48 is 0.0022398026194423437\n",
      "Epoch 48/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.2785\n",
      "Epoch 00048: val_loss did not improve from 7.34792\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 8.2785 - val_loss: 7.4013\n",
      "\n",
      "[20210227-2308-44] Learning rate for epoch 49 is 0.002285047434270382\n",
      "Epoch 49/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.1979\n",
      "Epoch 00049: val_loss did not improve from 7.34792\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 8.1979 - val_loss: 7.5435\n",
      "\n",
      "[20210227-2308-50] Learning rate for epoch 50 is 0.0023301925975829363\n",
      "Epoch 50/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.1257\n",
      "Epoch 00050: val_loss did not improve from 7.34792\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 8.1257 - val_loss: 7.3727\n",
      "\n",
      "[20210227-2308-55] Learning rate for epoch 51 is 0.002375237410888076\n",
      "Epoch 51/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.0421\n",
      "Epoch 00051: val_loss improved from 7.34792 to 6.98204, saving model to ./20210227-225757/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 2s 146ms/step - loss: 8.0421 - val_loss: 6.9820\n",
      "\n",
      "[20210227-2309-01] Learning rate for epoch 52 is 0.0024201825726777315\n",
      "Epoch 52/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.0085\n",
      "Epoch 00052: val_loss improved from 6.98204 to 5.93307, saving model to ./20210227-225757/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 2s 137ms/step - loss: 8.0085 - val_loss: 5.9331\n",
      "\n",
      "[20210227-2309-07] Learning rate for epoch 53 is 0.002465027617290616\n",
      "Epoch 53/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.8712\n",
      "Epoch 00053: val_loss did not improve from 5.93307\n",
      "17/17 [==============================] - 2s 102ms/step - loss: 7.8712 - val_loss: 6.4879\n",
      "\n",
      "[20210227-2309-11] Learning rate for epoch 54 is 0.0025097725447267294\n",
      "Epoch 54/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.8037\n",
      "Epoch 00054: val_loss did not improve from 5.93307\n",
      "17/17 [==============================] - 2s 100ms/step - loss: 7.8037 - val_loss: 6.9171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[20210227-2309-16] Learning rate for epoch 55 is 0.0025544175878167152\n",
      "Epoch 55/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.8683\n",
      "Epoch 00055: val_loss did not improve from 5.93307\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 7.8683 - val_loss: 6.7177\n",
      "\n",
      "[20210227-2309-21] Learning rate for epoch 56 is 0.00259896251372993\n",
      "Epoch 56/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.0219\n",
      "Epoch 00056: val_loss did not improve from 5.93307\n",
      "17/17 [==============================] - 2s 105ms/step - loss: 8.0219 - val_loss: 6.4784\n",
      "\n",
      "[20210227-2309-26] Learning rate for epoch 57 is 0.0026434077881276608\n",
      "Epoch 57/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.9142\n",
      "Epoch 00057: val_loss did not improve from 5.93307\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 7.9142 - val_loss: 6.3373\n",
      "\n",
      "[20210227-2309-31] Learning rate for epoch 58 is 0.0026877527125179768\n",
      "Epoch 58/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.8441\n",
      "Epoch 00058: val_loss did not improve from 5.93307\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 7.8441 - val_loss: 7.4548\n",
      "\n",
      "[20210227-2309-36] Learning rate for epoch 59 is 0.0027319977525621653\n",
      "Epoch 59/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.8285\n",
      "Epoch 00059: val_loss did not improve from 5.93307\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 7.8285 - val_loss: 7.1069\n",
      "\n",
      "[20210227-2309-42] Learning rate for epoch 60 is 0.0027761429082602262\n",
      "Epoch 60/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.7858\n",
      "Epoch 00060: val_loss did not improve from 5.93307\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 7.7858 - val_loss: 6.5810\n",
      "\n",
      "[20210227-2309-47] Learning rate for epoch 61 is 0.002820187946781516\n",
      "Epoch 61/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.9026\n",
      "Epoch 00061: val_loss did not improve from 5.93307\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 7.9026 - val_loss: 7.2285\n",
      "\n",
      "[20210227-2309-52] Learning rate for epoch 62 is 0.0028641331009566784\n",
      "Epoch 62/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.7872\n",
      "Epoch 00062: val_loss did not improve from 5.93307\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 7.7872 - val_loss: 8.2907\n",
      "\n",
      "[20210227-2309-58] Learning rate for epoch 63 is 0.0029079781379550695\n",
      "Epoch 63/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.7492\n",
      "Epoch 00063: val_loss did not improve from 5.93307\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 7.7492 - val_loss: 7.6697\n",
      "\n",
      "[20210227-2310-03] Learning rate for epoch 64 is 0.002951723290607333\n",
      "Epoch 64/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.9782\n",
      "Epoch 00064: val_loss did not improve from 5.93307\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 7.9782 - val_loss: 7.4449\n",
      "\n",
      "[20210227-2310-09] Learning rate for epoch 65 is 0.0029953685589134693\n",
      "Epoch 65/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.5350\n",
      "Epoch 00065: val_loss did not improve from 5.93307\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 7.5350 - val_loss: 6.5117\n",
      "\n",
      "[20210227-2310-14] Learning rate for epoch 66 is 0.0030389137100428343\n",
      "Epoch 66/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.0322\n",
      "Epoch 00066: val_loss did not improve from 5.93307\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 8.0322 - val_loss: 6.9255\n",
      "\n",
      "[20210227-2310-19] Learning rate for epoch 67 is 0.003082358743995428\n",
      "Epoch 67/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.9290\n",
      "Epoch 00067: val_loss did not improve from 5.93307\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 7.9290 - val_loss: 6.1377\n",
      "\n",
      "[20210227-2310-24] Learning rate for epoch 68 is 0.0031257038936018944\n",
      "Epoch 68/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.7352\n",
      "Epoch 00068: val_loss did not improve from 5.93307\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 7.7352 - val_loss: 6.7704\n",
      "\n",
      "[20210227-2310-29] Learning rate for epoch 69 is 0.003168949158862233\n",
      "Epoch 69/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.3603\n",
      "Epoch 00069: val_loss did not improve from 5.93307\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 7.3603 - val_loss: 7.0455\n",
      "\n",
      "[20210227-2310-34] Learning rate for epoch 70 is 0.0032120943069458008\n",
      "Epoch 70/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.3975\n",
      "Epoch 00070: val_loss did not improve from 5.93307\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 7.3975 - val_loss: 6.5252\n",
      "\n",
      "[20210227-2310-39] Learning rate for epoch 71 is 0.003255139570683241\n",
      "Epoch 71/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.7328\n",
      "Epoch 00071: val_loss did not improve from 5.93307\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 7.7328 - val_loss: 7.6158\n",
      "\n",
      "[20210227-2310-44] Learning rate for epoch 72 is 0.00329808471724391\n",
      "Epoch 72/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.8608\n",
      "Epoch 00072: val_loss did not improve from 5.93307\n",
      "17/17 [==============================] - 2s 101ms/step - loss: 7.8608 - val_loss: 7.2843\n",
      "\n",
      "[20210227-2310-50] Learning rate for epoch 73 is 0.0033409299794584513\n",
      "Epoch 73/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.4840\n",
      "Epoch 00073: val_loss did not improve from 5.93307\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 7.4840 - val_loss: 7.5205\n",
      "\n",
      "[20210227-2310-55] Learning rate for epoch 74 is 0.0033836751244962215\n",
      "Epoch 74/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.5413\n",
      "Epoch 00074: val_loss did not improve from 5.93307\n",
      "17/17 [==============================] - 2s 105ms/step - loss: 7.5413 - val_loss: 7.6911\n",
      "\n",
      "[20210227-2311-00] Learning rate for epoch 75 is 0.0034263203851878643\n",
      "Epoch 75/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.6515\n",
      "Epoch 00075: val_loss did not improve from 5.93307\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 7.6515 - val_loss: 7.0418\n",
      "\n",
      "[20210227-2311-05] Learning rate for epoch 76 is 0.003468865528702736\n",
      "Epoch 76/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.6014\n",
      "Epoch 00076: val_loss did not improve from 5.93307\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 7.6014 - val_loss: 6.6596\n",
      "\n",
      "[20210227-2311-10] Learning rate for epoch 77 is 0.00351131078787148\n",
      "Epoch 77/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.5779\n",
      "Epoch 00077: val_loss did not improve from 5.93307\n",
      "17/17 [==============================] - 2s 103ms/step - loss: 7.5779 - val_loss: 5.9687\n",
      "\n",
      "[20210227-2311-15] Learning rate for epoch 78 is 0.0035536561626940966\n",
      "Epoch 78/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.3957\n",
      "Epoch 00078: val_loss did not improve from 5.93307\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 7.3957 - val_loss: 6.2191\n",
      "\n",
      "[20210227-2311-20] Learning rate for epoch 79 is 0.003595901420339942\n",
      "Epoch 79/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.6921\n",
      "Epoch 00079: val_loss did not improve from 5.93307\n",
      "17/17 [==============================] - 2s 103ms/step - loss: 7.6921 - val_loss: 7.0747\n",
      "\n",
      "[20210227-2311-25] Learning rate for epoch 80 is 0.00363804679363966\n",
      "Epoch 80/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.3746\n",
      "Epoch 00080: val_loss did not improve from 5.93307\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 7.3746 - val_loss: 6.2852\n",
      "\n",
      "[20210227-2311-30] Learning rate for epoch 81 is 0.0036800920497626066\n",
      "Epoch 81/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.7194\n",
      "Epoch 00081: val_loss did not improve from 5.93307\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 7.7194 - val_loss: 6.9196\n",
      "\n",
      "[20210227-2311-36] Learning rate for epoch 82 is 0.003722037188708782\n",
      "Epoch 82/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.3898\n",
      "Epoch 00082: val_loss did not improve from 5.93307\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 7.3898 - val_loss: 6.6846\n",
      "\n",
      "[20210227-2311-41] Learning rate for epoch 83 is 0.003763882676139474\n",
      "Epoch 83/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - ETA: 0s - loss: 7.6623\n",
      "Epoch 00083: val_loss did not improve from 5.93307\n",
      "17/17 [==============================] - 2s 105ms/step - loss: 7.6623 - val_loss: 8.0168\n",
      "\n",
      "[20210227-2311-46] Learning rate for epoch 84 is 0.0038056280463933945\n",
      "Epoch 84/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.6476\n",
      "Epoch 00084: val_loss did not improve from 5.93307\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 7.6476 - val_loss: 6.9934\n",
      "\n",
      "[20210227-2311-51] Learning rate for epoch 85 is 0.003847273299470544\n",
      "Epoch 85/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.5902\n",
      "Epoch 00085: val_loss did not improve from 5.93307\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 7.5902 - val_loss: 6.2523\n",
      "\n",
      "[20210227-2311-57] Learning rate for epoch 86 is 0.0038888186682015657\n",
      "Epoch 86/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.5774\n",
      "Epoch 00086: val_loss did not improve from 5.93307\n",
      "17/17 [==============================] - 2s 116ms/step - loss: 7.5774 - val_loss: 7.9151\n",
      "\n",
      "[20210227-2312-02] Learning rate for epoch 87 is 0.00393026415258646\n",
      "Epoch 87/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.7956\n",
      "Epoch 00087: val_loss did not improve from 5.93307\n",
      "17/17 [==============================] - 2s 102ms/step - loss: 7.7956 - val_loss: 7.0293\n",
      "\n",
      "[20210227-2312-07] Learning rate for epoch 88 is 0.00397160928696394\n",
      "Epoch 88/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.4081\n",
      "Epoch 00088: val_loss did not improve from 5.93307\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 7.4081 - val_loss: 6.2852\n",
      "\n",
      "[20210227-2312-11] Learning rate for epoch 89 is 0.004012854769825935\n",
      "Epoch 89/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.3086\n",
      "Epoch 00089: val_loss did not improve from 5.93307\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 7.3086 - val_loss: 7.2812\n",
      "\n",
      "[20210227-2312-16] Learning rate for epoch 90 is 0.00405400013551116\n",
      "Epoch 90/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.5220\n",
      "Epoch 00090: val_loss did not improve from 5.93307\n",
      "17/17 [==============================] - 2s 105ms/step - loss: 7.5220 - val_loss: 6.3063\n",
      "\n",
      "[20210227-2312-22] Learning rate for epoch 91 is 0.004095045384019613\n",
      "Epoch 91/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.0641\n",
      "Epoch 00091: val_loss did not improve from 5.93307\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 8.0641 - val_loss: 7.3452\n",
      "\n",
      "[20210227-2312-26] Learning rate for epoch 92 is 0.004135990981012583\n",
      "Epoch 92/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.6013\n",
      "Epoch 00092: val_loss did not improve from 5.93307\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 7.6013 - val_loss: 6.9003\n",
      "\n",
      "[20210227-2312-31] Learning rate for epoch 93 is 0.004176836460828781\n",
      "Epoch 93/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.4409\n",
      "Epoch 00093: val_loss did not improve from 5.93307\n",
      "17/17 [==============================] - 2s 104ms/step - loss: 7.4409 - val_loss: 8.0348\n",
      "\n",
      "[20210227-2312-37] Learning rate for epoch 94 is 0.004217581823468208\n",
      "Epoch 94/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.5237\n",
      "Epoch 00094: val_loss did not improve from 5.93307\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 7.5237 - val_loss: 8.0295\n",
      "\n",
      "[20210227-2312-42] Learning rate for epoch 95 is 0.004258227068930864\n",
      "Epoch 95/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.4293\n",
      "Epoch 00095: val_loss did not improve from 5.93307\n",
      "17/17 [==============================] - 2s 104ms/step - loss: 7.4293 - val_loss: 6.7608\n",
      "\n",
      "[20210227-2312-47] Learning rate for epoch 96 is 0.0042987726628780365\n",
      "Epoch 96/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.3189\n",
      "Epoch 00096: val_loss did not improve from 5.93307\n",
      "17/17 [==============================] - 2s 105ms/step - loss: 7.3189 - val_loss: 6.6418\n",
      "\n",
      "[20210227-2312-52] Learning rate for epoch 97 is 0.0043392181396484375\n",
      "Epoch 97/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.3443\n",
      "Epoch 00097: val_loss did not improve from 5.93307\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 7.3443 - val_loss: 7.1508\n",
      "\n",
      "[20210227-2312-56] Learning rate for epoch 98 is 0.004379563499242067\n",
      "Epoch 98/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.2362\n",
      "Epoch 00098: val_loss did not improve from 5.93307\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 7.2362 - val_loss: 7.1321\n",
      "\n",
      "[20210227-2313-01] Learning rate for epoch 99 is 0.004419809207320213\n",
      "Epoch 99/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.2304\n",
      "Epoch 00099: val_loss did not improve from 5.93307\n",
      "17/17 [==============================] - 2s 105ms/step - loss: 7.2304 - val_loss: 7.0762\n",
      "\n",
      "[20210227-2313-06] Learning rate for epoch 100 is 0.004459954332560301\n",
      "Epoch 100/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.4130\n",
      "Epoch 00100: val_loss did not improve from 5.93307\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 7.4130 - val_loss: 6.5293\n",
      "\n",
      "[20210227-2313-11] Learning rate for epoch 101 is 4.4999998749517545e-07\n",
      "Epoch 101/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.2217\n",
      "Epoch 00101: val_loss did not improve from 5.93307\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 7.2217 - val_loss: 6.2805\n",
      "\n",
      "[20210227-2313-16] Learning rate for epoch 102 is 0.00018023152370005846\n",
      "Epoch 102/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.3263\n",
      "Epoch 00102: val_loss improved from 5.93307 to 5.87759, saving model to ./20210227-225757/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 2s 146ms/step - loss: 7.3263 - val_loss: 5.8776\n",
      "\n",
      "[20210227-2313-22] Learning rate for epoch 103 is 0.000359613070031628\n",
      "Epoch 103/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.9526\n",
      "Epoch 00103: val_loss improved from 5.87759 to 5.78996, saving model to ./20210227-225757/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 149ms/step - loss: 6.9526 - val_loss: 5.7900\n",
      "\n",
      "[20210227-2313-28] Learning rate for epoch 104 is 0.0005385946715250611\n",
      "Epoch 104/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.8339\n",
      "Epoch 00104: val_loss improved from 5.78996 to 5.75475, saving model to ./20210227-225757/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 2s 144ms/step - loss: 6.8339 - val_loss: 5.7547\n",
      "\n",
      "[20210227-2313-34] Learning rate for epoch 105 is 0.0007171762990765274\n",
      "Epoch 105/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.9283\n",
      "Epoch 00105: val_loss improved from 5.75475 to 5.58911, saving model to ./20210227-225757/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 172ms/step - loss: 6.9283 - val_loss: 5.5891\n",
      "\n",
      "[20210227-2313-40] Learning rate for epoch 106 is 0.0008953579817898571\n",
      "Epoch 106/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.9917\n",
      "Epoch 00106: val_loss improved from 5.58911 to 5.57376, saving model to ./20210227-225757/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 2s 146ms/step - loss: 6.9917 - val_loss: 5.5738\n",
      "\n",
      "[20210227-2313-46] Learning rate for epoch 107 is 0.0010731397196650505\n",
      "Epoch 107/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.9893\n",
      "Epoch 00107: val_loss improved from 5.57376 to 5.46092, saving model to ./20210227-225757/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 2s 144ms/step - loss: 6.9893 - val_loss: 5.4609\n",
      "\n",
      "[20210227-2313-51] Learning rate for epoch 108 is 0.0012505215127021074\n",
      "Epoch 108/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.6116\n",
      "Epoch 00108: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 6.6116 - val_loss: 5.6122\n",
      "\n",
      "[20210227-2313-57] Learning rate for epoch 109 is 0.001427503302693367\n",
      "Epoch 109/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.8981\n",
      "Epoch 00109: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 6.8981 - val_loss: 6.0494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[20210227-2314-02] Learning rate for epoch 110 is 0.0016040850896388292\n",
      "Epoch 110/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.9503\n",
      "Epoch 00110: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 6.9503 - val_loss: 5.6278\n",
      "\n",
      "[20210227-2314-07] Learning rate for epoch 111 is 0.001780266989953816\n",
      "Epoch 111/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.8942\n",
      "Epoch 00111: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 6.8942 - val_loss: 5.9808\n",
      "\n",
      "[20210227-2314-12] Learning rate for epoch 112 is 0.0019560488872230053\n",
      "Epoch 112/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.7443\n",
      "Epoch 00112: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 6.7443 - val_loss: 5.9416\n",
      "\n",
      "[20210227-2314-18] Learning rate for epoch 113 is 0.002131430897861719\n",
      "Epoch 113/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.8396\n",
      "Epoch 00113: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 6.8396 - val_loss: 6.5230\n",
      "\n",
      "[20210227-2314-23] Learning rate for epoch 114 is 0.0023064129054546356\n",
      "Epoch 114/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.6557\n",
      "Epoch 00114: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 6.6557 - val_loss: 5.5588\n",
      "\n",
      "[20210227-2314-28] Learning rate for epoch 115 is 0.0024809949100017548\n",
      "Epoch 115/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.0264\n",
      "Epoch 00115: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 7.0264 - val_loss: 6.0106\n",
      "\n",
      "[20210227-2314-33] Learning rate for epoch 116 is 0.0026551769115030766\n",
      "Epoch 116/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.1218\n",
      "Epoch 00116: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 7.1218 - val_loss: 5.8435\n",
      "\n",
      "[20210227-2314-38] Learning rate for epoch 117 is 0.0028289591427892447\n",
      "Epoch 117/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.7829\n",
      "Epoch 00117: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 102ms/step - loss: 6.7829 - val_loss: 6.1078\n",
      "\n",
      "[20210227-2314-43] Learning rate for epoch 118 is 0.0030023413710296154\n",
      "Epoch 118/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.1391\n",
      "Epoch 00118: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 7.1391 - val_loss: 6.0953\n",
      "\n",
      "[20210227-2314-49] Learning rate for epoch 119 is 0.003175323596224189\n",
      "Epoch 119/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.9195\n",
      "Epoch 00119: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 6.9195 - val_loss: 6.5228\n",
      "\n",
      "[20210227-2314-54] Learning rate for epoch 120 is 0.003347905818372965\n",
      "Epoch 120/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.1079\n",
      "Epoch 00120: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 7.1079 - val_loss: 6.6002\n",
      "\n",
      "[20210227-2314-59] Learning rate for epoch 121 is 0.0035200880374759436\n",
      "Epoch 121/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.0465\n",
      "Epoch 00121: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 7.0465 - val_loss: 6.3404\n",
      "\n",
      "[20210227-2315-04] Learning rate for epoch 122 is 0.003691870253533125\n",
      "Epoch 122/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.1468\n",
      "Epoch 00122: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 7.1468 - val_loss: 6.9900\n",
      "\n",
      "[20210227-2315-10] Learning rate for epoch 123 is 0.0038632526993751526\n",
      "Epoch 123/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.0827\n",
      "Epoch 00123: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 7.0827 - val_loss: 6.6988\n",
      "\n",
      "[20210227-2315-15] Learning rate for epoch 124 is 0.004034235142171383\n",
      "Epoch 124/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.0982\n",
      "Epoch 00124: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 104ms/step - loss: 7.0982 - val_loss: 6.3191\n",
      "\n",
      "[20210227-2315-20] Learning rate for epoch 125 is 0.004204817581921816\n",
      "Epoch 125/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.2375\n",
      "Epoch 00125: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 7.2375 - val_loss: 5.9076\n",
      "\n",
      "[20210227-2315-26] Learning rate for epoch 126 is 0.0043750000186264515\n",
      "Epoch 126/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.9438\n",
      "Epoch 00126: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 6.9438 - val_loss: 6.1272\n",
      "\n",
      "[20210227-2315-31] Learning rate for epoch 127 is 0.0041952175088226795\n",
      "Epoch 127/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.8497\n",
      "Epoch 00127: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 104ms/step - loss: 6.8497 - val_loss: 6.5475\n",
      "\n",
      "[20210227-2315-36] Learning rate for epoch 128 is 0.004015835002064705\n",
      "Epoch 128/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.1904\n",
      "Epoch 00128: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 104ms/step - loss: 7.1904 - val_loss: 6.2209\n",
      "\n",
      "[20210227-2315-41] Learning rate for epoch 129 is 0.003836852265521884\n",
      "Epoch 129/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.7903\n",
      "Epoch 00129: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 105ms/step - loss: 6.7903 - val_loss: 6.8842\n",
      "\n",
      "[20210227-2315-46] Learning rate for epoch 130 is 0.003658269764855504\n",
      "Epoch 130/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.3432\n",
      "Epoch 00130: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 7.3432 - val_loss: 6.3451\n",
      "\n",
      "[20210227-2315-51] Learning rate for epoch 131 is 0.003480087034404278\n",
      "Epoch 131/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.0353\n",
      "Epoch 00131: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 7.0353 - val_loss: 6.0939\n",
      "\n",
      "[20210227-2315-55] Learning rate for epoch 132 is 0.003302304306998849\n",
      "Epoch 132/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.9292\n",
      "Epoch 00132: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 6.9292 - val_loss: 6.8748\n",
      "\n",
      "[20210227-2316-00] Learning rate for epoch 133 is 0.0031249215826392174\n",
      "Epoch 133/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.7648\n",
      "Epoch 00133: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 103ms/step - loss: 6.7648 - val_loss: 5.9984\n",
      "\n",
      "[20210227-2316-05] Learning rate for epoch 134 is 0.0029479386284947395\n",
      "Epoch 134/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.7650\n",
      "Epoch 00134: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 105ms/step - loss: 6.7650 - val_loss: 6.7028\n",
      "\n",
      "[20210227-2316-10] Learning rate for epoch 135 is 0.0027713559102267027\n",
      "Epoch 135/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.6181\n",
      "Epoch 00135: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 104ms/step - loss: 6.6181 - val_loss: 7.0578\n",
      "\n",
      "[20210227-2316-15] Learning rate for epoch 136 is 0.0025951729621738195\n",
      "Epoch 136/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.8634\n",
      "Epoch 00136: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 105ms/step - loss: 6.8634 - val_loss: 6.0323\n",
      "\n",
      "[20210227-2316-20] Learning rate for epoch 137 is 0.0024193900171667337\n",
      "Epoch 137/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.7468\n",
      "Epoch 00137: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 6.7468 - val_loss: 6.0842\n",
      "\n",
      "[20210227-2316-24] Learning rate for epoch 138 is 0.0022440070752054453\n",
      "Epoch 138/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - ETA: 0s - loss: 6.9194\n",
      "Epoch 00138: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 6.9194 - val_loss: 6.1610\n",
      "\n",
      "[20210227-2316-29] Learning rate for epoch 139 is 0.002069024136289954\n",
      "Epoch 139/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.5938\n",
      "Epoch 00139: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 6.5938 - val_loss: 5.8101\n",
      "\n",
      "[20210227-2316-35] Learning rate for epoch 140 is 0.0018944410840049386\n",
      "Epoch 140/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.4557\n",
      "Epoch 00140: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 103ms/step - loss: 6.4557 - val_loss: 6.0604\n",
      "\n",
      "[20210227-2316-39] Learning rate for epoch 141 is 0.0017202580347657204\n",
      "Epoch 141/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.4906\n",
      "Epoch 00141: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 6.4906 - val_loss: 6.1818\n",
      "\n",
      "[20210227-2316-45] Learning rate for epoch 142 is 0.0015464748721569777\n",
      "Epoch 142/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.7301\n",
      "Epoch 00142: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 6.7301 - val_loss: 6.0900\n",
      "\n",
      "[20210227-2316-50] Learning rate for epoch 143 is 0.0013730917125940323\n",
      "Epoch 143/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.3952\n",
      "Epoch 00143: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 6.3952 - val_loss: 6.2489\n",
      "\n",
      "[20210227-2316-55] Learning rate for epoch 144 is 0.0012001085560768843\n",
      "Epoch 144/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.0620\n",
      "Epoch 00144: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 104ms/step - loss: 7.0620 - val_loss: 5.8651\n",
      "\n",
      "[20210227-2317-01] Learning rate for epoch 145 is 0.0010275252861902118\n",
      "Epoch 145/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.5437\n",
      "Epoch 00145: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 6.5437 - val_loss: 5.8124\n",
      "\n",
      "[20210227-2317-06] Learning rate for epoch 146 is 0.0008553420193493366\n",
      "Epoch 146/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.2592\n",
      "Epoch 00146: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 105ms/step - loss: 6.2592 - val_loss: 5.6899\n",
      "\n",
      "[20210227-2317-11] Learning rate for epoch 147 is 0.0006835586973465979\n",
      "Epoch 147/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.4088\n",
      "Epoch 00147: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 103ms/step - loss: 6.4088 - val_loss: 5.6586\n",
      "\n",
      "[20210227-2317-16] Learning rate for epoch 148 is 0.0005121753201819956\n",
      "Epoch 148/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.2518\n",
      "Epoch 00148: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 6.2518 - val_loss: 5.8953\n",
      "\n",
      "[20210227-2317-21] Learning rate for epoch 149 is 0.00034119191695936024\n",
      "Epoch 149/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 5.9794\n",
      "Epoch 00149: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 105ms/step - loss: 5.9794 - val_loss: 5.8423\n",
      "\n",
      "[20210227-2317-25] Learning rate for epoch 150 is 0.00017060847312677652\n",
      "Epoch 150/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.2277\n",
      "Epoch 00150: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 105ms/step - loss: 6.2277 - val_loss: 5.7301\n",
      "\n",
      "[20210227-2317-31] Learning rate for epoch 151 is 4.249999960848072e-07\n",
      "Epoch 151/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.3372\n",
      "Epoch 00151: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 6.3372 - val_loss: 5.7161\n",
      "\n",
      "[20210227-2317-36] Learning rate for epoch 152 is 0.00017020752420648932\n",
      "Epoch 152/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.0397\n",
      "Epoch 00152: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 105ms/step - loss: 6.0397 - val_loss: 5.7599\n",
      "\n",
      "[20210227-2317-41] Learning rate for epoch 153 is 0.0003395900712348521\n",
      "Epoch 153/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.0820\n",
      "Epoch 00153: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 105ms/step - loss: 6.0820 - val_loss: 5.7449\n",
      "\n",
      "[20210227-2317-47] Learning rate for epoch 154 is 0.0005085726734250784\n",
      "Epoch 154/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.2761\n",
      "Epoch 00154: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 6.2761 - val_loss: 5.7367\n",
      "\n",
      "[20210227-2317-52] Learning rate for epoch 155 is 0.0006771553307771683\n",
      "Epoch 155/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.1074\n",
      "Epoch 00155: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 104ms/step - loss: 6.1074 - val_loss: 5.5528\n",
      "\n",
      "[20210227-2317-57] Learning rate for epoch 156 is 0.0008453379850834608\n",
      "Epoch 156/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.3031\n",
      "Epoch 00156: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 104ms/step - loss: 6.3031 - val_loss: 5.7476\n",
      "\n",
      "[20210227-2318-02] Learning rate for epoch 157 is 0.0010131207527592778\n",
      "Epoch 157/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.1867\n",
      "Epoch 00157: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 6.1867 - val_loss: 5.8271\n",
      "\n",
      "[20210227-2318-07] Learning rate for epoch 158 is 0.0011805035173892975\n",
      "Epoch 158/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 5.9977\n",
      "Epoch 00158: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 5.9977 - val_loss: 5.8279\n",
      "\n",
      "[20210227-2318-12] Learning rate for epoch 159 is 0.0013474862789735198\n",
      "Epoch 159/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.2834\n",
      "Epoch 00159: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 104ms/step - loss: 6.2834 - val_loss: 5.9824\n",
      "\n",
      "[20210227-2318-17] Learning rate for epoch 160 is 0.0015140691539272666\n",
      "Epoch 160/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.3607\n",
      "Epoch 00160: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 105ms/step - loss: 6.3607 - val_loss: 5.9355\n",
      "\n",
      "[20210227-2318-21] Learning rate for epoch 161 is 0.001680252025835216\n",
      "Epoch 161/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.5044\n",
      "Epoch 00161: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 6.5044 - val_loss: 6.3311\n",
      "\n",
      "[20210227-2318-27] Learning rate for epoch 162 is 0.0018460348946973681\n",
      "Epoch 162/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.1998\n",
      "Epoch 00162: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 6.1998 - val_loss: 5.8687\n",
      "\n",
      "[20210227-2318-32] Learning rate for epoch 163 is 0.0020114178769290447\n",
      "Epoch 163/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.3697\n",
      "Epoch 00163: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 6.3697 - val_loss: 5.6791\n",
      "\n",
      "[20210227-2318-37] Learning rate for epoch 164 is 0.0021764009725302458\n",
      "Epoch 164/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.4413\n",
      "Epoch 00164: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 104ms/step - loss: 6.4413 - val_loss: 6.2778\n",
      "\n",
      "[20210227-2318-42] Learning rate for epoch 165 is 0.002340983832255006\n",
      "Epoch 165/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.5947\n",
      "Epoch 00165: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 6.5947 - val_loss: 6.4305\n",
      "\n",
      "[20210227-2318-47] Learning rate for epoch 166 is 0.002505166921764612\n",
      "Epoch 166/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - ETA: 0s - loss: 6.4558\n",
      "Epoch 00166: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 101ms/step - loss: 6.4558 - val_loss: 6.4437\n",
      "\n",
      "[20210227-2318-52] Learning rate for epoch 167 is 0.002668950008228421\n",
      "Epoch 167/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.5840\n",
      "Epoch 00167: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 6.5840 - val_loss: 6.8316\n",
      "\n",
      "[20210227-2318-56] Learning rate for epoch 168 is 0.0028323333244770765\n",
      "Epoch 168/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.5119\n",
      "Epoch 00168: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 6.5119 - val_loss: 6.2649\n",
      "\n",
      "[20210227-2319-02] Learning rate for epoch 169 is 0.002995316404849291\n",
      "Epoch 169/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.6605\n",
      "Epoch 00169: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 102ms/step - loss: 6.6605 - val_loss: 6.3325\n",
      "\n",
      "[20210227-2319-07] Learning rate for epoch 170 is 0.0031578997150063515\n",
      "Epoch 170/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.6634\n",
      "Epoch 00170: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 6.6634 - val_loss: 7.1025\n",
      "\n",
      "[20210227-2319-11] Learning rate for epoch 171 is 0.0033200830221176147\n",
      "Epoch 171/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.8513\n",
      "Epoch 00171: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 6.8513 - val_loss: 6.2765\n",
      "\n",
      "[20210227-2319-17] Learning rate for epoch 172 is 0.0034818663261830807\n",
      "Epoch 172/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.0087\n",
      "Epoch 00172: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 7.0087 - val_loss: 7.3723\n",
      "\n",
      "[20210227-2319-22] Learning rate for epoch 173 is 0.0036432496272027493\n",
      "Epoch 173/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.9380\n",
      "Epoch 00173: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 102ms/step - loss: 6.9380 - val_loss: 7.6079\n",
      "\n",
      "[20210227-2319-27] Learning rate for epoch 174 is 0.003804233158007264\n",
      "Epoch 174/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.8539\n",
      "Epoch 00174: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 6.8539 - val_loss: 7.4643\n",
      "\n",
      "[20210227-2319-32] Learning rate for epoch 175 is 0.003964816685765982\n",
      "Epoch 175/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.1550\n",
      "Epoch 00175: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 99ms/step - loss: 7.1550 - val_loss: 7.4205\n",
      "\n",
      "[20210227-2319-37] Learning rate for epoch 176 is 0.004124999977648258\n",
      "Epoch 176/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.7315\n",
      "Epoch 00176: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 6.7315 - val_loss: 6.9198\n",
      "\n",
      "[20210227-2319-42] Learning rate for epoch 177 is 0.003955216612666845\n",
      "Epoch 177/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.8144\n",
      "Epoch 00177: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 6.8144 - val_loss: 6.0547\n",
      "\n",
      "[20210227-2319-48] Learning rate for epoch 178 is 0.003785833017900586\n",
      "Epoch 178/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.9347\n",
      "Epoch 00178: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 6.9347 - val_loss: 6.6086\n",
      "\n",
      "[20210227-2319-53] Learning rate for epoch 179 is 0.0036168494261801243\n",
      "Epoch 179/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.6429\n",
      "Epoch 00179: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 6.6429 - val_loss: 6.9649\n",
      "\n",
      "[20210227-2319-58] Learning rate for epoch 180 is 0.003448265604674816\n",
      "Epoch 180/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.4627\n",
      "Epoch 00180: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 6.4627 - val_loss: 6.6476\n",
      "\n",
      "[20210227-2320-03] Learning rate for epoch 181 is 0.003280082019045949\n",
      "Epoch 181/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.7053\n",
      "Epoch 00181: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 6.7053 - val_loss: 6.8569\n",
      "\n",
      "[20210227-2320-08] Learning rate for epoch 182 is 0.0031122982036322355\n",
      "Epoch 182/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.8372\n",
      "Epoch 00182: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 6.8372 - val_loss: 6.4787\n",
      "\n",
      "[20210227-2320-13] Learning rate for epoch 183 is 0.002944914624094963\n",
      "Epoch 183/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.7887\n",
      "Epoch 00183: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 6.7887 - val_loss: 6.6671\n",
      "\n",
      "[20210227-2320-18] Learning rate for epoch 184 is 0.0027779308147728443\n",
      "Epoch 184/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.5128\n",
      "Epoch 00184: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 6.5128 - val_loss: 6.4348\n",
      "\n",
      "[20210227-2320-23] Learning rate for epoch 185 is 0.0026113467756658792\n",
      "Epoch 185/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.2766\n",
      "Epoch 00185: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 102ms/step - loss: 6.2766 - val_loss: 6.6217\n",
      "\n",
      "[20210227-2320-28] Learning rate for epoch 186 is 0.002445162972435355\n",
      "Epoch 186/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.3880\n",
      "Epoch 00186: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 6.3880 - val_loss: 6.1700\n",
      "\n",
      "[20210227-2320-33] Learning rate for epoch 187 is 0.0022793791722506285\n",
      "Epoch 187/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.6725\n",
      "Epoch 00187: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 6.6725 - val_loss: 6.4422\n",
      "\n",
      "[20210227-2320-38] Learning rate for epoch 188 is 0.0021139951422810555\n",
      "Epoch 188/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.5025\n",
      "Epoch 00188: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 6.5025 - val_loss: 5.9877\n",
      "\n",
      "[20210227-2320-43] Learning rate for epoch 189 is 0.0019490111153572798\n",
      "Epoch 189/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.1277\n",
      "Epoch 00189: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 6.1277 - val_loss: 5.6963\n",
      "\n",
      "[20210227-2320-48] Learning rate for epoch 190 is 0.0017844270914793015\n",
      "Epoch 190/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.3137\n",
      "Epoch 00190: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 6.3137 - val_loss: 6.0917\n",
      "\n",
      "[20210227-2320-54] Learning rate for epoch 191 is 0.0016202429542317986\n",
      "Epoch 191/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.1996\n",
      "Epoch 00191: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 6.1996 - val_loss: 6.0929\n",
      "\n",
      "[20210227-2320-59] Learning rate for epoch 192 is 0.001456458936445415\n",
      "Epoch 192/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.0957\n",
      "Epoch 00192: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 6.0957 - val_loss: 6.1111\n",
      "\n",
      "[20210227-2321-05] Learning rate for epoch 193 is 0.001293074688874185\n",
      "Epoch 193/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 5.8043\n",
      "Epoch 00193: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 5.8043 - val_loss: 5.6926\n",
      "\n",
      "[20210227-2321-10] Learning rate for epoch 194 is 0.0011300905607640743\n",
      "Epoch 194/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - ETA: 0s - loss: 5.9553\n",
      "Epoch 00194: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 5.9553 - val_loss: 5.7295\n",
      "\n",
      "[20210227-2321-15] Learning rate for epoch 195 is 0.0009675062610767782\n",
      "Epoch 195/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.0610\n",
      "Epoch 00195: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 6.0610 - val_loss: 5.7497\n",
      "\n",
      "[20210227-2321-21] Learning rate for epoch 196 is 0.0008053220226429403\n",
      "Epoch 196/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.0788\n",
      "Epoch 00196: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 6.0788 - val_loss: 5.8530\n",
      "\n",
      "[20210227-2321-26] Learning rate for epoch 197 is 0.0006435376708395779\n",
      "Epoch 197/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.1408\n",
      "Epoch 00197: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 6.1408 - val_loss: 5.7484\n",
      "\n",
      "[20210227-2321-31] Learning rate for epoch 198 is 0.0004821533220820129\n",
      "Epoch 198/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 5.8740\n",
      "Epoch 00198: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 5.8740 - val_loss: 5.7141\n",
      "\n",
      "[20210227-2321-36] Learning rate for epoch 199 is 0.0003211689181625843\n",
      "Epoch 199/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.0151\n",
      "Epoch 00199: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 6.0151 - val_loss: 5.7416\n",
      "\n",
      "[20210227-2321-41] Learning rate for epoch 200 is 0.00016058447363320738\n",
      "Epoch 200/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 5.9910\n",
      "Epoch 00200: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 104ms/step - loss: 5.9910 - val_loss: 5.7237\n",
      "\n",
      "[20210227-2321-46] Learning rate for epoch 201 is 4.0000000467443897e-07\n",
      "Epoch 201/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 5.8247\n",
      "Epoch 00201: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 5.8247 - val_loss: 5.7304\n",
      "\n",
      "[20210227-2321-51] Learning rate for epoch 202 is 0.0001601835247129202\n",
      "Epoch 202/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 5.9607\n",
      "Epoch 00202: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 104ms/step - loss: 5.9607 - val_loss: 5.7001\n",
      "\n",
      "[20210227-2321-55] Learning rate for epoch 203 is 0.00031956707243807614\n",
      "Epoch 203/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.0093\n",
      "Epoch 00203: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 104ms/step - loss: 6.0093 - val_loss: 5.6363\n",
      "\n",
      "[20210227-2322-01] Learning rate for epoch 204 is 0.00047855067532509565\n",
      "Epoch 204/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 5.9739\n",
      "Epoch 00204: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 5.9739 - val_loss: 5.6327\n",
      "\n",
      "[20210227-2322-05] Learning rate for epoch 205 is 0.0006371343042701483\n",
      "Epoch 205/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 5.8583\n",
      "Epoch 00205: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 105ms/step - loss: 5.8583 - val_loss: 5.7743\n",
      "\n",
      "[20210227-2322-10] Learning rate for epoch 206 is 0.0007953179883770645\n",
      "Epoch 206/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 5.7942\n",
      "Epoch 00206: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 105ms/step - loss: 5.7942 - val_loss: 5.9530\n",
      "\n",
      "[20210227-2322-15] Learning rate for epoch 207 is 0.0009531017276458442\n",
      "Epoch 207/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.0478\n",
      "Epoch 00207: val_loss did not improve from 5.46092\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 6.0478 - val_loss: 5.7698\n",
      "\n",
      " \n",
      " K =  1 \n",
      "\n",
      "Check cache in memory:Y True\n",
      "Check keypoints process01: Tensor(\"args_1:0\", shape=(2,), dtype=int64) (2,) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Check keypoints process02: Tensor(\"Reshape:0\", shape=(1, 2), dtype=int64) (1, 2) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Check keypoints process03: Tensor(\"PyFunc:1\", dtype=float32, device=/job:localhost/replica:0/task:0)\n",
      "Check keypoints process04: Tensor(\"Reshape_1:0\", shape=(2,), dtype=float32)\n",
      "Check augment :Y True\n",
      "Check ds cache[True] and augment[True]\n",
      "Check cache in memory:Y True\n",
      "Check augment :N False\n",
      "Check ds cache[True] and augment[False]\n",
      "best_model_name: ./20210227-225757/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "base_model.trainable :  False\n",
      "\n",
      "[20210227-2322-23] Learning rate for epoch 1 is 0.009999999776482582\n",
      "Epoch 1/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 77.2073\n",
      "Epoch 00001: val_loss improved from inf to 61.62582, saving model to ./20210227-225757/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 9s 518ms/step - loss: 76.1594 - val_loss: 61.6258\n",
      "\n",
      "[20210227-2322-44] Learning rate for epoch 2 is 0.009960000403225422\n",
      "Epoch 2/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 40.8197\n",
      "Epoch 00002: val_loss improved from 61.62582 to 14.95483, saving model to ./20210227-225757/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 47ms/step - loss: 40.8197 - val_loss: 14.9548\n",
      "\n",
      "[20210227-2322-48] Learning rate for epoch 3 is 0.009920000098645687\n",
      "Epoch 3/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 22.0354\n",
      "Epoch 00003: val_loss improved from 14.95483 to 14.64859, saving model to ./20210227-225757/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 6s 358ms/step - loss: 22.0354 - val_loss: 14.6486\n",
      "\n",
      "[20210227-2322-57] Learning rate for epoch 4 is 0.009879999794065952\n",
      "Epoch 4/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 19.6660\n",
      "Epoch 00004: val_loss improved from 14.64859 to 12.80405, saving model to ./20210227-225757/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 46ms/step - loss: 19.4845 - val_loss: 12.8040\n",
      "\n",
      "[20210227-2323-01] Learning rate for epoch 5 is 0.009840000420808792\n",
      "Epoch 5/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 18.1193\n",
      "Epoch 00005: val_loss improved from 12.80405 to 11.98467, saving model to ./20210227-225757/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 43ms/step - loss: 18.2109 - val_loss: 11.9847\n",
      "\n",
      "[20210227-2323-05] Learning rate for epoch 6 is 0.009800000116229057\n",
      "Epoch 6/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 17.8982\n",
      "Epoch 00006: val_loss improved from 11.98467 to 11.80596, saving model to ./20210227-225757/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 49ms/step - loss: 17.8732 - val_loss: 11.8060\n",
      "\n",
      "[20210227-2323-09] Learning rate for epoch 7 is 0.009759999811649323\n",
      "Epoch 7/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 17.4681\n",
      "Epoch 00007: val_loss improved from 11.80596 to 11.67410, saving model to ./20210227-225757/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 47ms/step - loss: 17.4681 - val_loss: 11.6741\n",
      "\n",
      "[20210227-2323-13] Learning rate for epoch 8 is 0.009720000438392162\n",
      "Epoch 8/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 16.4131\n",
      "Epoch 00008: val_loss did not improve from 11.67410\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 16.4642 - val_loss: 11.9033\n",
      "\n",
      "[20210227-2323-16] Learning rate for epoch 9 is 0.009680000133812428\n",
      "Epoch 9/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 17.1033\n",
      "Epoch 00009: val_loss improved from 11.67410 to 11.45988, saving model to ./20210227-225757/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 50ms/step - loss: 17.1033 - val_loss: 11.4599\n",
      "\n",
      "[20210227-2323-20] Learning rate for epoch 10 is 0.009639999829232693\n",
      "Epoch 10/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/17 [===========================>..] - ETA: 0s - loss: 16.2347\n",
      "Epoch 00010: val_loss did not improve from 11.45988\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 16.2905 - val_loss: 11.5885\n",
      "\n",
      "[20210227-2323-24] Learning rate for epoch 11 is 0.009600000455975533\n",
      "Epoch 11/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 16.0961\n",
      "Epoch 00011: val_loss did not improve from 11.45988\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 16.1006 - val_loss: 11.4710\n",
      "\n",
      "[20210227-2323-27] Learning rate for epoch 12 is 0.009560000151395798\n",
      "Epoch 12/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 15.6164\n",
      "Epoch 00012: val_loss did not improve from 11.45988\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 15.6573 - val_loss: 11.4704\n",
      "\n",
      "[20210227-2323-31] Learning rate for epoch 13 is 0.009519999846816063\n",
      "Epoch 13/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 15.4588\n",
      "Epoch 00013: val_loss improved from 11.45988 to 11.15343, saving model to ./20210227-225757/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 49ms/step - loss: 15.4588 - val_loss: 11.1534\n",
      "\n",
      "[20210227-2323-35] Learning rate for epoch 14 is 0.009479999542236328\n",
      "Epoch 14/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 15.7253\n",
      "Epoch 00014: val_loss did not improve from 11.15343\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 15.7481 - val_loss: 11.2087\n",
      "\n",
      "[20210227-2323-39] Learning rate for epoch 15 is 0.009440000168979168\n",
      "Epoch 15/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.9687\n",
      "Epoch 00015: val_loss did not improve from 11.15343\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 14.9687 - val_loss: 11.4504\n",
      "\n",
      "[20210227-2323-43] Learning rate for epoch 16 is 0.009399999864399433\n",
      "Epoch 16/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.8457\n",
      "Epoch 00016: val_loss improved from 11.15343 to 10.98237, saving model to ./20210227-225757/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 46ms/step - loss: 14.8457 - val_loss: 10.9824\n",
      "\n",
      "[20210227-2323-47] Learning rate for epoch 17 is 0.009359999559819698\n",
      "Epoch 17/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 15.2468\n",
      "Epoch 00017: val_loss improved from 10.98237 to 10.82258, saving model to ./20210227-225757/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 48ms/step - loss: 15.3530 - val_loss: 10.8226\n",
      "\n",
      "[20210227-2323-51] Learning rate for epoch 18 is 0.009320000186562538\n",
      "Epoch 18/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 15.0926\n",
      "Epoch 00018: val_loss improved from 10.82258 to 10.78129, saving model to ./20210227-225757/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 52ms/step - loss: 15.1649 - val_loss: 10.7813\n",
      "\n",
      "[20210227-2323-55] Learning rate for epoch 19 is 0.009279999881982803\n",
      "Epoch 19/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 14.7249\n",
      "Epoch 00019: val_loss improved from 10.78129 to 10.72524, saving model to ./20210227-225757/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 45ms/step - loss: 14.6086 - val_loss: 10.7252\n",
      "\n",
      "[20210227-2323-59] Learning rate for epoch 20 is 0.009239999577403069\n",
      "Epoch 20/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 14.7724\n",
      "Epoch 00020: val_loss did not improve from 10.72524\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 14.6218 - val_loss: 10.8656\n",
      "\n",
      "[20210227-2324-03] Learning rate for epoch 21 is 0.009200000204145908\n",
      "Epoch 21/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.6289\n",
      "Epoch 00021: val_loss improved from 10.72524 to 10.52298, saving model to ./20210227-225757/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 47ms/step - loss: 14.6289 - val_loss: 10.5230\n",
      "\n",
      "[20210227-2324-07] Learning rate for epoch 22 is 0.009159999899566174\n",
      "Epoch 22/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 14.2419\n",
      "Epoch 00022: val_loss improved from 10.52298 to 10.42004, saving model to ./20210227-225757/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 48ms/step - loss: 14.1918 - val_loss: 10.4200\n",
      "\n",
      "[20210227-2324-11] Learning rate for epoch 23 is 0.009119999594986439\n",
      "Epoch 23/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.4331\n",
      "Epoch 00023: val_loss improved from 10.42004 to 10.35843, saving model to ./20210227-225757/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 46ms/step - loss: 14.4331 - val_loss: 10.3584\n",
      "\n",
      "[20210227-2324-15] Learning rate for epoch 24 is 0.009080000221729279\n",
      "Epoch 24/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 14.7168\n",
      "Epoch 00024: val_loss did not improve from 10.35843\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 14.7356 - val_loss: 10.4218\n",
      "\n",
      "[20210227-2324-19] Learning rate for epoch 25 is 0.009039999917149544\n",
      "Epoch 25/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 14.2190\n",
      "Epoch 00025: val_loss did not improve from 10.35843\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 14.2889 - val_loss: 10.5751\n",
      "\n",
      "[20210227-2324-23] Learning rate for epoch 26 is 0.008999999612569809\n",
      "Epoch 26/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 14.1827\n",
      "Epoch 00026: val_loss did not improve from 10.35843\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 14.2601 - val_loss: 10.9642\n",
      "\n",
      "[20210227-2324-27] Learning rate for epoch 27 is 0.008960000239312649\n",
      "Epoch 27/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.4419\n",
      "Epoch 00027: val_loss did not improve from 10.35843\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 14.4419 - val_loss: 10.7074\n",
      "\n",
      "[20210227-2324-30] Learning rate for epoch 28 is 0.008919999934732914\n",
      "Epoch 28/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 14.4745\n",
      "Epoch 00028: val_loss did not improve from 10.35843\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 14.4792 - val_loss: 11.2605\n",
      "\n",
      "[20210227-2324-34] Learning rate for epoch 29 is 0.00887999963015318\n",
      "Epoch 29/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.1421\n",
      "Epoch 00029: val_loss did not improve from 10.35843\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 14.1421 - val_loss: 10.9585\n",
      "\n",
      "[20210227-2324-38] Learning rate for epoch 30 is 0.008840000256896019\n",
      "Epoch 30/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 14.2984\n",
      "Epoch 00030: val_loss did not improve from 10.35843\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 14.2891 - val_loss: 10.9314\n",
      "\n",
      "[20210227-2324-42] Learning rate for epoch 31 is 0.008799999952316284\n",
      "Epoch 31/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 14.5347\n",
      "Epoch 00031: val_loss did not improve from 10.35843\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 14.3193 - val_loss: 10.6435\n",
      "\n",
      "[20210227-2324-45] Learning rate for epoch 32 is 0.00875999964773655\n",
      "Epoch 32/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 14.2071\n",
      "Epoch 00032: val_loss did not improve from 10.35843\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 14.1493 - val_loss: 10.8040\n",
      "\n",
      "[20210227-2324-49] Learning rate for epoch 33 is 0.00872000027447939\n",
      "Epoch 33/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 13.8961\n",
      "Epoch 00033: val_loss did not improve from 10.35843\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 13.9871 - val_loss: 10.5934\n",
      "\n",
      "[20210227-2324-52] Learning rate for epoch 34 is 0.008679999969899654\n",
      "Epoch 34/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 14.1381\n",
      "Epoch 00034: val_loss did not improve from 10.35843\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 14.1234 - val_loss: 10.4032\n",
      "\n",
      "[20210227-2324-55] Learning rate for epoch 35 is 0.00863999966531992\n",
      "Epoch 35/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.1576\n",
      "Epoch 00035: val_loss improved from 10.35843 to 10.23055, saving model to ./20210227-225757/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 50ms/step - loss: 14.1576 - val_loss: 10.2306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[20210227-2325-00] Learning rate for epoch 36 is 0.00860000029206276\n",
      "Epoch 36/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.1945\n",
      "Epoch 00036: val_loss did not improve from 10.23055\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 14.1945 - val_loss: 10.2686\n",
      "\n",
      "[20210227-2325-03] Learning rate for epoch 37 is 0.008559999987483025\n",
      "Epoch 37/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 13.9114\n",
      "Epoch 00037: val_loss did not improve from 10.23055\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 13.9343 - val_loss: 10.2311\n",
      "\n",
      "[20210227-2325-07] Learning rate for epoch 38 is 0.00851999968290329\n",
      "Epoch 38/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 14.1329\n",
      "Epoch 00038: val_loss did not improve from 10.23055\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 14.0743 - val_loss: 10.3531\n",
      "\n",
      "[20210227-2325-11] Learning rate for epoch 39 is 0.00848000030964613\n",
      "Epoch 39/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 13.9540\n",
      "Epoch 00039: val_loss did not improve from 10.23055\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 13.9640 - val_loss: 10.7480\n",
      "\n",
      "[20210227-2325-15] Learning rate for epoch 40 is 0.008440000005066395\n",
      "Epoch 40/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 14.2579\n",
      "Epoch 00040: val_loss did not improve from 10.23055\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 14.2152 - val_loss: 10.4703\n",
      "\n",
      "[20210227-2325-18] Learning rate for epoch 41 is 0.00839999970048666\n",
      "Epoch 41/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 14.1140\n",
      "Epoch 00041: val_loss did not improve from 10.23055\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 14.1162 - val_loss: 10.5816\n",
      "\n",
      "[20210227-2325-22] Learning rate for epoch 42 is 0.0083600003272295\n",
      "Epoch 42/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 13.9284\n",
      "Epoch 00042: val_loss did not improve from 10.23055\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 13.9731 - val_loss: 10.4898\n",
      "\n",
      "[20210227-2325-26] Learning rate for epoch 43 is 0.008320000022649765\n",
      "Epoch 43/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 13.8529\n",
      "Epoch 00043: val_loss did not improve from 10.23055\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 13.8815 - val_loss: 10.5612\n",
      "\n",
      "[20210227-2325-30] Learning rate for epoch 44 is 0.00827999971807003\n",
      "Epoch 44/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.9275\n",
      "Epoch 00044: val_loss did not improve from 10.23055\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 13.9275 - val_loss: 10.6543\n",
      "\n",
      "[20210227-2325-33] Learning rate for epoch 45 is 0.00824000034481287\n",
      "Epoch 45/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 13.9560\n",
      "Epoch 00045: val_loss did not improve from 10.23055\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 13.9674 - val_loss: 10.3566\n",
      "\n",
      "[20210227-2325-37] Learning rate for epoch 46 is 0.008200000040233135\n",
      "Epoch 46/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.2155\n",
      "Epoch 00046: val_loss did not improve from 10.23055\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 14.2155 - val_loss: 10.6426\n",
      "\n",
      "[20210227-2325-41] Learning rate for epoch 47 is 0.0081599997356534\n",
      "Epoch 47/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 13.8006\n",
      "Epoch 00047: val_loss did not improve from 10.23055\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 13.7146 - val_loss: 10.6355\n",
      "\n",
      "[20210227-2325-45] Learning rate for epoch 48 is 0.00812000036239624\n",
      "Epoch 48/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 13.8682\n",
      "Epoch 00048: val_loss did not improve from 10.23055\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 13.8486 - val_loss: 10.3835\n",
      "\n",
      "[20210227-2325-49] Learning rate for epoch 49 is 0.008080000057816505\n",
      "Epoch 49/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.7074\n",
      "Epoch 00049: val_loss did not improve from 10.23055\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 13.7074 - val_loss: 10.6310\n",
      "\n",
      "[20210227-2325-52] Learning rate for epoch 50 is 0.00803999975323677\n",
      "Epoch 50/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 14.0761\n",
      "Epoch 00050: val_loss did not improve from 10.23055\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 13.9899 - val_loss: 10.9302\n",
      "\n",
      "[20210227-2325-56] Learning rate for epoch 51 is 0.00800000037997961\n",
      "Epoch 51/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.6562\n",
      "Epoch 00051: val_loss did not improve from 10.23055\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 13.6562 - val_loss: 11.2754\n",
      "\n",
      "[20210227-2325-59] Learning rate for epoch 52 is 0.007960000075399876\n",
      "Epoch 52/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 13.9710\n",
      "Epoch 00052: val_loss did not improve from 10.23055\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 13.9847 - val_loss: 11.4579\n",
      "\n",
      "[20210227-2326-03] Learning rate for epoch 53 is 0.00791999977082014\n",
      "Epoch 53/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 13.7078\n",
      "Epoch 00053: val_loss did not improve from 10.23055\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 13.6985 - val_loss: 10.7108\n",
      "\n",
      "[20210227-2326-07] Learning rate for epoch 54 is 0.00788000039756298\n",
      "Epoch 54/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 13.8698\n",
      "Epoch 00054: val_loss did not improve from 10.23055\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 13.8510 - val_loss: 10.6860\n",
      "\n",
      "[20210227-2326-11] Learning rate for epoch 55 is 0.007840000092983246\n",
      "Epoch 55/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 13.4976\n",
      "Epoch 00055: val_loss did not improve from 10.23055\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 13.5917 - val_loss: 10.5504\n",
      "\n",
      "*********************************** Start fine tune ***********************************\n",
      "tt: 241, nt:0, total layers:241\n",
      "*********************************** Start fine tune ***********************************\n",
      "\n",
      "[20210227-2326-14] Learning rate for epoch 1 is 4.999999987376214e-07\n",
      "Epoch 1/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 15.3122\n",
      "Epoch 00001: val_loss did not improve from 10.23055\n",
      "17/17 [==============================] - 22s 1s/step - loss: 15.3122 - val_loss: 10.6508\n",
      "\n",
      "[20210227-2326-55] Learning rate for epoch 2 is 5.044450517743826e-05\n",
      "Epoch 2/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 15.3671\n",
      "Epoch 00002: val_loss did not improve from 10.23055\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 15.3671 - val_loss: 10.8457\n",
      "\n",
      "[20210227-2327-01] Learning rate for epoch 3 is 0.00010028902033809572\n",
      "Epoch 3/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.9333\n",
      "Epoch 00003: val_loss did not improve from 10.23055\n",
      "17/17 [==============================] - 20s 1s/step - loss: 14.9333 - val_loss: 11.1571\n",
      "\n",
      "[20210227-2327-24] Learning rate for epoch 4 is 0.00015003354928921908\n",
      "Epoch 4/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.1645\n",
      "Epoch 00004: val_loss did not improve from 10.23055\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 14.1645 - val_loss: 11.9134\n",
      "\n",
      "[20210227-2327-29] Learning rate for epoch 5 is 0.0001996780774788931\n",
      "Epoch 5/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.6801\n",
      "Epoch 00005: val_loss did not improve from 10.23055\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 13.6801 - val_loss: 12.4843\n",
      "\n",
      "[20210227-2327-35] Learning rate for epoch 6 is 0.000249222619459033\n",
      "Epoch 6/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.3613\n",
      "Epoch 00006: val_loss did not improve from 10.23055\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 13.3613 - val_loss: 11.9628\n",
      "\n",
      "[20210227-2327-40] Learning rate for epoch 7 is 0.0002986671752296388\n",
      "Epoch 7/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.7251\n",
      "Epoch 00007: val_loss improved from 10.23055 to 10.17150, saving model to ./20210227-225757/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 152ms/step - loss: 12.7251 - val_loss: 10.1715\n",
      "\n",
      "[20210227-2327-46] Learning rate for epoch 8 is 0.00034801175934262574\n",
      "Epoch 8/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - ETA: 0s - loss: 11.7983\n",
      "Epoch 00008: val_loss improved from 10.17150 to 9.05953, saving model to ./20210227-225757/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 149ms/step - loss: 11.7983 - val_loss: 9.0595\n",
      "\n",
      "[20210227-2327-52] Learning rate for epoch 9 is 0.00039725631359033287\n",
      "Epoch 9/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.0462\n",
      "Epoch 00009: val_loss improved from 9.05953 to 8.65797, saving model to ./20210227-225757/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 2s 146ms/step - loss: 12.0462 - val_loss: 8.6580\n",
      "\n",
      "[20210227-2327-58] Learning rate for epoch 10 is 0.0004464008961804211\n",
      "Epoch 10/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 11.6007\n",
      "Epoch 00010: val_loss improved from 8.65797 to 8.45679, saving model to ./20210227-225757/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 2s 147ms/step - loss: 11.6007 - val_loss: 8.4568\n",
      "\n",
      "[20210227-2328-04] Learning rate for epoch 11 is 0.0004954455071128905\n",
      "Epoch 11/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 11.8063\n",
      "Epoch 00011: val_loss improved from 8.45679 to 8.31973, saving model to ./20210227-225757/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 152ms/step - loss: 11.8063 - val_loss: 8.3197\n",
      "\n",
      "[20210227-2328-09] Learning rate for epoch 12 is 0.0005443901172839105\n",
      "Epoch 12/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 11.4139\n",
      "Epoch 00012: val_loss improved from 8.31973 to 8.28479, saving model to ./20210227-225757/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 2s 146ms/step - loss: 11.4139 - val_loss: 8.2848\n",
      "\n",
      "[20210227-2328-15] Learning rate for epoch 13 is 0.0005932347266934812\n",
      "Epoch 13/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 11.2465\n",
      "Epoch 00013: val_loss improved from 8.28479 to 8.13172, saving model to ./20210227-225757/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 2s 145ms/step - loss: 11.2465 - val_loss: 8.1317\n",
      "\n",
      "[20210227-2328-21] Learning rate for epoch 14 is 0.0006419793353416026\n",
      "Epoch 14/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 11.3834\n",
      "Epoch 00014: val_loss did not improve from 8.13172\n",
      "17/17 [==============================] - 2s 102ms/step - loss: 11.3834 - val_loss: 8.5435\n",
      "\n",
      "[20210227-2328-26] Learning rate for epoch 15 is 0.0006906240014359355\n",
      "Epoch 15/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 10.8612\n",
      "Epoch 00015: val_loss did not improve from 8.13172\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 10.8612 - val_loss: 9.1019\n",
      "\n",
      "[20210227-2328-31] Learning rate for epoch 16 is 0.0007391686085611582\n",
      "Epoch 16/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 10.8426\n",
      "Epoch 00016: val_loss improved from 8.13172 to 8.00991, saving model to ./20210227-225757/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 2s 146ms/step - loss: 10.8426 - val_loss: 8.0099\n",
      "\n",
      "[20210227-2328-37] Learning rate for epoch 17 is 0.0007876132731325924\n",
      "Epoch 17/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 10.7361\n",
      "Epoch 00017: val_loss did not improve from 8.00991\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 10.7361 - val_loss: 8.6664\n",
      "\n",
      "[20210227-2328-42] Learning rate for epoch 18 is 0.0008359579369425774\n",
      "Epoch 18/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 11.2088\n",
      "Epoch 00018: val_loss did not improve from 8.00991\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 11.2088 - val_loss: 8.4475\n",
      "\n",
      "[20210227-2328-47] Learning rate for epoch 19 is 0.000884202599991113\n",
      "Epoch 19/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 10.6480\n",
      "Epoch 00019: val_loss did not improve from 8.00991\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 10.6480 - val_loss: 8.7886\n",
      "\n",
      "[20210227-2328-52] Learning rate for epoch 20 is 0.0009323473204858601\n",
      "Epoch 20/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 10.5794\n",
      "Epoch 00020: val_loss did not improve from 8.00991\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 10.5794 - val_loss: 8.0550\n",
      "\n",
      "[20210227-2328-58] Learning rate for epoch 21 is 0.000980391982011497\n",
      "Epoch 21/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 10.6733\n",
      "Epoch 00021: val_loss did not improve from 8.00991\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 10.6733 - val_loss: 10.2984\n",
      "\n",
      "[20210227-2329-03] Learning rate for epoch 22 is 0.0010283367009833455\n",
      "Epoch 22/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 10.2178\n",
      "Epoch 00022: val_loss did not improve from 8.00991\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 10.2178 - val_loss: 8.8184\n",
      "\n",
      "[20210227-2329-08] Learning rate for epoch 23 is 0.0010761814191937447\n",
      "Epoch 23/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 10.1965\n",
      "Epoch 00023: val_loss did not improve from 8.00991\n",
      "17/17 [==============================] - 2s 101ms/step - loss: 10.1965 - val_loss: 8.3808\n",
      "\n",
      "[20210227-2329-13] Learning rate for epoch 24 is 0.0011239261366426945\n",
      "Epoch 24/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 10.2645\n",
      "Epoch 00024: val_loss did not improve from 8.00991\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 10.2645 - val_loss: 8.8214\n",
      "\n",
      "[20210227-2329-18] Learning rate for epoch 25 is 0.001171570853330195\n",
      "Epoch 25/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 9.8736\n",
      "Epoch 00025: val_loss improved from 8.00991 to 7.88686, saving model to ./20210227-225757/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 2s 146ms/step - loss: 9.8736 - val_loss: 7.8869\n",
      "\n",
      "[20210227-2329-23] Learning rate for epoch 26 is 0.001219115569256246\n",
      "Epoch 26/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 9.6847\n",
      "Epoch 00026: val_loss did not improve from 7.88686\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 9.6847 - val_loss: 10.1854\n",
      "\n",
      "[20210227-2329-29] Learning rate for epoch 27 is 0.0012665604008361697\n",
      "Epoch 27/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 9.5678\n",
      "Epoch 00027: val_loss improved from 7.88686 to 7.46311, saving model to ./20210227-225757/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 149ms/step - loss: 9.5678 - val_loss: 7.4631\n",
      "\n",
      "[20210227-2329-34] Learning rate for epoch 28 is 0.0013139051152393222\n",
      "Epoch 28/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 9.7047\n",
      "Epoch 00028: val_loss did not improve from 7.46311\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 9.7047 - val_loss: 8.1511\n",
      "\n",
      "[20210227-2329-40] Learning rate for epoch 29 is 0.0013611499452963471\n",
      "Epoch 29/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 9.3449\n",
      "Epoch 00029: val_loss improved from 7.46311 to 7.22906, saving model to ./20210227-225757/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 2s 143ms/step - loss: 9.3449 - val_loss: 7.2291\n",
      "\n",
      "[20210227-2329-46] Learning rate for epoch 30 is 0.001408294658176601\n",
      "Epoch 30/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 9.4974\n",
      "Epoch 00030: val_loss did not improve from 7.22906\n",
      "17/17 [==============================] - 2s 105ms/step - loss: 9.4974 - val_loss: 7.8185\n",
      "\n",
      "[20210227-2329-51] Learning rate for epoch 31 is 0.0014553394867107272\n",
      "Epoch 31/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 9.3018\n",
      "Epoch 00031: val_loss did not improve from 7.22906\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 9.3018 - val_loss: 8.3751\n",
      "\n",
      "[20210227-2329-56] Learning rate for epoch 32 is 0.0015022843144834042\n",
      "Epoch 32/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 9.5799\n",
      "Epoch 00032: val_loss did not improve from 7.22906\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 9.5799 - val_loss: 7.7343\n",
      "\n",
      "[20210227-2330-01] Learning rate for epoch 33 is 0.0015491291414946318\n",
      "Epoch 33/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - ETA: 0s - loss: 9.1752\n",
      "Epoch 00033: val_loss did not improve from 7.22906\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 9.1752 - val_loss: 7.6835\n",
      "\n",
      "[20210227-2330-06] Learning rate for epoch 34 is 0.00159587396774441\n",
      "Epoch 34/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 9.0621\n",
      "Epoch 00034: val_loss did not improve from 7.22906\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 9.0621 - val_loss: 7.9188\n",
      "\n",
      "[20210227-2330-11] Learning rate for epoch 35 is 0.001642518793232739\n",
      "Epoch 35/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 9.0708\n",
      "Epoch 00035: val_loss improved from 7.22906 to 6.57207, saving model to ./20210227-225757/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 154ms/step - loss: 9.0708 - val_loss: 6.5721\n",
      "\n",
      "[20210227-2330-17] Learning rate for epoch 36 is 0.0016890636179596186\n",
      "Epoch 36/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.9269\n",
      "Epoch 00036: val_loss did not improve from 6.57207\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 8.9269 - val_loss: 8.2368\n",
      "\n",
      "[20210227-2330-23] Learning rate for epoch 37 is 0.0017355084419250488\n",
      "Epoch 37/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 9.0783\n",
      "Epoch 00037: val_loss did not improve from 6.57207\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 9.0783 - val_loss: 8.9624\n",
      "\n",
      "[20210227-2330-28] Learning rate for epoch 38 is 0.0017818533815443516\n",
      "Epoch 38/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 9.0152\n",
      "Epoch 00038: val_loss did not improve from 6.57207\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 9.0152 - val_loss: 8.2458\n",
      "\n",
      "[20210227-2330-33] Learning rate for epoch 39 is 0.0018280982039868832\n",
      "Epoch 39/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.8355\n",
      "Epoch 00039: val_loss did not improve from 6.57207\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 8.8355 - val_loss: 8.3291\n",
      "\n",
      "[20210227-2330-38] Learning rate for epoch 40 is 0.0018742431420832872\n",
      "Epoch 40/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.5544\n",
      "Epoch 00040: val_loss did not improve from 6.57207\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 8.5544 - val_loss: 8.2485\n",
      "\n",
      "[20210227-2330-43] Learning rate for epoch 41 is 0.0019202879630029202\n",
      "Epoch 41/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 9.0372\n",
      "Epoch 00041: val_loss did not improve from 6.57207\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 9.0372 - val_loss: 7.2057\n",
      "\n",
      "[20210227-2330-49] Learning rate for epoch 42 is 0.0019662328995764256\n",
      "Epoch 42/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.6349\n",
      "Epoch 00042: val_loss did not improve from 6.57207\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 8.6349 - val_loss: 7.7026\n",
      "\n",
      "[20210227-2330-54] Learning rate for epoch 43 is 0.00201207771897316\n",
      "Epoch 43/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.9088\n",
      "Epoch 00043: val_loss did not improve from 6.57207\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 8.9088 - val_loss: 7.5146\n",
      "\n",
      "[20210227-2330-59] Learning rate for epoch 44 is 0.0020578226540237665\n",
      "Epoch 44/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.3938\n",
      "Epoch 00044: val_loss improved from 6.57207 to 6.49951, saving model to ./20210227-225757/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 2s 144ms/step - loss: 8.3938 - val_loss: 6.4995\n",
      "\n",
      "[20210227-2331-05] Learning rate for epoch 45 is 0.0021034677047282457\n",
      "Epoch 45/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.5001\n",
      "Epoch 00045: val_loss improved from 6.49951 to 6.46798, saving model to ./20210227-225757/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 149ms/step - loss: 8.5001 - val_loss: 6.4680\n",
      "\n",
      "[20210227-2331-11] Learning rate for epoch 46 is 0.002149012638255954\n",
      "Epoch 46/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.3716\n",
      "Epoch 00046: val_loss did not improve from 6.46798\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 8.3716 - val_loss: 7.7744\n",
      "\n",
      "[20210227-2331-16] Learning rate for epoch 47 is 0.0021944576874375343\n",
      "Epoch 47/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.3372\n",
      "Epoch 00047: val_loss did not improve from 6.46798\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 8.3372 - val_loss: 6.9859\n",
      "\n",
      "[20210227-2331-21] Learning rate for epoch 48 is 0.0022398026194423437\n",
      "Epoch 48/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.2893\n",
      "Epoch 00048: val_loss did not improve from 6.46798\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 8.2893 - val_loss: 6.5162\n",
      "\n",
      "[20210227-2331-27] Learning rate for epoch 49 is 0.002285047434270382\n",
      "Epoch 49/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.3519\n",
      "Epoch 00049: val_loss did not improve from 6.46798\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 8.3519 - val_loss: 7.2004\n",
      "\n",
      "[20210227-2331-32] Learning rate for epoch 50 is 0.0023301925975829363\n",
      "Epoch 50/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.2910\n",
      "Epoch 00050: val_loss did not improve from 6.46798\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 8.2910 - val_loss: 6.4920\n",
      "\n",
      "[20210227-2331-37] Learning rate for epoch 51 is 0.002375237410888076\n",
      "Epoch 51/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.4562\n",
      "Epoch 00051: val_loss improved from 6.46798 to 6.41318, saving model to ./20210227-225757/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 2s 147ms/step - loss: 8.4562 - val_loss: 6.4132\n",
      "\n",
      "[20210227-2331-43] Learning rate for epoch 52 is 0.0024201825726777315\n",
      "Epoch 52/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.9729\n",
      "Epoch 00052: val_loss improved from 6.41318 to 6.38706, saving model to ./20210227-225757/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 2s 147ms/step - loss: 7.9729 - val_loss: 6.3871\n",
      "\n",
      "[20210227-2331-49] Learning rate for epoch 53 is 0.002465027617290616\n",
      "Epoch 53/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.2326\n",
      "Epoch 00053: val_loss improved from 6.38706 to 6.13984, saving model to ./20210227-225757/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 147ms/step - loss: 8.2326 - val_loss: 6.1398\n",
      "\n",
      "[20210227-2331-54] Learning rate for epoch 54 is 0.0025097725447267294\n",
      "Epoch 54/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.2023\n",
      "Epoch 00054: val_loss improved from 6.13984 to 6.09357, saving model to ./20210227-225757/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 2s 145ms/step - loss: 8.2023 - val_loss: 6.0936\n",
      "\n",
      "[20210227-2332-00] Learning rate for epoch 55 is 0.0025544175878167152\n",
      "Epoch 55/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.9113\n",
      "Epoch 00055: val_loss did not improve from 6.09357\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 7.9113 - val_loss: 6.9958\n",
      "\n",
      "[20210227-2332-05] Learning rate for epoch 56 is 0.00259896251372993\n",
      "Epoch 56/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.0790\n",
      "Epoch 00056: val_loss did not improve from 6.09357\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 8.0790 - val_loss: 7.4609\n",
      "\n",
      "[20210227-2332-11] Learning rate for epoch 57 is 0.0026434077881276608\n",
      "Epoch 57/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.9913\n",
      "Epoch 00057: val_loss did not improve from 6.09357\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 7.9913 - val_loss: 6.1597\n",
      "\n",
      "[20210227-2332-16] Learning rate for epoch 58 is 0.0026877527125179768\n",
      "Epoch 58/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.3460\n",
      "Epoch 00058: val_loss did not improve from 6.09357\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 8.3460 - val_loss: 6.6048\n",
      "\n",
      "[20210227-2332-21] Learning rate for epoch 59 is 0.0027319977525621653\n",
      "Epoch 59/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - ETA: 0s - loss: 8.2637\n",
      "Epoch 00059: val_loss did not improve from 6.09357\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 8.2637 - val_loss: 7.1456\n",
      "\n",
      "[20210227-2332-26] Learning rate for epoch 60 is 0.0027761429082602262\n",
      "Epoch 60/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.2742\n",
      "Epoch 00060: val_loss did not improve from 6.09357\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 8.2742 - val_loss: 6.5693\n",
      "\n",
      "[20210227-2332-31] Learning rate for epoch 61 is 0.002820187946781516\n",
      "Epoch 61/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.2804\n",
      "Epoch 00061: val_loss did not improve from 6.09357\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 8.2804 - val_loss: 6.6838\n",
      "\n",
      "[20210227-2332-36] Learning rate for epoch 62 is 0.0028641331009566784\n",
      "Epoch 62/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.1057\n",
      "Epoch 00062: val_loss did not improve from 6.09357\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 8.1057 - val_loss: 7.5298\n",
      "\n",
      "[20210227-2332-41] Learning rate for epoch 63 is 0.0029079781379550695\n",
      "Epoch 63/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.0698\n",
      "Epoch 00063: val_loss did not improve from 6.09357\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 8.0698 - val_loss: 7.3844\n",
      "\n",
      "[20210227-2332-46] Learning rate for epoch 64 is 0.002951723290607333\n",
      "Epoch 64/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.8523\n",
      "Epoch 00064: val_loss did not improve from 6.09357\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 7.8523 - val_loss: 6.8297\n",
      "\n",
      "[20210227-2332-52] Learning rate for epoch 65 is 0.0029953685589134693\n",
      "Epoch 65/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.8684\n",
      "Epoch 00065: val_loss did not improve from 6.09357\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 7.8684 - val_loss: 7.0338\n",
      "\n",
      "[20210227-2332-57] Learning rate for epoch 66 is 0.0030389137100428343\n",
      "Epoch 66/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.9502\n",
      "Epoch 00066: val_loss did not improve from 6.09357\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 7.9502 - val_loss: 7.3644\n",
      "\n",
      "[20210227-2333-02] Learning rate for epoch 67 is 0.003082358743995428\n",
      "Epoch 67/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.9374\n",
      "Epoch 00067: val_loss did not improve from 6.09357\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 7.9374 - val_loss: 7.0128\n",
      "\n",
      "[20210227-2333-08] Learning rate for epoch 68 is 0.0031257038936018944\n",
      "Epoch 68/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.0806\n",
      "Epoch 00068: val_loss did not improve from 6.09357\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 8.0806 - val_loss: 6.4226\n",
      "\n",
      "[20210227-2333-12] Learning rate for epoch 69 is 0.003168949158862233\n",
      "Epoch 69/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.7979\n",
      "Epoch 00069: val_loss did not improve from 6.09357\n",
      "17/17 [==============================] - 2s 103ms/step - loss: 7.7979 - val_loss: 7.8662\n",
      "\n",
      "[20210227-2333-18] Learning rate for epoch 70 is 0.0032120943069458008\n",
      "Epoch 70/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.6926\n",
      "Epoch 00070: val_loss did not improve from 6.09357\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 7.6926 - val_loss: 7.1163\n",
      "\n",
      "[20210227-2333-22] Learning rate for epoch 71 is 0.003255139570683241\n",
      "Epoch 71/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.1311\n",
      "Epoch 00071: val_loss did not improve from 6.09357\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 8.1311 - val_loss: 6.3278\n",
      "\n",
      "[20210227-2333-27] Learning rate for epoch 72 is 0.00329808471724391\n",
      "Epoch 72/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.0268\n",
      "Epoch 00072: val_loss did not improve from 6.09357\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 8.0268 - val_loss: 6.8047\n",
      "\n",
      "[20210227-2333-33] Learning rate for epoch 73 is 0.0033409299794584513\n",
      "Epoch 73/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.6306\n",
      "Epoch 00073: val_loss did not improve from 6.09357\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 7.6306 - val_loss: 6.1028\n",
      "\n",
      "[20210227-2333-38] Learning rate for epoch 74 is 0.0033836751244962215\n",
      "Epoch 74/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.6611\n",
      "Epoch 00074: val_loss did not improve from 6.09357\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 7.6611 - val_loss: 6.8108\n",
      "\n",
      "[20210227-2333-43] Learning rate for epoch 75 is 0.0034263203851878643\n",
      "Epoch 75/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.7411\n",
      "Epoch 00075: val_loss did not improve from 6.09357\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 7.7411 - val_loss: 7.0491\n",
      "\n",
      "[20210227-2333-48] Learning rate for epoch 76 is 0.003468865528702736\n",
      "Epoch 76/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.8198\n",
      "Epoch 00076: val_loss did not improve from 6.09357\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 7.8198 - val_loss: 6.3654\n",
      "\n",
      "[20210227-2333-54] Learning rate for epoch 77 is 0.00351131078787148\n",
      "Epoch 77/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.9924\n",
      "Epoch 00077: val_loss did not improve from 6.09357\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 7.9924 - val_loss: 7.6029\n",
      "\n",
      "[20210227-2333-59] Learning rate for epoch 78 is 0.0035536561626940966\n",
      "Epoch 78/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.0357\n",
      "Epoch 00078: val_loss did not improve from 6.09357\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 8.0357 - val_loss: 6.8434\n",
      "\n",
      "[20210227-2334-04] Learning rate for epoch 79 is 0.003595901420339942\n",
      "Epoch 79/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.6596\n",
      "Epoch 00079: val_loss did not improve from 6.09357\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 7.6596 - val_loss: 7.1020\n",
      "\n",
      "[20210227-2334-09] Learning rate for epoch 80 is 0.00363804679363966\n",
      "Epoch 80/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.8122\n",
      "Epoch 00080: val_loss did not improve from 6.09357\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 7.8122 - val_loss: 7.2777\n",
      "\n",
      "[20210227-2334-14] Learning rate for epoch 81 is 0.0036800920497626066\n",
      "Epoch 81/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.5818\n",
      "Epoch 00081: val_loss did not improve from 6.09357\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 7.5818 - val_loss: 6.1603\n",
      "\n",
      "[20210227-2334-19] Learning rate for epoch 82 is 0.003722037188708782\n",
      "Epoch 82/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.6753\n",
      "Epoch 00082: val_loss did not improve from 6.09357\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 7.6753 - val_loss: 15.6797\n",
      "\n",
      "[20210227-2334-24] Learning rate for epoch 83 is 0.003763882676139474\n",
      "Epoch 83/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.7506\n",
      "Epoch 00083: val_loss improved from 6.09357 to 6.03329, saving model to ./20210227-225757/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 149ms/step - loss: 7.7506 - val_loss: 6.0333\n",
      "\n",
      "[20210227-2334-30] Learning rate for epoch 84 is 0.0038056280463933945\n",
      "Epoch 84/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.7426\n",
      "Epoch 00084: val_loss improved from 6.03329 to 6.02343, saving model to ./20210227-225757/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 193ms/step - loss: 7.7426 - val_loss: 6.0234\n",
      "\n",
      "[20210227-2334-37] Learning rate for epoch 85 is 0.003847273299470544\n",
      "Epoch 85/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.5282\n",
      "Epoch 00085: val_loss did not improve from 6.02343\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 7.5282 - val_loss: 6.2770\n",
      "\n",
      "[20210227-2334-42] Learning rate for epoch 86 is 0.0038888186682015657\n",
      "Epoch 86/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.0274\n",
      "Epoch 00086: val_loss did not improve from 6.02343\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 8.0274 - val_loss: 6.1321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[20210227-2334-47] Learning rate for epoch 87 is 0.00393026415258646\n",
      "Epoch 87/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.6179\n",
      "Epoch 00087: val_loss did not improve from 6.02343\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 7.6179 - val_loss: 8.1309\n",
      "\n",
      "[20210227-2334-52] Learning rate for epoch 88 is 0.00397160928696394\n",
      "Epoch 88/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.5988\n",
      "Epoch 00088: val_loss did not improve from 6.02343\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 7.5988 - val_loss: 6.3847\n",
      "\n",
      "[20210227-2334-58] Learning rate for epoch 89 is 0.004012854769825935\n",
      "Epoch 89/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.1198\n",
      "Epoch 00089: val_loss did not improve from 6.02343\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 8.1198 - val_loss: 6.3829\n",
      "\n",
      "[20210227-2335-03] Learning rate for epoch 90 is 0.00405400013551116\n",
      "Epoch 90/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.5821\n",
      "Epoch 00090: val_loss did not improve from 6.02343\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 7.5821 - val_loss: 6.0469\n",
      "\n",
      "[20210227-2335-08] Learning rate for epoch 91 is 0.004095045384019613\n",
      "Epoch 91/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.7470\n",
      "Epoch 00091: val_loss did not improve from 6.02343\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 7.7470 - val_loss: 9.3333\n",
      "\n",
      "[20210227-2335-13] Learning rate for epoch 92 is 0.004135990981012583\n",
      "Epoch 92/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.7577\n",
      "Epoch 00092: val_loss did not improve from 6.02343\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 7.7577 - val_loss: 6.9092\n",
      "\n",
      "[20210227-2335-17] Learning rate for epoch 93 is 0.004176836460828781\n",
      "Epoch 93/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.5425\n",
      "Epoch 00093: val_loss did not improve from 6.02343\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 7.5425 - val_loss: 6.5274\n",
      "\n",
      "[20210227-2335-23] Learning rate for epoch 94 is 0.004217581823468208\n",
      "Epoch 94/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.3688\n",
      "Epoch 00094: val_loss did not improve from 6.02343\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 7.3688 - val_loss: 6.5646\n",
      "\n",
      "[20210227-2335-28] Learning rate for epoch 95 is 0.004258227068930864\n",
      "Epoch 95/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.2765\n",
      "Epoch 00095: val_loss did not improve from 6.02343\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 7.2765 - val_loss: 6.1217\n",
      "\n",
      "[20210227-2335-33] Learning rate for epoch 96 is 0.0042987726628780365\n",
      "Epoch 96/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.4859\n",
      "Epoch 00096: val_loss did not improve from 6.02343\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 7.4859 - val_loss: 6.0288\n",
      "\n",
      "[20210227-2335-38] Learning rate for epoch 97 is 0.0043392181396484375\n",
      "Epoch 97/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.9271\n",
      "Epoch 00097: val_loss did not improve from 6.02343\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 7.9271 - val_loss: 6.3232\n",
      "\n",
      "[20210227-2335-43] Learning rate for epoch 98 is 0.004379563499242067\n",
      "Epoch 98/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.3847\n",
      "Epoch 00098: val_loss did not improve from 6.02343\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 7.3847 - val_loss: 6.9998\n",
      "\n",
      "[20210227-2335-48] Learning rate for epoch 99 is 0.004419809207320213\n",
      "Epoch 99/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.4829\n",
      "Epoch 00099: val_loss did not improve from 6.02343\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 7.4829 - val_loss: 6.3889\n",
      "\n",
      "[20210227-2335-53] Learning rate for epoch 100 is 0.004459954332560301\n",
      "Epoch 100/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.6617\n",
      "Epoch 00100: val_loss improved from 6.02343 to 6.00762, saving model to ./20210227-225757/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 2s 142ms/step - loss: 7.6617 - val_loss: 6.0076\n",
      "\n",
      "[20210227-2335-59] Learning rate for epoch 101 is 4.4999998749517545e-07\n",
      "Epoch 101/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.6967\n",
      "Epoch 00101: val_loss improved from 6.00762 to 5.92848, saving model to ./20210227-225757/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 151ms/step - loss: 7.6967 - val_loss: 5.9285\n",
      "\n",
      "[20210227-2336-05] Learning rate for epoch 102 is 0.00018023152370005846\n",
      "Epoch 102/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.5442\n",
      "Epoch 00102: val_loss improved from 5.92848 to 5.64298, saving model to ./20210227-225757/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 2s 143ms/step - loss: 7.5442 - val_loss: 5.6430\n",
      "\n",
      "[20210227-2336-11] Learning rate for epoch 103 is 0.000359613070031628\n",
      "Epoch 103/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.5714\n",
      "Epoch 00103: val_loss did not improve from 5.64298\n",
      "17/17 [==============================] - 2s 105ms/step - loss: 7.5714 - val_loss: 5.6806\n",
      "\n",
      "[20210227-2336-16] Learning rate for epoch 104 is 0.0005385946715250611\n",
      "Epoch 104/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.3899\n",
      "Epoch 00104: val_loss improved from 5.64298 to 5.58288, saving model to ./20210227-225757/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 2s 144ms/step - loss: 7.3899 - val_loss: 5.5829\n",
      "\n",
      "[20210227-2336-21] Learning rate for epoch 105 is 0.0007171762990765274\n",
      "Epoch 105/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.1615\n",
      "Epoch 00105: val_loss improved from 5.58288 to 5.53113, saving model to ./20210227-225757/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 2s 144ms/step - loss: 7.1615 - val_loss: 5.5311\n",
      "\n",
      "[20210227-2336-27] Learning rate for epoch 106 is 0.0008953579817898571\n",
      "Epoch 106/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.9931\n",
      "Epoch 00106: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 6.9931 - val_loss: 6.0115\n",
      "\n",
      "[20210227-2336-32] Learning rate for epoch 107 is 0.0010731397196650505\n",
      "Epoch 107/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.1931\n",
      "Epoch 00107: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 7.1931 - val_loss: 5.7886\n",
      "\n",
      "[20210227-2336-38] Learning rate for epoch 108 is 0.0012505215127021074\n",
      "Epoch 108/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.2366\n",
      "Epoch 00108: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 7.2366 - val_loss: 5.9336\n",
      "\n",
      "[20210227-2336-43] Learning rate for epoch 109 is 0.001427503302693367\n",
      "Epoch 109/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.1334\n",
      "Epoch 00109: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 7.1334 - val_loss: 6.0328\n",
      "\n",
      "[20210227-2336-48] Learning rate for epoch 110 is 0.0016040850896388292\n",
      "Epoch 110/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.9973\n",
      "Epoch 00110: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 6.9973 - val_loss: 5.8558\n",
      "\n",
      "[20210227-2336-54] Learning rate for epoch 111 is 0.001780266989953816\n",
      "Epoch 111/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.8370\n",
      "Epoch 00111: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 6.8370 - val_loss: 5.7070\n",
      "\n",
      "[20210227-2336-59] Learning rate for epoch 112 is 0.0019560488872230053\n",
      "Epoch 112/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.9339\n",
      "Epoch 00112: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 6.9339 - val_loss: 6.3017\n",
      "\n",
      "[20210227-2337-04] Learning rate for epoch 113 is 0.002131430897861719\n",
      "Epoch 113/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.2519\n",
      "Epoch 00113: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 7.2519 - val_loss: 6.0774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[20210227-2337-10] Learning rate for epoch 114 is 0.0023064129054546356\n",
      "Epoch 114/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.8526\n",
      "Epoch 00114: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 6.8526 - val_loss: 5.9909\n",
      "\n",
      "[20210227-2337-15] Learning rate for epoch 115 is 0.0024809949100017548\n",
      "Epoch 115/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.2246\n",
      "Epoch 00115: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 7.2246 - val_loss: 6.1055\n",
      "\n",
      "[20210227-2337-20] Learning rate for epoch 116 is 0.0026551769115030766\n",
      "Epoch 116/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.8891\n",
      "Epoch 00116: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 6.8891 - val_loss: 5.9636\n",
      "\n",
      "[20210227-2337-25] Learning rate for epoch 117 is 0.0028289591427892447\n",
      "Epoch 117/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.2357\n",
      "Epoch 00117: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 7.2357 - val_loss: 6.5306\n",
      "\n",
      "[20210227-2337-31] Learning rate for epoch 118 is 0.0030023413710296154\n",
      "Epoch 118/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.4535\n",
      "Epoch 00118: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 7.4535 - val_loss: 5.9136\n",
      "\n",
      "[20210227-2337-36] Learning rate for epoch 119 is 0.003175323596224189\n",
      "Epoch 119/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.9861\n",
      "Epoch 00119: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 6.9861 - val_loss: 5.9941\n",
      "\n",
      "[20210227-2337-41] Learning rate for epoch 120 is 0.003347905818372965\n",
      "Epoch 120/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.3572\n",
      "Epoch 00120: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 116ms/step - loss: 7.3572 - val_loss: 6.5750\n",
      "\n",
      "[20210227-2337-47] Learning rate for epoch 121 is 0.0035200880374759436\n",
      "Epoch 121/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.2492\n",
      "Epoch 00121: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 7.2492 - val_loss: 5.8078\n",
      "\n",
      "[20210227-2337-52] Learning rate for epoch 122 is 0.003691870253533125\n",
      "Epoch 122/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.5201\n",
      "Epoch 00122: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 7.5201 - val_loss: 6.4247\n",
      "\n",
      "[20210227-2337-57] Learning rate for epoch 123 is 0.0038632526993751526\n",
      "Epoch 123/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.3597\n",
      "Epoch 00123: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 7.3597 - val_loss: 7.7674\n",
      "\n",
      "[20210227-2338-02] Learning rate for epoch 124 is 0.004034235142171383\n",
      "Epoch 124/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.1642\n",
      "Epoch 00124: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 7.1642 - val_loss: 6.8165\n",
      "\n",
      "[20210227-2338-07] Learning rate for epoch 125 is 0.004204817581921816\n",
      "Epoch 125/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.1977\n",
      "Epoch 00125: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 7.1977 - val_loss: 6.9293\n",
      "\n",
      "[20210227-2338-12] Learning rate for epoch 126 is 0.0043750000186264515\n",
      "Epoch 126/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.5340\n",
      "Epoch 00126: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 7.5340 - val_loss: 6.0057\n",
      "\n",
      "[20210227-2338-17] Learning rate for epoch 127 is 0.0041952175088226795\n",
      "Epoch 127/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.0391\n",
      "Epoch 00127: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 105ms/step - loss: 7.0391 - val_loss: 6.3985\n",
      "\n",
      "[20210227-2338-22] Learning rate for epoch 128 is 0.004015835002064705\n",
      "Epoch 128/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.2186\n",
      "Epoch 00128: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 7.2186 - val_loss: 6.4749\n",
      "\n",
      "[20210227-2338-27] Learning rate for epoch 129 is 0.003836852265521884\n",
      "Epoch 129/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.1697\n",
      "Epoch 00129: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 7.1697 - val_loss: 6.0758\n",
      "\n",
      "[20210227-2338-32] Learning rate for epoch 130 is 0.003658269764855504\n",
      "Epoch 130/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.1030\n",
      "Epoch 00130: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 7.1030 - val_loss: 5.9307\n",
      "\n",
      "[20210227-2338-38] Learning rate for epoch 131 is 0.003480087034404278\n",
      "Epoch 131/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.2224\n",
      "Epoch 00131: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 7.2224 - val_loss: 5.9162\n",
      "\n",
      "[20210227-2338-43] Learning rate for epoch 132 is 0.003302304306998849\n",
      "Epoch 132/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.3652\n",
      "Epoch 00132: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 7.3652 - val_loss: 6.2353\n",
      "\n",
      "[20210227-2338-48] Learning rate for epoch 133 is 0.0031249215826392174\n",
      "Epoch 133/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.0936\n",
      "Epoch 00133: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 7.0936 - val_loss: 6.0557\n",
      "\n",
      "[20210227-2338-53] Learning rate for epoch 134 is 0.0029479386284947395\n",
      "Epoch 134/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.1003\n",
      "Epoch 00134: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 7.1003 - val_loss: 5.8584\n",
      "\n",
      "[20210227-2338-58] Learning rate for epoch 135 is 0.0027713559102267027\n",
      "Epoch 135/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.8248\n",
      "Epoch 00135: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 6.8248 - val_loss: 5.7654\n",
      "\n",
      "[20210227-2339-04] Learning rate for epoch 136 is 0.0025951729621738195\n",
      "Epoch 136/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.8435\n",
      "Epoch 00136: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 6.8435 - val_loss: 5.9832\n",
      "\n",
      "[20210227-2339-09] Learning rate for epoch 137 is 0.0024193900171667337\n",
      "Epoch 137/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.8698\n",
      "Epoch 00137: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 6.8698 - val_loss: 5.9681\n",
      "\n",
      "[20210227-2339-14] Learning rate for epoch 138 is 0.0022440070752054453\n",
      "Epoch 138/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.5539\n",
      "Epoch 00138: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 6.5539 - val_loss: 6.5625\n",
      "\n",
      "[20210227-2339-19] Learning rate for epoch 139 is 0.002069024136289954\n",
      "Epoch 139/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.6747\n",
      "Epoch 00139: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 6.6747 - val_loss: 5.6350\n",
      "\n",
      "[20210227-2339-24] Learning rate for epoch 140 is 0.0018944410840049386\n",
      "Epoch 140/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.5765\n",
      "Epoch 00140: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 6.5765 - val_loss: 5.7235\n",
      "\n",
      "[20210227-2339-29] Learning rate for epoch 141 is 0.0017202580347657204\n",
      "Epoch 141/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.6549\n",
      "Epoch 00141: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 6.6549 - val_loss: 5.6805\n",
      "\n",
      "[20210227-2339-35] Learning rate for epoch 142 is 0.0015464748721569777\n",
      "Epoch 142/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - ETA: 0s - loss: 6.7984\n",
      "Epoch 00142: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 6.7984 - val_loss: 5.9710\n",
      "\n",
      "[20210227-2339-40] Learning rate for epoch 143 is 0.0013730917125940323\n",
      "Epoch 143/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.9521\n",
      "Epoch 00143: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 6.9521 - val_loss: 5.8537\n",
      "\n",
      "[20210227-2339-45] Learning rate for epoch 144 is 0.0012001085560768843\n",
      "Epoch 144/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.7512\n",
      "Epoch 00144: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 104ms/step - loss: 6.7512 - val_loss: 5.7308\n",
      "\n",
      "[20210227-2339-50] Learning rate for epoch 145 is 0.0010275252861902118\n",
      "Epoch 145/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.3427\n",
      "Epoch 00145: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 105ms/step - loss: 6.3427 - val_loss: 5.6400\n",
      "\n",
      "[20210227-2339-55] Learning rate for epoch 146 is 0.0008553420193493366\n",
      "Epoch 146/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.5145\n",
      "Epoch 00146: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 105ms/step - loss: 6.5145 - val_loss: 5.6989\n",
      "\n",
      "[20210227-2340-00] Learning rate for epoch 147 is 0.0006835586973465979\n",
      "Epoch 147/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.4663\n",
      "Epoch 00147: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 104ms/step - loss: 6.4663 - val_loss: 5.6469\n",
      "\n",
      "[20210227-2340-05] Learning rate for epoch 148 is 0.0005121753201819956\n",
      "Epoch 148/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.2775\n",
      "Epoch 00148: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 105ms/step - loss: 6.2775 - val_loss: 5.7004\n",
      "\n",
      "[20210227-2340-10] Learning rate for epoch 149 is 0.00034119191695936024\n",
      "Epoch 149/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.4202\n",
      "Epoch 00149: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 6.4202 - val_loss: 5.7228\n",
      "\n",
      "[20210227-2340-15] Learning rate for epoch 150 is 0.00017060847312677652\n",
      "Epoch 150/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.0745\n",
      "Epoch 00150: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 103ms/step - loss: 6.0745 - val_loss: 5.6306\n",
      "\n",
      "[20210227-2340-20] Learning rate for epoch 151 is 4.249999960848072e-07\n",
      "Epoch 151/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.1622\n",
      "Epoch 00151: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 6.1622 - val_loss: 5.6290\n",
      "\n",
      "[20210227-2340-25] Learning rate for epoch 152 is 0.00017020752420648932\n",
      "Epoch 152/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.3928\n",
      "Epoch 00152: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 6.3928 - val_loss: 5.6503\n",
      "\n",
      "[20210227-2340-30] Learning rate for epoch 153 is 0.0003395900712348521\n",
      "Epoch 153/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.2306\n",
      "Epoch 00153: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 105ms/step - loss: 6.2306 - val_loss: 5.5573\n",
      "\n",
      "[20210227-2340-35] Learning rate for epoch 154 is 0.0005085726734250784\n",
      "Epoch 154/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.3709\n",
      "Epoch 00154: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 6.3709 - val_loss: 5.6418\n",
      "\n",
      "[20210227-2340-40] Learning rate for epoch 155 is 0.0006771553307771683\n",
      "Epoch 155/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.2127\n",
      "Epoch 00155: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 104ms/step - loss: 6.2127 - val_loss: 5.6020\n",
      "\n",
      "[20210227-2340-46] Learning rate for epoch 156 is 0.0008453379850834608\n",
      "Epoch 156/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.2246\n",
      "Epoch 00156: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 6.2246 - val_loss: 5.8952\n",
      "\n",
      "[20210227-2340-51] Learning rate for epoch 157 is 0.0010131207527592778\n",
      "Epoch 157/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.3178\n",
      "Epoch 00157: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 6.3178 - val_loss: 5.7372\n",
      "\n",
      "[20210227-2340-56] Learning rate for epoch 158 is 0.0011805035173892975\n",
      "Epoch 158/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.4151\n",
      "Epoch 00158: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 6.4151 - val_loss: 6.0398\n",
      "\n",
      "[20210227-2341-01] Learning rate for epoch 159 is 0.0013474862789735198\n",
      "Epoch 159/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.3538\n",
      "Epoch 00159: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 105ms/step - loss: 6.3538 - val_loss: 5.9092\n",
      "\n",
      "[20210227-2341-06] Learning rate for epoch 160 is 0.0015140691539272666\n",
      "Epoch 160/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.2982\n",
      "Epoch 00160: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 6.2982 - val_loss: 6.2799\n",
      "\n",
      "[20210227-2341-11] Learning rate for epoch 161 is 0.001680252025835216\n",
      "Epoch 161/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.8540\n",
      "Epoch 00161: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 6.8540 - val_loss: 6.0814\n",
      "\n",
      "[20210227-2341-16] Learning rate for epoch 162 is 0.0018460348946973681\n",
      "Epoch 162/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.1831\n",
      "Epoch 00162: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 105ms/step - loss: 6.1831 - val_loss: 5.8760\n",
      "\n",
      "[20210227-2341-21] Learning rate for epoch 163 is 0.0020114178769290447\n",
      "Epoch 163/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.4406\n",
      "Epoch 00163: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 6.4406 - val_loss: 6.0201\n",
      "\n",
      "[20210227-2341-26] Learning rate for epoch 164 is 0.0021764009725302458\n",
      "Epoch 164/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.5653\n",
      "Epoch 00164: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 6.5653 - val_loss: 6.3092\n",
      "\n",
      "[20210227-2341-31] Learning rate for epoch 165 is 0.002340983832255006\n",
      "Epoch 165/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.6675\n",
      "Epoch 00165: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 6.6675 - val_loss: 5.7692\n",
      "\n",
      "[20210227-2341-36] Learning rate for epoch 166 is 0.002505166921764612\n",
      "Epoch 166/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.6807\n",
      "Epoch 00166: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 6.6807 - val_loss: 6.0479\n",
      "\n",
      "[20210227-2341-42] Learning rate for epoch 167 is 0.002668950008228421\n",
      "Epoch 167/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.5300\n",
      "Epoch 00167: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 6.5300 - val_loss: 6.1747\n",
      "\n",
      "[20210227-2341-47] Learning rate for epoch 168 is 0.0028323333244770765\n",
      "Epoch 168/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.5609\n",
      "Epoch 00168: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 104ms/step - loss: 6.5609 - val_loss: 5.9936\n",
      "\n",
      "[20210227-2341-52] Learning rate for epoch 169 is 0.002995316404849291\n",
      "Epoch 169/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.7795\n",
      "Epoch 00169: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 6.7795 - val_loss: 5.8692\n",
      "\n",
      "[20210227-2341-57] Learning rate for epoch 170 is 0.0031578997150063515\n",
      "Epoch 170/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - ETA: 0s - loss: 6.9084\n",
      "Epoch 00170: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 6.9084 - val_loss: 6.1732\n",
      "\n",
      "[20210227-2342-02] Learning rate for epoch 171 is 0.0033200830221176147\n",
      "Epoch 171/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.5386\n",
      "Epoch 00171: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 6.5386 - val_loss: 6.2326\n",
      "\n",
      "[20210227-2342-07] Learning rate for epoch 172 is 0.0034818663261830807\n",
      "Epoch 172/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.8550\n",
      "Epoch 00172: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 6.8550 - val_loss: 6.4094\n",
      "\n",
      "[20210227-2342-13] Learning rate for epoch 173 is 0.0036432496272027493\n",
      "Epoch 173/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.6913\n",
      "Epoch 00173: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 6.6913 - val_loss: 6.2595\n",
      "\n",
      "[20210227-2342-18] Learning rate for epoch 174 is 0.003804233158007264\n",
      "Epoch 174/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.0636\n",
      "Epoch 00174: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 7.0636 - val_loss: 6.5006\n",
      "\n",
      "[20210227-2342-23] Learning rate for epoch 175 is 0.003964816685765982\n",
      "Epoch 175/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.8686\n",
      "Epoch 00175: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 6.8686 - val_loss: 7.5829\n",
      "\n",
      "[20210227-2342-28] Learning rate for epoch 176 is 0.004124999977648258\n",
      "Epoch 176/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.7250\n",
      "Epoch 00176: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 6.7250 - val_loss: 6.4947\n",
      "\n",
      "[20210227-2342-34] Learning rate for epoch 177 is 0.003955216612666845\n",
      "Epoch 177/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.8131\n",
      "Epoch 00177: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 6.8131 - val_loss: 6.0735\n",
      "\n",
      "[20210227-2342-39] Learning rate for epoch 178 is 0.003785833017900586\n",
      "Epoch 178/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.9219\n",
      "Epoch 00178: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 6.9219 - val_loss: 7.5996\n",
      "\n",
      "[20210227-2342-44] Learning rate for epoch 179 is 0.0036168494261801243\n",
      "Epoch 179/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.9406\n",
      "Epoch 00179: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 6.9406 - val_loss: 6.1451\n",
      "\n",
      "[20210227-2342-50] Learning rate for epoch 180 is 0.003448265604674816\n",
      "Epoch 180/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.8712\n",
      "Epoch 00180: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 6.8712 - val_loss: 6.3384\n",
      "\n",
      "[20210227-2342-55] Learning rate for epoch 181 is 0.003280082019045949\n",
      "Epoch 181/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.5401\n",
      "Epoch 00181: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 6.5401 - val_loss: 6.4286\n",
      "\n",
      "[20210227-2343-00] Learning rate for epoch 182 is 0.0031122982036322355\n",
      "Epoch 182/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.7268\n",
      "Epoch 00182: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 6.7268 - val_loss: 6.6986\n",
      "\n",
      "[20210227-2343-05] Learning rate for epoch 183 is 0.002944914624094963\n",
      "Epoch 183/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.8101\n",
      "Epoch 00183: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 6.8101 - val_loss: 6.9724\n",
      "\n",
      "[20210227-2343-10] Learning rate for epoch 184 is 0.0027779308147728443\n",
      "Epoch 184/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.4841\n",
      "Epoch 00184: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 6.4841 - val_loss: 6.3232\n",
      "\n",
      "[20210227-2343-16] Learning rate for epoch 185 is 0.0026113467756658792\n",
      "Epoch 185/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.5184\n",
      "Epoch 00185: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 6.5184 - val_loss: 5.8288\n",
      "\n",
      "[20210227-2343-21] Learning rate for epoch 186 is 0.002445162972435355\n",
      "Epoch 186/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.3015\n",
      "Epoch 00186: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 6.3015 - val_loss: 6.0411\n",
      "\n",
      "[20210227-2343-26] Learning rate for epoch 187 is 0.0022793791722506285\n",
      "Epoch 187/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.3773\n",
      "Epoch 00187: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 6.3773 - val_loss: 5.9711\n",
      "\n",
      "[20210227-2343-32] Learning rate for epoch 188 is 0.0021139951422810555\n",
      "Epoch 188/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.1787\n",
      "Epoch 00188: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 6.1787 - val_loss: 6.4730\n",
      "\n",
      "[20210227-2343-37] Learning rate for epoch 189 is 0.0019490111153572798\n",
      "Epoch 189/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.4129\n",
      "Epoch 00189: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 100ms/step - loss: 6.4129 - val_loss: 6.1143\n",
      "\n",
      "[20210227-2343-42] Learning rate for epoch 190 is 0.0017844270914793015\n",
      "Epoch 190/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.6234\n",
      "Epoch 00190: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 6.6234 - val_loss: 6.0217\n",
      "\n",
      "[20210227-2343-47] Learning rate for epoch 191 is 0.0016202429542317986\n",
      "Epoch 191/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.0502\n",
      "Epoch 00191: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 6.0502 - val_loss: 5.9716\n",
      "\n",
      "[20210227-2343-52] Learning rate for epoch 192 is 0.001456458936445415\n",
      "Epoch 192/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.2293\n",
      "Epoch 00192: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 6.2293 - val_loss: 6.1562\n",
      "\n",
      "[20210227-2343-58] Learning rate for epoch 193 is 0.001293074688874185\n",
      "Epoch 193/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.2765\n",
      "Epoch 00193: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 6.2765 - val_loss: 6.2680\n",
      "\n",
      "[20210227-2344-03] Learning rate for epoch 194 is 0.0011300905607640743\n",
      "Epoch 194/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.2035\n",
      "Epoch 00194: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 6.2035 - val_loss: 6.1205\n",
      "\n",
      "[20210227-2344-09] Learning rate for epoch 195 is 0.0009675062610767782\n",
      "Epoch 195/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.0155\n",
      "Epoch 00195: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 116ms/step - loss: 6.0155 - val_loss: 6.1193\n",
      "\n",
      "[20210227-2344-14] Learning rate for epoch 196 is 0.0008053220226429403\n",
      "Epoch 196/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.2256\n",
      "Epoch 00196: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 6.2256 - val_loss: 6.0782\n",
      "\n",
      "[20210227-2344-19] Learning rate for epoch 197 is 0.0006435376708395779\n",
      "Epoch 197/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.0410\n",
      "Epoch 00197: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 6.0410 - val_loss: 5.8972\n",
      "\n",
      "[20210227-2344-25] Learning rate for epoch 198 is 0.0004821533220820129\n",
      "Epoch 198/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - ETA: 0s - loss: 6.0668\n",
      "Epoch 00198: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 6.0668 - val_loss: 5.9552\n",
      "\n",
      "[20210227-2344-30] Learning rate for epoch 199 is 0.0003211689181625843\n",
      "Epoch 199/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.0423\n",
      "Epoch 00199: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 6.0423 - val_loss: 6.0287\n",
      "\n",
      "[20210227-2344-35] Learning rate for epoch 200 is 0.00016058447363320738\n",
      "Epoch 200/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 5.9654\n",
      "Epoch 00200: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 5.9654 - val_loss: 5.9236\n",
      "\n",
      "[20210227-2344-41] Learning rate for epoch 201 is 4.0000000467443897e-07\n",
      "Epoch 201/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 5.9222\n",
      "Epoch 00201: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 5.9222 - val_loss: 5.8976\n",
      "\n",
      "[20210227-2344-46] Learning rate for epoch 202 is 0.0001601835247129202\n",
      "Epoch 202/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 5.9150\n",
      "Epoch 00202: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 5.9150 - val_loss: 5.8613\n",
      "\n",
      "[20210227-2344-51] Learning rate for epoch 203 is 0.00031956707243807614\n",
      "Epoch 203/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.1199\n",
      "Epoch 00203: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 6.1199 - val_loss: 5.8408\n",
      "\n",
      "[20210227-2344-56] Learning rate for epoch 204 is 0.00047855067532509565\n",
      "Epoch 204/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 5.8758\n",
      "Epoch 00204: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 5.8758 - val_loss: 5.7530\n",
      "\n",
      "[20210227-2345-02] Learning rate for epoch 205 is 0.0006371343042701483\n",
      "Epoch 205/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 5.9838\n",
      "Epoch 00205: val_loss did not improve from 5.53113\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 5.9838 - val_loss: 5.8493\n",
      "\n",
      " \n",
      " K =  2 \n",
      "\n",
      "Check cache in memory:Y True\n",
      "Check keypoints process01: Tensor(\"args_1:0\", shape=(2,), dtype=int64) (2,) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Check keypoints process02: Tensor(\"Reshape:0\", shape=(1, 2), dtype=int64) (1, 2) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Check keypoints process03: Tensor(\"PyFunc:1\", dtype=float32, device=/job:localhost/replica:0/task:0)\n",
      "Check keypoints process04: Tensor(\"Reshape_1:0\", shape=(2,), dtype=float32)\n",
      "Check augment :Y True\n",
      "Check ds cache[True] and augment[True]\n",
      "Check cache in memory:Y True\n",
      "Check augment :N False\n",
      "Check ds cache[True] and augment[False]\n",
      "best_model_name: ./20210227-225757/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "base_model.trainable :  False\n",
      "\n",
      "[20210227-2345-10] Learning rate for epoch 1 is 0.009999999776482582\n",
      "Epoch 1/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 76.7638\n",
      "Epoch 00001: val_loss improved from inf to 64.68189, saving model to ./20210227-225757/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 9s 536ms/step - loss: 75.7215 - val_loss: 64.6819\n",
      "\n",
      "[20210227-2345-31] Learning rate for epoch 2 is 0.009960000403225422\n",
      "Epoch 2/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 41.9104\n",
      "Epoch 00002: val_loss improved from 64.68189 to 13.77287, saving model to ./20210227-225757/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 50ms/step - loss: 40.8460 - val_loss: 13.7729\n",
      "\n",
      "[20210227-2345-36] Learning rate for epoch 3 is 0.009920000098645687\n",
      "Epoch 3/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 21.9779\n",
      "Epoch 00003: val_loss did not improve from 13.77287\n",
      "17/17 [==============================] - 6s 332ms/step - loss: 21.9779 - val_loss: 14.2324\n",
      "\n",
      "[20210227-2345-45] Learning rate for epoch 4 is 0.009879999794065952\n",
      "Epoch 4/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 19.8253\n",
      "Epoch 00004: val_loss improved from 13.77287 to 12.00152, saving model to ./20210227-225757/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 47ms/step - loss: 19.5696 - val_loss: 12.0015\n",
      "\n",
      "[20210227-2345-49] Learning rate for epoch 5 is 0.009840000420808792\n",
      "Epoch 5/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 18.5330\n",
      "Epoch 00005: val_loss improved from 12.00152 to 11.92546, saving model to ./20210227-225757/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 45ms/step - loss: 18.4119 - val_loss: 11.9255\n",
      "\n",
      "[20210227-2345-53] Learning rate for epoch 6 is 0.009800000116229057\n",
      "Epoch 6/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 17.4370\n",
      "Epoch 00006: val_loss improved from 11.92546 to 11.29395, saving model to ./20210227-225757/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 50ms/step - loss: 17.4370 - val_loss: 11.2939\n",
      "\n",
      "[20210227-2345-56] Learning rate for epoch 7 is 0.009759999811649323\n",
      "Epoch 7/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 16.9261\n",
      "Epoch 00007: val_loss did not improve from 11.29395\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 16.9391 - val_loss: 11.6086\n",
      "\n",
      "[20210227-2346-00] Learning rate for epoch 8 is 0.009720000438392162\n",
      "Epoch 8/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 16.6996\n",
      "Epoch 00008: val_loss did not improve from 11.29395\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 16.6648 - val_loss: 11.8952\n",
      "\n",
      "[20210227-2346-04] Learning rate for epoch 9 is 0.009680000133812428\n",
      "Epoch 9/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 16.6067\n",
      "Epoch 00009: val_loss improved from 11.29395 to 11.07800, saving model to ./20210227-225757/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 46ms/step - loss: 16.6067 - val_loss: 11.0780\n",
      "\n",
      "[20210227-2346-07] Learning rate for epoch 10 is 0.009639999829232693\n",
      "Epoch 10/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 16.4462\n",
      "Epoch 00010: val_loss did not improve from 11.07800\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 16.4776 - val_loss: 11.2827\n",
      "\n",
      "[20210227-2346-11] Learning rate for epoch 11 is 0.009600000455975533\n",
      "Epoch 11/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 16.0168\n",
      "Epoch 00011: val_loss improved from 11.07800 to 10.79741, saving model to ./20210227-225757/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 48ms/step - loss: 16.0541 - val_loss: 10.7974\n",
      "\n",
      "[20210227-2346-15] Learning rate for epoch 12 is 0.009560000151395798\n",
      "Epoch 12/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 15.5752\n",
      "Epoch 00012: val_loss improved from 10.79741 to 10.78285, saving model to ./20210227-225757/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 46ms/step - loss: 15.7706 - val_loss: 10.7828\n",
      "\n",
      "[20210227-2346-19] Learning rate for epoch 13 is 0.009519999846816063\n",
      "Epoch 13/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 15.5963\n",
      "Epoch 00013: val_loss did not improve from 10.78285\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 15.5963 - val_loss: 11.2069\n",
      "\n",
      "[20210227-2346-22] Learning rate for epoch 14 is 0.009479999542236328\n",
      "Epoch 14/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 15.1232\n",
      "Epoch 00014: val_loss did not improve from 10.78285\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 15.0959 - val_loss: 11.0795\n",
      "\n",
      "[20210227-2346-26] Learning rate for epoch 15 is 0.009440000168979168\n",
      "Epoch 15/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 15.6389\n",
      "Epoch 00015: val_loss improved from 10.78285 to 10.42015, saving model to ./20210227-225757/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 47ms/step - loss: 15.6389 - val_loss: 10.4201\n",
      "\n",
      "[20210227-2346-30] Learning rate for epoch 16 is 0.009399999864399433\n",
      "Epoch 16/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/17 [===========================>..] - ETA: 0s - loss: 15.0732\n",
      "Epoch 00016: val_loss improved from 10.42015 to 10.34877, saving model to ./20210227-225757/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 45ms/step - loss: 15.0745 - val_loss: 10.3488\n",
      "\n",
      "[20210227-2346-33] Learning rate for epoch 17 is 0.009359999559819698\n",
      "Epoch 17/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 14.9234\n",
      "Epoch 00017: val_loss did not improve from 10.34877\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 14.8872 - val_loss: 10.6700\n",
      "\n",
      "[20210227-2346-37] Learning rate for epoch 18 is 0.009320000186562538\n",
      "Epoch 18/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.8691\n",
      "Epoch 00018: val_loss did not improve from 10.34877\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 14.8691 - val_loss: 11.3337\n",
      "\n",
      "[20210227-2346-40] Learning rate for epoch 19 is 0.009279999881982803\n",
      "Epoch 19/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 15.3135\n",
      "Epoch 00019: val_loss improved from 10.34877 to 10.05763, saving model to ./20210227-225757/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 44ms/step - loss: 15.2178 - val_loss: 10.0576\n",
      "\n",
      "[20210227-2346-44] Learning rate for epoch 20 is 0.009239999577403069\n",
      "Epoch 20/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 14.2693\n",
      "Epoch 00020: val_loss improved from 10.05763 to 9.90464, saving model to ./20210227-225757/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 52ms/step - loss: 14.2859 - val_loss: 9.9046\n",
      "\n",
      "[20210227-2346-48] Learning rate for epoch 21 is 0.009200000204145908\n",
      "Epoch 21/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.8506\n",
      "Epoch 00021: val_loss did not improve from 9.90464\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 14.8506 - val_loss: 9.9144\n",
      "\n",
      "[20210227-2346-51] Learning rate for epoch 22 is 0.009159999899566174\n",
      "Epoch 22/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 14.3894\n",
      "Epoch 00022: val_loss did not improve from 9.90464\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 14.4951 - val_loss: 10.6051\n",
      "\n",
      "[20210227-2346-55] Learning rate for epoch 23 is 0.009119999594986439\n",
      "Epoch 23/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.6011\n",
      "Epoch 00023: val_loss did not improve from 9.90464\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 14.6011 - val_loss: 10.2409\n",
      "\n",
      "[20210227-2346-59] Learning rate for epoch 24 is 0.009080000221729279\n",
      "Epoch 24/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 14.4514\n",
      "Epoch 00024: val_loss did not improve from 9.90464\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 14.4294 - val_loss: 10.2695\n",
      "\n",
      "[20210227-2347-02] Learning rate for epoch 25 is 0.009039999917149544\n",
      "Epoch 25/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.3125\n",
      "Epoch 00025: val_loss did not improve from 9.90464\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 14.3125 - val_loss: 10.1425\n",
      "\n",
      "[20210227-2347-06] Learning rate for epoch 26 is 0.008999999612569809\n",
      "Epoch 26/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 14.7889\n",
      "Epoch 00026: val_loss did not improve from 9.90464\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 14.6903 - val_loss: 10.1153\n",
      "\n",
      "[20210227-2347-10] Learning rate for epoch 27 is 0.008960000239312649\n",
      "Epoch 27/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 14.2889\n",
      "Epoch 00027: val_loss did not improve from 9.90464\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 14.3118 - val_loss: 10.1014\n",
      "\n",
      "[20210227-2347-14] Learning rate for epoch 28 is 0.008919999934732914\n",
      "Epoch 28/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 14.0267\n",
      "Epoch 00028: val_loss did not improve from 9.90464\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 14.0532 - val_loss: 10.2042\n",
      "\n",
      "[20210227-2347-17] Learning rate for epoch 29 is 0.00887999963015318\n",
      "Epoch 29/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.2003\n",
      "Epoch 00029: val_loss did not improve from 9.90464\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 14.2003 - val_loss: 10.5292\n",
      "\n",
      "[20210227-2347-21] Learning rate for epoch 30 is 0.008840000256896019\n",
      "Epoch 30/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 14.2444\n",
      "Epoch 00030: val_loss did not improve from 9.90464\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 14.2746 - val_loss: 10.1331\n",
      "\n",
      "[20210227-2347-24] Learning rate for epoch 31 is 0.008799999952316284\n",
      "Epoch 31/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 14.6942\n",
      "Epoch 00031: val_loss did not improve from 9.90464\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 14.7522 - val_loss: 10.1593\n",
      "\n",
      "[20210227-2347-28] Learning rate for epoch 32 is 0.00875999964773655\n",
      "Epoch 32/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 14.2169\n",
      "Epoch 00032: val_loss did not improve from 9.90464\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 14.1757 - val_loss: 9.9882\n",
      "\n",
      "[20210227-2347-32] Learning rate for epoch 33 is 0.00872000027447939\n",
      "Epoch 33/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 14.1069\n",
      "Epoch 00033: val_loss improved from 9.90464 to 9.87630, saving model to ./20210227-225757/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 45ms/step - loss: 14.0351 - val_loss: 9.8763\n",
      "\n",
      "[20210227-2347-36] Learning rate for epoch 34 is 0.008679999969899654\n",
      "Epoch 34/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 14.1612\n",
      "Epoch 00034: val_loss improved from 9.87630 to 9.85829, saving model to ./20210227-225757/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 46ms/step - loss: 14.3109 - val_loss: 9.8583\n",
      "\n",
      "[20210227-2347-40] Learning rate for epoch 35 is 0.00863999966531992\n",
      "Epoch 35/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 14.0156\n",
      "Epoch 00035: val_loss did not improve from 9.85829\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 14.0236 - val_loss: 9.9123\n",
      "\n",
      "[20210227-2347-43] Learning rate for epoch 36 is 0.00860000029206276\n",
      "Epoch 36/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 13.9386\n",
      "Epoch 00036: val_loss did not improve from 9.85829\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 13.8864 - val_loss: 9.9243\n",
      "\n",
      "[20210227-2347-47] Learning rate for epoch 37 is 0.008559999987483025\n",
      "Epoch 37/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.9253\n",
      "Epoch 00037: val_loss did not improve from 9.85829\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 13.9253 - val_loss: 9.8949\n",
      "\n",
      "[20210227-2347-50] Learning rate for epoch 38 is 0.00851999968290329\n",
      "Epoch 38/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 14.0627\n",
      "Epoch 00038: val_loss did not improve from 9.85829\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 14.1763 - val_loss: 10.6772\n",
      "\n",
      "[20210227-2347-54] Learning rate for epoch 39 is 0.00848000030964613\n",
      "Epoch 39/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 14.4863\n",
      "Epoch 00039: val_loss did not improve from 9.85829\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 14.4241 - val_loss: 10.3460\n",
      "\n",
      "[20210227-2347-58] Learning rate for epoch 40 is 0.008440000005066395\n",
      "Epoch 40/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 13.9292\n",
      "Epoch 00040: val_loss did not improve from 9.85829\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 13.9578 - val_loss: 10.4748\n",
      "\n",
      "[20210227-2348-01] Learning rate for epoch 41 is 0.00839999970048666\n",
      "Epoch 41/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 14.0616\n",
      "Epoch 00041: val_loss did not improve from 9.85829\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 14.1385 - val_loss: 10.0464\n",
      "\n",
      "[20210227-2348-05] Learning rate for epoch 42 is 0.0083600003272295\n",
      "Epoch 42/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.1768\n",
      "Epoch 00042: val_loss did not improve from 9.85829\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 14.1768 - val_loss: 10.3527\n",
      "\n",
      "[20210227-2348-08] Learning rate for epoch 43 is 0.008320000022649765\n",
      "Epoch 43/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - ETA: 0s - loss: 13.8369\n",
      "Epoch 00043: val_loss did not improve from 9.85829\n",
      "17/17 [==============================] - 1s 66ms/step - loss: 13.8369 - val_loss: 10.0448\n",
      "\n",
      "[20210227-2348-12] Learning rate for epoch 44 is 0.00827999971807003\n",
      "Epoch 44/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 14.1603\n",
      "Epoch 00044: val_loss did not improve from 9.85829\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 14.1675 - val_loss: 9.8966\n",
      "\n",
      "[20210227-2348-16] Learning rate for epoch 45 is 0.00824000034481287\n",
      "Epoch 45/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 14.2054\n",
      "Epoch 00045: val_loss did not improve from 9.85829\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 14.1921 - val_loss: 10.2465\n",
      "\n",
      "[20210227-2348-19] Learning rate for epoch 46 is 0.008200000040233135\n",
      "Epoch 46/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 13.8762\n",
      "Epoch 00046: val_loss did not improve from 9.85829\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 13.9259 - val_loss: 10.0417\n",
      "\n",
      "[20210227-2348-23] Learning rate for epoch 47 is 0.0081599997356534\n",
      "Epoch 47/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.6458\n",
      "Epoch 00047: val_loss improved from 9.85829 to 9.85687, saving model to ./20210227-225757/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 47ms/step - loss: 13.6458 - val_loss: 9.8569\n",
      "\n",
      "[20210227-2348-27] Learning rate for epoch 48 is 0.00812000036239624\n",
      "Epoch 48/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 14.0526\n",
      "Epoch 00048: val_loss improved from 9.85687 to 9.76812, saving model to ./20210227-225757/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 45ms/step - loss: 14.0983 - val_loss: 9.7681\n",
      "\n",
      "[20210227-2348-31] Learning rate for epoch 49 is 0.008080000057816505\n",
      "Epoch 49/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.7706\n",
      "Epoch 00049: val_loss did not improve from 9.76812\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 13.7706 - val_loss: 9.8909\n",
      "\n",
      "[20210227-2348-35] Learning rate for epoch 50 is 0.00803999975323677\n",
      "Epoch 50/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 13.6538\n",
      "Epoch 00050: val_loss improved from 9.76812 to 9.75404, saving model to ./20210227-225757/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 46ms/step - loss: 13.6660 - val_loss: 9.7540\n",
      "\n",
      "[20210227-2348-38] Learning rate for epoch 51 is 0.00800000037997961\n",
      "Epoch 51/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 13.4418\n",
      "Epoch 00051: val_loss improved from 9.75404 to 9.74887, saving model to ./20210227-225757/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 46ms/step - loss: 13.4467 - val_loss: 9.7489\n",
      "\n",
      "[20210227-2348-42] Learning rate for epoch 52 is 0.007960000075399876\n",
      "Epoch 52/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.0172\n",
      "Epoch 00052: val_loss did not improve from 9.74887\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 14.0172 - val_loss: 10.2705\n",
      "\n",
      "[20210227-2348-46] Learning rate for epoch 53 is 0.00791999977082014\n",
      "Epoch 53/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 13.7291\n",
      "Epoch 00053: val_loss did not improve from 9.74887\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 13.8602 - val_loss: 9.8522\n",
      "\n",
      "[20210227-2348-50] Learning rate for epoch 54 is 0.00788000039756298\n",
      "Epoch 54/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 13.9003\n",
      "Epoch 00054: val_loss improved from 9.74887 to 9.74243, saving model to ./20210227-225757/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 46ms/step - loss: 13.8364 - val_loss: 9.7424\n",
      "\n",
      "[20210227-2348-54] Learning rate for epoch 55 is 0.007840000092983246\n",
      "Epoch 55/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 14.0517\n",
      "Epoch 00055: val_loss improved from 9.74243 to 9.68594, saving model to ./20210227-225757/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 1s 47ms/step - loss: 14.0005 - val_loss: 9.6859\n",
      "\n",
      "[20210227-2348-58] Learning rate for epoch 56 is 0.007799999788403511\n",
      "Epoch 56/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 14.0026\n",
      "Epoch 00056: val_loss did not improve from 9.68594\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 13.9174 - val_loss: 9.8684\n",
      "\n",
      "[20210227-2349-02] Learning rate for epoch 57 is 0.0077599999494850636\n",
      "Epoch 57/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 13.7709\n",
      "Epoch 00057: val_loss did not improve from 9.68594\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 13.7953 - val_loss: 10.2726\n",
      "\n",
      "[20210227-2349-06] Learning rate for epoch 58 is 0.007720000110566616\n",
      "Epoch 58/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 13.7857\n",
      "Epoch 00058: val_loss did not improve from 9.68594\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 13.7638 - val_loss: 9.8562\n",
      "\n",
      "[20210227-2349-10] Learning rate for epoch 59 is 0.007679999805986881\n",
      "Epoch 59/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 13.1455\n",
      "Epoch 00059: val_loss did not improve from 9.68594\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 13.1574 - val_loss: 9.7285\n",
      "\n",
      "[20210227-2349-13] Learning rate for epoch 60 is 0.007639999967068434\n",
      "Epoch 60/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 13.6689\n",
      "Epoch 00060: val_loss did not improve from 9.68594\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 13.7677 - val_loss: 9.7991\n",
      "\n",
      "[20210227-2349-17] Learning rate for epoch 61 is 0.007600000128149986\n",
      "Epoch 61/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 13.4194\n",
      "Epoch 00061: val_loss did not improve from 9.68594\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 13.3895 - val_loss: 10.1635\n",
      "\n",
      "[20210227-2349-21] Learning rate for epoch 62 is 0.0075599998235702515\n",
      "Epoch 62/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 13.8159\n",
      "Epoch 00062: val_loss did not improve from 9.68594\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 13.7794 - val_loss: 10.5039\n",
      "\n",
      "[20210227-2349-25] Learning rate for epoch 63 is 0.007519999984651804\n",
      "Epoch 63/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 13.7435\n",
      "Epoch 00063: val_loss did not improve from 9.68594\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 13.6823 - val_loss: 10.3349\n",
      "\n",
      "[20210227-2349-29] Learning rate for epoch 64 is 0.0074800001457333565\n",
      "Epoch 64/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 13.5263\n",
      "Epoch 00064: val_loss did not improve from 9.68594\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 13.5188 - val_loss: 9.8778\n",
      "\n",
      "[20210227-2349-32] Learning rate for epoch 65 is 0.007439999841153622\n",
      "Epoch 65/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 13.2702\n",
      "Epoch 00065: val_loss did not improve from 9.68594\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 13.3215 - val_loss: 9.8639\n",
      "\n",
      "[20210227-2349-36] Learning rate for epoch 66 is 0.007400000002235174\n",
      "Epoch 66/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 13.6444\n",
      "Epoch 00066: val_loss did not improve from 9.68594\n",
      "17/17 [==============================] - 0s 22ms/step - loss: 13.5115 - val_loss: 9.9890\n",
      "\n",
      "[20210227-2349-39] Learning rate for epoch 67 is 0.007360000163316727\n",
      "Epoch 67/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 13.2543\n",
      "Epoch 00067: val_loss did not improve from 9.68594\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 13.2950 - val_loss: 9.8065\n",
      "\n",
      "[20210227-2349-42] Learning rate for epoch 68 is 0.007319999858736992\n",
      "Epoch 68/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 13.6705\n",
      "Epoch 00068: val_loss did not improve from 9.68594\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 13.7047 - val_loss: 9.8742\n",
      "\n",
      "[20210227-2349-46] Learning rate for epoch 69 is 0.007280000019818544\n",
      "Epoch 69/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.4950\n",
      "Epoch 00069: val_loss did not improve from 9.68594\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 13.4950 - val_loss: 9.8638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[20210227-2349-49] Learning rate for epoch 70 is 0.007240000180900097\n",
      "Epoch 70/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 13.4904\n",
      "Epoch 00070: val_loss did not improve from 9.68594\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 13.4700 - val_loss: 9.7789\n",
      "\n",
      "[20210227-2349-53] Learning rate for epoch 71 is 0.007199999876320362\n",
      "Epoch 71/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.5312\n",
      "Epoch 00071: val_loss did not improve from 9.68594\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 13.5312 - val_loss: 9.8723\n",
      "\n",
      "[20210227-2349-56] Learning rate for epoch 72 is 0.007160000037401915\n",
      "Epoch 72/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 13.0561\n",
      "Epoch 00072: val_loss did not improve from 9.68594\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 13.0927 - val_loss: 9.7918\n",
      "\n",
      "[20210227-2350-00] Learning rate for epoch 73 is 0.007120000198483467\n",
      "Epoch 73/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.6940\n",
      "Epoch 00073: val_loss did not improve from 9.68594\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 13.6940 - val_loss: 9.9159\n",
      "\n",
      "[20210227-2350-04] Learning rate for epoch 74 is 0.007079999893903732\n",
      "Epoch 74/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 13.2992\n",
      "Epoch 00074: val_loss did not improve from 9.68594\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 13.2558 - val_loss: 9.7270\n",
      "\n",
      "[20210227-2350-07] Learning rate for epoch 75 is 0.007040000054985285\n",
      "Epoch 75/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 13.8029\n",
      "Epoch 00075: val_loss did not improve from 9.68594\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 13.7703 - val_loss: 9.8273\n",
      "\n",
      "*********************************** Start fine tune ***********************************\n",
      "tt: 241, nt:0, total layers:241\n",
      "*********************************** Start fine tune ***********************************\n",
      "\n",
      "[20210227-2350-11] Learning rate for epoch 1 is 4.999999987376214e-07\n",
      "Epoch 1/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.9121\n",
      "Epoch 00001: val_loss did not improve from 9.68594\n",
      "17/17 [==============================] - 22s 1s/step - loss: 14.9121 - val_loss: 9.8164\n",
      "\n",
      "[20210227-2350-52] Learning rate for epoch 2 is 5.044450517743826e-05\n",
      "Epoch 2/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.7063\n",
      "Epoch 00002: val_loss did not improve from 9.68594\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 14.7063 - val_loss: 9.8274\n",
      "\n",
      "[20210227-2350-57] Learning rate for epoch 3 is 0.00010028902033809572\n",
      "Epoch 3/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.3956\n",
      "Epoch 00003: val_loss did not improve from 9.68594\n",
      "17/17 [==============================] - 19s 1s/step - loss: 14.3956 - val_loss: 10.0249\n",
      "\n",
      "[20210227-2351-19] Learning rate for epoch 4 is 0.00015003354928921908\n",
      "Epoch 4/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 14.4000\n",
      "Epoch 00004: val_loss did not improve from 9.68594\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 14.4000 - val_loss: 10.3840\n",
      "\n",
      "[20210227-2351-24] Learning rate for epoch 5 is 0.0001996780774788931\n",
      "Epoch 5/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 13.2216\n",
      "Epoch 00005: val_loss did not improve from 9.68594\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 13.2216 - val_loss: 10.4960\n",
      "\n",
      "[20210227-2351-30] Learning rate for epoch 6 is 0.000249222619459033\n",
      "Epoch 6/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.6621\n",
      "Epoch 00006: val_loss did not improve from 9.68594\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 12.6621 - val_loss: 9.9190\n",
      "\n",
      "[20210227-2351-35] Learning rate for epoch 7 is 0.0002986671752296388\n",
      "Epoch 7/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.5948\n",
      "Epoch 00007: val_loss improved from 9.68594 to 9.40452, saving model to ./20210227-225757/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 2s 147ms/step - loss: 12.5948 - val_loss: 9.4045\n",
      "\n",
      "[20210227-2351-41] Learning rate for epoch 8 is 0.00034801175934262574\n",
      "Epoch 8/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 12.2178\n",
      "Epoch 00008: val_loss improved from 9.40452 to 8.76482, saving model to ./20210227-225757/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 2s 144ms/step - loss: 12.2178 - val_loss: 8.7648\n",
      "\n",
      "[20210227-2351-47] Learning rate for epoch 9 is 0.00039725631359033287\n",
      "Epoch 9/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 11.9651\n",
      "Epoch 00009: val_loss improved from 8.76482 to 8.37027, saving model to ./20210227-225757/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 2s 146ms/step - loss: 11.9651 - val_loss: 8.3703\n",
      "\n",
      "[20210227-2351-52] Learning rate for epoch 10 is 0.0004464008961804211\n",
      "Epoch 10/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 11.1731\n",
      "Epoch 00010: val_loss improved from 8.37027 to 8.15196, saving model to ./20210227-225757/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 2s 147ms/step - loss: 11.1731 - val_loss: 8.1520\n",
      "\n",
      "[20210227-2351-58] Learning rate for epoch 11 is 0.0004954455071128905\n",
      "Epoch 11/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 11.6420\n",
      "Epoch 00011: val_loss did not improve from 8.15196\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 11.6420 - val_loss: 9.0026\n",
      "\n",
      "[20210227-2352-03] Learning rate for epoch 12 is 0.0005443901172839105\n",
      "Epoch 12/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 11.3785\n",
      "Epoch 00012: val_loss did not improve from 8.15196\n",
      "17/17 [==============================] - 2s 104ms/step - loss: 11.3785 - val_loss: 8.3656\n",
      "\n",
      "[20210227-2352-07] Learning rate for epoch 13 is 0.0005932347266934812\n",
      "Epoch 13/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 11.4615\n",
      "Epoch 00013: val_loss did not improve from 8.15196\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 11.4615 - val_loss: 8.4913\n",
      "\n",
      "[20210227-2352-12] Learning rate for epoch 14 is 0.0006419793353416026\n",
      "Epoch 14/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 10.9833\n",
      "Epoch 00014: val_loss did not improve from 8.15196\n",
      "17/17 [==============================] - 2s 105ms/step - loss: 10.9833 - val_loss: 8.3799\n",
      "\n",
      "[20210227-2352-17] Learning rate for epoch 15 is 0.0006906240014359355\n",
      "Epoch 15/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 10.8223\n",
      "Epoch 00015: val_loss did not improve from 8.15196\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 10.8223 - val_loss: 8.1862\n",
      "\n",
      "[20210227-2352-22] Learning rate for epoch 16 is 0.0007391686085611582\n",
      "Epoch 16/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 11.0092\n",
      "Epoch 00016: val_loss improved from 8.15196 to 7.49425, saving model to ./20210227-225757/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "17/17 [==============================] - 3s 147ms/step - loss: 11.0092 - val_loss: 7.4942\n",
      "\n",
      "[20210227-2352-28] Learning rate for epoch 17 is 0.0007876132731325924\n",
      "Epoch 17/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 10.5427\n",
      "Epoch 00017: val_loss did not improve from 7.49425\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 10.5427 - val_loss: 8.9784\n",
      "\n",
      "[20210227-2352-33] Learning rate for epoch 18 is 0.0008359579369425774\n",
      "Epoch 18/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 10.7421\n",
      "Epoch 00018: val_loss did not improve from 7.49425\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 10.7421 - val_loss: 8.4012\n",
      "\n",
      "[20210227-2352-38] Learning rate for epoch 19 is 0.000884202599991113\n",
      "Epoch 19/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 10.3954\n",
      "Epoch 00019: val_loss did not improve from 7.49425\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 10.3954 - val_loss: 8.7819\n",
      "\n",
      "[20210227-2352-43] Learning rate for epoch 20 is 0.0009323473204858601\n",
      "Epoch 20/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 10.3848\n",
      "Epoch 00020: val_loss did not improve from 7.49425\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 10.3848 - val_loss: 9.6659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[20210227-2352-49] Learning rate for epoch 21 is 0.000980391982011497\n",
      "Epoch 21/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 10.2819\n",
      "Epoch 00021: val_loss did not improve from 7.49425\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 10.2819 - val_loss: 10.2306\n",
      "\n",
      "[20210227-2352-54] Learning rate for epoch 22 is 0.0010283367009833455\n",
      "Epoch 22/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 10.2060\n",
      "Epoch 00022: val_loss did not improve from 7.49425\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 10.2060 - val_loss: 9.3239\n",
      "\n",
      "[20210227-2352-59] Learning rate for epoch 23 is 0.0010761814191937447\n",
      "Epoch 23/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 10.1419\n",
      "Epoch 00023: val_loss did not improve from 7.49425\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 10.1419 - val_loss: 9.0815\n",
      "\n",
      "[20210227-2353-05] Learning rate for epoch 24 is 0.0011239261366426945\n",
      "Epoch 24/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 9.9197\n",
      "Epoch 00024: val_loss did not improve from 7.49425\n",
      "17/17 [==============================] - 2s 105ms/step - loss: 9.9197 - val_loss: 8.4563\n",
      "\n",
      "[20210227-2353-10] Learning rate for epoch 25 is 0.001171570853330195\n",
      "Epoch 25/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 10.0577\n",
      "Epoch 00025: val_loss did not improve from 7.49425\n",
      "17/17 [==============================] - 2s 118ms/step - loss: 10.0577 - val_loss: 8.7904\n",
      "\n",
      "[20210227-2353-15] Learning rate for epoch 26 is 0.001219115569256246\n",
      "Epoch 26/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 9.6328\n",
      "Epoch 00026: val_loss did not improve from 7.49425\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 9.6328 - val_loss: 8.9047\n",
      "\n",
      "[20210227-2353-21] Learning rate for epoch 27 is 0.0012665604008361697\n",
      "Epoch 27/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 9.8165\n",
      "Epoch 00027: val_loss did not improve from 7.49425\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 9.8165 - val_loss: 9.0962\n",
      "\n",
      "[20210227-2353-26] Learning rate for epoch 28 is 0.0013139051152393222\n",
      "Epoch 28/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 9.8254\n",
      "Epoch 00028: val_loss did not improve from 7.49425\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 9.8254 - val_loss: 8.6250\n",
      "\n",
      "[20210227-2353-31] Learning rate for epoch 29 is 0.0013611499452963471\n",
      "Epoch 29/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 9.6281\n",
      "Epoch 00029: val_loss did not improve from 7.49425\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 9.6281 - val_loss: 9.6009\n",
      "\n",
      "[20210227-2353-36] Learning rate for epoch 30 is 0.001408294658176601\n",
      "Epoch 30/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 9.3011\n",
      "Epoch 00030: val_loss did not improve from 7.49425\n",
      "17/17 [==============================] - 2s 105ms/step - loss: 9.3011 - val_loss: 9.5683\n",
      "\n",
      "[20210227-2353-42] Learning rate for epoch 31 is 0.0014553394867107272\n",
      "Epoch 31/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 9.4453\n",
      "Epoch 00031: val_loss did not improve from 7.49425\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 9.4453 - val_loss: 8.8885\n",
      "\n",
      "[20210227-2353-47] Learning rate for epoch 32 is 0.0015022843144834042\n",
      "Epoch 32/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 9.4747\n",
      "Epoch 00032: val_loss did not improve from 7.49425\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 9.4747 - val_loss: 8.4569\n",
      "\n",
      "[20210227-2353-52] Learning rate for epoch 33 is 0.0015491291414946318\n",
      "Epoch 33/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.9985\n",
      "Epoch 00033: val_loss did not improve from 7.49425\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 8.9985 - val_loss: 8.4714\n",
      "\n",
      "[20210227-2353-57] Learning rate for epoch 34 is 0.00159587396774441\n",
      "Epoch 34/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 9.1000\n",
      "Epoch 00034: val_loss did not improve from 7.49425\n",
      "17/17 [==============================] - 2s 116ms/step - loss: 9.1000 - val_loss: 9.8396\n",
      "\n",
      "[20210227-2354-02] Learning rate for epoch 35 is 0.001642518793232739\n",
      "Epoch 35/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 9.1050\n",
      "Epoch 00035: val_loss did not improve from 7.49425\n",
      "17/17 [==============================] - 2s 100ms/step - loss: 9.1050 - val_loss: 9.2395\n",
      "\n",
      "[20210227-2354-08] Learning rate for epoch 36 is 0.0016890636179596186\n",
      "Epoch 36/1000\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 10.9193"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# big K = 5 (fold 0 ~ 4) \n",
    "KFlodNum = 5\n",
    "\n",
    "\n",
    "\n",
    "history_toe = []\n",
    "history_toe_finetune = []\n",
    "\n",
    "#above until 'train_ds_map_toe now' to 'train_ds_map_toe_s', 'valid_ds_map_toe_s'\n",
    "for k in range(KFlodNum):\n",
    "    \n",
    "    \n",
    "    # Split data to train/valid with K-Fold #\n",
    "    print(\"\\n \\n K = \", k, \"\\n\")\n",
    "    # Toe split\n",
    "    train_ds_map_toe_s, valid_ds_map_toe_s = get_KFold_ds(train_ds_map_toe, K=k)\n",
    "    \n",
    "    # Toe ds_pre\n",
    "    train_ds_pre_toe_s = configure_for_performance_cache_train(train_ds_map_toe_s, augment=True)\n",
    "    valid_ds_pre_toe_s = configure_for_performance_cache_val(valid_ds_map_toe_s)\n",
    "    \n",
    "    \n",
    "#     # heel split\n",
    "#     train_ds_map_heel_s, valid_ds_map_heel_s = get_KFold_ds(train_ds_map_heel, K=k)\n",
    "#     # Heel ds_pre\n",
    "#     train_ds_pre_heel_s = configure_for_performance_cache_train(train_ds_map_heel_s, augment=True)\n",
    "#     valid_ds_pre_heel_s = configure_for_performance_cache_val(valid_ds_map_heel_s)\n",
    "    \n",
    "    \n",
    "    # Train K-Model with transfer learnling #\n",
    "    \n",
    "    # Toe model, TL\n",
    "    th = 'toe'\n",
    "    # th = 'heel'\n",
    "    best_model_name = get_best_model_name(th, K=str(k))\n",
    "    best_model_save = tf.keras.callbacks.ModelCheckpoint(filepath=best_model_name, \n",
    "                                 save_best_only = True, \n",
    "                                 save_weights_only = False,\n",
    "                                 monitor = monitor, \n",
    "                                 mode = 'auto', verbose = 1)\n",
    "    callbacks_toe_tl = [\n",
    "                    #     tensorboard_callback,\n",
    "                        best_model_save,\n",
    "                        tf.keras.callbacks.EarlyStopping(monitor=monitor, patience=20), #patience=step_size or ep_num\n",
    "                    #     lr_reduceonplateau,\n",
    "                        tf.keras.callbacks.LearningRateScheduler(lrdump),#lrdump, decay or lrfn or lrfn2. clr\n",
    "                        PrintLRtoe()\n",
    "                        ]\n",
    "    callbacks_toe_fn = [\n",
    "                    #     tensorboard_callback,\n",
    "                        best_model_save,\n",
    "                        tf.keras.callbacks.EarlyStopping(monitor=monitor, patience=100), #patience=step_size or ep_num\n",
    "                    #     lr_reduceonplateau,\n",
    "                        tf.keras.callbacks.LearningRateScheduler(clr3),#lrdump, decay or lrfn or lrfn2. clr, CosineDecayCLRWarmUp, CosineDecayCLRWarmUpLSW\n",
    "                        PrintLRtoe()\n",
    "                    ]\n",
    "    print('best_model_name:', best_model_name)\n",
    "\n",
    "\n",
    "    top_dropout_rate = 0.8 #less dp rate, say 0.1, train_loss will lower than val_loss\n",
    "    drop_connect_rate = 0.9 #for efnet This parameter serves as a toggle for extra regularization in finetuning, but does not affect loaded weights.\n",
    "    outputnum = 2\n",
    "    with strategy.scope():\n",
    "        model_toe = build_efn_model(outputnum, top_dropout_rate, drop_connect_rate)\n",
    "#         model_toe = load_pretrained_efn_model() # from 20210224-200728 ed5.3\n",
    "#         count_model_trainOrNot_layers(model_toe)\n",
    "        \n",
    "    # fit the model on all data\n",
    "    hist = model_toe.fit(train_ds_pre_toe_s, \n",
    "                          verbose=1, \n",
    "                          epochs=ep_num_transf, \n",
    "                          validation_data=valid_ds_pre_toe_s, \n",
    "                          callbacks=callbacks_toe_tl)#, validation_split=0.1)\n",
    "    history_toe.append(hist)\n",
    "    \n",
    "      \n",
    "    # Train K-Model with fine tune #\n",
    "    \n",
    "    # Toe model, FT\n",
    "    unfreeze_model(model_toe)\n",
    "    count_model_trainOrNot_layers(model_toe)\n",
    "    # fit the model on all data\n",
    "    hist = model_toe.fit(train_ds_pre_toe_s, \n",
    "                          verbose=1, \n",
    "                          epochs=ep_num, \n",
    "                          validation_data=valid_ds_pre_toe_s, \n",
    "                          callbacks=callbacks_toe_fn)#, validation_split=0.1)\n",
    "    history_toe_finetune.append(hist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ED sum\n",
    "def get_valloss(his_v_l):  \n",
    "    return np.min(his_v_l), np.argmin(his_v_l)\n",
    "\n",
    "t_vl = []\n",
    "# h_vl = []\n",
    "for k in range(KFlodNum):\n",
    "    print(f'K:{k}')\n",
    "    t_v, _ = get_valloss(history_toe_finetune[k].history['val_loss'])\n",
    "#     h_v, _ = get_valloss(history_heel_finetune[k].history['val_loss'])\n",
    "    \n",
    "    t_vl.append(t_v)\n",
    "#     h_vl.append(h_v)\n",
    "\n",
    "# t_vl = np.mean(t_vl, axis=0)\n",
    "# h_vl = np.mean(h_vl, axis=0)\n",
    "# print(f'{round(t_vl,5)} + {round(h_vl,5)} = {round(t_vl + h_vl,5)}')\n",
    "\n",
    "t_vl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(f'{log_dir_name}/toe_FNED.txt', t_vl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum plot losses toe-tl\n",
    "\n",
    "plt.figure(figsize=(25, 10))\n",
    "\n",
    "for k in range(KFlodNum):\n",
    "\n",
    "    plt.plot(history_toe[k].history['loss'])\n",
    "    plt.plot(history_toe[k].history['val_loss'])\n",
    "\n",
    "    \n",
    "plt.title('K-model ed loss toe-TL')\n",
    "plt.ylabel('ed loss'), plt.ylim(5, 80)# for too large loss\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['k0 loss', 'k0 val_loss', 'k1 loss', 'k1 val_loss', 'k2 loss', 'k2 val_loss', \n",
    "            'k3 loss', 'k3 val_loss', 'k4 loss', 'k4 val_loss'], loc='upper left') \n",
    "# plt.show()\n",
    "\n",
    "# save plot : comment plo.show in jupyter notebook.\n",
    "plt.savefig(f'{log_dir_name}/{log_dir_name}_toe_Ksum_TL.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Single plot loss toe-tl\n",
    "\n",
    "for k in range(KFlodNum):\n",
    "    plt.figure(figsize=(25, 10))\n",
    "    plt.plot(history_toe[k].history['loss'])\n",
    "    plt.plot(history_toe[k].history['val_loss'])\n",
    "    plt.title('K-model ed loss toe-TL')\n",
    "    plt.ylabel('ed loss'), plt.ylim(5, 20)# for too large loss\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['loss', 'val_loss'], loc='upper left') \n",
    "    # plt.show()\n",
    "\n",
    "    # save plot : comment plo.show in jupyter notebook.\n",
    "    def get_valloss(his_v_l):   \n",
    "        return np.min(his_v_l), np.argmin(his_v_l)\n",
    "\n",
    "    vl, ep = get_valloss(history_toe[k].history['val_loss'])\n",
    "    plt.savefig(f'{log_dir_name}/{log_dir_name}_toe_K{k}_TL_clr_ed{round(vl,4)}@{ep}.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the loos the model trained.\n",
    "\n",
    "def get_valloss(his_v_l):  \n",
    "    return np.min(his_v_l), np.argmin(his_v_l)\n",
    "\n",
    "t_vl = []\n",
    "# h_vl = []\n",
    "for k in range(KFlodNum):\n",
    "    print(f'K:{k}')\n",
    "    t_v, _ = get_valloss(history_toe_finetune[k].history['val_loss'])\n",
    "#     h_v, _ = get_valloss(history_heel_finetune[k].history['val_loss'])\n",
    "    \n",
    "    t_vl.append(t_v)\n",
    "    \n",
    "# for different scales (different Y-axes)\n",
    "# fig, ax1 = plt.subplots()\n",
    "fig, ax1 = plt.subplots(figsize=(25, 10))\n",
    "\n",
    "# nice to have this colorful tip.\n",
    "color = 'tab:red'\n",
    "\n",
    "ax1.set_title('[ toe_finetune ] \\n ED loss')\n",
    "\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.set_ylabel('ed loss', color=color)\n",
    "\n",
    "for k in range(KFlodNum):\n",
    "\n",
    "    plt.plot(history_toe_finetune[k].history['loss'])\n",
    "    plt.plot(history_toe_finetune[k].history['val_loss'])\n",
    "\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "# ax1.legend(['loss', 'val_loss'], loc='upper center') # legend may ocvered by next ax ploting. moved to end.\n",
    "ax1.legend(['k0 loss', 'k0 val_loss', 'k1 loss', 'k1 val_loss', 'k2 loss', 'k2 val_loss', \n",
    "            'k3 loss', 'k3 val_loss', 'k4 loss', 'k4 val_loss'], loc='upper left') \n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('learning rate', color=color)\n",
    "ax2.plot(history_toe_finetune[0].history['lr'], color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.legend(['lr'], loc='upper right') \n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# # save plot : comment plo.show in jupyter notebook.\n",
    "# def get_valloss(his_v_l):   \n",
    "#     return np.min(his_v_l), np.argmin(his_v_l)\n",
    "\n",
    "# vl, ep = get_valloss(history_toe_finetune.history['val_loss'])\n",
    "\n",
    "\n",
    "t_vl = np.mean(t_vl, axis=0)\n",
    "plt.savefig(f'{log_dir_name}/{log_dir_name}_toe_ft_Ksum-clr_ed{round(t_vl,4)}.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum plot losses toe-ft\n",
    "\n",
    "plt.figure(figsize=(25, 10))\n",
    "\n",
    "for k in range(KFlodNum):\n",
    "\n",
    "    plt.plot(history_toe_finetune[k].history['loss'])\n",
    "    plt.plot(history_toe_finetune[k].history['val_loss'])\n",
    "\n",
    "    \n",
    "plt.title('K-model ed loss toe-FT')\n",
    "plt.ylabel('ed loss'), plt.ylim(4, 20)# for too large loss\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['k0 loss', 'k0 val_loss', 'k1 loss', 'k1 val_loss', 'k2 loss', 'k2 val_loss', \n",
    "            'k3 loss', 'k3 val_loss', 'k4 loss', 'k4 val_loss'], loc='upper left') \n",
    "# plt.show()\n",
    "\n",
    "# save plot : comment plo.show in jupyter notebook.\n",
    "plt.savefig(f'{log_dir_name}/{log_dir_name}_toe_Ksum_FT.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Single plot loss toe-FT\n",
    "\n",
    "for k in range(KFlodNum):\n",
    "    plt.figure(figsize=(25, 10))\n",
    "    plt.plot(history_toe_finetune[k].history['loss'])\n",
    "    plt.plot(history_toe_finetune[k].history['val_loss'])\n",
    "    plt.title('K-model ed loss toe-FT')\n",
    "    plt.ylabel('ed loss'), plt.ylim(4, 20)# for too large loss\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['loss', 'val_loss'], loc='upper left') \n",
    "    # plt.show()\n",
    "\n",
    "    # save plot : comment plo.show in jupyter notebook.\n",
    "    def get_valloss(his_v_l):   \n",
    "        return np.min(his_v_l), np.argmin(his_v_l)\n",
    "\n",
    "    vl, ep = get_valloss(history_toe_finetune[k].history['val_loss'])\n",
    "    plt.savefig(f'{log_dir_name}/{log_dir_name}_toe_K{k}_FT_clr_ed{round(vl,4)}@{ep}.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heel K-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# big K = 5 (fold 0 ~ 4) \n",
    "# KFlodNum = 5 # follow Toe's K.\n",
    "\n",
    "\n",
    "\n",
    "history_heel = []\n",
    "history_heel_finetune = []\n",
    "\n",
    "#above until 'train_ds_map_toe now' to 'train_ds_map_toe_s', 'valid_ds_map_toe_s'\n",
    "for k in range(KFlodNum):\n",
    "    \n",
    "    \n",
    "    # Split data to train/valid with K-Fold #\n",
    "    print(\"K=\", k)\n",
    "#     # Toe split\n",
    "#     train_ds_map_toe_s, valid_ds_map_toe_s = get_KFold_ds(train_ds_map_toe, K=k)\n",
    "    \n",
    "#     # Toe ds_pre\n",
    "#     train_ds_pre_toe_s = configure_for_performance_cache_train(train_ds_map_toe_s, augment=True)\n",
    "#     valid_ds_pre_toe_s = configure_for_performance_cache_val(valid_ds_map_toe_s)\n",
    "    \n",
    "    \n",
    "    # heel split\n",
    "    train_ds_map_heel_s, valid_ds_map_heel_s = get_KFold_ds(train_ds_map_heel, K=k)\n",
    "    # Heel ds_pre\n",
    "    train_ds_pre_heel_s = configure_for_performance_cache_train_AToe(train_ds_map_heel_s, augment=True)\n",
    "    valid_ds_pre_heel_s = configure_for_performance_cache_val(valid_ds_map_heel_s)\n",
    "    \n",
    "    \n",
    "    # Train K-Model with transfer learnling #\n",
    "    \n",
    "    # Toe model, TL\n",
    "    #th = 'toe'\n",
    "    th = 'heel'\n",
    "    best_model_name = get_best_model_name(th, K=str(k))\n",
    "    best_model_save = tf.keras.callbacks.ModelCheckpoint(filepath=best_model_name, \n",
    "                                 save_best_only = True, \n",
    "                                 save_weights_only = False,\n",
    "                                 monitor = monitor, \n",
    "                                 mode = 'auto', verbose = 1)\n",
    "    callbacks_heel_tl = [\n",
    "                    #     tensorboard_callback,\n",
    "                        best_model_save,\n",
    "                        tf.keras.callbacks.EarlyStopping(monitor=monitor, patience=20), #patience=step_size or ep_num\n",
    "                    #     lr_reduceonplateau,\n",
    "                        tf.keras.callbacks.LearningRateScheduler(lrdump),#lrdump, decay or lrfn or lrfn2. clr\n",
    "                        PrintLRtoe()\n",
    "                        ]\n",
    "    callbacks_heel_fn = [\n",
    "                    #     tensorboard_callback,\n",
    "                        best_model_save,\n",
    "                        tf.keras.callbacks.EarlyStopping(monitor=monitor, patience=100), #patience=step_size or ep_num\n",
    "                    #     lr_reduceonplateau,\n",
    "                        tf.keras.callbacks.LearningRateScheduler(clr3),#lrdump, decay or lrfn or lrfn2. clr, CosineDecayCLRWarmUp, CosineDecayCLRWarmUpLSW\n",
    "                        PrintLRheel()\n",
    "                    ]\n",
    "    print('best_model_name:', best_model_name)\n",
    "\n",
    "\n",
    "    top_dropout_rate = 0.4 #less dp rate, say 0.1, train_loss will lower than val_loss\n",
    "    drop_connect_rate = 0.4 #for efnet This parameter serves as a toggle for extra regularization in finetuning, but does not affect loaded weights.\n",
    "    outputnum = 2\n",
    "    with strategy.scope():\n",
    "        model_heel = build_efn_model(outputnum, top_dropout_rate, drop_connect_rate)\n",
    "    # fit the model on all data\n",
    "    hist = model_heel.fit(train_ds_pre_heel_s, \n",
    "                          verbose=1, \n",
    "                          epochs=ep_num_transf, \n",
    "                          validation_data=valid_ds_pre_heel_s, \n",
    "                          callbacks=callbacks_heel_tl)#, validation_split=0.1)\n",
    "    history_heel.append(hist)\n",
    "    \n",
    "      \n",
    "    # Train K-Model with fine tune #\n",
    "    \n",
    "    # Toe model, FT\n",
    "    unfreeze_model(model_heel)\n",
    "    count_model_trainOrNot_layers(model_heel)\n",
    "    # fit the model on all data\n",
    "    hist = model_heel.fit(train_ds_pre_heel_s, \n",
    "                          verbose=1, \n",
    "                          epochs=ep_num, \n",
    "                          validation_data=valid_ds_pre_heel_s, \n",
    "                          callbacks=callbacks_heel_fn)#, validation_split=0.1)\n",
    "    history_heel_finetune.append(hist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ED sum\n",
    "def get_valloss(his_v_l):  \n",
    "    return np.min(his_v_l), np.argmin(his_v_l)\n",
    "\n",
    "# t_vl = []\n",
    "h_vl = []\n",
    "for k in range(KFlodNum):\n",
    "    print(f'K:{k}')\n",
    "#     t_v, _ = get_valloss(history_toe_finetune[k].history['val_loss'])\n",
    "    h_v, _ = get_valloss(history_heel_finetune[k].history['val_loss'])\n",
    "    \n",
    "#     t_vl.append(t_v)\n",
    "    h_vl.append(h_v)\n",
    "\n",
    "# t_vl = np.mean(t_vl, axis=0)\n",
    "# h_vl = np.mean(h_vl, axis=0)\n",
    "# print(f'{round(t_vl,5)} + {round(h_vl,5)} = {round(t_vl + h_vl,5)}')\n",
    "\n",
    "# t_vl\n",
    "h_vl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(f'{log_dir_name}/heel_FNED.txt', h_vl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum plot losses heel-tl\n",
    "\n",
    "plt.figure(figsize=(25, 10))\n",
    "\n",
    "for k in range(KFlodNum):\n",
    "\n",
    "    plt.plot(history_heel[k].history['loss'])\n",
    "    plt.plot(history_heel[k].history['val_loss'])\n",
    "\n",
    "    \n",
    "plt.title('K-model ed loss heel-TL')\n",
    "plt.ylabel('ed loss'), plt.ylim(5, 50)# for too large loss\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['k0 loss', 'k0 val_loss', 'k1 loss', 'k1 val_loss', 'k2 loss', 'k2 val_loss', \n",
    "            'k3 loss', 'k3 val_loss', 'k4 loss', 'k4 val_loss'], loc='upper left') \n",
    "# plt.show()\n",
    "\n",
    "# save plot : comment plo.show in jupyter notebook.\n",
    "plt.savefig(f'{log_dir_name}/{log_dir_name}_heel_Ksum_TL.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Single plot loss heel-tl\n",
    "\n",
    "for k in range(KFlodNum):\n",
    "    plt.figure(figsize=(25, 10))\n",
    "    plt.plot(history_heel[k].history['loss'])\n",
    "    plt.plot(history_heel[k].history['val_loss'])\n",
    "    plt.title('K-model ed loss heel-TL')\n",
    "    plt.ylabel('ed loss'), plt.ylim(5, 80)# for too large loss\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['loss', 'val_loss'], loc='upper left') \n",
    "    # plt.show()\n",
    "\n",
    "    # save plot : comment plo.show in jupyter notebook.\n",
    "    def get_valloss(his_v_l):   \n",
    "        return np.min(his_v_l), np.argmin(his_v_l)\n",
    "\n",
    "    vl, ep = get_valloss(history_heel[k].history['val_loss'])\n",
    "    plt.savefig(f'{log_dir_name}/{log_dir_name}_heel_K{k}_TL_clr_ed{round(vl,4)}@{ep}.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the loos the model trained.\n",
    "\n",
    "def get_valloss(his_v_l):  \n",
    "    return np.min(his_v_l), np.argmin(his_v_l)\n",
    "\n",
    "# t_vl = []\n",
    "h_vl = []\n",
    "for k in range(KFlodNum):\n",
    "    print(f'K:{k}')\n",
    "#     t_v, _ = get_valloss(history_toe_finetune[k].history['val_loss'])\n",
    "    h_v, _ = get_valloss(history_heel_finetune[k].history['val_loss'])\n",
    "    \n",
    "    h_vl.append(h_v)\n",
    "    \n",
    "# for different scales (different Y-axes)\n",
    "# fig, ax1 = plt.subplots()\n",
    "fig, ax1 = plt.subplots(figsize=(25, 10))\n",
    "\n",
    "# nice to have this colorful tip.\n",
    "color = 'tab:red'\n",
    "\n",
    "ax1.set_title('[ heel_finetune ] \\n ED loss')\n",
    "\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.set_ylabel('ed loss', color=color)\n",
    "\n",
    "for k in range(KFlodNum):\n",
    "\n",
    "    plt.plot(history_heel_finetune[k].history['loss'])\n",
    "    plt.plot(history_heel_finetune[k].history['val_loss'])\n",
    "\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "# ax1.legend(['loss', 'val_loss'], loc='upper center') # legend may ocvered by next ax ploting. moved to end.\n",
    "ax1.legend(['k0 loss', 'k0 val_loss', 'k1 loss', 'k1 val_loss', 'k2 loss', 'k2 val_loss', \n",
    "            'k3 loss', 'k3 val_loss', 'k4 loss', 'k4 val_loss'], loc='upper right') \n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('learning rate', color=color)\n",
    "ax2.plot(history_heel_finetune[0].history['lr'], color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.legend(['lr'], loc='upper right') \n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# # save plot : comment plo.show in jupyter notebook.\n",
    "# def get_valloss(his_v_l):   \n",
    "#     return np.min(his_v_l), np.argmin(his_v_l)\n",
    "\n",
    "# vl, ep = get_valloss(history_toe_finetune.history['val_loss'])\n",
    "\n",
    "\n",
    "# t_vl = np.mean(t_vl, axis=0)\n",
    "h_vl = np.mean(h_vl, axis=0)\n",
    "plt.savefig(f'{log_dir_name}/{log_dir_name}_heel_ft_Ksum-clr_ed{round(h_vl,4)}.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum plot losses heel-ft\n",
    "\n",
    "plt.figure(figsize=(25, 10))\n",
    "\n",
    "for k in range(KFlodNum):\n",
    "\n",
    "    plt.plot(history_heel_finetune[k].history['loss'])\n",
    "    plt.plot(history_heel_finetune[k].history['val_loss'])\n",
    "\n",
    "    \n",
    "plt.title('K-model ed loss heel-FT')\n",
    "plt.ylabel('ed loss'), plt.ylim(2, 20)# for too large loss\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['k0 loss', 'k0 val_loss', 'k1 loss', 'k1 val_loss', 'k2 loss', 'k2 val_loss', \n",
    "            'k3 loss', 'k3 val_loss', 'k4 loss', 'k4 val_loss'], loc='upper left') \n",
    "# plt.show()\n",
    "\n",
    "# save plot : comment plo.show in jupyter notebook.\n",
    "plt.savefig(f'{log_dir_name}/{log_dir_name}_heel_Ksum_FT.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Single plot loss heel-FT\n",
    "\n",
    "for k in range(KFlodNum):\n",
    "    plt.figure(figsize=(25, 10))\n",
    "    plt.plot(history_heel_finetune[k].history['loss'])\n",
    "    plt.plot(history_heel_finetune[k].history['val_loss'])\n",
    "    plt.title('K-model ed loss heel-FT')\n",
    "    plt.ylabel('ed loss'), plt.ylim(2, 20)# for too large loss\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['loss', 'val_loss'], loc='upper left') \n",
    "    # plt.show()\n",
    "\n",
    "    # save plot : comment plo.show in jupyter notebook.\n",
    "    def get_valloss(his_v_l):   \n",
    "        return np.min(his_v_l), np.argmin(his_v_l)\n",
    "\n",
    "    vl, ep = get_valloss(history_heel_finetune[k].history['val_loss'])\n",
    "    plt.savefig(f'{log_dir_name}/{log_dir_name}_heel_K{k}_FT_clr_ed{round(vl,4)}@{ep}.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ploting your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model_toe.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # show after modl.fit\n",
    "# model_toe.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check metrics the model have.\n",
    "# history_toe.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.utils.plot_model(model_toe, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from tensorflow.keras.utils import plot_model\n",
    "# plot_model(model_toe, to_file='model_toe_conv_layer_blocks.png', show_shapes=True)\n",
    "# from IPython.display import Image\n",
    "# Image(filename='model_toe_conv_layer_blocks.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## show pp pred\n",
    "\n",
    "* we can switch toe/hell by comment it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EFN Toe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it_valid_ds_pre_toe_s = iter(valid_ds_pre_toe_s)\n",
    "# # it_valid_ds_pre_heel_s = iter(valid_ds_pre_heel_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # image_batch, label_batch = next(valid_ds_pre_toe_s)\n",
    "\n",
    "# image_batch, label_batch = next(it_valid_ds_pre_toe_s)\n",
    "# # image_batch, label_batch = next(it_valid_ds_pre_heel_s)\n",
    "\n",
    "\n",
    "# pred = model_toe.predict_on_batch(image_batch) #predictions\n",
    "# # pred = model.predict_on_batch(image_batch) #Simple 2D CNN model predictions\n",
    "\n",
    "# plt.figure(figsize=(20, 20))\n",
    "# for i in range(64):\n",
    "#     ax = plt.subplot(8, 8, i + 1)\n",
    "#     plt.imshow(image_batch[i])\n",
    "#     plt.title(label_batch[i].numpy())\n",
    "#     plt.axis(\"off\")\n",
    "    \n",
    "#     # note: y_offset_toe for ds image\n",
    "    \n",
    "#     #ground truth\n",
    "#     plt.plot(label_batch[i].numpy()[0], label_batch[i].numpy()[1], 'r+', markersize=15, mew=2)\n",
    "\n",
    "#     #pred\n",
    "#     plt.plot(pred[i][0], pred[i][1], 'k+', markersize=15, mew=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #test_ds一次做完即可不用分batch\n",
    "# neg = label_batch - pred\n",
    "# neg[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.abs(neg)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neg_abs = np.abs(neg)\n",
    "# neg_abs.mean(axis=0)#所有x 所有y個別平均  neg.mean(axis=0)#所有x 所有y個別平均"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ED 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # y_pred = neg_abs.mean(axis=0)\n",
    "# ed_metric_2d([0,0], [neg_abs.mean(axis=0)]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EFN Heel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # it_valid_ds_pre_toe_s = iter(valid_ds_pre_toe_s)\n",
    "# it_valid_ds_pre_heel_s = iter(valid_ds_pre_heel_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # image_batch, label_batch = next(valid_ds_pre_toe_s)\n",
    "\n",
    "# # image_batch, label_batch = next(it_valid_ds_pre_toe_s)\n",
    "# image_batch, label_batch = next(it_valid_ds_pre_heel_s)\n",
    "\n",
    "\n",
    "# pred = model_heel.predict_on_batch(image_batch) #predictions\n",
    "# # pred = model.predict_on_batch(image_batch) #Simple 2D CNN model predictions\n",
    "\n",
    "# plt.figure(figsize=(20, 20))\n",
    "# for i in range(64):\n",
    "#     ax = plt.subplot(8, 8, i + 1)\n",
    "#     plt.imshow(image_batch[i])\n",
    "#     plt.title(label_batch[i].numpy())\n",
    "#     plt.axis(\"off\")\n",
    "    \n",
    "#     # note: y_offset_toe for ds image\n",
    "    \n",
    "#     #ground truth\n",
    "#     plt.plot(label_batch[i].numpy()[0], label_batch[i].numpy()[1], 'r+', markersize=15, mew=2)\n",
    "\n",
    "#     #pred\n",
    "#     plt.plot(pred[i][0], pred[i][1], 'k+', markersize=15, mew=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_batch[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #test_ds一次做完即可不用分batch\n",
    "# neg = label_batch - pred\n",
    "# neg[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.abs(neg)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neg_abs = np.abs(neg)\n",
    "# neg_abs.mean(axis=0)#所有x 所有y個別平均  neg.mean(axis=0)#所有x 所有y個別平均"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ED 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # y_pred = neg_abs.mean(axis=0)\n",
    "# ed_metric_2d([0,0], [neg_abs.mean(axis=0)]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merg Toe/Heel model and predict the Test data at once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TEST DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = 'test_submission.csv'\n",
    "df_ts = pd.read_csv(ts)\n",
    "df_ts.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ts['images']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataframe\n",
    "list_ds_test = tf.data.Dataset.from_tensor_slices(df_ts['images'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_ds_test)#.shape() #take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check The type specification of an element of this dataset.\n",
    "list_ds_test.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in list_ds_test.take(5):\n",
    "    print(f'take test sample: {f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST DS: Process TEST path to image tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST-DS: re-used from train/val-ds\n",
    "\n",
    "im_test = 'test_images/'\n",
    "\n",
    "'''\n",
    "def get_label(file_path):\n",
    "    # convert the path to a list of path components\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    # The second to last is the class-directory\n",
    "    boolen = parts[-2] == class_names\n",
    "    #one_hot_num = np.array(boolen, dtype=np.int) not works should use tf.x repalced.\n",
    "    one_hot_num = tf.dtypes.cast(boolen, tf.int64)\n",
    "    one_num = tf.argmax(one_hot_num)\n",
    "    print('one_num:', one_num)\n",
    "    # Integer encode the label\n",
    "    return one_num\n",
    "\n",
    "def decode_img(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    # resize the image to the desired size\n",
    "#     return tf.image.resize(img, [img_height, img_width])# augment 已經resize過一次了 但這邊不先做會比較慢\n",
    "    return tf.cast(tf.image.resize(img, [img_height, img_width]), tf.uint8)# 避免float over at augment\n",
    "'''\n",
    "\n",
    "#\n",
    "# map list to ds, Toe part.\n",
    "#\n",
    "\n",
    "def decode_crop_png_toe_test(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.io.decode_png(img, channels=3)\n",
    "    # crop the toe from top-left corner [image, offset_height y1, offset_width x1, target_height, target_width]\n",
    "    y1=y_offset_toe;    x1=0;    h=img_height;    w=img_width # not the pp location\n",
    "    img = tf.image.crop_to_bounding_box(img, int(y1), int(x1), h, w)\n",
    "    #img = tf.image.crop_to_bounding_box(img, int(y1), int(x1), int(y2)-int(y1), int(x2)-int(x1))\n",
    "    # resize the image to the desired size\n",
    "    return img\n",
    "\n",
    "def process_path_toe_test(file_name):\n",
    "    file_path = im_test + file_name\n",
    "    #label = get_label(file_path)\n",
    "    # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)#can read the byte string paths b'image_0001.png'\n",
    "    img = decode_crop_png_toe_test(img)\n",
    "    return img, file_name\n",
    "\n",
    "#\n",
    "# map list to ds, Heel part.\n",
    "#\n",
    "\n",
    "def decode_crop_png_heel_test(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.io.decode_png(img, channels=3)\n",
    "    # crop the toe from top-left corner [image, offset_height y1, offset_width x1, target_height, target_width]\n",
    "    y2=y_offset_heel;    x2=0;    h=img_height;    w=img_width # not the pp location\n",
    "    img = tf.image.crop_to_bounding_box(img, int(y2), int(x2), h, w)\n",
    "    #img = tf.image.crop_to_bounding_box(img, int(y1), int(x1), int(y2)-int(y1), int(x2)-int(x1))\n",
    "    # resize the image to the desired size\n",
    "    return img\n",
    "\n",
    "def process_path_heel_test(file_name):\n",
    "    file_path = im_test + file_name\n",
    "    #label = get_label(file_path)\n",
    "    # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)#can read the byte string paths b'image_0001.png'\n",
    "    img = decode_crop_png_heel_test(img)\n",
    "    return img, file_name\n",
    "\n",
    "\n",
    "#\n",
    "# test how to put parameters to map\n",
    "#\n",
    "\n",
    "def t_ds_map(file_path,x1,y1,x2,y2):\n",
    "#     img = get_img('train/images/' + str(file_path))\n",
    "#     print(file_path)\n",
    "    return file_path,x1,y1,x2,y2 #img, [x1,y1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Toe ds\n",
    "test_ds_map_toe = list_ds_test.map(process_path_toe_test, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "# TEST Heel ds\n",
    "test_ds_map_heel = list_ds_test.map(process_path_heel_test, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, file_name in test_ds_map_toe.take(5):\n",
    "    print(f'take sample: {img.shape} {file_name}')\n",
    "    \n",
    "# print('f', f.dtype)\n",
    "# print('xy', xy.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare TEST_ds_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_for_performance_cache_test(ds, cache=True):\n",
    "\n",
    "    if cache:\n",
    "        ds = ds.cache()\n",
    "        print(\"Check cache in memory:Y\", cache)\n",
    "    else:\n",
    "        print(\"Check cache in memory:N\", cache)\n",
    "        \n",
    "#     if augment:\n",
    "# #         ds = ds.map(data_augment, num_parallel_calls=AUTOTUNE)\n",
    "#         ds = ds.map(AA, num_parallel_calls=AUTOTUNE)\n",
    "# #         ds = ds.map(RA, num_parallel_calls=AUTOTUNE)\n",
    "#         print(\"Check augment :Y\", augment)\n",
    "#     else:\n",
    "#         print(\"Check augment :N\", augment)\n",
    "    \n",
    "#     #ds = ds.repeat()#TODO:2020-12-14: test\n",
    "#     ds = ds.shuffle(buffer_size=MULTI_BATCH_SIZE*2) # (buffer_size=MULTI_BATCH_SIZE*5) 6sec,buffer_size=1000 take few sec. or buffer_size=image_count <- take too long # each take ds take 30~45 sec, TODO!!\n",
    "#     ds = ds.shuffle(len(list_ds), reshuffle_each_iteration=False) #todo: move to ds_pre. see https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle\n",
    "    ds = ds.batch(1000)# 1k for foot test images #MULTI_BATCH_SIZE for multi-GPUs\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE) #buffer_size=AUTOTUNE seem no speed improve\n",
    "    \n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prepare the ds properties (cache, augment, bs, shuffle, prefetch, etc.) for better performance.\n",
    "\"\"\"\n",
    "# TEST Toe ds_pre\n",
    "test_ds_pre_toe = configure_for_performance_cache_test(test_ds_map_toe)\n",
    "\n",
    "# TEST Heel ds_pre\n",
    "test_ds_pre_heel = configure_for_performance_cache_test(test_ds_map_heel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Best-K-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if K-models are in last time frame\n",
    "# best_model_name = get_best_model_name(th, K=str(k))\n",
    "\n",
    "predictions_toe = []\n",
    "predictions_heel = []\n",
    "\n",
    "for k in range(KFlodNum):\n",
    "    print(f'K:{k}')\n",
    "    best_model_toe_name = get_best_model_name('toe', K=str(k))\n",
    "    best_model_heel_name = get_best_model_name('heel', K=str(k))\n",
    "\n",
    "# # if models are in last time frame\n",
    "# best_model_toe_name = get_best_model_name('toe')\n",
    "# best_model_heel_name = get_best_model_name('heel')\n",
    "\n",
    "# # if toe/heel are in different time frame\n",
    "# best_model_toe_name = '20210118-212454/toe_EfficientNetB0_bs64_w120_best_val_loss.h5'#6.3318 @e393\n",
    "# best_model_heel_name = '20210122-084854/heel_EfficientNetB0_bs64_w120_best_val_loss.h5'#3.27979@152\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(best_model_toe_name)\n",
    "    print(best_model_heel_name)\n",
    "    # log_dir_name + '/' + 'leaf-2020-12-01-EfficientNetB7_top-layer50_lr_lrfn_val-acc.8352_wh512_e37.h5'\n",
    "\n",
    "    best_model_toe = tf.keras.models.load_model(best_model_toe_name,compile=False)\n",
    "    best_model_heel = tf.keras.models.load_model(best_model_heel_name,compile=False)\n",
    "    \n",
    "    best_model_toe.compile(optimizer = tf.keras.optimizers.Adam(),#RMSprop , Adam\n",
    "                loss=ed_metric_2d_mean)#, ed_loss ed_metric_2d ed_metric_2d_mean            \n",
    "                #metrics=['mae', 'accuracy'])\n",
    "    best_model_heel.compile(optimizer = tf.keras.optimizers.Adam(),#RMSprop , Adam\n",
    "                loss=ed_metric_2d_mean)#, ed_loss ed_metric_2d ed_metric_2d_mean            \n",
    "                #metrics=['mae', 'accuracy'])\n",
    "        \n",
    "        \n",
    "    pred_toe = best_model_toe.predict(test_ds_pre_toe)\n",
    "    pred_toe[:,1] = pred_toe[:,1] + y_offset_toe\n",
    "    predictions_toe.append(pred_toe)\n",
    "    \n",
    "    pred_heel = best_model_heel.predict(test_ds_pre_heel)\n",
    "    pred_heel[:,1] = pred_heel[:,1] + y_offset_heel\n",
    "    predictions_heel.append(pred_heel)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictions_toe[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure we got the k-pred as k models.\n",
    "for i, _ in enumerate(predictions_toe):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(predictions_toe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_toe[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_toe[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_heel[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_heel[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean the k-predictions\n",
    "k_predictions_toe = np.mean(predictions_toe, axis=0)\n",
    "k_predictions_toe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(k_predictions_toe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean the k-predictions\n",
    "k_predictions_heel = np.mean(predictions_heel, axis=0)\n",
    "k_predictions_heel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge toe/hell pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_th = np.append(k_predictions_toe, k_predictions_heel, axis=1)#左右接\n",
    "predictions_th.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_name = np.expand_dims(df_ts['images'], axis=1)\n",
    "images_name.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_merge = np.append(images_name, predictions_th, axis=1)#左右接\n",
    "predictions_merge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.DataFrame(predictions_merge)\n",
    "df_submission.columns = ['images','x1','y1','x2','y2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submi_name = CSVNAME + '.' + log_dir_name +'.csv'\n",
    "\n",
    "df_submission.to_csv(submi_name, index=False)\n",
    "print('Save {} as submission CSV.'.format(submi_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ED sum\n",
    "def get_valloss(his_v_l):  \n",
    "    return np.min(his_v_l), np.argmin(his_v_l)\n",
    "\n",
    "t_vl = []\n",
    "h_vl = []\n",
    "for k in range(KFlodNum):\n",
    "    print(f'K:{k}')\n",
    "    t_v, _ = get_valloss(history_toe_finetune[k].history['val_loss'])\n",
    "    h_v, _ = get_valloss(history_heel_finetune[k].history['val_loss'])\n",
    "    \n",
    "    t_vl.append(t_v)\n",
    "    h_vl.append(h_v)\n",
    "\n",
    "t_vl = np.mean(t_vl, axis=0)\n",
    "h_vl = np.mean(h_vl, axis=0)\n",
    "print(f'{round(t_vl,5)} + {round(h_vl,5)} = {round(t_vl + h_vl,5)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K134520210224-114845.csv\n",
    "# 5.63922 + 3.34466 = 8.98389 LB:8.4890610"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_vl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_vl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_timer.toc() #Time elapsed since t.tic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compile the model separately afterwards. to load model with custom loss function\n",
    "\n",
    "* https://github.com/tensorflow/tensorflow/issues/32348"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model_toe.compile(optimizer = tf.keras.optimizers.Adam(),#RMSprop , Adam\n",
    "#                 loss=ed_metric_2d_mean)#, ed_loss ed_metric_2d ed_metric_2d_mean            \n",
    "#                 #metrics=['mae', 'accuracy'])\n",
    "# best_model_heel.compile(optimizer = tf.keras.optimizers.Adam(),#RMSprop , Adam\n",
    "#                 loss=ed_metric_2d_mean)#, ed_loss ed_metric_2d ed_metric_2d_mean            \n",
    "#                 #metrics=['mae', 'accuracy'])\n",
    "\n",
    "# best_model_toe.compile(optimizer = tf.keras.optimizers.Adam(),#RMSprop , Adam\n",
    "#                 loss=tf.keras.losses.MeanSquaredError())#, ed_loss ed_metric_2d ed_metric_2d_mean            \n",
    "#                 #metrics=['mae', 'accuracy'])\n",
    "# best_model_heel.compile(optimizer = tf.keras.optimizers.Adam(),#RMSprop , Adam\n",
    "#                 loss=tf.keras.losses.MeanSquaredError())#, ed_loss ed_metric_2d ed_metric_2d_mean            \n",
    "#                 #metrics=['mae', 'accuracy'])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # inference all test_ds once\n",
    "# predictions_toe = best_model_toe.predict(test_ds_pre_toe)\n",
    "# predictions_toe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offset Toe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_toe[:,1] = predictions_toe[:,1] + y_offset_toe\n",
    "\n",
    "# # for [0,1]\n",
    "# # predictions_toe[:,0] = predictions_toe[:,0]*120\n",
    "# # predictions_toe[:,1] = predictions_toe[:,1]*120 + y_offset_toe\n",
    "\n",
    "# # # for [-1,1]\n",
    "# # # for re-scale back xy \n",
    "# # # return img, [(x1-60)/60,((y1-y_offset_toe)-60)/60]#normalized [-1,1] \n",
    "# # # return img, [(x2-60)/60,((y2-y_offset_heel)-60)/60]#normalized [-1,1] \n",
    "# # predictions_toe[:,0] = (predictions_toe[:,0]*60)+60\n",
    "# # predictions_toe[:,1] = (predictions_toe[:,1]*60)+60 + y_offset_toe\n",
    "\n",
    "# predictions_toe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # inference all test_ds once\n",
    "# predictions_heel = best_model_heel.predict(test_ds_pre_heel)\n",
    "# predictions_heel.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offset Heel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_heel[:,1] = predictions_heel[:,1] + y_offset_heel\n",
    "\n",
    "# # for [0,1]\n",
    "# # predictions_heel[:,0] = predictions_heel[:,0]*120\n",
    "# # predictions_heel[:,1] = predictions_heel[:,1]*120 + y_offset_heel\n",
    "\n",
    "# # # for [-1,1]\n",
    "# # predictions_heel[:,0] = (predictions_heel[:,0]*60)+60\n",
    "# # predictions_heel[:,1] = (predictions_heel[:,1]*60)+60 + y_offset_heel\n",
    "\n",
    "# predictions_heel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge toe/hell pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_th = np.append(predictions_toe, predictions_heel, axis=1)#左右接\n",
    "# predictions_th.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images_name = np.expand_dims(df_ts['images'], axis=1)\n",
    "# images_name.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_merge = np.append(images_name, predictions_th, axis=1)#左右接\n",
    "# predictions_merge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_submission = pd.DataFrame(predictions_merge)\n",
    "# df_submission.columns = ['images','x1','y1','x2','y2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submi_name = '0000_ft_' + log_dir_name +'.csv'\n",
    "# # submi_name = 'Bth_clr3_2690_XYnorm[0-1]_' + log_dir_name +'.csv'\n",
    "# df_submission.to_csv(submi_name, index=False)\n",
    "# print('Save {} as submission CSV.'.format(submi_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bth_clr3_2690_ed_findtune_20210202-141718.csv\n",
    "\n",
    "#toe.9.9/heel.4.4 109 trainable LB:9.3411759 比heel保持top-20略高0.04 (9.3084957)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #ED sum\n",
    "# def get_valloss(his_v_l):  \n",
    "#     return np.min(his_v_l), np.argmin(his_v_l)\n",
    "\n",
    "# t_vl, _ = get_valloss(history_toe_finetune.history['val_loss'])\n",
    "# h_vl, _ = get_valloss(history_heel_finetune.history['val_loss'])\n",
    "\n",
    "# print(f'{round(t_vl,5)} + {round(h_vl,5)} = {round(t_vl + h_vl,5)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# best_model_name = './cop_' + model_name + '_bs' + str(BATCH_SIZE) + '_w' + str(img_width) + '_e' + str(ep_num) + '_' + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + '_.h5'\n",
    "# # model.save(best_model_name)\n",
    "# print(\"Save model: \", best_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "multi output model:\n",
    "https://navoshta.com/end-to-end-deep-learning/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
