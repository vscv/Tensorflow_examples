{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K20L195 20210301-225844"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 101 Course of transfer learning and Fine tune 2021-01-01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [How-to] 1. k-fold for cross validation\n",
    "\n",
    "#### Create a simple k-fold for train classification model.\n",
    "\n",
    "* In this short course you learned:\n",
    "\n",
    "* data pipline\n",
    "\n",
    "* transfer learning\n",
    "\n",
    "* fine tune\n",
    "\n",
    "* callbacks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo: move to note.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.style.use(\"bmh\")\n",
    "\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "\n",
    "import errno\n",
    "\n",
    "# albumentations\n",
    "from functools import partial\n",
    "# from albumentations import (\n",
    "#     Compose, RandomBrightness, JpegCompression, HueSaturationValue, RandomContrast, HorizontalFlip,\n",
    "#     Rotate\n",
    "# )\n",
    "import albumentations as A\n",
    "\n",
    "# from adabelief_tf import AdaBeliefOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n",
      "4.3.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(cv2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytictoc import TicToc\n",
    "\n",
    "t_timer = TicToc() #create instance of class\n",
    "\n",
    "t_timer.tic() #Start timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image size, Batch size, toe/heel-offset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64 # 16 #32 #64 #128 #32 #64 #todo: reduce the BS maybe help to reduce the loss\n",
    "img_height = 120 #512 #224 #100\n",
    "img_width = 120 #512 #224 #100\n",
    "\n",
    "y_offset_toe = 80\n",
    "y_offset_heel = 280 #400-120=280"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf MirroredStrategy seting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "\n",
      "Number of REPLICAS: 1\n",
      "\n",
      "BATCH_SIZE: 64, MULTI_BATCH_SIZE: 64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# tf MirroredStrategy seting\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "REPLICAS = strategy.num_replicas_in_sync\n",
    "print('\\nNumber of REPLICAS: {}\\n'.format(REPLICAS))\n",
    "\n",
    "\n",
    "MULTI_BATCH_SIZE = BATCH_SIZE * strategy.num_replicas_in_sync\n",
    "print('BATCH_SIZE: {}, MULTI_BATCH_SIZE: {}'.format(BATCH_SIZE, MULTI_BATCH_SIZE))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自動調節tf.data管道\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create the training dataset W/ croped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load samples as data-farame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>images</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1120</th>\n",
       "      <td>image_6195.jpg</td>\n",
       "      <td>52</td>\n",
       "      <td>127</td>\n",
       "      <td>75</td>\n",
       "      <td>370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1121</th>\n",
       "      <td>image_6196.jpg</td>\n",
       "      <td>62</td>\n",
       "      <td>138</td>\n",
       "      <td>29</td>\n",
       "      <td>383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1122</th>\n",
       "      <td>image_6197.jpg</td>\n",
       "      <td>54</td>\n",
       "      <td>135</td>\n",
       "      <td>78</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123</th>\n",
       "      <td>image_6198.jpg</td>\n",
       "      <td>60</td>\n",
       "      <td>125</td>\n",
       "      <td>29</td>\n",
       "      <td>384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1124</th>\n",
       "      <td>image_6199.jpg</td>\n",
       "      <td>51</td>\n",
       "      <td>147</td>\n",
       "      <td>70</td>\n",
       "      <td>353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1340</th>\n",
       "      <td>image_6419.jpg</td>\n",
       "      <td>60</td>\n",
       "      <td>135</td>\n",
       "      <td>70</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1341</th>\n",
       "      <td>image_6421.jpg</td>\n",
       "      <td>53</td>\n",
       "      <td>157</td>\n",
       "      <td>76</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1342</th>\n",
       "      <td>image_6422.jpg</td>\n",
       "      <td>49</td>\n",
       "      <td>154</td>\n",
       "      <td>33</td>\n",
       "      <td>375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1343</th>\n",
       "      <td>image_6423.jpg</td>\n",
       "      <td>64</td>\n",
       "      <td>149</td>\n",
       "      <td>76</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1344</th>\n",
       "      <td>image_6424.jpg</td>\n",
       "      <td>55</td>\n",
       "      <td>147</td>\n",
       "      <td>36</td>\n",
       "      <td>385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>225 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              images  x1   y1  x2   y2\n",
       "1120  image_6195.jpg  52  127  75  370\n",
       "1121  image_6196.jpg  62  138  29  383\n",
       "1122  image_6197.jpg  54  135  78  380\n",
       "1123  image_6198.jpg  60  125  29  384\n",
       "1124  image_6199.jpg  51  147  70  353\n",
       "...              ...  ..  ...  ..  ...\n",
       "1340  image_6419.jpg  60  135  70  381\n",
       "1341  image_6421.jpg  53  157  76  376\n",
       "1342  image_6422.jpg  49  154  33  375\n",
       "1343  image_6423.jpg  64  149  76  381\n",
       "1344  image_6424.jpg  55  147  36  385\n",
       "\n",
       "[225 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# # data-org #\n",
    "# ann = 'annotation_1424_merge.csv'\n",
    "# im_p = 'train/images/'\n",
    "# CSVNAME = \"\"\n",
    "\n",
    "\n",
    "# # data-org-augm#\n",
    "# ann = 'annotation_2848_augm.csv'\n",
    "# im_p = 'train_augm/images/'\n",
    "# CSVNAME = \"\"\n",
    "\n",
    "\n",
    "# data-train # # current best dataset.1424-79.\n",
    "ann = 'annotation_1345_good.csv'\n",
    "im_p = 'train/images/'\n",
    "CSVNAME = \"K1345K20L195\"\n",
    "\n",
    "# data-augm #\n",
    "# ann = 'annotation_2690_augm.csv'\n",
    "# im_p = 'train_augm/images/'\n",
    "# CSVNAME = \"\"\n",
    "\n",
    "\n",
    "# # data-train-HPL-1123\n",
    "# ann = 'annotation_1123_HPL_Good.csv'\n",
    "# im_p = 'train/images/'\n",
    "# CSVNAME = \"\"\n",
    "\n",
    "\n",
    "df = pd.read_csv(ann)\n",
    "df[1120:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1345"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffle and reset index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index\n",
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>images</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>image_6169.jpg</td>\n",
       "      <td>63</td>\n",
       "      <td>156</td>\n",
       "      <td>83</td>\n",
       "      <td>383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>image_6291.jpg</td>\n",
       "      <td>54</td>\n",
       "      <td>140</td>\n",
       "      <td>86</td>\n",
       "      <td>385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>image_6233.jpg</td>\n",
       "      <td>52</td>\n",
       "      <td>146</td>\n",
       "      <td>30</td>\n",
       "      <td>384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>image_0322.png</td>\n",
       "      <td>44</td>\n",
       "      <td>118</td>\n",
       "      <td>34</td>\n",
       "      <td>369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>image_0535.png</td>\n",
       "      <td>64</td>\n",
       "      <td>134</td>\n",
       "      <td>42</td>\n",
       "      <td>369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>image_0442.png</td>\n",
       "      <td>48</td>\n",
       "      <td>138</td>\n",
       "      <td>30</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>image_6230.jpg</td>\n",
       "      <td>61</td>\n",
       "      <td>137</td>\n",
       "      <td>76</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>image_0509.png</td>\n",
       "      <td>52</td>\n",
       "      <td>147</td>\n",
       "      <td>79</td>\n",
       "      <td>383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>image_6274.jpg</td>\n",
       "      <td>60</td>\n",
       "      <td>140</td>\n",
       "      <td>41</td>\n",
       "      <td>353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>image_0002.png</td>\n",
       "      <td>62</td>\n",
       "      <td>139</td>\n",
       "      <td>42</td>\n",
       "      <td>385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           images  x1   y1  x2   y2\n",
       "0  image_6169.jpg  63  156  83  383\n",
       "1  image_6291.jpg  54  140  86  385\n",
       "2  image_6233.jpg  52  146  30  384\n",
       "3  image_0322.png  44  118  34  369\n",
       "4  image_0535.png  64  134  42  369\n",
       "5  image_0442.png  48  138  30  380\n",
       "6  image_6230.jpg  61  137  76  380\n",
       "7  image_0509.png  52  147  79  383\n",
       "8  image_6274.jpg  60  140  41  353\n",
       "9  image_0002.png  62  139  42  385"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See, the image_####.jpg now are random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create tf.dataset (DS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataframe\n",
    "list_ds = tf.data.Dataset.from_tensor_slices((df['images'], df['x1'], df['y1'], df['x2'], df['y2']))\n",
    "# list_ds = list_ds.shuffle(image_count, reshuffle_each_iteration=True) #todo: move to ds_pre. see https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1345"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_ds)#.shape() #take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check The type specification of an element of this dataset.\n",
    "list_ds.element_spec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "take sample: b'image_6169.jpg' 63 156 83 383\n",
      "take sample: b'image_6291.jpg' 54 140 86 385\n",
      "take sample: b'image_6233.jpg' 52 146 30 384\n",
      "take sample: b'image_0322.png' 44 118 34 369\n",
      "take sample: b'image_0535.png' 64 134 42 369\n"
     ]
    }
   ],
   "source": [
    "for f,x1,y1,x2,y2 in list_ds.take(5):\n",
    "    print(f'take sample: {f} {x1} {y1} {x2} {y2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_0535.png\n"
     ]
    }
   ],
   "source": [
    "# use np decode to UTF-8\n",
    "print(f.numpy().decode('UTF-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1345"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check ds iterator for consuming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Python iterator\n",
    "\n",
    "it_list_ds = iter(list_ds) # Make sure iter ds only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'image_6169.jpg' 63 156\n",
      "b'image_6291.jpg' 54 140\n",
      "b'image_6233.jpg' 52 146\n",
      "b'image_0322.png' 44 118\n"
     ]
    }
   ],
   "source": [
    "# using iter and consuming its elements using next: every print different image name.\n",
    "\n",
    "for i in range(4):\n",
    "    image, x1, y1, x2, y2 = next(it_list_ds)\n",
    "    print(image.numpy(), x1.numpy(), y1.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'image_6169.jpg' 63 156 83 383\n",
      "b'image_6169.jpg' 63 156 83 383\n",
      "b'image_6169.jpg' 63 156 83 383\n",
      "b'image_6169.jpg' 63 156 83 383\n",
      "===== Create iterator once and pull out to above cell. =====\n",
      "b'image_6169.jpg' 63 156 83 383\n",
      "b'image_6291.jpg' 54 140 86 385\n",
      "b'image_6233.jpg' 52 146 30 384\n",
      "b'image_0322.png' 44 118 34 369\n"
     ]
    }
   ],
   "source": [
    "# image_batch, label_batch = valid_ds_pre_s.as_numpy_iterator().next()\n",
    "# pred = model.predict_on_batch(image_batch)\n",
    "\n",
    "\n",
    "for i in range(4):\n",
    "    image, x1, y1, x2, y2 = list_ds.as_numpy_iterator().next()# every time create a new iter, so need put iter out of above cell.\n",
    "    print(image, x1, y1, x2, y2)\n",
    "    \n",
    "iter_test_list = list_ds.as_numpy_iterator()\n",
    "print(\"===== Create iterator once and pull out to above cell. =====\")\n",
    "for i in range(4):\n",
    "    image, x1, y1, x2, y2 = iter_test_list.next()\n",
    "    print(image, x1, y1, x2, y2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process path to image tensor in DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "def get_label(file_path):\n",
    "    # convert the path to a list of path components\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    # The second to last is the class-directory\n",
    "    boolen = parts[-2] == class_names\n",
    "    #one_hot_num = np.array(boolen, dtype=np.int) not works should use tf.x repalced.\n",
    "    one_hot_num = tf.dtypes.cast(boolen, tf.int64)\n",
    "    one_num = tf.argmax(one_hot_num)\n",
    "    print('one_num:', one_num)\n",
    "    # Integer encode the label\n",
    "    return one_num\n",
    "\n",
    "def decode_img(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    # resize the image to the desired size\n",
    "#     return tf.image.resize(img, [img_height, img_width])# augment 已經resize過一次了 但這邊不先做會比較慢\n",
    "    return tf.cast(tf.image.resize(img, [img_height, img_width]), tf.uint8)# 避免float over at augment\n",
    "'''\n",
    "\n",
    "#\n",
    "# map list to ds, Toe part.\n",
    "#\n",
    "\n",
    "def decode_crop_png_toe(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.io.decode_png(img, channels=3)\n",
    "    # crop the toe from top-left corner [image, offset_height y1, offset_width x1, target_height, target_width]\n",
    "    y1=y_offset_toe;    x1=0;    h=img_height;    w=img_width # not the pp location\n",
    "    img = tf.image.crop_to_bounding_box(img, int(y1), int(x1), h, w)\n",
    "    #img = tf.image.crop_to_bounding_box(img, int(y1), int(x1), int(y2)-int(y1), int(x2)-int(x1))\n",
    "    # resize the image to the desired size\n",
    "    return img\n",
    "\n",
    "def process_path_toe(file_path,x1,y1,x2,y2):\n",
    "    file_path = im_p + file_path\n",
    "    #label = get_label(file_path)\n",
    "    # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)#can read the byte string paths b'image_0001.png'\n",
    "    img = decode_crop_png_toe(img)\n",
    "    return img, [x1,y1-y_offset_toe]#Original [0,120]\n",
    "    #return img, x1, y1-y_offset_toe #Original [0,120] #貌似ed不用改，蛋mse變超大\n",
    "    #return img, [x1/120,(y1-y_offset_toe)/120]#normalized [0,1] xy <dtype: 'float64'>, no help.\n",
    "    #return img, [(x1-60)/60,((y1-y_offset_toe)-60)/60]#normalized [-1,1] , no help.\n",
    "\n",
    "#\n",
    "# map list to ds, Heel part.\n",
    "#\n",
    "\n",
    "def decode_crop_png_heel(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.io.decode_png(img, channels=3)\n",
    "    # crop the toe from top-left corner [image, offset_height y1, offset_width x1, target_height, target_width]\n",
    "    y2=y_offset_heel;    x2=0;    h=img_height;    w=img_width # not the pp location\n",
    "    img = tf.image.crop_to_bounding_box(img, int(y2), int(x2), h, w)\n",
    "    #img = tf.image.crop_to_bounding_box(img, int(y1), int(x1), int(y2)-int(y1), int(x2)-int(x1))\n",
    "    # resize the image to the desired size\n",
    "    return img\n",
    "\n",
    "def process_path_heel(file_path,x1,y1,x2,y2):\n",
    "    file_path = im_p + file_path\n",
    "    #label = get_label(file_path)\n",
    "    # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)#can read the byte string paths b'image_0001.png'\n",
    "    img = decode_crop_png_heel(img)\n",
    "    return img, [x2,y2-y_offset_heel]#Original [0,120]\n",
    "    #return img, x2, y2-y_offset_heel #Original [0,120] #貌似ed不用改，蛋mse變超大\n",
    "    #return img, [x2/120,(y2-y_offset_heel)/120]#normalized [0,1] xy <dtype: 'float64'>, no help.\n",
    "    #return img, [(x2-60)/60,((y2-y_offset_heel)-60)/60]#normalized [-1,1] , no help.\n",
    "\n",
    "#\n",
    "# test how to put parameters to map\n",
    "#\n",
    "\n",
    "def t_ds_map(file_path,x1,y1,x2,y2):\n",
    "#     img = get_img('train/images/' + str(file_path))\n",
    "#     print(file_path)\n",
    "    return file_path,x1,y1,x2,y2 #img, [x1,y1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toe ds\n",
    "train_ds_map_toe = list_ds.map(process_path_toe, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "# Heel ds\n",
    "train_ds_map_heel = list_ds.map(process_path_heel, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "take sample: 63 76\n",
      "f <dtype: 'string'>\n",
      "x <dtype: 'int64'>\n"
     ]
    }
   ],
   "source": [
    "# for img, xy in train_ds_map_toe.take(1):\n",
    "#     print(f'take sample: {xy}')\n",
    "    \n",
    "# print('f', f.dtype)\n",
    "# print('xy', xy.dtype)\n",
    "\n",
    "# for img, x, y in train_ds_map_toe.take(1):\n",
    "#     print(f'take sample: {x} {y}')\n",
    "    \n",
    "# print('img', img.dtype)\n",
    "# print('x', x.dtype)\n",
    "# x\n",
    "\n",
    "for img, [x, y] in train_ds_map_toe.take(1):\n",
    "    print(f'take sample: {x} {y}')\n",
    "    \n",
    "print('f', f.dtype)\n",
    "print('x', x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int64, numpy=63>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for f,x1,y1,x2,y2 in train_ds_map.take(5):\n",
    "#     print(f'take sample: {f} {x1} {y1} {x2} {y2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [new] Split train_ds_pre with ratio of validation %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ToDo 20210201] keep orignal validation in 0.1, but augmenting train_ds in input layer or in the tf.ds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2021-02-23] New k-split ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # split Toe\n",
    "# val_size = int(tf.data.experimental.cardinality(train_ds_map_toe).numpy() * 0.2)\n",
    "# # val_size = int(tf.data.experimental.cardinality(train_ds_map_toe).numpy() * 0.1)#no help\n",
    "\n",
    "# train_ds_map_toe_s = train_ds_map_toe.skip(val_size)\n",
    "# valid_ds_map_toe_s = train_ds_map_toe.take(val_size)\n",
    "\n",
    "# print(f'whole samples = {len(train_ds_map_toe)}')\n",
    "# print(f'val_size = {val_size}')\n",
    "\n",
    "# print('ds_train = ', tf.data.experimental.cardinality(train_ds_map_toe_s).numpy())\n",
    "# print('ds_valid = ', tf.data.experimental.cardinality(valid_ds_map_toe_s).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # split Heel\n",
    "# val_size = int(tf.data.experimental.cardinality(train_ds_map_heel).numpy() * 0.2)\n",
    "# # val_size = int(tf.data.experimental.cardinality(train_ds_map_heel).numpy() * 0.1)\n",
    "\n",
    "\n",
    "# train_ds_map_heel_s = train_ds_map_heel.skip(val_size)\n",
    "# valid_ds_map_heel_s = train_ds_map_heel.take(val_size)\n",
    "\n",
    "# print(len(train_ds_map_heel))\n",
    "# print(val_size)\n",
    "# print(tf.data.experimental.cardinality(train_ds_map_heel_s).numpy())\n",
    "# print(tf.data.experimental.cardinality(valid_ds_map_heel_s).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## testing cell\n",
    "# kf = []\n",
    "# for k in range(5):\n",
    "#     kf.append(train_ds_map_heel.shard(num_shards=5, index=k))\n",
    "#     print(\"k =\", k,\"num=\", tf.data.experimental.cardinality(kf[k]).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for img, [x,y] in kf[1].take(1):\n",
    "#     print(f'take sample: {x} {y}')\n",
    "    \n",
    "# print('img', img.dtype)\n",
    "# print('x', x.dtype)\n",
    "# print('y', y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## testing cell\n",
    "\n",
    "# range_k_0 = train_ds_map_heel.window(5)\n",
    "\n",
    "# print(len(range_k_0))\n",
    "# print(tf.data.experimental.cardinality(range_k_0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## testing cell\n",
    "\n",
    "# def get_train_valid_k_split():\n",
    "#     x = tf.data.Dataset.range(1000)\n",
    "#     val_size = int(tf.data.experimental.cardinality(x).numpy() * 0.2)\n",
    "    \n",
    "#     for k in range(5):\n",
    "#         train_num = x.take(val_size + k*val_size)\n",
    "#         valid_num = x.skip(k*val_size)\n",
    "        \n",
    "#         print(\"k=\", k)\n",
    "#         print(tf.data.experimental.cardinality(train_num).numpy())\n",
    "#         print(tf.data.experimental.cardinality(valid_num).numpy())\n",
    "    \n",
    "    \n",
    "# get_train_valid_k_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_size= 0\n",
      "k = 0 k*val_size+val_size 0 k_train num= 10\n",
      "k = 1 k*val_size+val_size 0 k_train num= 10\n",
      "k = 2 k*val_size+val_size 0 k_train num= 10\n",
      "k = 3 k*val_size+val_size 0 k_train num= 10\n",
      "k = 4 k*val_size+val_size 0 k_train num= 10\n"
     ]
    }
   ],
   "source": [
    "## testing cell\n",
    "\n",
    "# \n",
    "# tf.slice\n",
    "# tf.data.experimental.choose_from_datasets\n",
    "\n",
    "\n",
    "# x = list_ds\n",
    "# val_size = int(tf.data.experimental.cardinality(list_ds).numpy() * 0.2)\n",
    "x = tf.data.Dataset.range(10)\n",
    "\n",
    "def check_KFold_ds(x, K=5):\n",
    "    \n",
    "    val_size = int(tf.data.experimental.cardinality(x).numpy() * 0.05)\n",
    "    print(\"val_size=\", val_size)\n",
    "    \n",
    "    for k in range(K):\n",
    "#         k_train = x.take(val_size + k*val_size)\n",
    "#         k_valid = x.skip(k*val_size)\n",
    "#         k_train = tf.slice(x, k*val_size, val_size) #only for pure tensors not \n",
    "#         k_valid = x.skip(k*val_size)\n",
    "\n",
    "        # may skip twicce to performe kflod\n",
    "        t_take = x.take(k*val_size)\n",
    "        t_skip = x.skip(k*val_size+val_size)\n",
    "        k_train = t_take.concatenate(t_skip)\n",
    "        \n",
    "        v_skip = x.skip(k*val_size)\n",
    "        k_valid = v_skip.take(val_size)\n",
    "\n",
    "        print(\"k =\", k,\"k*val_size+val_size\", k*val_size+val_size, \"k_train num=\", tf.data.experimental.cardinality(k_train).numpy())\n",
    "\n",
    "\n",
    "        # x = tf.data.Dataset.range(10)\n",
    "#         for n in k_train:\n",
    "#             print(n.numpy())\n",
    "#         for n in k_valid:\n",
    "#             print(n.numpy())\n",
    "        \n",
    "        # list_ds\n",
    "#         for img, x1, y1, x2, y2 in k_train:\n",
    "#             print(x1, y1)\n",
    "\n",
    "        # train_ds_map_toe\n",
    "#         for img, (x, y) in k_train:\n",
    "#             print(x.numpy(), y.numpy())\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "check_KFold_ds(x)\n",
    "# check_KFold_ds(list_ds)    \n",
    "# check_KFold_ds(train_ds_map_toe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = tf.data.Dataset.range(10)\n",
    "# val_size = int(tf.data.experimental.cardinality(x).numpy() * 0.2)\n",
    "# print(\"val_size=\", val_size)\n",
    "\n",
    "def get_KFold_ds(x, K=0):\n",
    "        \n",
    "    k = K\n",
    "    # may skip twicce to perform kflod\n",
    "    # train ds\n",
    "    t_take = x.take(k*val_size)\n",
    "    t_skip = x.skip(k*val_size+val_size)\n",
    "    k_train = t_take.concatenate(t_skip)\n",
    "    # val ds\n",
    "    v_skip = x.skip(k*val_size)\n",
    "    k_valid = v_skip.take(val_size)\n",
    "\n",
    "    return k_train, k_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1278\n",
      "67\n"
     ]
    }
   ],
   "source": [
    "# val_size = int(tf.data.experimental.cardinality(train_ds_map_toe).numpy() * 0.2)\n",
    "val_size = int(tf.data.experimental.cardinality(train_ds_map_toe).numpy() * 0.05)\n",
    "t, v = get_KFold_ds(train_ds_map_toe, 1)\n",
    "\n",
    "print(tf.data.experimental.cardinality(t).numpy())\n",
    "print(tf.data.experimental.cardinality(v).numpy())\n",
    "\n",
    "# for n in v:\n",
    "#     print(n.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ParallelMapDataset shapes: ((120, 120, 3), (2,)), types: (tf.uint8, tf.int64)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds_map_toe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset shapes: ((120, 120, 3), (2,)), types: (tf.uint8, tf.int64)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Albumentation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # for NO keypoint augment\n",
    "# transforms = A.Compose([\n",
    "# #             A.RandomBrightness(limit=0.1, p=0.5),\n",
    "#             A.JpegCompression(quality_lower=65, quality_upper=100, p=0.5),#A.JpegCompression(quality_lower=85, quality_upper=100, p=0.5)\n",
    "#             A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "# #             A.RandomContrast(limit=0.2, p=0.5),\n",
    "#             A.FancyPCA(alpha=0.1, always_apply=False, p=1),#A.FancyPCA(alpha=0.1, always_apply=False, p=0.5)\n",
    "#             A.Downscale(scale_min=0.7, scale_max=0.9, interpolation=0, always_apply=False, p=0.5), #0.8~0.99 may better\n",
    "#             A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, always_apply=False, p=0.5),\n",
    "#             A.CLAHE(clip_limit=(1, 8), tile_grid_size=(8, 8), always_apply=False, p=0.5), #A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), always_apply=False, p=0.5)\n",
    "# #             A.GlassBlur(sigma=0.9, max_delta=2, iterations=2, always_apply=False, mode='fast', p=0.5),\n",
    "# #             A.GaussNoise(var_limit=(10.0, 50.0), mean=0, always_apply=False, p=0.5),\n",
    "# #             A.GaussianBlur(blur_limit=(3, 7), sigma_limit=0, always_apply=False, p=0.5),\n",
    "# #             A.HorizontalFlip(),\n",
    "    \n",
    "#             # try other augm, seems to strong...\n",
    "#             A.RandomBrightnessContrast(always_apply=False, p=0.5, brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), brightness_by_max=True),\n",
    "#             A.Equalize(always_apply=False, p=0.5, mode='cv', by_channels=True),\n",
    "#             A.MultiplicativeNoise(always_apply=False, p=0.5, multiplier=(0.8, 1.5), per_channel=False, elementwise=False),\n",
    "#             A.RandomFog(always_apply=False, p=0.5, fog_coef_lower=0.2, fog_coef_upper=0.3, alpha_coef=0.25),\n",
    "\n",
    "# ])\n",
    "\n",
    "\n",
    "# def aug_fn(image, img_size):\n",
    "#     data = {\"image\":image}\n",
    "#     aug_data = transforms(**data)\n",
    "#     aug_img = aug_data[\"image\"]\n",
    "# #     aug_img = tf.cast(aug_img/255.0, tf.float32)\n",
    "#     aug_img = tf.cast(aug_img, tf.float32)\n",
    "#     aug_img = tf.image.resize(aug_img, size=[img_size, img_size])\n",
    "#     return aug_img\n",
    "\n",
    "# def process_data(image, label, img_size):\n",
    "#     aug_img = tf.numpy_function(func=aug_fn, inp=[image, img_size], Tout=tf.float32)\n",
    "#     return aug_img, label\n",
    "\n",
    "# def set_shapes(img, label, img_shape=(120,120,3)):\n",
    "#     img.set_shape(img_shape)\n",
    "# #     label.set_shape([]) # commited for go around error\n",
    "#     return img, label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # for NO keypoint augment AND for OneOf[] for better heel loss.\n",
    "# transforms_oneof = A.Compose(A.OneOf([\n",
    "#             A.RandomBrightness(limit=0.1, p=0.5),\n",
    "#             A.JpegCompression(quality_lower=85, quality_upper=100, p=0.5),\n",
    "#             A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "#             A.RandomContrast(limit=0.2, p=0.5),\n",
    "#             A.FancyPCA(alpha=0.1, always_apply=False, p=0.5),\n",
    "#             A.Downscale(scale_min=0.7, scale_max=0.9, interpolation=0, always_apply=False, p=0.5),\n",
    "#             A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, always_apply=False, p=0.5),\n",
    "#             A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), always_apply=False, p=0.5),\n",
    "#             A.GlassBlur(sigma=0.9, max_delta=2, iterations=2, always_apply=False, mode='fast', p=0.5),\n",
    "#             A.GaussNoise(var_limit=(10.0, 50.0), mean=0, always_apply=False, p=0.5),\n",
    "#             A.GaussianBlur(blur_limit=(3, 7), sigma_limit=0, always_apply=False, p=.5)\n",
    "# #             A.HorizontalFlip(),\n",
    "#             ]),p=0.5)\n",
    "\n",
    "\n",
    "# def aug_fn_oneof(image, img_size):\n",
    "#     data = {\"image\":image}\n",
    "#     aug_data = transforms_oneof(**data)\n",
    "#     aug_img = aug_data[\"image\"]\n",
    "# #     aug_img = tf.cast(aug_img/255.0, tf.float32)\n",
    "#     aug_img = tf.cast(aug_img, tf.float32)\n",
    "#     aug_img = tf.image.resize(aug_img, size=[img_size, img_size])\n",
    "#     return aug_img\n",
    "\n",
    "# def process_data_oneof(image, label, img_size):\n",
    "#     aug_img = tf.numpy_function(func=aug_fn_oneof, inp=[image, img_size], Tout=tf.float32)\n",
    "#     return aug_img, label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Testing keypoints augment\n",
    "# transforms = A.Compose([\n",
    "#             A.RandomBrightness(limit=0.1, p=0.5),\n",
    "#             A.JpegCompression(quality_lower=85, quality_upper=100, p=0.5),\n",
    "#             A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "#             A.RandomContrast(limit=0.2, p=0.5),\n",
    "#             A.FancyPCA (alpha=0.1, always_apply=False, p=1),\n",
    "#             A.Downscale (scale_min=0.7, scale_max=0.9, interpolation=0, always_apply=False, p=0.5),\n",
    "#             A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, always_apply=False, p=0.5),\n",
    "#             A.CLAHE (clip_limit=4.0, tile_grid_size=(8, 8), always_apply=False, p=0.5),\n",
    "#             A.HorizontalFlip(p=0.5),\n",
    "#             ]\n",
    "#             , \n",
    "#             keypoint_params=A.KeypointParams(format='xy'),  #currently not works for tf.ds yet.\n",
    "#             )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Testing keypoints augment\n",
    "transforms = A.Compose([\n",
    "            A.RandomBrightness(limit=0.1, p=0.5),\n",
    "            A.JpegCompression(quality_lower=85, quality_upper=100, p=0.5),\n",
    "            A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "            A.RandomContrast(limit=0.2, p=0.5),\n",
    "            A.FancyPCA(alpha=0.1, always_apply=False, p=0.5),\n",
    "            A.Downscale(scale_min=0.7, scale_max=0.9, interpolation=0, always_apply=False, p=0.5),\n",
    "            A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, always_apply=False, p=0.5),\n",
    "            A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), always_apply=False, p=0.5),\n",
    "    \n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.RandomResizedCrop(always_apply=False, height=120, width=120, scale=(0.9, 0.99), ratio=(1.0, 1.0), interpolation=0, p=0.5),#xy become double need change dtype of label. # pp will outside the image.\n",
    "            A.IAAAffine(scale=0.9, translate_percent=None, translate_px=None, rotate=0.0, shear=0.0, order=1, cval=0, mode='reflect', always_apply=False, p=0.5),\n",
    "#             A.ShiftScaleRotate(always_apply=False, p=0.5, shift_limit=(0.06, 0.06), scale_limit=(-0.1, 0.1), rotate_limit=(-5, 5), interpolation=1, border_mode=2, value=(0, 0, 0), mask_value=None),\n",
    "    #2021-02-26\n",
    "#             A.IAAPerspective(scale=(0.05, 0.1), keep_size=True, always_apply=False, p=0.5),#fallout image make train stop. NOT support keypoints!!!!!\n",
    "            ]\n",
    "            , \n",
    "            keypoint_params=A.KeypointParams(format='xy',remove_invisible=True),  #currently not works for tf.ds yet.\n",
    "            )\n",
    "\n",
    "# Testing keypoints augment\n",
    "# @tf.function\n",
    "def aug_fn(image, keypoints, img_size):\n",
    "#     print('Check keypoints aug_fun 00:', keypoints) # Check keypoints aug_fun 00: [[53 58]]\n",
    "#     data = {\"image\":image}\n",
    "    aug_data = transforms(image=image, keypoints=keypoints)\n",
    "    aug_img = aug_data[\"image\"]\n",
    "    aug_xy  = aug_data[\"keypoints\"]\n",
    "#     aug_img = tf.cast(aug_img/255.0, tf.float32)\n",
    "    aug_img = tf.cast(aug_img, tf.float32)\n",
    "    aug_img = tf.image.resize(aug_img, size=[img_size, img_size])\n",
    "    \n",
    "    aug_xy = tf.cast(aug_xy, tf.float32) #有些變形輸出是double\n",
    "#     print('Check aug_xy:', aug_xy) # Check aug_xy: [(95, 45)] #印到這邊都是對的\n",
    "    return aug_img, aug_xy \n",
    "\n",
    "# @tf.function\n",
    "def process_data(image, keypoints, img_size):\n",
    "    \n",
    "    print('Check keypoints process01:', keypoints, np.shape(keypoints), type(keypoints))\n",
    "        \n",
    "#     keypoints = tf.make_ndarray(keypoints)\n",
    "#     keypoints = np.array(keypoints)\n",
    "#     keypoints = list(keypoints)\n",
    "#     keypoints = np.asarray(keypoints, dtype=np.float32)\n",
    "#     keypoints = tf.make_ndarray(keypoints.op.get_attr('value'))\n",
    "\n",
    "#     keypoints = tf.reshape(keypoints, [1, 2])\n",
    "    keypoints = tf.reshape(keypoints, [1, 2]) # for 'convert_keypoint_to_albumentations'\n",
    "#     keypoints = np.reshape(keypoints, (1, 2))#not support tensor with np.call.\n",
    "\n",
    "    print('Check keypoints process02:', keypoints, np.shape(keypoints), type(keypoints))\n",
    "\n",
    "#     aug_img, aug_xy = tf.numpy_function(func=aug_fn, inp=[image, img_size], Tout=tf.float32)\n",
    "#     aug_img, aug_xy = tf.py_function(func=aug_fn, inp=[image, keypoints, img_size], Tout=[tf.float32, tf.int64])#for tensors.\n",
    "    aug_img, aug_xy = tf.numpy_function(func=aug_fn, inp=[image, keypoints, img_size], Tout=[tf.float32, tf.float32])\n",
    "    print('Check keypoints process03:', aug_xy)\n",
    "    \n",
    "    aug_xy = tf.reshape(aug_xy, [2,]) # for 'tf ds tarining'\n",
    "    print('Check keypoints process04:', aug_xy)\n",
    "        \n",
    "    return aug_img, aug_xy \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###  for AToe ###\n",
    "# some pp will outside of image bcs p2 is close to 400.\n",
    "\n",
    "\n",
    "\n",
    "# Testing keypoints augment\n",
    "transforms_AToe = A.Compose([\n",
    "#             A.RandomBrightness(limit=0.1, p=0.5),\n",
    "#             A.JpegCompression(quality_lower=85, quality_upper=100, p=0.5),\n",
    "#             A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "#             A.RandomContrast(limit=0.2, p=0.5),\n",
    "#             A.FancyPCA(alpha=0.1, always_apply=False, p=0.5),\n",
    "#             A.Downscale(scale_min=0.7, scale_max=0.9, interpolation=0, always_apply=False, p=0.5),\n",
    "#             A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, always_apply=False, p=0.5),\n",
    "#             A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), always_apply=False, p=0.5),\n",
    "    \n",
    "#             A.RandomBrightness(limit=0.1, p=0.5),\n",
    "            A.JpegCompression(quality_lower=65, quality_upper=100, p=0.5),#A.JpegCompression(quality_lower=85, quality_upper=100, p=0.5)\n",
    "            A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "#             A.RandomContrast(limit=0.2, p=0.5),\n",
    "            A.FancyPCA(alpha=0.1, always_apply=False, p=1),#A.FancyPCA(alpha=0.1, always_apply=False, p=0.5)\n",
    "            A.Downscale(scale_min=0.7, scale_max=0.9, interpolation=0, always_apply=False, p=0.5), #0.8~0.99 may better\n",
    "            A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, always_apply=False, p=0.5),\n",
    "            A.CLAHE(clip_limit=(1, 8), tile_grid_size=(8, 8), always_apply=False, p=0.5), #A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), always_apply=False, p=0.5)\n",
    "    \n",
    "            A.HorizontalFlip(p=0.5),\n",
    "#             A.RandomResizedCrop(always_apply=False, height=120, width=120, scale=(0.75, 0.9), ratio=(1.0, 1.0), interpolation=0, p=0.5),#xy become double need change dtype of label. # pp will outside the image.\n",
    "            A.IAAAffine (scale=0.9, translate_percent=None, translate_px=None, rotate=0.0, shear=0.0, order=1, cval=0, mode='reflect', always_apply=False, p=0.5),\n",
    "#             A.ShiftScaleRotate(always_apply=False, p=0.5, shift_limit=(0.06, 0.06), scale_limit=(-0.1, 0.1), rotate_limit=(-5, 5), interpolation=1, border_mode=2, value=(0, 0, 0), mask_value=None),\n",
    "            ]\n",
    "            , \n",
    "            keypoint_params=A.KeypointParams(format='xy',remove_invisible=True),  #currently not works for tf.ds yet.\n",
    "            )\n",
    "\n",
    "\n",
    "# Testing keypoints augment\n",
    "# @tf.function\n",
    "def aug_fn_AToe(image, keypoints, img_size):\n",
    "#     print('Check keypoints aug_fun 00:', keypoints) # Check keypoints aug_fun 00: [[53 58]]\n",
    "#     data = {\"image\":image}\n",
    "    aug_data = transforms_AToe(image=image, keypoints=keypoints)\n",
    "    aug_img = aug_data[\"image\"]\n",
    "    aug_xy  = aug_data[\"keypoints\"]\n",
    "#     aug_img = tf.cast(aug_img/255.0, tf.float32)\n",
    "    aug_img = tf.cast(aug_img, tf.float32)\n",
    "    aug_img = tf.image.resize(aug_img, size=[img_size, img_size])\n",
    "    \n",
    "    aug_xy = tf.cast(aug_xy, tf.float32) #有些變形輸出是double\n",
    "#     print('Check aug_xy:', aug_xy) # Check aug_xy: [(95, 45)] #印到這邊都是對的\n",
    "    return aug_img, aug_xy \n",
    "\n",
    "# @tf.function\n",
    "def process_data_AToe(image, keypoints, img_size):\n",
    "    \n",
    "    print('Check keypoints process01:', keypoints, np.shape(keypoints), type(keypoints))\n",
    "        \n",
    "#     keypoints = tf.make_ndarray(keypoints)\n",
    "#     keypoints = np.array(keypoints)\n",
    "#     keypoints = list(keypoints)\n",
    "#     keypoints = np.asarray(keypoints, dtype=np.float32)\n",
    "#     keypoints = tf.make_ndarray(keypoints.op.get_attr('value'))\n",
    "\n",
    "#     keypoints = tf.reshape(keypoints, [1, 2])\n",
    "    keypoints = tf.reshape(keypoints, [1, 2]) # for 'convert_keypoint_to_albumentations'\n",
    "#     keypoints = np.reshape(keypoints, (1, 2))#not support tensor with np.call.\n",
    "\n",
    "    print('Check keypoints process02:', keypoints, np.shape(keypoints), type(keypoints))\n",
    "\n",
    "#     aug_img, aug_xy = tf.numpy_function(func=aug_fn, inp=[image, img_size], Tout=tf.float32)\n",
    "#     aug_img, aug_xy = tf.py_function(func=aug_fn, inp=[image, keypoints, img_size], Tout=[tf.float32, tf.int64])#for tensors.\n",
    "    aug_img, aug_xy = tf.numpy_function(func=aug_fn_AToe, inp=[image, keypoints, img_size], Tout=[tf.float32, tf.float32])\n",
    "    print('Check keypoints process03:', aug_xy)\n",
    "    \n",
    "    aug_xy = tf.reshape(aug_xy, [2,]) # for 'tf ds tarining'\n",
    "    print('Check keypoints process04:', aug_xy)\n",
    "        \n",
    "    return aug_img, aug_xy \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def set_shapes(img, label, img_shape=(120,120,3)):\n",
    "    img.set_shape(img_shape)\n",
    "#     label.set_shape([]) # commited for go around error\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepare train_ds_prefetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_for_performance_cache_train(ds, cache=True, augment=False):\n",
    "\n",
    "    \n",
    "    \"\"\"#TODO: need to check the parse logic of ds.cache.\n",
    "    if cache:\n",
    "        print(\"Check cache-f1 to file:\", cache)\n",
    "        if isinstance(cache, str):\n",
    "            ds = ds.cache(cache)\n",
    "            print(\"Check cache-f2 to file:\", cache)\n",
    "    else:\n",
    "        ds = ds.cache()\n",
    "        print(\"Check cache in memory:\", cache)\n",
    "    \"\"\"    \n",
    "    if cache:\n",
    "        ds = ds.cache()\n",
    "        print(\"Check cache in memory:Y\", cache)\n",
    "    else:\n",
    "        print(\"Check cache in memory:N\", cache)\n",
    "        \n",
    "    if augment:\n",
    "        ds = ds.map(partial(process_data, img_size=120),num_parallel_calls=AUTOTUNE)\n",
    "        ds = ds.map(set_shapes, num_parallel_calls=AUTOTUNE)\n",
    "        print(\"Check augment :Y\", augment)\n",
    "    else:\n",
    "        print(\"Check augment :N\", augment)\n",
    "    \n",
    "    #ds = ds.repeat()#TODO:2020-12-14: test\n",
    "    #ds = ds.shuffle(buffer_size=MULTI_BATCH_SIZE*2) # (buffer_size=MULTI_BATCH_SIZE*5) 6sec,buffer_size=1000 take few sec. or buffer_size=image_count <- take too long # each take ds take 30~45 sec, TODO!!\n",
    "    ds = ds.shuffle(len(list_ds), reshuffle_each_iteration=True) #todo: move to ds_pre. see https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle\n",
    "    ds = ds.batch(MULTI_BATCH_SIZE)#MULTI_BATCH_SIZE for multi-GPUs\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE) #buffer_size=AUTOTUNE seem no speed improve\n",
    "    \n",
    "    print(\"Check ds cache[{}] and augment[{}]\".format(cache, augment))\n",
    "    \n",
    "    return ds\n",
    "\n",
    "\n",
    "def configure_for_performance_cache_train_AToe(ds, cache=True, augment=False):\n",
    "\n",
    "    \n",
    "    \"\"\"#TODO: need to check the parse logic of ds.cache.\n",
    "    if cache:\n",
    "        print(\"Check cache-f1 to file:\", cache)\n",
    "        if isinstance(cache, str):\n",
    "            ds = ds.cache(cache)\n",
    "            print(\"Check cache-f2 to file:\", cache)\n",
    "    else:\n",
    "        ds = ds.cache()\n",
    "        print(\"Check cache in memory:\", cache)\n",
    "    \"\"\"    \n",
    "    if cache:\n",
    "        ds = ds.cache()\n",
    "        print(\"Check cache in memory:Y\", cache)\n",
    "    else:\n",
    "        print(\"Check cache in memory:N\", cache)\n",
    "        \n",
    "    if augment:\n",
    "        ds = ds.map(partial(process_data_AToe, img_size=120),num_parallel_calls=AUTOTUNE)\n",
    "        ds = ds.map(set_shapes, num_parallel_calls=AUTOTUNE)\n",
    "        print(\"Check augment :Y\", augment)\n",
    "    else:\n",
    "        print(\"Check augment :N\", augment)\n",
    "    \n",
    "    #ds = ds.repeat()#TODO:2020-12-14: test\n",
    "    #ds = ds.shuffle(buffer_size=MULTI_BATCH_SIZE*2) # (buffer_size=MULTI_BATCH_SIZE*5) 6sec,buffer_size=1000 take few sec. or buffer_size=image_count <- take too long # each take ds take 30~45 sec, TODO!!\n",
    "    ds = ds.shuffle(len(list_ds), reshuffle_each_iteration=True) #todo: move to ds_pre. see https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle\n",
    "    ds = ds.batch(MULTI_BATCH_SIZE)#MULTI_BATCH_SIZE for multi-GPUs\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE) #buffer_size=AUTOTUNE seem no speed improve\n",
    "    \n",
    "    print(\"Check ds cache[{}] and augment[{}]\".format(cache, augment))\n",
    "    \n",
    "    return ds\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def configure_for_performance_cache_train_oneof(ds, cache=True, augment=False):\n",
    "\n",
    "    \n",
    "#     \"\"\"#TODO: need to check the parse logic of ds.cache.\n",
    "#     if cache:\n",
    "#         print(\"Check cache-f1 to file:\", cache)\n",
    "#         if isinstance(cache, str):\n",
    "#             ds = ds.cache(cache)\n",
    "#             print(\"Check cache-f2 to file:\", cache)\n",
    "#     else:\n",
    "#         ds = ds.cache()\n",
    "#         print(\"Check cache in memory:\", cache)\n",
    "#     \"\"\"    \n",
    "#     if cache:\n",
    "#         ds = ds.cache()\n",
    "#         print(\"Check cache in memory:Y\", cache)\n",
    "#     else:\n",
    "#         print(\"Check cache in memory:N\", cache)\n",
    "        \n",
    "#     if augment:\n",
    "#         ds = ds.map(partial(process_data_oneof, img_size=120),num_parallel_calls=AUTOTUNE)\n",
    "#         ds = ds.map(set_shapes, num_parallel_calls=AUTOTUNE)\n",
    "#         print(\"Check augment :Y\", augment)\n",
    "#     else:\n",
    "#         print(\"Check augment :N\", augment)\n",
    "    \n",
    "#     #ds = ds.repeat()#TODO:2020-12-14: test\n",
    "#     #ds = ds.shuffle(buffer_size=MULTI_BATCH_SIZE*2) # (buffer_size=MULTI_BATCH_SIZE*5) 6sec,buffer_size=1000 take few sec. or buffer_size=image_count <- take too long # each take ds take 30~45 sec, TODO!!\n",
    "#     ds = ds.shuffle(len(list_ds), reshuffle_each_iteration=True) #todo: move to ds_pre. see https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle\n",
    "#     ds = ds.batch(MULTI_BATCH_SIZE)#MULTI_BATCH_SIZE for multi-GPUs\n",
    "#     ds = ds.prefetch(buffer_size=AUTOTUNE) #buffer_size=AUTOTUNE seem no speed improve\n",
    "    \n",
    "#     print(\"Check ds cache[{}] and augment[{}]\".format(cache, augment))\n",
    "    \n",
    "#     return ds\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def configure_for_performance_cache_val(ds, cache=True, augment=False):\n",
    "\n",
    "    \n",
    "    \"\"\"#TODO: need to check the parse logic of ds.cache\n",
    "    TODO:test remove ds.shuffle from val_ds.\n",
    "    .\n",
    "    if cache:\n",
    "        print(\"Check cache-f1 to file:\", cache)\n",
    "        if isinstance(cache, str):\n",
    "            ds = ds.cache(cache)\n",
    "            print(\"Check cache-f2 to file:\", cache)\n",
    "    else:\n",
    "        ds = ds.cache()\n",
    "        print(\"Check cache in memory:\", cache)\n",
    "    \"\"\"    \n",
    "    if cache:\n",
    "        ds = ds.cache()\n",
    "        print(\"Check cache in memory:Y\", cache)\n",
    "    else:\n",
    "        print(\"Check cache in memory:N\", cache)\n",
    "        \n",
    "    if augment:\n",
    "#         ds = ds.map(data_augment, num_parallel_calls=AUTOTUNE)\n",
    "        ds = ds.map(AA, num_parallel_calls=AUTOTUNE)\n",
    "#         ds = ds.map(RA, num_parallel_calls=AUTOTUNE)\n",
    "        print(\"Check augment :Y\", augment)\n",
    "    else:\n",
    "        print(\"Check augment :N\", augment)\n",
    "    \n",
    "    #ds = ds.repeat()#TODO:2020-12-14: test\n",
    "    #ds = ds.shuffle(buffer_size=MULTI_BATCH_SIZE*2) # (buffer_size=MULTI_BATCH_SIZE*5) 6sec,buffer_size=1000 take few sec. or buffer_size=image_count <- take too long # each take ds take 30~45 sec, TODO!!\n",
    "#     ds = ds.shuffle(len(list_ds), reshuffle_each_iteration=False) #todo: move to ds_pre. see https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle\n",
    "    ds = ds.batch(MULTI_BATCH_SIZE)#MULTI_BATCH_SIZE for multi-GPUs\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE) #buffer_size=AUTOTUNE seem no speed improve\n",
    "    \n",
    "    print(\"Check ds cache[{}] and augment[{}]\".format(cache, augment))\n",
    "    \n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Prepare the ds properties (cache, augment, bs, shuffle, prefetch, etc.) for better performance.\n",
    "# \"\"\"\n",
    "# # Toe ds_pre\n",
    "# train_ds_pre_toe = configure_for_performance_cache_train(train_ds_map_toe)\n",
    "\n",
    "# # Heel ds_pre\n",
    "# train_ds_pre_heel = configure_for_performance_cache_val(train_ds_map_heel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All split ds_prefetch\n",
    "* train_ds_map_toe_s = train_ds_map_toe.skip(val_size)\n",
    "* valid_ds_map_toe_s = train_ds_map_toe.take(val_size)\n",
    "\n",
    "* train_ds_map_heel_s = train_ds_map_heel.skip(val_size)\n",
    "* valid_ds_map_heel_s = train_ds_map_heel.take(val_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Prepare the ds properties (cache, augment, bs, shuffle, prefetch, etc.) for better performance.\n",
    "# \"\"\"\n",
    "# # Toe ds_pre\n",
    "# train_ds_pre_toe_s = configure_for_performance_cache_train(train_ds_map_toe_s, augment=True)\n",
    "# valid_ds_pre_toe_s = configure_for_performance_cache_val(valid_ds_map_toe_s)\n",
    "\n",
    "# # Heel ds_pre\n",
    "# train_ds_pre_heel_s = configure_for_performance_cache_train(train_ds_map_heel_s, augment=True)\n",
    "# valid_ds_pre_heel_s = configure_for_performance_cache_val(valid_ds_map_heel_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check ds_prefetch samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create it_ds once\n",
    "# it_train_ds_pre_toe_s = iter(train_ds_pre_toe_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # for albu keypoint\n",
    "\n",
    "# # for original return aug_img, , aug_xy \n",
    "\n",
    "\n",
    "# image_batch, label_batch = next(it_train_ds_pre_toe_s)\n",
    "\n",
    "# fig = plt.figure(figsize=(8, 8))\n",
    "# print('batch * multi:', len(label_batch), ', MULTI_BATCH_SIZE=', MULTI_BATCH_SIZE)\n",
    "# for i in range(16):\n",
    "#     ax = plt.subplot(4, 4, i + 1)\n",
    "#     plt.imshow(image_batch[i].numpy().astype(\"uint8\"))\n",
    "#     plt.title(label_batch[i].numpy())\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.plot(label_batch[i].numpy()[0], label_batch[i].numpy()[1], 'r+', markersize=13, mew=2.5)\n",
    "\n",
    "#     print(f'Check lables: {label_batch[i]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create it_ds once\n",
    "# it_train_ds_pre_heel_s = iter(train_ds_pre_heel_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # for albu keypoint\n",
    "\n",
    "# # for original return aug_img, , aug_xy \n",
    "\n",
    "\n",
    "# image_batch, label_batch = next(it_train_ds_pre_heel_s)\n",
    "\n",
    "# fig = plt.figure(figsize=(8, 8))\n",
    "# print('batch * multi:', len(label_batch), ', MULTI_BATCH_SIZE=', MULTI_BATCH_SIZE)\n",
    "# for i in range(16):\n",
    "#     ax = plt.subplot(4, 4, i + 1)\n",
    "#     plt.imshow(image_batch[i].numpy().astype(\"uint8\"))\n",
    "#     plt.title(label_batch[i].numpy())\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.plot(label_batch[i].numpy()[0], label_batch[i].numpy()[1], 'r+', markersize=13, mew=2.5)\n",
    "\n",
    "#     print(f'Check lables: {label_batch[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create it_ds once\n",
    "# it_valid_ds_pre_toe_s = iter(valid_ds_pre_toe_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # for albu keypoint\n",
    "\n",
    "# # for return aug_img, aug_xy \n",
    "\n",
    "\n",
    "# image_batch, label_batch = next(it_valid_ds_pre_toe_s)\n",
    "\n",
    "# plt.figure(figsize=(8, 8))\n",
    "# # for images, labels in valid_ds_pre_toe_s.take(1):\n",
    "# print('batch * multi:', len(label_batch), ', MULTI_BATCH_SIZE=', MULTI_BATCH_SIZE)\n",
    "# for i in range(16):\n",
    "#     ax = plt.subplot(4, 4, i + 1)\n",
    "#     plt.imshow(image_batch[i].numpy().astype(\"uint8\"))\n",
    "#     plt.title(label_batch[i].numpy())\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.plot(label_batch[i].numpy()[0], label_batch[i].numpy()[1], 'r+', markersize=13, mew=2.5)\n",
    "\n",
    "#     print(f'Check lables: {label_batch[i]}')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Loss function\n",
    "\n",
    "* mae\n",
    "* euclidean distance\n",
    "* others\n",
    "\n",
    "\n",
    "        # 'x' is [[1, 1, 1]\n",
    "        #         [1, 1, 1]]\n",
    "        tf.reduce_sum(x) ==> 6\n",
    "        tf.reduce_sum(x, 0) ==> [2, 2, 2]\n",
    "        tf.reduce_sum(x, 1) ==> [3, 3]\n",
    "        the function is default for 2-D array, therefor, in our 1-D [x1,y1] to [x2,y2] the axis should be '0' or just leave it.\n",
    "        \n",
    "        tf.sqrt need \tA tf.Tensor of type bfloat16, half, float32, float64, complex64, complex128\n",
    "        so, convert it first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "should be -> tf.Tensor([56 39], shape=(2,), dtype=int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [10, 10]\n",
    "y_pred = [10, 20]\n",
    "\n",
    "# y_true = [1.00000000000000000000123, 10]\n",
    "# y_pred = [1.0, 10.000000000000000000000000001]\n",
    "\n",
    "# y_true = [1.0000123, 10]\n",
    "# y_pred = [1.0, 10.0000321]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=5>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mae\n",
    "\n",
    "loss_mae = tf.keras.losses.MAE(\n",
    "    y_true, y_pred\n",
    ")\n",
    "\n",
    "loss_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=10.0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ed\n",
    "\n",
    "# loss_ed = tf.sqrt(tf.reduce_sum(tf.square(tf.constant(y_true) - tf.constant(y_pred)), 0))\n",
    "\n",
    "# loss_ed = tf.sqrt(tf.reduce_sum(tf.square(tf.Variable(y_true) - tf.Variable(y_pred)), 0))\n",
    "\n",
    "loss_ed = tf.sqrt(tf.reduce_sum(tf.square(tf.cast(y_true, tf.float32) - tf.cast(y_pred, tf.float32)), 0))\n",
    "\n",
    "loss_ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ed_loss(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_sum(tf.square(tf.cast(y_true, tf.float32) - tf.cast(y_pred, tf.float32)), 0))\n",
    "\n",
    "# fix NaN in euclidean distance\n",
    "# tf.maximum(d, 1e-9), to keep atlease is 1e-9.\n",
    "# def ed_loss(y_true, y_pred):\n",
    "#     return tf.sqrt(tf.maximum(tf.reduce_sum(tf.square(tf.cast(y_true, tf.float32) - tf.cast(y_pred, tf.float32)), 0), 1e-9))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the euclidean distance loss\n",
    "ed_loss(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=10.0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ed_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mean Euclidean distance \n",
    "\n",
    "* here the y_true and y_pred is 2-D array. the axis use 1.\n",
    "\n",
    "\n",
    "* NOTE: LB評分的mean euclidean distance功能，應該跟model.evaluate()一樣so不需重新寫。evaluate()會自動用loss (model.metrics_names)計算後在自動平均，而模型loss我們是用ed-loss取代。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_true = [[60, 76],\n",
    "#        [58, 49 ],\n",
    "#        [63, 67 ],\n",
    "#        [58 , 57]]\n",
    "# y_pred = [[59.927303, 76.471214],\n",
    "#        [58.056904, 49.98754 ],\n",
    "#        [63.067844, 67.03861 ],\n",
    "#        [58.70202 , 57.372707]]\n",
    "\n",
    "y_true = [[60, 70],\n",
    "       [70, 80]]\n",
    "y_pred = [[61, 71],\n",
    "       [72, 82]]\n",
    "\n",
    "# y_true = [(60, 70),\n",
    "#        (70, 80)]\n",
    "# y_pred = [(61, 71),\n",
    "#        (72, 82)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[1., 1.],\n",
       "       [4., 4.]], dtype=float32)>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.square(tf.cast(y_true, tf.float32) - tf.cast(y_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([2., 8.], dtype=float32)>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(tf.square(tf.cast(y_true, tf.float32) - tf.cast(y_pred, tf.float32)), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ed_metric_2d(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_sum(tf.square(tf.cast(y_true, tf.float32) - tf.cast(y_pred, tf.float32)), 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.4142135, 2.828427 ], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ed_metric_2d(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1.4142135, 2.828427 ], dtype=float32)>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ed_metric_2d(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 等於true, pred點位ed的平均，LB評分方式。(toe/heel即p1,p2要個別算ed一次再相加)\n",
    "def ed_metric_2d_mean(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.sqrt(tf.reduce_sum(tf.square(tf.cast(y_true, tf.float32) - tf.cast(y_pred, tf.float32)), 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for re-scale back xy \n",
    "# return img, [(x1-60)/60,((y1-y_offset_toe)-60)/60]#normalized [-1,1] \n",
    "# return img, [(x2-60)/60,((y2-y_offset_toe)-60)/60]#normalized [-1,1] \n",
    "\n",
    "# 等於true, pred點位ed的平均，LB評分方式。(toe/heel即p1,p2要個別算ed一次再相加)\n",
    "def edRescal(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.sqrt(tf.reduce_sum(tf.square(tf.cast((y_true*60)+60, tf.float32) - tf.cast((y_pred*60)+60, tf.float32)), 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1213202"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ed_metric_2d_mean(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=2.1213202>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ed_metric_2d_mean(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EFNE Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maybe mae better than ed loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f73107cfbe0>]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"dump lr\n",
    "\"\"\"\n",
    "ep_num_transf = 500\n",
    "\n",
    "\n",
    "\n",
    "def lrdump(epoch):\n",
    "    \n",
    "    #step_size = 100\n",
    "    lr_max = 0.006\n",
    "    lr_min = 0.001\n",
    "    lr_start = 0.01\n",
    "\n",
    "    lr_init_ep = 0\n",
    "    lr_ramp_ep = 100\n",
    "    lr_sus_ep  = 0\n",
    "    lr_decay   = 0.8\n",
    "\n",
    "    \n",
    "    # warm up\n",
    "    if epoch < lr_init_ep:\n",
    "        lr = (lr_max - lr_min) / lr_ramp_ep * epoch + lr_min    \n",
    "        \n",
    "    elif lr_init_ep -1 < epoch < lr_ramp_ep:\n",
    "        lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "\n",
    "    elif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "        lr = lr_max\n",
    "\n",
    "    else:\n",
    "        lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
    "\n",
    "    return lr\n",
    "\n",
    "rng = [i for i in range(ep_num_transf)]\n",
    "y = [lrdump(x) for x in rng]\n",
    "sns.set(style='darkgrid')\n",
    "fig, ax = plt.subplots(figsize=(20, 6))\n",
    "plt.plot(rng, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 ~ 0.01\n"
     ]
    }
   ],
   "source": [
    "print('{} ~ {}'.format(min(y), max(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t 0.01\n",
      "\n",
      "1\t 0.00996\n",
      "\n",
      "2\t 0.00992\n",
      "\n",
      "3\t 0.00988\n",
      "\n",
      "4\t 0.00984\n",
      "\n",
      "5\t 0.0098\n",
      "\n",
      "6\t 0.00976\n",
      "\n",
      "7\t 0.00972\n",
      "\n",
      "8\t 0.00968\n",
      "\n",
      "9\t 0.009640000000000001\n",
      "\n",
      "10\t 0.009600000000000001\n",
      "\n",
      "11\t 0.00956\n",
      "\n",
      "12\t 0.00952\n",
      "\n",
      "13\t 0.00948\n",
      "\n",
      "14\t 0.00944\n",
      "\n",
      "15\t 0.0094\n",
      "\n",
      "16\t 0.00936\n",
      "\n",
      "17\t 0.00932\n",
      "\n",
      "18\t 0.00928\n",
      "\n",
      "19\t 0.00924\n",
      "\n",
      "20\t 0.0092\n",
      "\n",
      "21\t 0.00916\n",
      "\n",
      "22\t 0.00912\n",
      "\n",
      "23\t 0.00908\n",
      "\n",
      "24\t 0.00904\n",
      "\n",
      "25\t 0.009000000000000001\n",
      "\n",
      "26\t 0.00896\n",
      "\n",
      "27\t 0.00892\n",
      "\n",
      "28\t 0.00888\n",
      "\n",
      "29\t 0.00884\n",
      "\n",
      "30\t 0.0088\n",
      "\n",
      "31\t 0.00876\n",
      "\n",
      "32\t 0.00872\n",
      "\n",
      "33\t 0.00868\n",
      "\n",
      "34\t 0.00864\n",
      "\n",
      "35\t 0.0086\n",
      "\n",
      "36\t 0.00856\n",
      "\n",
      "37\t 0.00852\n",
      "\n",
      "38\t 0.00848\n",
      "\n",
      "39\t 0.00844\n",
      "\n",
      "40\t 0.0084\n",
      "\n",
      "41\t 0.00836\n",
      "\n",
      "42\t 0.008320000000000001\n",
      "\n",
      "43\t 0.00828\n",
      "\n",
      "44\t 0.00824\n",
      "\n",
      "45\t 0.0082\n",
      "\n",
      "46\t 0.00816\n",
      "\n",
      "47\t 0.00812\n",
      "\n",
      "48\t 0.00808\n",
      "\n",
      "49\t 0.00804\n",
      "\n",
      "50\t 0.008\n",
      "\n",
      "51\t 0.00796\n",
      "\n",
      "52\t 0.00792\n",
      "\n",
      "53\t 0.00788\n",
      "\n",
      "54\t 0.00784\n",
      "\n",
      "55\t 0.0078\n",
      "\n",
      "56\t 0.0077599999999999995\n",
      "\n",
      "57\t 0.007719999999999999\n",
      "\n",
      "58\t 0.00768\n",
      "\n",
      "59\t 0.00764\n",
      "\n",
      "60\t 0.0076\n",
      "\n",
      "61\t 0.00756\n",
      "\n",
      "62\t 0.007520000000000001\n",
      "\n",
      "63\t 0.0074800000000000005\n",
      "\n",
      "64\t 0.00744\n",
      "\n",
      "65\t 0.0074\n",
      "\n",
      "66\t 0.00736\n",
      "\n",
      "67\t 0.00732\n",
      "\n",
      "68\t 0.00728\n",
      "\n",
      "69\t 0.00724\n",
      "\n",
      "70\t 0.0072\n",
      "\n",
      "71\t 0.00716\n",
      "\n",
      "72\t 0.00712\n",
      "\n",
      "73\t 0.0070799999999999995\n",
      "\n",
      "74\t 0.007039999999999999\n",
      "\n",
      "75\t 0.007\n",
      "\n",
      "76\t 0.00696\n",
      "\n",
      "77\t 0.00692\n",
      "\n",
      "78\t 0.00688\n",
      "\n",
      "79\t 0.006840000000000001\n",
      "\n",
      "80\t 0.0068000000000000005\n",
      "\n",
      "81\t 0.00676\n",
      "\n",
      "82\t 0.00672\n",
      "\n",
      "83\t 0.00668\n",
      "\n",
      "84\t 0.00664\n",
      "\n",
      "85\t 0.0066\n",
      "\n",
      "86\t 0.00656\n",
      "\n",
      "87\t 0.00652\n",
      "\n",
      "88\t 0.00648\n",
      "\n",
      "89\t 0.0064399999999999995\n",
      "\n",
      "90\t 0.0063999999999999994\n",
      "\n",
      "91\t 0.006359999999999999\n",
      "\n",
      "92\t 0.00632\n",
      "\n",
      "93\t 0.00628\n",
      "\n",
      "94\t 0.00624\n",
      "\n",
      "95\t 0.0062\n",
      "\n",
      "96\t 0.00616\n",
      "\n",
      "97\t 0.0061200000000000004\n",
      "\n",
      "98\t 0.0060799999999999995\n",
      "\n",
      "99\t 0.00604\n",
      "\n",
      "100\t 0.006\n",
      "\n",
      "101\t 0.005\n",
      "\n",
      "102\t 0.004200000000000001\n",
      "\n",
      "103\t 0.0035600000000000007\n",
      "\n",
      "104\t 0.0030480000000000004\n",
      "\n",
      "105\t 0.0026384000000000004\n",
      "\n",
      "106\t 0.0023107200000000005\n",
      "\n",
      "107\t 0.0020485760000000007\n",
      "\n",
      "108\t 0.0018388608000000006\n",
      "\n",
      "109\t 0.0016710886400000003\n",
      "\n",
      "110\t 0.0015368709120000003\n",
      "\n",
      "111\t 0.0014294967296000004\n",
      "\n",
      "112\t 0.0013435973836800003\n",
      "\n",
      "113\t 0.0012748779069440002\n",
      "\n",
      "114\t 0.0012199023255552001\n",
      "\n",
      "115\t 0.0011759218604441603\n",
      "\n",
      "116\t 0.0011407374883553281\n",
      "\n",
      "117\t 0.0011125899906842625\n",
      "\n",
      "118\t 0.0010900719925474101\n",
      "\n",
      "119\t 0.001072057594037928\n",
      "\n",
      "120\t 0.0010576460752303425\n",
      "\n",
      "121\t 0.001046116860184274\n",
      "\n",
      "122\t 0.001036893488147419\n",
      "\n",
      "123\t 0.0010295147905179354\n",
      "\n",
      "124\t 0.0010236118324143482\n",
      "\n",
      "125\t 0.0010188894659314785\n",
      "\n",
      "126\t 0.0010151115727451828\n",
      "\n",
      "127\t 0.0010120892581961464\n",
      "\n",
      "128\t 0.001009671406556917\n",
      "\n",
      "129\t 0.0010077371252455336\n",
      "\n",
      "130\t 0.0010061897001964269\n",
      "\n",
      "131\t 0.0010049517601571415\n",
      "\n",
      "132\t 0.0010039614081257132\n",
      "\n",
      "133\t 0.0010031691265005706\n",
      "\n",
      "134\t 0.0010025353012004564\n",
      "\n",
      "135\t 0.0010020282409603652\n",
      "\n",
      "136\t 0.001001622592768292\n",
      "\n",
      "137\t 0.0010012980742146336\n",
      "\n",
      "138\t 0.001001038459371707\n",
      "\n",
      "139\t 0.0010008307674973657\n",
      "\n",
      "140\t 0.0010006646139978925\n",
      "\n",
      "141\t 0.001000531691198314\n",
      "\n",
      "142\t 0.0010004253529586511\n",
      "\n",
      "143\t 0.001000340282366921\n",
      "\n",
      "144\t 0.0010002722258935367\n",
      "\n",
      "145\t 0.0010002177807148294\n",
      "\n",
      "146\t 0.0010001742245718634\n",
      "\n",
      "147\t 0.001000139379657491\n",
      "\n",
      "148\t 0.0010001115037259927\n",
      "\n",
      "149\t 0.001000089202980794\n",
      "\n",
      "150\t 0.0010000713623846353\n",
      "\n",
      "151\t 0.0010000570899077082\n",
      "\n",
      "152\t 0.0010000456719261666\n",
      "\n",
      "153\t 0.0010000365375409334\n",
      "\n",
      "154\t 0.0010000292300327467\n",
      "\n",
      "155\t 0.0010000233840261974\n",
      "\n",
      "156\t 0.0010000187072209579\n",
      "\n",
      "157\t 0.0010000149657767663\n",
      "\n",
      "158\t 0.001000011972621413\n",
      "\n",
      "159\t 0.0010000095780971303\n",
      "\n",
      "160\t 0.0010000076624777044\n",
      "\n",
      "161\t 0.0010000061299821634\n",
      "\n",
      "162\t 0.0010000049039857308\n",
      "\n",
      "163\t 0.0010000039231885846\n",
      "\n",
      "164\t 0.0010000031385508678\n",
      "\n",
      "165\t 0.0010000025108406942\n",
      "\n",
      "166\t 0.0010000020086725553\n",
      "\n",
      "167\t 0.0010000016069380442\n",
      "\n",
      "168\t 0.0010000012855504354\n",
      "\n",
      "169\t 0.0010000010284403483\n",
      "\n",
      "170\t 0.0010000008227522787\n",
      "\n",
      "171\t 0.001000000658201823\n",
      "\n",
      "172\t 0.0010000005265614583\n",
      "\n",
      "173\t 0.0010000004212491666\n",
      "\n",
      "174\t 0.0010000003369993334\n",
      "\n",
      "175\t 0.0010000002695994667\n",
      "\n",
      "176\t 0.0010000002156795734\n",
      "\n",
      "177\t 0.0010000001725436586\n",
      "\n",
      "178\t 0.001000000138034927\n",
      "\n",
      "179\t 0.0010000001104279416\n",
      "\n",
      "180\t 0.0010000000883423532\n",
      "\n",
      "181\t 0.0010000000706738826\n",
      "\n",
      "182\t 0.001000000056539106\n",
      "\n",
      "183\t 0.0010000000452312849\n",
      "\n",
      "184\t 0.0010000000361850279\n",
      "\n",
      "185\t 0.0010000000289480224\n",
      "\n",
      "186\t 0.001000000023158418\n",
      "\n",
      "187\t 0.0010000000185267342\n",
      "\n",
      "188\t 0.0010000000148213874\n",
      "\n",
      "189\t 0.00100000001185711\n",
      "\n",
      "190\t 0.0010000000094856879\n",
      "\n",
      "191\t 0.0010000000075885505\n",
      "\n",
      "192\t 0.0010000000060708402\n",
      "\n",
      "193\t 0.0010000000048566722\n",
      "\n",
      "194\t 0.0010000000038853378\n",
      "\n",
      "195\t 0.0010000000031082702\n",
      "\n",
      "196\t 0.0010000000024866162\n",
      "\n",
      "197\t 0.0010000000019892929\n",
      "\n",
      "198\t 0.0010000000015914345\n",
      "\n",
      "199\t 0.0010000000012731474\n",
      "\n",
      "200\t 0.001000000001018518\n",
      "\n",
      "201\t 0.0010000000008148144\n",
      "\n",
      "202\t 0.0010000000006518516\n",
      "\n",
      "203\t 0.0010000000005214813\n",
      "\n",
      "204\t 0.001000000000417185\n",
      "\n",
      "205\t 0.001000000000333748\n",
      "\n",
      "206\t 0.0010000000002669985\n",
      "\n",
      "207\t 0.0010000000002135987\n",
      "\n",
      "208\t 0.001000000000170879\n",
      "\n",
      "209\t 0.0010000000001367032\n",
      "\n",
      "210\t 0.0010000000001093626\n",
      "\n",
      "211\t 0.0010000000000874901\n",
      "\n",
      "212\t 0.001000000000069992\n",
      "\n",
      "213\t 0.0010000000000559936\n",
      "\n",
      "214\t 0.001000000000044795\n",
      "\n",
      "215\t 0.001000000000035836\n",
      "\n",
      "216\t 0.0010000000000286687\n",
      "\n",
      "217\t 0.001000000000022935\n",
      "\n",
      "218\t 0.001000000000018348\n",
      "\n",
      "219\t 0.0010000000000146784\n",
      "\n",
      "220\t 0.0010000000000117428\n",
      "\n",
      "221\t 0.0010000000000093942\n",
      "\n",
      "222\t 0.0010000000000075153\n",
      "\n",
      "223\t 0.0010000000000060124\n",
      "\n",
      "224\t 0.0010000000000048098\n",
      "\n",
      "225\t 0.0010000000000038479\n",
      "\n",
      "226\t 0.0010000000000030783\n",
      "\n",
      "227\t 0.0010000000000024627\n",
      "\n",
      "228\t 0.00100000000000197\n",
      "\n",
      "229\t 0.001000000000001576\n",
      "\n",
      "230\t 0.001000000000001261\n",
      "\n",
      "231\t 0.0010000000000010088\n",
      "\n",
      "232\t 0.0010000000000008069\n",
      "\n",
      "233\t 0.0010000000000006456\n",
      "\n",
      "234\t 0.0010000000000005165\n",
      "\n",
      "235\t 0.001000000000000413\n",
      "\n",
      "236\t 0.0010000000000003305\n",
      "\n",
      "237\t 0.0010000000000002643\n",
      "\n",
      "238\t 0.0010000000000002117\n",
      "\n",
      "239\t 0.0010000000000001692\n",
      "\n",
      "240\t 0.0010000000000001353\n",
      "\n",
      "241\t 0.0010000000000001082\n",
      "\n",
      "242\t 0.0010000000000000868\n",
      "\n",
      "243\t 0.0010000000000000694\n",
      "\n",
      "244\t 0.0010000000000000555\n",
      "\n",
      "245\t 0.0010000000000000445\n",
      "\n",
      "246\t 0.0010000000000000356\n",
      "\n",
      "247\t 0.0010000000000000284\n",
      "\n",
      "248\t 0.0010000000000000228\n",
      "\n",
      "249\t 0.0010000000000000182\n",
      "\n",
      "250\t 0.0010000000000000145\n",
      "\n",
      "251\t 0.0010000000000000117\n",
      "\n",
      "252\t 0.0010000000000000093\n",
      "\n",
      "253\t 0.0010000000000000074\n",
      "\n",
      "254\t 0.0010000000000000059\n",
      "\n",
      "255\t 0.0010000000000000048\n",
      "\n",
      "256\t 0.001000000000000004\n",
      "\n",
      "257\t 0.001000000000000003\n",
      "\n",
      "258\t 0.0010000000000000024\n",
      "\n",
      "259\t 0.001000000000000002\n",
      "\n",
      "260\t 0.0010000000000000015\n",
      "\n",
      "261\t 0.0010000000000000013\n",
      "\n",
      "262\t 0.001000000000000001\n",
      "\n",
      "263\t 0.0010000000000000009\n",
      "\n",
      "264\t 0.0010000000000000007\n",
      "\n",
      "265\t 0.0010000000000000005\n",
      "\n",
      "266\t 0.0010000000000000005\n",
      "\n",
      "267\t 0.0010000000000000005\n",
      "\n",
      "268\t 0.0010000000000000002\n",
      "\n",
      "269\t 0.0010000000000000002\n",
      "\n",
      "270\t 0.0010000000000000002\n",
      "\n",
      "271\t 0.0010000000000000002\n",
      "\n",
      "272\t 0.001\n",
      "\n",
      "273\t 0.001\n",
      "\n",
      "274\t 0.001\n",
      "\n",
      "275\t 0.001\n",
      "\n",
      "276\t 0.001\n",
      "\n",
      "277\t 0.001\n",
      "\n",
      "278\t 0.001\n",
      "\n",
      "279\t 0.001\n",
      "\n",
      "280\t 0.001\n",
      "\n",
      "281\t 0.001\n",
      "\n",
      "282\t 0.001\n",
      "\n",
      "283\t 0.001\n",
      "\n",
      "284\t 0.001\n",
      "\n",
      "285\t 0.001\n",
      "\n",
      "286\t 0.001\n",
      "\n",
      "287\t 0.001\n",
      "\n",
      "288\t 0.001\n",
      "\n",
      "289\t 0.001\n",
      "\n",
      "290\t 0.001\n",
      "\n",
      "291\t 0.001\n",
      "\n",
      "292\t 0.001\n",
      "\n",
      "293\t 0.001\n",
      "\n",
      "294\t 0.001\n",
      "\n",
      "295\t 0.001\n",
      "\n",
      "296\t 0.001\n",
      "\n",
      "297\t 0.001\n",
      "\n",
      "298\t 0.001\n",
      "\n",
      "299\t 0.001\n",
      "\n",
      "300\t 0.001\n",
      "\n",
      "301\t 0.001\n",
      "\n",
      "302\t 0.001\n",
      "\n",
      "303\t 0.001\n",
      "\n",
      "304\t 0.001\n",
      "\n",
      "305\t 0.001\n",
      "\n",
      "306\t 0.001\n",
      "\n",
      "307\t 0.001\n",
      "\n",
      "308\t 0.001\n",
      "\n",
      "309\t 0.001\n",
      "\n",
      "310\t 0.001\n",
      "\n",
      "311\t 0.001\n",
      "\n",
      "312\t 0.001\n",
      "\n",
      "313\t 0.001\n",
      "\n",
      "314\t 0.001\n",
      "\n",
      "315\t 0.001\n",
      "\n",
      "316\t 0.001\n",
      "\n",
      "317\t 0.001\n",
      "\n",
      "318\t 0.001\n",
      "\n",
      "319\t 0.001\n",
      "\n",
      "320\t 0.001\n",
      "\n",
      "321\t 0.001\n",
      "\n",
      "322\t 0.001\n",
      "\n",
      "323\t 0.001\n",
      "\n",
      "324\t 0.001\n",
      "\n",
      "325\t 0.001\n",
      "\n",
      "326\t 0.001\n",
      "\n",
      "327\t 0.001\n",
      "\n",
      "328\t 0.001\n",
      "\n",
      "329\t 0.001\n",
      "\n",
      "330\t 0.001\n",
      "\n",
      "331\t 0.001\n",
      "\n",
      "332\t 0.001\n",
      "\n",
      "333\t 0.001\n",
      "\n",
      "334\t 0.001\n",
      "\n",
      "335\t 0.001\n",
      "\n",
      "336\t 0.001\n",
      "\n",
      "337\t 0.001\n",
      "\n",
      "338\t 0.001\n",
      "\n",
      "339\t 0.001\n",
      "\n",
      "340\t 0.001\n",
      "\n",
      "341\t 0.001\n",
      "\n",
      "342\t 0.001\n",
      "\n",
      "343\t 0.001\n",
      "\n",
      "344\t 0.001\n",
      "\n",
      "345\t 0.001\n",
      "\n",
      "346\t 0.001\n",
      "\n",
      "347\t 0.001\n",
      "\n",
      "348\t 0.001\n",
      "\n",
      "349\t 0.001\n",
      "\n",
      "350\t 0.001\n",
      "\n",
      "351\t 0.001\n",
      "\n",
      "352\t 0.001\n",
      "\n",
      "353\t 0.001\n",
      "\n",
      "354\t 0.001\n",
      "\n",
      "355\t 0.001\n",
      "\n",
      "356\t 0.001\n",
      "\n",
      "357\t 0.001\n",
      "\n",
      "358\t 0.001\n",
      "\n",
      "359\t 0.001\n",
      "\n",
      "360\t 0.001\n",
      "\n",
      "361\t 0.001\n",
      "\n",
      "362\t 0.001\n",
      "\n",
      "363\t 0.001\n",
      "\n",
      "364\t 0.001\n",
      "\n",
      "365\t 0.001\n",
      "\n",
      "366\t 0.001\n",
      "\n",
      "367\t 0.001\n",
      "\n",
      "368\t 0.001\n",
      "\n",
      "369\t 0.001\n",
      "\n",
      "370\t 0.001\n",
      "\n",
      "371\t 0.001\n",
      "\n",
      "372\t 0.001\n",
      "\n",
      "373\t 0.001\n",
      "\n",
      "374\t 0.001\n",
      "\n",
      "375\t 0.001\n",
      "\n",
      "376\t 0.001\n",
      "\n",
      "377\t 0.001\n",
      "\n",
      "378\t 0.001\n",
      "\n",
      "379\t 0.001\n",
      "\n",
      "380\t 0.001\n",
      "\n",
      "381\t 0.001\n",
      "\n",
      "382\t 0.001\n",
      "\n",
      "383\t 0.001\n",
      "\n",
      "384\t 0.001\n",
      "\n",
      "385\t 0.001\n",
      "\n",
      "386\t 0.001\n",
      "\n",
      "387\t 0.001\n",
      "\n",
      "388\t 0.001\n",
      "\n",
      "389\t 0.001\n",
      "\n",
      "390\t 0.001\n",
      "\n",
      "391\t 0.001\n",
      "\n",
      "392\t 0.001\n",
      "\n",
      "393\t 0.001\n",
      "\n",
      "394\t 0.001\n",
      "\n",
      "395\t 0.001\n",
      "\n",
      "396\t 0.001\n",
      "\n",
      "397\t 0.001\n",
      "\n",
      "398\t 0.001\n",
      "\n",
      "399\t 0.001\n",
      "\n",
      "400\t 0.001\n",
      "\n",
      "401\t 0.001\n",
      "\n",
      "402\t 0.001\n",
      "\n",
      "403\t 0.001\n",
      "\n",
      "404\t 0.001\n",
      "\n",
      "405\t 0.001\n",
      "\n",
      "406\t 0.001\n",
      "\n",
      "407\t 0.001\n",
      "\n",
      "408\t 0.001\n",
      "\n",
      "409\t 0.001\n",
      "\n",
      "410\t 0.001\n",
      "\n",
      "411\t 0.001\n",
      "\n",
      "412\t 0.001\n",
      "\n",
      "413\t 0.001\n",
      "\n",
      "414\t 0.001\n",
      "\n",
      "415\t 0.001\n",
      "\n",
      "416\t 0.001\n",
      "\n",
      "417\t 0.001\n",
      "\n",
      "418\t 0.001\n",
      "\n",
      "419\t 0.001\n",
      "\n",
      "420\t 0.001\n",
      "\n",
      "421\t 0.001\n",
      "\n",
      "422\t 0.001\n",
      "\n",
      "423\t 0.001\n",
      "\n",
      "424\t 0.001\n",
      "\n",
      "425\t 0.001\n",
      "\n",
      "426\t 0.001\n",
      "\n",
      "427\t 0.001\n",
      "\n",
      "428\t 0.001\n",
      "\n",
      "429\t 0.001\n",
      "\n",
      "430\t 0.001\n",
      "\n",
      "431\t 0.001\n",
      "\n",
      "432\t 0.001\n",
      "\n",
      "433\t 0.001\n",
      "\n",
      "434\t 0.001\n",
      "\n",
      "435\t 0.001\n",
      "\n",
      "436\t 0.001\n",
      "\n",
      "437\t 0.001\n",
      "\n",
      "438\t 0.001\n",
      "\n",
      "439\t 0.001\n",
      "\n",
      "440\t 0.001\n",
      "\n",
      "441\t 0.001\n",
      "\n",
      "442\t 0.001\n",
      "\n",
      "443\t 0.001\n",
      "\n",
      "444\t 0.001\n",
      "\n",
      "445\t 0.001\n",
      "\n",
      "446\t 0.001\n",
      "\n",
      "447\t 0.001\n",
      "\n",
      "448\t 0.001\n",
      "\n",
      "449\t 0.001\n",
      "\n",
      "450\t 0.001\n",
      "\n",
      "451\t 0.001\n",
      "\n",
      "452\t 0.001\n",
      "\n",
      "453\t 0.001\n",
      "\n",
      "454\t 0.001\n",
      "\n",
      "455\t 0.001\n",
      "\n",
      "456\t 0.001\n",
      "\n",
      "457\t 0.001\n",
      "\n",
      "458\t 0.001\n",
      "\n",
      "459\t 0.001\n",
      "\n",
      "460\t 0.001\n",
      "\n",
      "461\t 0.001\n",
      "\n",
      "462\t 0.001\n",
      "\n",
      "463\t 0.001\n",
      "\n",
      "464\t 0.001\n",
      "\n",
      "465\t 0.001\n",
      "\n",
      "466\t 0.001\n",
      "\n",
      "467\t 0.001\n",
      "\n",
      "468\t 0.001\n",
      "\n",
      "469\t 0.001\n",
      "\n",
      "470\t 0.001\n",
      "\n",
      "471\t 0.001\n",
      "\n",
      "472\t 0.001\n",
      "\n",
      "473\t 0.001\n",
      "\n",
      "474\t 0.001\n",
      "\n",
      "475\t 0.001\n",
      "\n",
      "476\t 0.001\n",
      "\n",
      "477\t 0.001\n",
      "\n",
      "478\t 0.001\n",
      "\n",
      "479\t 0.001\n",
      "\n",
      "480\t 0.001\n",
      "\n",
      "481\t 0.001\n",
      "\n",
      "482\t 0.001\n",
      "\n",
      "483\t 0.001\n",
      "\n",
      "484\t 0.001\n",
      "\n",
      "485\t 0.001\n",
      "\n",
      "486\t 0.001\n",
      "\n",
      "487\t 0.001\n",
      "\n",
      "488\t 0.001\n",
      "\n",
      "489\t 0.001\n",
      "\n",
      "490\t 0.001\n",
      "\n",
      "491\t 0.001\n",
      "\n",
      "492\t 0.001\n",
      "\n",
      "493\t 0.001\n",
      "\n",
      "494\t 0.001\n",
      "\n",
      "495\t 0.001\n",
      "\n",
      "496\t 0.001\n",
      "\n",
      "497\t 0.001\n",
      "\n",
      "498\t 0.001\n",
      "\n",
      "499\t 0.001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for e, lr in zip(rng,y):\n",
    "    print('{}\\t {}\\n'.format(e, lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f73107cfa20>]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# clr3\n",
    "# warm up 10% of epoch: it can reduce fall in local min in inital steps.\n",
    "\n",
    "\n",
    "ep_num = 1000\n",
    "\n",
    "\n",
    "\n",
    "def clr3(epoch):\n",
    "    \n",
    "    \n",
    "    step_size = 25 # currently best for foot pp\n",
    "    max_lr = 0.01 # currently best for foot pp\n",
    "    base_lr = 1e-6 # 1e-6 1e-7\n",
    "\n",
    "    # warm up\n",
    "    lr_init_ep = 0\n",
    "    lr_ramp_ep = 100\n",
    "    lr_sus_ep  = 0\n",
    "    lr_decay   = 0.5\n",
    "\n",
    "    iterations = epoch\n",
    "    cycle = np.floor(1+iterations/(2*step_size))\n",
    "    x = np.abs(iterations/step_size - 2*cycle + 1)\n",
    "    lr = base_lr + (max_lr-base_lr)*np.maximum(0, (1-x))\n",
    "    \n",
    "    #todo: boost the lr at initial setps.\n",
    "#     initial_lr = lambda epoch: lr if epoch > step_size else max_lr\n",
    "#     initial_lr = lambda epoch: lr if epoch > step_size else boost_lr\n",
    "#     lr = initial_lr(epoch)\n",
    "    #todo: boost the lr at fist step_size.\n",
    "    \n",
    "    # warm up\n",
    "    if epoch < lr_ramp_ep:\n",
    "        lr = (max_lr - base_lr) / lr_ramp_ep * epoch + base_lr\n",
    "    \n",
    "    decay = ((epoch+1)/ep_num)\n",
    "    base_part = 1.001 #1.1\n",
    "#     print(decay)\n",
    "    return lr * (base_part-decay) * lr_decay # supressed the lr!\n",
    "\n",
    "\n",
    "rng = [i for i in range(ep_num)]\n",
    "y = [clr3(x) for x in rng]\n",
    "sns.set(style='darkgrid')\n",
    "fig, ax = plt.subplots(figsize=(20, 6))\n",
    "plt.plot(rng, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4999999999999966e-08 ~ 0.004459954505\n"
     ]
    }
   ],
   "source": [
    "print('{} ~ {}'.format(min(y), max(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4999999999999966e-08 ~ 0.004459954505 1e-2~1e-6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# \"\"\"\n",
    "# cosine_decay_restarts是cosine_decay的cycle版本。\n",
    "# first_decay_steps是指第一次完全下降的step數，\n",
    "# t_mul是指每一次循環的步數都將乘以t_mul倍，\n",
    "# m_mul指每一次循環重新開始時的初始lr是上一次循環初始值的m_mul倍。\n",
    "# alpha\n",
    "# \"\"\"\n",
    "\n",
    "# from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "\n",
    "\n",
    "# ep_num = 1000\n",
    "\n",
    "\n",
    "\n",
    "# def CosineDecayCLRWarmUp(epoch):\n",
    "    \n",
    "#     #step_size = 25 # currently best for foot pp\n",
    "#     max_lr = 1e-2 # currently best for foot pp\n",
    "#     base_lr = 1e-8# 1e-6 1e-7\n",
    "\n",
    "#     # warm up\n",
    "#     lr_init_ep = 0\n",
    "#     lr_ramp_ep = 100\n",
    "#     lr_sus_ep  = 0\n",
    "#     lr_decay   = 0.8\n",
    "\n",
    "\n",
    "#     initial_learning_rate = 1e-2\n",
    "#     first_decay_steps = 100\n",
    "\n",
    "\n",
    "#     lr_decayed_fn = (\n",
    "#       tf.keras.experimental.CosineDecayRestarts(\n",
    "#           initial_learning_rate,\n",
    "#           first_decay_steps,\n",
    "#           t_mul=1.0,\n",
    "#           m_mul=0.8,\n",
    "#           alpha = 0.000001,\n",
    "#           name=\"CCosineDecayRestarts\"))\n",
    "    \n",
    "#     # warm up\n",
    "#     if epoch < lr_ramp_ep:\n",
    "#         lr = (max_lr - base_lr) / lr_ramp_ep * epoch + base_lr    \n",
    "#     else:\n",
    "#         lr = lr_decayed_fn(epoch)\n",
    "#     return lr\n",
    "\n",
    "\n",
    "\n",
    "# rng = [i for i in range(ep_num)]\n",
    "# y = [CosineDecayCLRWarmUp(x) for x in rng]\n",
    "# sns.set(style='darkgrid')\n",
    "# fig, ax = plt.subplots(figsize=(20, 6))\n",
    "# # plt.ylim(.0000000000000001, .01)# for too large loss\n",
    "# ax.yaxis.set_major_formatter(FormatStrFormatter('%.12f'))# for too small loss\n",
    "# plt.plot(rng, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# \"\"\"\n",
    "# cosine_decay_restarts是cosine_decay的cycle版本。\n",
    "# first_decay_steps是指第一次完全下降的step數，\n",
    "# t_mul是指每一次循環的步數都將乘以t_mul倍，\n",
    "# m_mul指每一次循環重新開始時的初始lr是上一次循環初始值的m_mul倍。\n",
    "# alpha\n",
    "# \"\"\"\n",
    "\n",
    "# from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "\n",
    "\n",
    "# ep_num = 1000\n",
    "\n",
    "\n",
    "\n",
    "# def CosineDecayCLRWarmUpLSW(epoch):\n",
    "    \n",
    "#     #step_size = 25 # currently best for foot pp\n",
    "#     max_lr = 1e-3 # currently best for foot pp\n",
    "#     base_lr = 1e-6# 1e-6 1e-7\n",
    "\n",
    "#     # warm up\n",
    "#     lr_init_ep = 0\n",
    "#     lr_ramp_ep = 20\n",
    "#     lr_sus_ep  = 0\n",
    "#     lr_decay   = 0.8\n",
    "\n",
    "\n",
    "#     initial_learning_rate = 1e-3\n",
    "#     first_decay_steps = 50\n",
    "\n",
    "\n",
    "#     lr_decayed_fn = (\n",
    "#       tf.keras.experimental.CosineDecayRestarts(\n",
    "#           initial_learning_rate,\n",
    "#           first_decay_steps,\n",
    "#           t_mul=1.0,\n",
    "#           m_mul=0.8,\n",
    "#           alpha = 0.000001,\n",
    "#           name=\"CCosineDecayRestarts\"))\n",
    "    \n",
    "#     # warm up\n",
    "#     if epoch < lr_ramp_ep:\n",
    "#         lr = (max_lr - base_lr) / lr_ramp_ep * epoch + base_lr    \n",
    "#     else:\n",
    "#         lr = lr_decayed_fn(epoch-lr_ramp_ep)\n",
    "#     return lr\n",
    "\n",
    "\n",
    "\n",
    "# rng = [i for i in range(ep_num)]\n",
    "# y = [CosineDecayCLRWarmUpLSW(x) for x in rng]\n",
    "# sns.set(style='darkgrid')\n",
    "# fig, ax = plt.subplots(figsize=(20, 6))\n",
    "# # plt.ylim(.0000000000000001, .01)# for too large loss\n",
    "# ax.yaxis.set_major_formatter(FormatStrFormatter('%.12f'))# for too small loss\n",
    "# plt.plot(rng, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 590 ms, sys: 71.5 ms, total: 662 ms\n",
      "Wall time: 554 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f7310253780>]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "cosine_decay_restarts是cosine_decay的cycle版本。\n",
    "first_decay_steps是指第一次完全下降的step數，\n",
    "t_mul是指每一次循環的步數都將乘以t_mul倍，\n",
    "m_mul指每一次循環重新開始時的初始lr是上一次循環初始值的m_mul倍。\n",
    "alpha\n",
    "\"\"\"\n",
    "\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "\n",
    "\n",
    "ep_num = 1000\n",
    "\n",
    "\n",
    "\n",
    "def CosineDecayCLRWarmUpLSW_2(epoch):\n",
    "    \n",
    "    #step_size = 25 # currently best for foot pp\n",
    "    max_lr = 1e-2 # currently best for foot pp\n",
    "    base_lr = 1e-6# 1e-6 1e-7\n",
    "\n",
    "    # warm up\n",
    "    lr_init_ep = 0\n",
    "    lr_ramp_ep = 20\n",
    "    lr_sus_ep  = 0\n",
    "    #lr_decay   = 0.8\n",
    "\n",
    "\n",
    "    initial_learning_rate = 1e-2\n",
    "    first_decay_steps = 50\n",
    "\n",
    "\n",
    "    lr_decayed_fn = (\n",
    "      tf.keras.experimental.CosineDecayRestarts(\n",
    "          initial_learning_rate,\n",
    "          first_decay_steps,\n",
    "          t_mul=1,\n",
    "          m_mul=1,\n",
    "          alpha = 0.000001,\n",
    "          name=\"CCosineDecayRestarts\"))\n",
    "    \n",
    "    # warm up\n",
    "    if epoch < lr_ramp_ep:\n",
    "        lr = (max_lr - base_lr) / lr_ramp_ep * epoch + base_lr    \n",
    "    else:\n",
    "        lr = lr_decayed_fn(epoch-lr_ramp_ep)\n",
    "    return lr\n",
    "\n",
    "\n",
    "\n",
    "rng = [i for i in range(ep_num)]\n",
    "y = [CosineDecayCLRWarmUpLSW_2(x) for x in rng]\n",
    "sns.set(style='darkgrid')\n",
    "fig, ax = plt.subplots(figsize=(20, 6))\n",
    "# plt.ylim(.0000000000000001, .01)# for too large loss\n",
    "ax.yaxis.set_major_formatter(FormatStrFormatter('%.12f'))# for too small loss\n",
    "plt.plot(rng, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-06 ~ 0.009999999776482582\n"
     ]
    }
   ],
   "source": [
    "print('{} ~ {}'.format(min(y), max(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.8774175103430935e-08 ~ 0.0010000000474974513 1e-3 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_reduceonplateau = tf.keras.callbacks.ReduceLROnPlateau(monitor=monitor, factor=0.1, patience=5, verbose=1, mode='auto', min_delta=0.0001, cooldown=0, min_lr=1e-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback for printing the LR at the end of each epoch.\n",
    "class PrintLRtoe(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "#     def on_epoch_end(self, epoch, logs=None):\n",
    "#         print('\\n[{}] Learning rate for epoch {} is {}'.format(\n",
    "#             datetime.now().strftime(\"%Y%m%d-%H%M-%S\"), \n",
    "#             epoch + 1,\n",
    "#             self.model.optimizer.lr.numpy()))\n",
    "        print('\\n[{}] Learning rate for epoch {} is {}'.format(\n",
    "        datetime.now().strftime(\"%Y%m%d-%H%M-%S\"), \n",
    "        epoch + 1,\n",
    "        model_toe.optimizer._decayed_lr(tf.float32).numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback for printing the LR at the end of each epoch.\n",
    "class PrintLRheel(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "#         print('\\nLearning rate for epoch {} is {}'.format(epoch + 1,\n",
    "#                                               model_heel.optimizer.lr.numpy()))\n",
    "        print('\\n[{}] Learning rate for epoch {} is {}'.format(\n",
    "        datetime.now().strftime(\"%Y%m%d-%H%M-%S\"), \n",
    "        epoch + 1,\n",
    "        model_heel.optimizer._decayed_lr(tf.float32).numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output dir and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_log_dir(log_dir_name):\n",
    "    try:\n",
    "        os.makedirs(log_dir_name)\n",
    "    except OSError as e:\n",
    "        print(\"This log dir exist.\")\n",
    "        if e.errno != errno.EEXIST:\n",
    "            raise ValueError(\"we got problem.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor = 'val_loss' #'val_loss' 'val_accuracy' if use ed_loss it still the loss here.\n",
    "\n",
    "log_dir_name = datetime.now().strftime(\"%Y%m%d-%H%M%S\") \n",
    "\n",
    "# mk_log_dir(datetime.now().strftime(\"%Y%m%d-%H%M%S\") )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use once at the time\n",
    "mk_log_dir(log_dir_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'EfficientNetB0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_best_model_name\n",
    "\n",
    "# best_model_name = './' + model_name + '_bs-' + str(BATCH_SIZE) + '_s-' + str(img_height) + '_' + \"ep-{epoch:02d}-vloss-{val_loss:.2f}\" +'_best-weight.h5'\n",
    "# best_model_name = '{model_name}-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
    "#best_model_name = './' + model_name + '_bs' + str(BATCH_SIZE) + '_w' + str(img_width) + '_' + monitor + '_best.h5'\n",
    "# best_model_name = './Leaf_' + model_name + '_bs' + str(BATCH_SIZE) + '_w' + str(img_width) + '_' + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + '_best_' + monitor + '.h5'\n",
    "\n",
    "# best_model_name = './cop' + model_name + '_bs' + str(BATCH_SIZE) + '_w' + str(img_width) + '_' + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + '_best_' + monitor + '.h5'\n",
    "\n",
    "def get_best_model_name(th, K):\n",
    "    return './' + log_dir_name + '/' + th + '_K' + K + '_' + model_name + '_bs' + str(BATCH_SIZE) + '_w' + str(img_width) + '_best_' + monitor + '.h5'\n",
    "\n",
    "# th = 'toe'\n",
    "# # th = 'heel'\n",
    "\n",
    "# # print(get_best_model_name(th,K))\n",
    "\n",
    "# best_model_name = get_best_model_name(th, K)\n",
    "\n",
    "\n",
    "# best_model_save = tf.keras.callbacks.ModelCheckpoint(filepath=best_model_name, \n",
    "#                              save_best_only = True, \n",
    "#                              save_weights_only = False,\n",
    "#                              monitor = monitor, \n",
    "#                              mode = 'auto', verbose = 1)\n",
    "# print('best_model_name:', best_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = log_dir_name + \"/logs/toe/\"\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks = [\n",
    "# #     tensorboard_callback,\n",
    "#     best_model_save,\n",
    "#     tf.keras.callbacks.EarlyStopping(monitor=monitor, patience=20), #patience=step_size or ep_num\n",
    "# #     lr_reduceonplateau,\n",
    "#     tf.keras.callbacks.LearningRateScheduler(lrdump),#lrdump, decay or lrfn or lrfn2. clr\n",
    "#     PrintLRtoe()\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transfer learning from pre-trained weights\n",
    "def build_efn_model(outputnum, top_dropout_rate, drop_connect_rate):\n",
    "    base_model = tf.keras.applications.EfficientNetB0(include_top=False, weights=\"imagenet\", input_shape=(120,120,3),drop_connect_rate=drop_connect_rate) #{'imagenet', None}\n",
    "\n",
    "    # Freeze the pretrained weights\n",
    "    base_model.trainable = False\n",
    "    print(\"base_model.trainable : \", base_model.trainable)\n",
    "\n",
    "    # Rebuild top\n",
    "    gap2d = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "    BNL = tf.keras.layers.BatchNormalization()(gap2d) #tood: remove#\n",
    "    dropout = tf.keras.layers.Dropout(top_dropout_rate)(BNL)#tood: remove# J add dropout, for flood 0.2 is ok. for leaf 0.4 is better.\n",
    "    outputs = tf.keras.layers.Dense(outputnum)(dropout)# remove activation for regression output (to default, the linear), , activation = 'relu' no help\n",
    "\n",
    "    # Compile new model\n",
    "    model = tf.keras.Model(base_model.input, outputs, name=model_name)\n",
    "\n",
    "\n",
    "#     # unfreeze the top #fine_tune_at# layers while leaving BatchNorm layers frozen\n",
    "#     fine_tune_at = 20 #10 #241 #20\n",
    "#     print('[Note] Now create model fine tuneing at Top-{} layers!'.format(fine_tune_at))\n",
    "#     for layer in model_toe.layers[-fine_tune_at:]:\n",
    "#         if not isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "#             layer.trainable = True\n",
    "\n",
    "    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),#RMSprop , Adam, SGD Adadelta(learning_rate=0.001), if set lr_callback the learning_rate=0.001 will not effeced.\n",
    "                    loss=ed_metric_2d_mean)\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Supervised pre-training 減少每次fold都要重新train的時間\n",
    "# 只先改toe\"\"\"\n",
    "\n",
    "# # Transfer learning from pre-trained weights\n",
    "# def load_pretrained_efn_model():\n",
    "#     pre_model_toe_name = \"20210224-200728/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\"\n",
    "#     model = tf.keras.models.load_model(pre_model_toe_name,compile=False)\n",
    "\n",
    "#     # Freeze the pretrained weights\n",
    "#     model.trainable = False\n",
    "# #     print(\"base_model.trainable : \", base_model.trainable)\n",
    "\n",
    "# #     # Rebuild top\n",
    "# #     gap2d = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "# #     BNL = tf.keras.layers.BatchNormalization()(gap2d) #tood: remove#\n",
    "# #     dropout = tf.keras.layers.Dropout(top_dropout_rate)(BNL)#tood: remove# J add dropout, for flood 0.2 is ok. for leaf 0.4 is better.\n",
    "# #     outputs = tf.keras.layers.Dense(outputnum)(dropout)# remove activation for regression output (to default, the linear), , activation = 'relu' no help\n",
    "\n",
    "# #     # Compile new model\n",
    "# #     model = tf.keras.Model(base_model.input, outputs, name=model_name)\n",
    "\n",
    "\n",
    "# #     # unfreeze the top #fine_tune_at# layers while leaving BatchNorm layers frozen\n",
    "#     fine_tune_at = 4 #10 #241 #20\n",
    "#     print('[Note] Now create model fine tuneing at Top-{} layers!'.format(fine_tune_at))\n",
    "#     for layer in model.layers[-fine_tune_at:]:\n",
    "#         if not isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "#             print('layer trainable +1', layer.name)\n",
    "#             layer.trainable = True\n",
    "\n",
    "#     model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),#RMSprop , Adam, SGD Adadelta(learning_rate=0.001), if set lr_callback the learning_rate=0.001 will not effeced.\n",
    "#                     loss=ed_metric_2d_mean)\n",
    "\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unfreeze_model(model):\n",
    "#\n",
    "#'block7a_expand_conv'20 'block6c_expand_conv'50 'block6a_expand_conv'79 'block5b_expand_conv'109 'block4a_expand_conv' 166  block3a_expand_conv 195\n",
    "#\n",
    "    model.trainable = True\n",
    "    set_trainable = False\n",
    "    for layer in model.layers:\n",
    "        if layer.name == 'block3a_expand_conv': \n",
    "            set_trainable = True\n",
    "        if set_trainable:\n",
    "            layer.trainable = True\n",
    "        else:\n",
    "            layer.trainable = False\n",
    "\n",
    "    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),#RMSprop , Adam, SGD Adadelta(learning_rate=0.001), if set lr_callback the learning_rate=0.001 will not effeced.\n",
    "                    loss=ed_metric_2d_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# top_dropout_rate = 0.8 #less dp rate, say 0.1, train_loss will lower than val_loss\n",
    "# drop_connect_rate = 0.9 #for efnet This parameter serves as a toggle for extra regularization in finetuning, but does not affect loaded weights.\n",
    "# outputnum = 2\n",
    "# with strategy.scope():\n",
    "#     model_toe = build_efn_model(outputnum, top_dropout_rate, drop_connect_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(model_toe.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tt = 0\n",
    "# nt = 0\n",
    "# for layer in model_toe.layers:\n",
    "#     if layer.trainable:\n",
    "#         tt +=1\n",
    "#         print(f'{layer.name}')\n",
    "#     else:\n",
    "#         nt +=1\n",
    "# print(f'tt: {tt}, nt:{nt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_model_trainOrNot_layers(model, printlayers=False):\n",
    "    tt = 0\n",
    "    nt = 0\n",
    "    for layer in model.layers:\n",
    "        if layer.trainable:\n",
    "            tt +=1\n",
    "            if printlayers:\n",
    "                print(f'{layer.name}')\n",
    "        else:\n",
    "            nt +=1\n",
    "    print('\\n*********************************** Start fine tune ***********************************')\n",
    "    print(f'tt: {tt}, nt:{nt}, total layers:{tt+nt}')\n",
    "    print('*********************************** Start fine tune ***********************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_model_trainOrNot_layers(model_toe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model_toe.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # fit the model on all data\n",
    "# history_toe = model_toe.fit(train_ds_pre_toe_s, \n",
    "#                       verbose=1, \n",
    "#                       epochs=ep_num_transf, \n",
    "#                       validation_data=valid_ds_pre_toe_s, \n",
    "#                       callbacks=callbacks)#, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold Training\n",
    "\n",
    "2021-02-23 v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toe K-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " K =  0 \n",
      "\n",
      "Check cache in memory:Y True\n",
      "Check keypoints process01: Tensor(\"args_1:0\", shape=(2,), dtype=int64) (2,) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Check keypoints process02: Tensor(\"Reshape:0\", shape=(1, 2), dtype=int64) (1, 2) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Check keypoints process03: Tensor(\"PyFunc:1\", dtype=float32, device=/job:localhost/replica:0/task:0)\n",
      "Check keypoints process04: Tensor(\"Reshape_1:0\", shape=(2,), dtype=float32)\n",
      "Check augment :Y True\n",
      "Check ds cache[True] and augment[True]\n",
      "Check cache in memory:Y True\n",
      "Check augment :N False\n",
      "Check ds cache[True] and augment[False]\n",
      "best_model_name: ./20210301-225844/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "base_model.trainable :  False\n",
      "\n",
      "[20210301-2258-47] Learning rate for epoch 1 is 0.009999999776482582\n",
      "Epoch 1/500\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Iterator.get_next_as_optional()` instead.\n",
      "20/20 [==============================] - ETA: 0s - loss: 73.7960\n",
      "Epoch 00001: val_loss improved from inf to 53.82478, saving model to ./20210301-225844/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 10s 500ms/step - loss: 73.7960 - val_loss: 53.8248\n",
      "\n",
      "[20210301-2259-12] Learning rate for epoch 2 is 0.009960000403225422\n",
      "Epoch 2/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 33.1979\n",
      "Epoch 00002: val_loss improved from 53.82478 to 22.51966, saving model to ./20210301-225844/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 33.1979 - val_loss: 22.5197\n",
      "\n",
      "[20210301-2259-16] Learning rate for epoch 3 is 0.009920000098645687\n",
      "Epoch 3/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 21.0461\n",
      "Epoch 00003: val_loss improved from 22.51966 to 13.56431, saving model to ./20210301-225844/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 12s 603ms/step - loss: 21.0461 - val_loss: 13.5643\n",
      "\n",
      "[20210301-2259-31] Learning rate for epoch 4 is 0.009879999794065952\n",
      "Epoch 4/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 18.7073\n",
      "Epoch 00004: val_loss improved from 13.56431 to 11.77637, saving model to ./20210301-225844/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 18.7073 - val_loss: 11.7764\n",
      "\n",
      "[20210301-2259-35] Learning rate for epoch 5 is 0.009840000420808792\n",
      "Epoch 5/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 18.0789\n",
      "Epoch 00005: val_loss improved from 11.77637 to 11.61687, saving model to ./20210301-225844/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 18.1735 - val_loss: 11.6169\n",
      "\n",
      "[20210301-2259-39] Learning rate for epoch 6 is 0.009800000116229057\n",
      "Epoch 6/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.9508\n",
      "Epoch 00006: val_loss improved from 11.61687 to 11.14332, saving model to ./20210301-225844/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 17.0324 - val_loss: 11.1433\n",
      "\n",
      "[20210301-2259-43] Learning rate for epoch 7 is 0.009759999811649323\n",
      "Epoch 7/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 17.0524\n",
      "Epoch 00007: val_loss improved from 11.14332 to 10.97493, saving model to ./20210301-225844/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 17.0524 - val_loss: 10.9749\n",
      "\n",
      "[20210301-2259-48] Learning rate for epoch 8 is 0.009720000438392162\n",
      "Epoch 8/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.6177\n",
      "Epoch 00008: val_loss did not improve from 10.97493\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.6177 - val_loss: 11.0209\n",
      "\n",
      "[20210301-2259-51] Learning rate for epoch 9 is 0.009680000133812428\n",
      "Epoch 9/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.5475\n",
      "Epoch 00009: val_loss did not improve from 10.97493\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.5780 - val_loss: 11.0478\n",
      "\n",
      "[20210301-2259-55] Learning rate for epoch 10 is 0.009639999829232693\n",
      "Epoch 10/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.8966\n",
      "Epoch 00010: val_loss improved from 10.97493 to 10.86716, saving model to ./20210301-225844/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 15.7773 - val_loss: 10.8672\n",
      "\n",
      "[20210301-2259-59] Learning rate for epoch 11 is 0.009600000455975533\n",
      "Epoch 11/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.6141\n",
      "Epoch 00011: val_loss improved from 10.86716 to 10.79177, saving model to ./20210301-225844/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 15.5762 - val_loss: 10.7918\n",
      "\n",
      "[20210301-2300-03] Learning rate for epoch 12 is 0.009560000151395798\n",
      "Epoch 12/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.3536\n",
      "Epoch 00012: val_loss did not improve from 10.79177\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.3426 - val_loss: 10.9329\n",
      "\n",
      "[20210301-2300-07] Learning rate for epoch 13 is 0.009519999846816063\n",
      "Epoch 13/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.1399\n",
      "Epoch 00013: val_loss did not improve from 10.79177\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.2566 - val_loss: 11.2147\n",
      "\n",
      "[20210301-2300-10] Learning rate for epoch 14 is 0.009479999542236328\n",
      "Epoch 14/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.3350\n",
      "Epoch 00014: val_loss did not improve from 10.79177\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.3609 - val_loss: 11.2205\n",
      "\n",
      "[20210301-2300-14] Learning rate for epoch 15 is 0.009440000168979168\n",
      "Epoch 15/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 14.8677\n",
      "Epoch 00015: val_loss did not improve from 10.79177\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 14.8952 - val_loss: 11.1592\n",
      "\n",
      "[20210301-2300-18] Learning rate for epoch 16 is 0.009399999864399433\n",
      "Epoch 16/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 14.8618\n",
      "Epoch 00016: val_loss did not improve from 10.79177\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 14.8985 - val_loss: 10.9093\n",
      "\n",
      "[20210301-2300-22] Learning rate for epoch 17 is 0.009359999559819698\n",
      "Epoch 17/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.0320\n",
      "Epoch 00017: val_loss did not improve from 10.79177\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.0320 - val_loss: 10.8700\n",
      "\n",
      "[20210301-2300-25] Learning rate for epoch 18 is 0.009320000186562538\n",
      "Epoch 18/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 15.2765\n",
      "Epoch 00018: val_loss improved from 10.79177 to 10.67581, saving model to ./20210301-225844/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 15.2485 - val_loss: 10.6758\n",
      "\n",
      "[20210301-2300-29] Learning rate for epoch 19 is 0.009279999881982803\n",
      "Epoch 19/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 14.5634\n",
      "Epoch 00019: val_loss did not improve from 10.67581\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 14.6618 - val_loss: 10.8430\n",
      "\n",
      "[20210301-2300-33] Learning rate for epoch 20 is 0.009239999577403069\n",
      "Epoch 20/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.6729\n",
      "Epoch 00020: val_loss did not improve from 10.67581\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 14.6729 - val_loss: 10.6952\n",
      "\n",
      "[20210301-2300-37] Learning rate for epoch 21 is 0.009200000204145908\n",
      "Epoch 21/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 14.6196\n",
      "Epoch 00021: val_loss did not improve from 10.67581\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 14.6038 - val_loss: 10.7754\n",
      "\n",
      "[20210301-2300-41] Learning rate for epoch 22 is 0.009159999899566174\n",
      "Epoch 22/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.5883\n",
      "Epoch 00022: val_loss improved from 10.67581 to 10.52807, saving model to ./20210301-225844/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 14.5883 - val_loss: 10.5281\n",
      "\n",
      "[20210301-2300-45] Learning rate for epoch 23 is 0.009119999594986439\n",
      "Epoch 23/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.4787\n",
      "Epoch 00023: val_loss improved from 10.52807 to 10.51972, saving model to ./20210301-225844/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 14.4787 - val_loss: 10.5197\n",
      "\n",
      "[20210301-2300-49] Learning rate for epoch 24 is 0.009080000221729279\n",
      "Epoch 24/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 14.1439\n",
      "Epoch 00024: val_loss did not improve from 10.51972\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 14.1965 - val_loss: 10.5440\n",
      "\n",
      "[20210301-2300-53] Learning rate for epoch 25 is 0.009039999917149544\n",
      "Epoch 25/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 14.1610\n",
      "Epoch 00025: val_loss improved from 10.51972 to 10.48089, saving model to ./20210301-225844/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 14.2034 - val_loss: 10.4809\n",
      "\n",
      "[20210301-2300-57] Learning rate for epoch 26 is 0.008999999612569809\n",
      "Epoch 26/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 14.4607\n",
      "Epoch 00026: val_loss did not improve from 10.48089\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 14.3860 - val_loss: 10.5857\n",
      "\n",
      "[20210301-2301-00] Learning rate for epoch 27 is 0.008960000239312649\n",
      "Epoch 27/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 14.4586\n",
      "Epoch 00027: val_loss did not improve from 10.48089\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 14.4808 - val_loss: 10.6767\n",
      "\n",
      "[20210301-2301-04] Learning rate for epoch 28 is 0.008919999934732914\n",
      "Epoch 28/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 14.2990\n",
      "Epoch 00028: val_loss did not improve from 10.48089\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 14.2565 - val_loss: 10.6480\n",
      "\n",
      "[20210301-2301-08] Learning rate for epoch 29 is 0.00887999963015318\n",
      "Epoch 29/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 14.5877\n",
      "Epoch 00029: val_loss did not improve from 10.48089\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 14.6030 - val_loss: 10.9515\n",
      "\n",
      "[20210301-2301-11] Learning rate for epoch 30 is 0.008840000256896019\n",
      "Epoch 30/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 14.2307\n",
      "Epoch 00030: val_loss did not improve from 10.48089\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 14.2076 - val_loss: 10.5914\n",
      "\n",
      "[20210301-2301-15] Learning rate for epoch 31 is 0.008799999952316284\n",
      "Epoch 31/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 14.3513\n",
      "Epoch 00031: val_loss did not improve from 10.48089\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 14.4140 - val_loss: 11.0819\n",
      "\n",
      "[20210301-2301-19] Learning rate for epoch 32 is 0.00875999964773655\n",
      "Epoch 32/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 14.2305\n",
      "Epoch 00032: val_loss did not improve from 10.48089\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 14.2570 - val_loss: 10.5910\n",
      "\n",
      "[20210301-2301-22] Learning rate for epoch 33 is 0.00872000027447939\n",
      "Epoch 33/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 14.5117\n",
      "Epoch 00033: val_loss did not improve from 10.48089\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 14.4241 - val_loss: 10.7747\n",
      "\n",
      "[20210301-2301-26] Learning rate for epoch 34 is 0.008679999969899654\n",
      "Epoch 34/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 14.2415\n",
      "Epoch 00034: val_loss did not improve from 10.48089\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 14.3316 - val_loss: 10.6937\n",
      "\n",
      "[20210301-2301-30] Learning rate for epoch 35 is 0.00863999966531992\n",
      "Epoch 35/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.1741\n",
      "Epoch 00035: val_loss did not improve from 10.48089\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 14.1741 - val_loss: 11.0917\n",
      "\n",
      "[20210301-2301-34] Learning rate for epoch 36 is 0.00860000029206276\n",
      "Epoch 36/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 14.2183\n",
      "Epoch 00036: val_loss did not improve from 10.48089\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 14.1955 - val_loss: 10.6365\n",
      "\n",
      "[20210301-2301-37] Learning rate for epoch 37 is 0.008559999987483025\n",
      "Epoch 37/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 14.1728\n",
      "Epoch 00037: val_loss did not improve from 10.48089\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 14.1660 - val_loss: 10.5152\n",
      "\n",
      "[20210301-2301-41] Learning rate for epoch 38 is 0.00851999968290329\n",
      "Epoch 38/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 13.9224\n",
      "Epoch 00038: val_loss improved from 10.48089 to 10.41647, saving model to ./20210301-225844/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 13.8994 - val_loss: 10.4165\n",
      "\n",
      "[20210301-2301-45] Learning rate for epoch 39 is 0.00848000030964613\n",
      "Epoch 39/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 13.8857\n",
      "Epoch 00039: val_loss improved from 10.41647 to 10.35073, saving model to ./20210301-225844/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 14.0110 - val_loss: 10.3507\n",
      "\n",
      "[20210301-2301-49] Learning rate for epoch 40 is 0.008440000005066395\n",
      "Epoch 40/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 14.0786\n",
      "Epoch 00040: val_loss did not improve from 10.35073\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 14.0519 - val_loss: 10.3710\n",
      "\n",
      "[20210301-2301-53] Learning rate for epoch 41 is 0.00839999970048666\n",
      "Epoch 41/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 13.6931\n",
      "Epoch 00041: val_loss did not improve from 10.35073\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 13.7886 - val_loss: 10.8622\n",
      "\n",
      "[20210301-2301-57] Learning rate for epoch 42 is 0.0083600003272295\n",
      "Epoch 42/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 13.8723\n",
      "Epoch 00042: val_loss did not improve from 10.35073\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 13.8606 - val_loss: 10.5562\n",
      "\n",
      "[20210301-2302-01] Learning rate for epoch 43 is 0.008320000022649765\n",
      "Epoch 43/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 13.7397\n",
      "Epoch 00043: val_loss improved from 10.35073 to 10.28841, saving model to ./20210301-225844/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 13.7380 - val_loss: 10.2884\n",
      "\n",
      "[20210301-2302-05] Learning rate for epoch 44 is 0.00827999971807003\n",
      "Epoch 44/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 13.5564\n",
      "Epoch 00044: val_loss did not improve from 10.28841\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 13.5595 - val_loss: 10.3633\n",
      "\n",
      "[20210301-2302-08] Learning rate for epoch 45 is 0.00824000034481287\n",
      "Epoch 45/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.6148\n",
      "Epoch 00045: val_loss did not improve from 10.28841\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 13.6148 - val_loss: 10.5189\n",
      "\n",
      "[20210301-2302-12] Learning rate for epoch 46 is 0.008200000040233135\n",
      "Epoch 46/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 13.4453\n",
      "Epoch 00046: val_loss did not improve from 10.28841\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 13.4948 - val_loss: 10.6371\n",
      "\n",
      "[20210301-2302-16] Learning rate for epoch 47 is 0.0081599997356534\n",
      "Epoch 47/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 13.5796\n",
      "Epoch 00047: val_loss improved from 10.28841 to 10.11194, saving model to ./20210301-225844/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 13.5993 - val_loss: 10.1119\n",
      "\n",
      "[20210301-2302-20] Learning rate for epoch 48 is 0.00812000036239624\n",
      "Epoch 48/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 14.0196\n",
      "Epoch 00048: val_loss did not improve from 10.11194\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 14.0499 - val_loss: 10.4717\n",
      "\n",
      "[20210301-2302-23] Learning rate for epoch 49 is 0.008080000057816505\n",
      "Epoch 49/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 13.8890\n",
      "Epoch 00049: val_loss did not improve from 10.11194\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 13.9629 - val_loss: 10.2153\n",
      "\n",
      "[20210301-2302-27] Learning rate for epoch 50 is 0.00803999975323677\n",
      "Epoch 50/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 13.6964\n",
      "Epoch 00050: val_loss did not improve from 10.11194\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 13.6770 - val_loss: 10.3583\n",
      "\n",
      "[20210301-2302-31] Learning rate for epoch 51 is 0.00800000037997961\n",
      "Epoch 51/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 13.9330\n",
      "Epoch 00051: val_loss did not improve from 10.11194\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 13.8744 - val_loss: 10.7944\n",
      "\n",
      "[20210301-2302-34] Learning rate for epoch 52 is 0.007960000075399876\n",
      "Epoch 52/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 13.9025\n",
      "Epoch 00052: val_loss did not improve from 10.11194\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 13.8976 - val_loss: 10.4467\n",
      "\n",
      "[20210301-2302-38] Learning rate for epoch 53 is 0.00791999977082014\n",
      "Epoch 53/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 13.7538\n",
      "Epoch 00053: val_loss did not improve from 10.11194\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 13.7588 - val_loss: 10.5317\n",
      "\n",
      "[20210301-2302-42] Learning rate for epoch 54 is 0.00788000039756298\n",
      "Epoch 54/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 13.5040\n",
      "Epoch 00054: val_loss did not improve from 10.11194\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 13.4698 - val_loss: 10.6142\n",
      "\n",
      "[20210301-2302-46] Learning rate for epoch 55 is 0.007840000092983246\n",
      "Epoch 55/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 13.4430\n",
      "Epoch 00055: val_loss did not improve from 10.11194\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 13.4689 - val_loss: 10.4769\n",
      "\n",
      "[20210301-2302-49] Learning rate for epoch 56 is 0.007799999788403511\n",
      "Epoch 56/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 13.4916\n",
      "Epoch 00056: val_loss did not improve from 10.11194\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 13.5281 - val_loss: 11.2402\n",
      "\n",
      "[20210301-2302-53] Learning rate for epoch 57 is 0.0077599999494850636\n",
      "Epoch 57/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 13.6009\n",
      "Epoch 00057: val_loss did not improve from 10.11194\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 13.7147 - val_loss: 10.6459\n",
      "\n",
      "[20210301-2302-57] Learning rate for epoch 58 is 0.007720000110566616\n",
      "Epoch 58/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 13.5555\n",
      "Epoch 00058: val_loss did not improve from 10.11194\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 13.6666 - val_loss: 10.7295\n",
      "\n",
      "[20210301-2303-01] Learning rate for epoch 59 is 0.007679999805986881\n",
      "Epoch 59/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.6070\n",
      "Epoch 00059: val_loss did not improve from 10.11194\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 13.6070 - val_loss: 10.6688\n",
      "\n",
      "[20210301-2303-04] Learning rate for epoch 60 is 0.007639999967068434\n",
      "Epoch 60/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 13.9769\n",
      "Epoch 00060: val_loss did not improve from 10.11194\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 13.9023 - val_loss: 10.3415\n",
      "\n",
      "[20210301-2303-08] Learning rate for epoch 61 is 0.007600000128149986\n",
      "Epoch 61/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 13.5906\n",
      "Epoch 00061: val_loss did not improve from 10.11194\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 13.6193 - val_loss: 10.5619\n",
      "\n",
      "[20210301-2303-12] Learning rate for epoch 62 is 0.0075599998235702515\n",
      "Epoch 62/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.6362\n",
      "Epoch 00062: val_loss did not improve from 10.11194\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 13.6362 - val_loss: 10.2416\n",
      "\n",
      "[20210301-2303-15] Learning rate for epoch 63 is 0.007519999984651804\n",
      "Epoch 63/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 13.6803\n",
      "Epoch 00063: val_loss did not improve from 10.11194\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 13.7184 - val_loss: 10.9095\n",
      "\n",
      "[20210301-2303-19] Learning rate for epoch 64 is 0.0074800001457333565\n",
      "Epoch 64/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 13.3152\n",
      "Epoch 00064: val_loss did not improve from 10.11194\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 13.3232 - val_loss: 10.3956\n",
      "\n",
      "[20210301-2303-23] Learning rate for epoch 65 is 0.007439999841153622\n",
      "Epoch 65/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 13.5414\n",
      "Epoch 00065: val_loss did not improve from 10.11194\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 13.5422 - val_loss: 10.3313\n",
      "\n",
      "[20210301-2303-26] Learning rate for epoch 66 is 0.007400000002235174\n",
      "Epoch 66/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 13.2386\n",
      "Epoch 00066: val_loss did not improve from 10.11194\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 13.3090 - val_loss: 10.1577\n",
      "\n",
      "[20210301-2303-30] Learning rate for epoch 67 is 0.007360000163316727\n",
      "Epoch 67/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.6851\n",
      "Epoch 00067: val_loss did not improve from 10.11194\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 13.6851 - val_loss: 10.3448\n",
      "\n",
      "*********************************** Start fine tune ***********************************\n",
      "tt: 195, nt:46, total layers:241\n",
      "*********************************** Start fine tune ***********************************\n",
      "\n",
      "[20210301-2303-34] Learning rate for epoch 1 is 4.999999987376214e-07\n",
      "Epoch 1/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.8265\n",
      "Epoch 00001: val_loss did not improve from 10.11194\n",
      "20/20 [==============================] - 27s 1s/step - loss: 14.8265 - val_loss: 10.4991\n",
      "\n",
      "[20210301-2304-18] Learning rate for epoch 2 is 5.044450517743826e-05\n",
      "Epoch 2/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.9669\n",
      "Epoch 00002: val_loss did not improve from 10.11194\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 14.9669 - val_loss: 10.6093\n",
      "\n",
      "[20210301-2304-23] Learning rate for epoch 3 is 0.00010028902033809572\n",
      "Epoch 3/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.3495\n",
      "Epoch 00003: val_loss did not improve from 10.11194\n",
      "20/20 [==============================] - 29s 1s/step - loss: 14.3495 - val_loss: 10.9920\n",
      "\n",
      "[20210301-2304-55] Learning rate for epoch 4 is 0.00015003354928921908\n",
      "Epoch 4/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.6723\n",
      "Epoch 00004: val_loss did not improve from 10.11194\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 13.6723 - val_loss: 11.3311\n",
      "\n",
      "[20210301-2304-59] Learning rate for epoch 5 is 0.0001996780774788931\n",
      "Epoch 5/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.0056\n",
      "Epoch 00005: val_loss did not improve from 10.11194\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 13.0056 - val_loss: 11.2963\n",
      "\n",
      "[20210301-2305-04] Learning rate for epoch 6 is 0.000249222619459033\n",
      "Epoch 6/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 12.7310\n",
      "Epoch 00006: val_loss improved from 10.11194 to 10.08152, saving model to ./20210301-225844/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 12.7310 - val_loss: 10.0815\n",
      "\n",
      "[20210301-2305-09] Learning rate for epoch 7 is 0.0002986671752296388\n",
      "Epoch 7/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 12.0546\n",
      "Epoch 00007: val_loss improved from 10.08152 to 8.57980, saving model to ./20210301-225844/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 87ms/step - loss: 12.0546 - val_loss: 8.5798\n",
      "\n",
      "[20210301-2305-14] Learning rate for epoch 8 is 0.00034801175934262574\n",
      "Epoch 8/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.9606\n",
      "Epoch 00008: val_loss improved from 8.57980 to 8.38975, saving model to ./20210301-225844/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 93ms/step - loss: 11.9606 - val_loss: 8.3898\n",
      "\n",
      "[20210301-2305-20] Learning rate for epoch 9 is 0.00039725631359033287\n",
      "Epoch 9/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.6021\n",
      "Epoch 00009: val_loss did not improve from 8.38975\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 11.6021 - val_loss: 9.2904\n",
      "\n",
      "[20210301-2305-24] Learning rate for epoch 10 is 0.0004464008961804211\n",
      "Epoch 10/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.1485\n",
      "Epoch 00010: val_loss improved from 8.38975 to 8.01771, saving model to ./20210301-225844/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 11.1485 - val_loss: 8.0177\n",
      "\n",
      "[20210301-2305-30] Learning rate for epoch 11 is 0.0004954455071128905\n",
      "Epoch 11/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.9772\n",
      "Epoch 00011: val_loss did not improve from 8.01771\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 10.9772 - val_loss: 8.7880\n",
      "\n",
      "[20210301-2305-34] Learning rate for epoch 12 is 0.0005443901172839105\n",
      "Epoch 12/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.7567\n",
      "Epoch 00012: val_loss did not improve from 8.01771\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 10.7567 - val_loss: 8.5829\n",
      "\n",
      "[20210301-2305-39] Learning rate for epoch 13 is 0.0005932347266934812\n",
      "Epoch 13/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.9786\n",
      "Epoch 00013: val_loss did not improve from 8.01771\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 10.9786 - val_loss: 8.4011\n",
      "\n",
      "[20210301-2305-43] Learning rate for epoch 14 is 0.0006419793353416026\n",
      "Epoch 14/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.9163\n",
      "Epoch 00014: val_loss did not improve from 8.01771\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 10.9163 - val_loss: 8.4351\n",
      "\n",
      "[20210301-2305-48] Learning rate for epoch 15 is 0.0006906240014359355\n",
      "Epoch 15/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.8378\n",
      "Epoch 00015: val_loss improved from 8.01771 to 7.91735, saving model to ./20210301-225844/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 93ms/step - loss: 10.8378 - val_loss: 7.9173\n",
      "\n",
      "[20210301-2305-53] Learning rate for epoch 16 is 0.0007391686085611582\n",
      "Epoch 16/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.7234\n",
      "Epoch 00016: val_loss did not improve from 7.91735\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 10.7234 - val_loss: 7.9960\n",
      "\n",
      "[20210301-2305-58] Learning rate for epoch 17 is 0.0007876132731325924\n",
      "Epoch 17/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.3758\n",
      "Epoch 00017: val_loss did not improve from 7.91735\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 10.3758 - val_loss: 8.8109\n",
      "\n",
      "[20210301-2306-02] Learning rate for epoch 18 is 0.0008359579369425774\n",
      "Epoch 18/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.5158\n",
      "Epoch 00018: val_loss did not improve from 7.91735\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 10.5158 - val_loss: 9.1436\n",
      "\n",
      "[20210301-2306-07] Learning rate for epoch 19 is 0.000884202599991113\n",
      "Epoch 19/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.0077\n",
      "Epoch 00019: val_loss did not improve from 7.91735\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 10.0077 - val_loss: 8.1930\n",
      "\n",
      "[20210301-2306-12] Learning rate for epoch 20 is 0.0009323473204858601\n",
      "Epoch 20/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.1225\n",
      "Epoch 00020: val_loss did not improve from 7.91735\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 10.1225 - val_loss: 8.1380\n",
      "\n",
      "[20210301-2306-16] Learning rate for epoch 21 is 0.000980391982011497\n",
      "Epoch 21/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.0807\n",
      "Epoch 00021: val_loss did not improve from 7.91735\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 10.0807 - val_loss: 9.5727\n",
      "\n",
      "[20210301-2306-21] Learning rate for epoch 22 is 0.0010283367009833455\n",
      "Epoch 22/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.7856\n",
      "Epoch 00022: val_loss did not improve from 7.91735\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 9.7856 - val_loss: 9.5144\n",
      "\n",
      "[20210301-2306-25] Learning rate for epoch 23 is 0.0010761814191937447\n",
      "Epoch 23/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.7054\n",
      "Epoch 00023: val_loss did not improve from 7.91735\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 9.7054 - val_loss: 9.8554\n",
      "\n",
      "[20210301-2306-30] Learning rate for epoch 24 is 0.0011239261366426945\n",
      "Epoch 24/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.7819\n",
      "Epoch 00024: val_loss did not improve from 7.91735\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 9.7819 - val_loss: 7.9398\n",
      "\n",
      "[20210301-2306-34] Learning rate for epoch 25 is 0.001171570853330195\n",
      "Epoch 25/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.4923\n",
      "Epoch 00025: val_loss did not improve from 7.91735\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 9.4923 - val_loss: 10.5664\n",
      "\n",
      "[20210301-2306-39] Learning rate for epoch 26 is 0.001219115569256246\n",
      "Epoch 26/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.6091\n",
      "Epoch 00026: val_loss did not improve from 7.91735\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 9.6091 - val_loss: 9.8574\n",
      "\n",
      "[20210301-2306-43] Learning rate for epoch 27 is 0.0012665604008361697\n",
      "Epoch 27/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.2270\n",
      "Epoch 00027: val_loss did not improve from 7.91735\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 9.2270 - val_loss: 9.1594\n",
      "\n",
      "[20210301-2306-48] Learning rate for epoch 28 is 0.0013139051152393222\n",
      "Epoch 28/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.4353\n",
      "Epoch 00028: val_loss did not improve from 7.91735\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 9.4353 - val_loss: 8.7512\n",
      "\n",
      "[20210301-2306-53] Learning rate for epoch 29 is 0.0013611499452963471\n",
      "Epoch 29/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.9984\n",
      "Epoch 00029: val_loss did not improve from 7.91735\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 8.9984 - val_loss: 8.9006\n",
      "\n",
      "[20210301-2306-57] Learning rate for epoch 30 is 0.001408294658176601\n",
      "Epoch 30/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.3049\n",
      "Epoch 00030: val_loss did not improve from 7.91735\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 9.3049 - val_loss: 9.6279\n",
      "\n",
      "[20210301-2307-02] Learning rate for epoch 31 is 0.0014553394867107272\n",
      "Epoch 31/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.1041\n",
      "Epoch 00031: val_loss did not improve from 7.91735\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 9.1041 - val_loss: 8.6519\n",
      "\n",
      "[20210301-2307-06] Learning rate for epoch 32 is 0.0015022843144834042\n",
      "Epoch 32/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.0957\n",
      "Epoch 00032: val_loss did not improve from 7.91735\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 9.0957 - val_loss: 9.9519\n",
      "\n",
      "[20210301-2307-11] Learning rate for epoch 33 is 0.0015491291414946318\n",
      "Epoch 33/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.0629\n",
      "Epoch 00033: val_loss did not improve from 7.91735\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 9.0629 - val_loss: 8.4478\n",
      "\n",
      "[20210301-2307-15] Learning rate for epoch 34 is 0.00159587396774441\n",
      "Epoch 34/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.8272\n",
      "Epoch 00034: val_loss improved from 7.91735 to 7.68922, saving model to ./20210301-225844/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 87ms/step - loss: 8.8272 - val_loss: 7.6892\n",
      "\n",
      "[20210301-2307-20] Learning rate for epoch 35 is 0.001642518793232739\n",
      "Epoch 35/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.6103\n",
      "Epoch 00035: val_loss improved from 7.68922 to 7.36794, saving model to ./20210301-225844/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 89ms/step - loss: 8.6103 - val_loss: 7.3679\n",
      "\n",
      "[20210301-2307-26] Learning rate for epoch 36 is 0.0016890636179596186\n",
      "Epoch 36/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.7079\n",
      "Epoch 00036: val_loss did not improve from 7.36794\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 8.7079 - val_loss: 7.9842\n",
      "\n",
      "[20210301-2307-30] Learning rate for epoch 37 is 0.0017355084419250488\n",
      "Epoch 37/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.4544\n",
      "Epoch 00037: val_loss did not improve from 7.36794\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 8.4544 - val_loss: 7.6061\n",
      "\n",
      "[20210301-2307-35] Learning rate for epoch 38 is 0.0017818533815443516\n",
      "Epoch 38/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.3173\n",
      "Epoch 00038: val_loss did not improve from 7.36794\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 8.3173 - val_loss: 7.7065\n",
      "\n",
      "[20210301-2307-40] Learning rate for epoch 39 is 0.0018280982039868832\n",
      "Epoch 39/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.5110\n",
      "Epoch 00039: val_loss did not improve from 7.36794\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 8.5110 - val_loss: 8.9628\n",
      "\n",
      "[20210301-2307-45] Learning rate for epoch 40 is 0.0018742431420832872\n",
      "Epoch 40/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.4631\n",
      "Epoch 00040: val_loss did not improve from 7.36794\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 8.4631 - val_loss: 7.6659\n",
      "\n",
      "[20210301-2307-49] Learning rate for epoch 41 is 0.0019202879630029202\n",
      "Epoch 41/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.3754\n",
      "Epoch 00041: val_loss improved from 7.36794 to 6.41583, saving model to ./20210301-225844/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 91ms/step - loss: 8.3754 - val_loss: 6.4158\n",
      "\n",
      "[20210301-2307-54] Learning rate for epoch 42 is 0.0019662328995764256\n",
      "Epoch 42/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.0885\n",
      "Epoch 00042: val_loss did not improve from 6.41583\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 8.0885 - val_loss: 8.1874\n",
      "\n",
      "[20210301-2307-59] Learning rate for epoch 43 is 0.00201207771897316\n",
      "Epoch 43/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.2943\n",
      "Epoch 00043: val_loss did not improve from 6.41583\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 8.2943 - val_loss: 7.2808\n",
      "\n",
      "[20210301-2308-04] Learning rate for epoch 44 is 0.0020578226540237665\n",
      "Epoch 44/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.2702\n",
      "Epoch 00044: val_loss did not improve from 6.41583\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 8.2702 - val_loss: 7.5645\n",
      "\n",
      "[20210301-2308-08] Learning rate for epoch 45 is 0.0021034677047282457\n",
      "Epoch 45/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.9374\n",
      "Epoch 00045: val_loss did not improve from 6.41583\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.9374 - val_loss: 7.7511\n",
      "\n",
      "[20210301-2308-13] Learning rate for epoch 46 is 0.002149012638255954\n",
      "Epoch 46/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.9829\n",
      "Epoch 00046: val_loss did not improve from 6.41583\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.9829 - val_loss: 7.4744\n",
      "\n",
      "[20210301-2308-17] Learning rate for epoch 47 is 0.0021944576874375343\n",
      "Epoch 47/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.1029\n",
      "Epoch 00047: val_loss did not improve from 6.41583\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 8.1029 - val_loss: 7.0851\n",
      "\n",
      "[20210301-2308-22] Learning rate for epoch 48 is 0.0022398026194423437\n",
      "Epoch 48/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.0062\n",
      "Epoch 00048: val_loss did not improve from 6.41583\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 8.0062 - val_loss: 6.7998\n",
      "\n",
      "[20210301-2308-27] Learning rate for epoch 49 is 0.002285047434270382\n",
      "Epoch 49/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.0257\n",
      "Epoch 00049: val_loss improved from 6.41583 to 5.87658, saving model to ./20210301-225844/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 8.0257 - val_loss: 5.8766\n",
      "\n",
      "[20210301-2308-32] Learning rate for epoch 50 is 0.0023301925975829363\n",
      "Epoch 50/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.9380\n",
      "Epoch 00050: val_loss did not improve from 5.87658\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.9380 - val_loss: 6.8441\n",
      "\n",
      "[20210301-2308-36] Learning rate for epoch 51 is 0.002375237410888076\n",
      "Epoch 51/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.0526\n",
      "Epoch 00051: val_loss did not improve from 5.87658\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 8.0526 - val_loss: 6.8097\n",
      "\n",
      "[20210301-2308-41] Learning rate for epoch 52 is 0.0024201825726777315\n",
      "Epoch 52/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.9997\n",
      "Epoch 00052: val_loss did not improve from 5.87658\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.9997 - val_loss: 6.4029\n",
      "\n",
      "[20210301-2308-45] Learning rate for epoch 53 is 0.002465027617290616\n",
      "Epoch 53/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7714\n",
      "Epoch 00053: val_loss did not improve from 5.87658\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.7714 - val_loss: 7.0665\n",
      "\n",
      "[20210301-2308-50] Learning rate for epoch 54 is 0.0025097725447267294\n",
      "Epoch 54/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7024\n",
      "Epoch 00054: val_loss did not improve from 5.87658\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.7024 - val_loss: 7.1644\n",
      "\n",
      "[20210301-2308-55] Learning rate for epoch 55 is 0.0025544175878167152\n",
      "Epoch 55/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8000\n",
      "Epoch 00055: val_loss did not improve from 5.87658\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.8000 - val_loss: 6.8511\n",
      "\n",
      "[20210301-2308-59] Learning rate for epoch 56 is 0.00259896251372993\n",
      "Epoch 56/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7143\n",
      "Epoch 00056: val_loss did not improve from 5.87658\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.7143 - val_loss: 6.3526\n",
      "\n",
      "[20210301-2309-04] Learning rate for epoch 57 is 0.0026434077881276608\n",
      "Epoch 57/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7696\n",
      "Epoch 00057: val_loss did not improve from 5.87658\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.7696 - val_loss: 6.3986\n",
      "\n",
      "[20210301-2309-09] Learning rate for epoch 58 is 0.0026877527125179768\n",
      "Epoch 58/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7273\n",
      "Epoch 00058: val_loss did not improve from 5.87658\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.7273 - val_loss: 7.0994\n",
      "\n",
      "[20210301-2309-13] Learning rate for epoch 59 is 0.0027319977525621653\n",
      "Epoch 59/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4702\n",
      "Epoch 00059: val_loss did not improve from 5.87658\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.4702 - val_loss: 5.9388\n",
      "\n",
      "[20210301-2309-18] Learning rate for epoch 60 is 0.0027761429082602262\n",
      "Epoch 60/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6747\n",
      "Epoch 00060: val_loss did not improve from 5.87658\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.6747 - val_loss: 8.5144\n",
      "\n",
      "[20210301-2309-22] Learning rate for epoch 61 is 0.002820187946781516\n",
      "Epoch 61/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.9191\n",
      "Epoch 00061: val_loss did not improve from 5.87658\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.9191 - val_loss: 6.0927\n",
      "\n",
      "[20210301-2309-27] Learning rate for epoch 62 is 0.0028641331009566784\n",
      "Epoch 62/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5151\n",
      "Epoch 00062: val_loss did not improve from 5.87658\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.5151 - val_loss: 6.5945\n",
      "\n",
      "[20210301-2309-31] Learning rate for epoch 63 is 0.0029079781379550695\n",
      "Epoch 63/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7078\n",
      "Epoch 00063: val_loss did not improve from 5.87658\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.7078 - val_loss: 7.0577\n",
      "\n",
      "[20210301-2309-36] Learning rate for epoch 64 is 0.002951723290607333\n",
      "Epoch 64/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5908\n",
      "Epoch 00064: val_loss did not improve from 5.87658\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.5908 - val_loss: 6.3083\n",
      "\n",
      "[20210301-2309-41] Learning rate for epoch 65 is 0.0029953685589134693\n",
      "Epoch 65/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5513\n",
      "Epoch 00065: val_loss did not improve from 5.87658\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.5513 - val_loss: 7.0498\n",
      "\n",
      "[20210301-2309-45] Learning rate for epoch 66 is 0.0030389137100428343\n",
      "Epoch 66/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6183\n",
      "Epoch 00066: val_loss did not improve from 5.87658\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.6183 - val_loss: 7.0764\n",
      "\n",
      "[20210301-2309-50] Learning rate for epoch 67 is 0.003082358743995428\n",
      "Epoch 67/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.9466\n",
      "Epoch 00067: val_loss did not improve from 5.87658\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.9466 - val_loss: 6.8320\n",
      "\n",
      "[20210301-2309-54] Learning rate for epoch 68 is 0.0031257038936018944\n",
      "Epoch 68/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4167\n",
      "Epoch 00068: val_loss did not improve from 5.87658\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.4167 - val_loss: 7.1862\n",
      "\n",
      "[20210301-2309-59] Learning rate for epoch 69 is 0.003168949158862233\n",
      "Epoch 69/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5458\n",
      "Epoch 00069: val_loss did not improve from 5.87658\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.5458 - val_loss: 6.4491\n",
      "\n",
      "[20210301-2310-04] Learning rate for epoch 70 is 0.0032120943069458008\n",
      "Epoch 70/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6286\n",
      "Epoch 00070: val_loss did not improve from 5.87658\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.6286 - val_loss: 7.0232\n",
      "\n",
      "[20210301-2310-08] Learning rate for epoch 71 is 0.003255139570683241\n",
      "Epoch 71/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.9449\n",
      "Epoch 00071: val_loss did not improve from 5.87658\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 7.9449 - val_loss: 6.3190\n",
      "\n",
      "[20210301-2310-13] Learning rate for epoch 72 is 0.00329808471724391\n",
      "Epoch 72/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4985\n",
      "Epoch 00072: val_loss did not improve from 5.87658\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.4985 - val_loss: 7.0646\n",
      "\n",
      "[20210301-2310-17] Learning rate for epoch 73 is 0.0033409299794584513\n",
      "Epoch 73/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5556\n",
      "Epoch 00073: val_loss did not improve from 5.87658\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.5556 - val_loss: 7.7795\n",
      "\n",
      "[20210301-2310-22] Learning rate for epoch 74 is 0.0033836751244962215\n",
      "Epoch 74/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5632\n",
      "Epoch 00074: val_loss did not improve from 5.87658\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.5632 - val_loss: 7.0486\n",
      "\n",
      "[20210301-2310-27] Learning rate for epoch 75 is 0.0034263203851878643\n",
      "Epoch 75/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4866\n",
      "Epoch 00075: val_loss did not improve from 5.87658\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.4866 - val_loss: 6.5632\n",
      "\n",
      "[20210301-2310-31] Learning rate for epoch 76 is 0.003468865528702736\n",
      "Epoch 76/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5620\n",
      "Epoch 00076: val_loss did not improve from 5.87658\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.5620 - val_loss: 6.1841\n",
      "\n",
      "[20210301-2310-36] Learning rate for epoch 77 is 0.00351131078787148\n",
      "Epoch 77/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3494\n",
      "Epoch 00077: val_loss did not improve from 5.87658\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.3494 - val_loss: 6.6841\n",
      "\n",
      "[20210301-2310-41] Learning rate for epoch 78 is 0.0035536561626940966\n",
      "Epoch 78/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6250\n",
      "Epoch 00078: val_loss improved from 5.87658 to 5.68678, saving model to ./20210301-225844/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 91ms/step - loss: 7.6250 - val_loss: 5.6868\n",
      "\n",
      "[20210301-2310-46] Learning rate for epoch 79 is 0.003595901420339942\n",
      "Epoch 79/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2162\n",
      "Epoch 00079: val_loss improved from 5.68678 to 5.54473, saving model to ./20210301-225844/toe_K0_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 89ms/step - loss: 7.2162 - val_loss: 5.5447\n",
      "\n",
      "[20210301-2310-51] Learning rate for epoch 80 is 0.00363804679363966\n",
      "Epoch 80/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4252\n",
      "Epoch 00080: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.4252 - val_loss: 6.4450\n",
      "\n",
      "[20210301-2310-56] Learning rate for epoch 81 is 0.0036800920497626066\n",
      "Epoch 81/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1508\n",
      "Epoch 00081: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.1508 - val_loss: 6.4993\n",
      "\n",
      "[20210301-2311-00] Learning rate for epoch 82 is 0.003722037188708782\n",
      "Epoch 82/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4050\n",
      "Epoch 00082: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.4050 - val_loss: 6.6466\n",
      "\n",
      "[20210301-2311-05] Learning rate for epoch 83 is 0.003763882676139474\n",
      "Epoch 83/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4706\n",
      "Epoch 00083: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.4706 - val_loss: 6.9025\n",
      "\n",
      "[20210301-2311-09] Learning rate for epoch 84 is 0.0038056280463933945\n",
      "Epoch 84/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4899\n",
      "Epoch 00084: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.4899 - val_loss: 6.1055\n",
      "\n",
      "[20210301-2311-14] Learning rate for epoch 85 is 0.003847273299470544\n",
      "Epoch 85/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2923\n",
      "Epoch 00085: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.2923 - val_loss: 6.2568\n",
      "\n",
      "[20210301-2311-19] Learning rate for epoch 86 is 0.0038888186682015657\n",
      "Epoch 86/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3930\n",
      "Epoch 00086: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.3930 - val_loss: 7.2043\n",
      "\n",
      "[20210301-2311-23] Learning rate for epoch 87 is 0.00393026415258646\n",
      "Epoch 87/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4000\n",
      "Epoch 00087: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.4000 - val_loss: 6.1982\n",
      "\n",
      "[20210301-2311-28] Learning rate for epoch 88 is 0.00397160928696394\n",
      "Epoch 88/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5278\n",
      "Epoch 00088: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.5278 - val_loss: 7.1832\n",
      "\n",
      "[20210301-2311-33] Learning rate for epoch 89 is 0.004012854769825935\n",
      "Epoch 89/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2460\n",
      "Epoch 00089: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.2460 - val_loss: 7.4119\n",
      "\n",
      "[20210301-2311-37] Learning rate for epoch 90 is 0.00405400013551116\n",
      "Epoch 90/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9956\n",
      "Epoch 00090: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.9956 - val_loss: 7.1349\n",
      "\n",
      "[20210301-2311-42] Learning rate for epoch 91 is 0.004095045384019613\n",
      "Epoch 91/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1410\n",
      "Epoch 00091: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.1410 - val_loss: 7.4015\n",
      "\n",
      "[20210301-2311-46] Learning rate for epoch 92 is 0.004135990981012583\n",
      "Epoch 92/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5836\n",
      "Epoch 00092: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.5836 - val_loss: 8.3300\n",
      "\n",
      "[20210301-2311-51] Learning rate for epoch 93 is 0.004176836460828781\n",
      "Epoch 93/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3511\n",
      "Epoch 00093: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.3511 - val_loss: 8.5998\n",
      "\n",
      "[20210301-2311-56] Learning rate for epoch 94 is 0.004217581823468208\n",
      "Epoch 94/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1390\n",
      "Epoch 00094: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.1390 - val_loss: 7.7860\n",
      "\n",
      "[20210301-2312-00] Learning rate for epoch 95 is 0.004258227068930864\n",
      "Epoch 95/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1303\n",
      "Epoch 00095: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.1303 - val_loss: 6.9513\n",
      "\n",
      "[20210301-2312-05] Learning rate for epoch 96 is 0.0042987726628780365\n",
      "Epoch 96/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9966\n",
      "Epoch 00096: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.9966 - val_loss: 5.7984\n",
      "\n",
      "[20210301-2312-09] Learning rate for epoch 97 is 0.0043392181396484375\n",
      "Epoch 97/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1922\n",
      "Epoch 00097: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.1922 - val_loss: 7.2228\n",
      "\n",
      "[20210301-2312-14] Learning rate for epoch 98 is 0.004379563499242067\n",
      "Epoch 98/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0972\n",
      "Epoch 00098: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.0972 - val_loss: 7.8977\n",
      "\n",
      "[20210301-2312-19] Learning rate for epoch 99 is 0.004419809207320213\n",
      "Epoch 99/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1375\n",
      "Epoch 00099: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.1375 - val_loss: 8.2029\n",
      "\n",
      "[20210301-2312-23] Learning rate for epoch 100 is 0.004459954332560301\n",
      "Epoch 100/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2958\n",
      "Epoch 00100: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.2958 - val_loss: 7.5506\n",
      "\n",
      "[20210301-2312-28] Learning rate for epoch 101 is 4.4999998749517545e-07\n",
      "Epoch 101/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1378\n",
      "Epoch 00101: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 7.1378 - val_loss: 7.1150\n",
      "\n",
      "[20210301-2312-32] Learning rate for epoch 102 is 0.00018023152370005846\n",
      "Epoch 102/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9401\n",
      "Epoch 00102: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.9401 - val_loss: 6.4966\n",
      "\n",
      "[20210301-2312-37] Learning rate for epoch 103 is 0.000359613070031628\n",
      "Epoch 103/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7223\n",
      "Epoch 00103: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.7223 - val_loss: 6.1214\n",
      "\n",
      "[20210301-2312-41] Learning rate for epoch 104 is 0.0005385946715250611\n",
      "Epoch 104/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5240\n",
      "Epoch 00104: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.5240 - val_loss: 5.9248\n",
      "\n",
      "[20210301-2312-46] Learning rate for epoch 105 is 0.0007171762990765274\n",
      "Epoch 105/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9236\n",
      "Epoch 00105: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.9236 - val_loss: 6.0735\n",
      "\n",
      "[20210301-2312-51] Learning rate for epoch 106 is 0.0008953579817898571\n",
      "Epoch 106/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8118\n",
      "Epoch 00106: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.8118 - val_loss: 6.2421\n",
      "\n",
      "[20210301-2312-55] Learning rate for epoch 107 is 0.0010731397196650505\n",
      "Epoch 107/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9571\n",
      "Epoch 00107: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.9571 - val_loss: 5.6515\n",
      "\n",
      "[20210301-2313-00] Learning rate for epoch 108 is 0.0012505215127021074\n",
      "Epoch 108/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6650\n",
      "Epoch 00108: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.6650 - val_loss: 5.7916\n",
      "\n",
      "[20210301-2313-05] Learning rate for epoch 109 is 0.001427503302693367\n",
      "Epoch 109/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9580\n",
      "Epoch 00109: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.9580 - val_loss: 5.6343\n",
      "\n",
      "[20210301-2313-09] Learning rate for epoch 110 is 0.0016040850896388292\n",
      "Epoch 110/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8999\n",
      "Epoch 00110: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.8999 - val_loss: 5.5916\n",
      "\n",
      "[20210301-2313-14] Learning rate for epoch 111 is 0.001780266989953816\n",
      "Epoch 111/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8780\n",
      "Epoch 00111: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.8780 - val_loss: 6.2758\n",
      "\n",
      "[20210301-2313-19] Learning rate for epoch 112 is 0.0019560488872230053\n",
      "Epoch 112/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5736\n",
      "Epoch 00112: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.5736 - val_loss: 5.6655\n",
      "\n",
      "[20210301-2313-23] Learning rate for epoch 113 is 0.002131430897861719\n",
      "Epoch 113/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7281\n",
      "Epoch 00113: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.7281 - val_loss: 5.8798\n",
      "\n",
      "[20210301-2313-28] Learning rate for epoch 114 is 0.0023064129054546356\n",
      "Epoch 114/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8643\n",
      "Epoch 00114: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.8643 - val_loss: 5.8685\n",
      "\n",
      "[20210301-2313-33] Learning rate for epoch 115 is 0.0024809949100017548\n",
      "Epoch 115/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9517\n",
      "Epoch 00115: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.9517 - val_loss: 5.7049\n",
      "\n",
      "[20210301-2313-37] Learning rate for epoch 116 is 0.0026551769115030766\n",
      "Epoch 116/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6034\n",
      "Epoch 00116: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.6034 - val_loss: 5.8221\n",
      "\n",
      "[20210301-2313-42] Learning rate for epoch 117 is 0.0028289591427892447\n",
      "Epoch 117/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6837\n",
      "Epoch 00117: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.6837 - val_loss: 5.8563\n",
      "\n",
      "[20210301-2313-46] Learning rate for epoch 118 is 0.0030023413710296154\n",
      "Epoch 118/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9559\n",
      "Epoch 00118: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.9559 - val_loss: 6.9202\n",
      "\n",
      "[20210301-2313-51] Learning rate for epoch 119 is 0.003175323596224189\n",
      "Epoch 119/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9022\n",
      "Epoch 00119: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.9022 - val_loss: 6.4052\n",
      "\n",
      "[20210301-2313-56] Learning rate for epoch 120 is 0.003347905818372965\n",
      "Epoch 120/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8718\n",
      "Epoch 00120: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.8718 - val_loss: 6.4879\n",
      "\n",
      "[20210301-2314-00] Learning rate for epoch 121 is 0.0035200880374759436\n",
      "Epoch 121/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7243\n",
      "Epoch 00121: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.7243 - val_loss: 5.9674\n",
      "\n",
      "[20210301-2314-05] Learning rate for epoch 122 is 0.003691870253533125\n",
      "Epoch 122/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7859\n",
      "Epoch 00122: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.7859 - val_loss: 6.7816\n",
      "\n",
      "[20210301-2314-09] Learning rate for epoch 123 is 0.0038632526993751526\n",
      "Epoch 123/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8906\n",
      "Epoch 00123: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.8906 - val_loss: 6.1076\n",
      "\n",
      "[20210301-2314-14] Learning rate for epoch 124 is 0.004034235142171383\n",
      "Epoch 124/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0459\n",
      "Epoch 00124: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.0459 - val_loss: 7.2140\n",
      "\n",
      "[20210301-2314-19] Learning rate for epoch 125 is 0.004204817581921816\n",
      "Epoch 125/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8453\n",
      "Epoch 00125: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.8453 - val_loss: 8.8012\n",
      "\n",
      "[20210301-2314-23] Learning rate for epoch 126 is 0.0043750000186264515\n",
      "Epoch 126/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1541\n",
      "Epoch 00126: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.1541 - val_loss: 6.1138\n",
      "\n",
      "[20210301-2314-28] Learning rate for epoch 127 is 0.0041952175088226795\n",
      "Epoch 127/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8028\n",
      "Epoch 00127: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.8028 - val_loss: 6.5276\n",
      "\n",
      "[20210301-2314-33] Learning rate for epoch 128 is 0.004015835002064705\n",
      "Epoch 128/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8787\n",
      "Epoch 00128: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.8787 - val_loss: 8.3625\n",
      "\n",
      "[20210301-2314-37] Learning rate for epoch 129 is 0.003836852265521884\n",
      "Epoch 129/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9157\n",
      "Epoch 00129: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.9157 - val_loss: 6.2647\n",
      "\n",
      "[20210301-2314-42] Learning rate for epoch 130 is 0.003658269764855504\n",
      "Epoch 130/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8907\n",
      "Epoch 00130: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.8907 - val_loss: 7.2324\n",
      "\n",
      "[20210301-2314-47] Learning rate for epoch 131 is 0.003480087034404278\n",
      "Epoch 131/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9750\n",
      "Epoch 00131: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.9750 - val_loss: 7.0697\n",
      "\n",
      "[20210301-2314-51] Learning rate for epoch 132 is 0.003302304306998849\n",
      "Epoch 132/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6840\n",
      "Epoch 00132: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.6840 - val_loss: 7.0074\n",
      "\n",
      "[20210301-2314-56] Learning rate for epoch 133 is 0.0031249215826392174\n",
      "Epoch 133/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7571\n",
      "Epoch 00133: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.7571 - val_loss: 6.2234\n",
      "\n",
      "[20210301-2315-00] Learning rate for epoch 134 is 0.0029479386284947395\n",
      "Epoch 134/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5903\n",
      "Epoch 00134: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.5903 - val_loss: 6.3556\n",
      "\n",
      "[20210301-2315-05] Learning rate for epoch 135 is 0.0027713559102267027\n",
      "Epoch 135/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7178\n",
      "Epoch 00135: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.7178 - val_loss: 6.5875\n",
      "\n",
      "[20210301-2315-10] Learning rate for epoch 136 is 0.0025951729621738195\n",
      "Epoch 136/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5732\n",
      "Epoch 00136: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.5732 - val_loss: 6.6337\n",
      "\n",
      "[20210301-2315-14] Learning rate for epoch 137 is 0.0024193900171667337\n",
      "Epoch 137/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4440\n",
      "Epoch 00137: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.4440 - val_loss: 6.7345\n",
      "\n",
      "[20210301-2315-19] Learning rate for epoch 138 is 0.0022440070752054453\n",
      "Epoch 138/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5505\n",
      "Epoch 00138: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.5505 - val_loss: 6.3974\n",
      "\n",
      "[20210301-2315-24] Learning rate for epoch 139 is 0.002069024136289954\n",
      "Epoch 139/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4445\n",
      "Epoch 00139: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.4445 - val_loss: 6.9753\n",
      "\n",
      "[20210301-2315-28] Learning rate for epoch 140 is 0.0018944410840049386\n",
      "Epoch 140/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6682\n",
      "Epoch 00140: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.6682 - val_loss: 6.9151\n",
      "\n",
      "[20210301-2315-33] Learning rate for epoch 141 is 0.0017202580347657204\n",
      "Epoch 141/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6750\n",
      "Epoch 00141: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.6750 - val_loss: 6.5467\n",
      "\n",
      "[20210301-2315-37] Learning rate for epoch 142 is 0.0015464748721569777\n",
      "Epoch 142/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2278\n",
      "Epoch 00142: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.2278 - val_loss: 7.1133\n",
      "\n",
      "[20210301-2315-42] Learning rate for epoch 143 is 0.0013730917125940323\n",
      "Epoch 143/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5384\n",
      "Epoch 00143: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.5384 - val_loss: 6.2087\n",
      "\n",
      "[20210301-2315-47] Learning rate for epoch 144 is 0.0012001085560768843\n",
      "Epoch 144/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1841\n",
      "Epoch 00144: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.1841 - val_loss: 6.0273\n",
      "\n",
      "[20210301-2315-52] Learning rate for epoch 145 is 0.0010275252861902118\n",
      "Epoch 145/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5353\n",
      "Epoch 00145: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.5353 - val_loss: 6.2267\n",
      "\n",
      "[20210301-2315-56] Learning rate for epoch 146 is 0.0008553420193493366\n",
      "Epoch 146/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4880\n",
      "Epoch 00146: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.4880 - val_loss: 6.4742\n",
      "\n",
      "[20210301-2316-01] Learning rate for epoch 147 is 0.0006835586973465979\n",
      "Epoch 147/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2167\n",
      "Epoch 00147: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.2167 - val_loss: 6.4793\n",
      "\n",
      "[20210301-2316-06] Learning rate for epoch 148 is 0.0005121753201819956\n",
      "Epoch 148/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3166\n",
      "Epoch 00148: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.3166 - val_loss: 6.2313\n",
      "\n",
      "[20210301-2316-10] Learning rate for epoch 149 is 0.00034119191695936024\n",
      "Epoch 149/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1142\n",
      "Epoch 00149: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.1142 - val_loss: 6.0813\n",
      "\n",
      "[20210301-2316-15] Learning rate for epoch 150 is 0.00017060847312677652\n",
      "Epoch 150/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2887\n",
      "Epoch 00150: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.2887 - val_loss: 6.0562\n",
      "\n",
      "[20210301-2316-20] Learning rate for epoch 151 is 4.249999960848072e-07\n",
      "Epoch 151/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8821\n",
      "Epoch 00151: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.8821 - val_loss: 6.0410\n",
      "\n",
      "[20210301-2316-24] Learning rate for epoch 152 is 0.00017020752420648932\n",
      "Epoch 152/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9864\n",
      "Epoch 00152: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.9864 - val_loss: 5.9943\n",
      "\n",
      "[20210301-2316-29] Learning rate for epoch 153 is 0.0003395900712348521\n",
      "Epoch 153/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0731\n",
      "Epoch 00153: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.0731 - val_loss: 5.9914\n",
      "\n",
      "[20210301-2316-33] Learning rate for epoch 154 is 0.0005085726734250784\n",
      "Epoch 154/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0504\n",
      "Epoch 00154: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.0504 - val_loss: 6.0230\n",
      "\n",
      "[20210301-2316-38] Learning rate for epoch 155 is 0.0006771553307771683\n",
      "Epoch 155/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1835\n",
      "Epoch 00155: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.1835 - val_loss: 5.9362\n",
      "\n",
      "[20210301-2316-42] Learning rate for epoch 156 is 0.0008453379850834608\n",
      "Epoch 156/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9810\n",
      "Epoch 00156: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.9810 - val_loss: 5.8728\n",
      "\n",
      "[20210301-2316-47] Learning rate for epoch 157 is 0.0010131207527592778\n",
      "Epoch 157/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1435\n",
      "Epoch 00157: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.1435 - val_loss: 6.3394\n",
      "\n",
      "[20210301-2316-52] Learning rate for epoch 158 is 0.0011805035173892975\n",
      "Epoch 158/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0067\n",
      "Epoch 00158: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.0067 - val_loss: 5.9966\n",
      "\n",
      "[20210301-2316-56] Learning rate for epoch 159 is 0.0013474862789735198\n",
      "Epoch 159/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1405\n",
      "Epoch 00159: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.1405 - val_loss: 5.8805\n",
      "\n",
      "[20210301-2317-01] Learning rate for epoch 160 is 0.0015140691539272666\n",
      "Epoch 160/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2769\n",
      "Epoch 00160: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.2769 - val_loss: 6.3788\n",
      "\n",
      "[20210301-2317-06] Learning rate for epoch 161 is 0.001680252025835216\n",
      "Epoch 161/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3902\n",
      "Epoch 00161: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.3902 - val_loss: 6.2488\n",
      "\n",
      "[20210301-2317-10] Learning rate for epoch 162 is 0.0018460348946973681\n",
      "Epoch 162/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1597\n",
      "Epoch 00162: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.1597 - val_loss: 5.7858\n",
      "\n",
      "[20210301-2317-15] Learning rate for epoch 163 is 0.0020114178769290447\n",
      "Epoch 163/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2944\n",
      "Epoch 00163: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.2944 - val_loss: 7.3272\n",
      "\n",
      "[20210301-2317-19] Learning rate for epoch 164 is 0.0021764009725302458\n",
      "Epoch 164/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4953\n",
      "Epoch 00164: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.4953 - val_loss: 6.6714\n",
      "\n",
      "[20210301-2317-24] Learning rate for epoch 165 is 0.002340983832255006\n",
      "Epoch 165/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2555\n",
      "Epoch 00165: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.2555 - val_loss: 6.2865\n",
      "\n",
      "[20210301-2317-29] Learning rate for epoch 166 is 0.002505166921764612\n",
      "Epoch 166/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2568\n",
      "Epoch 00166: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.2568 - val_loss: 6.4726\n",
      "\n",
      "[20210301-2317-33] Learning rate for epoch 167 is 0.002668950008228421\n",
      "Epoch 167/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2420\n",
      "Epoch 00167: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.2420 - val_loss: 6.8472\n",
      "\n",
      "[20210301-2317-38] Learning rate for epoch 168 is 0.0028323333244770765\n",
      "Epoch 168/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2731\n",
      "Epoch 00168: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.2731 - val_loss: 6.1386\n",
      "\n",
      "[20210301-2317-43] Learning rate for epoch 169 is 0.002995316404849291\n",
      "Epoch 169/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5829\n",
      "Epoch 00169: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.5829 - val_loss: 6.1236\n",
      "\n",
      "[20210301-2317-47] Learning rate for epoch 170 is 0.0031578997150063515\n",
      "Epoch 170/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5269\n",
      "Epoch 00170: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.5269 - val_loss: 6.3705\n",
      "\n",
      "[20210301-2317-52] Learning rate for epoch 171 is 0.0033200830221176147\n",
      "Epoch 171/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7537\n",
      "Epoch 00171: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.7537 - val_loss: 6.5609\n",
      "\n",
      "[20210301-2317-56] Learning rate for epoch 172 is 0.0034818663261830807\n",
      "Epoch 172/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8548\n",
      "Epoch 00172: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.8548 - val_loss: 7.8814\n",
      "\n",
      "[20210301-2318-01] Learning rate for epoch 173 is 0.0036432496272027493\n",
      "Epoch 173/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3078\n",
      "Epoch 00173: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.3078 - val_loss: 7.3789\n",
      "\n",
      "[20210301-2318-06] Learning rate for epoch 174 is 0.003804233158007264\n",
      "Epoch 174/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5569\n",
      "Epoch 00174: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.5569 - val_loss: 6.8089\n",
      "\n",
      "[20210301-2318-10] Learning rate for epoch 175 is 0.003964816685765982\n",
      "Epoch 175/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7365\n",
      "Epoch 00175: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.7365 - val_loss: 6.1477\n",
      "\n",
      "[20210301-2318-15] Learning rate for epoch 176 is 0.004124999977648258\n",
      "Epoch 176/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9250\n",
      "Epoch 00176: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.9250 - val_loss: 6.1562\n",
      "\n",
      "[20210301-2318-19] Learning rate for epoch 177 is 0.003955216612666845\n",
      "Epoch 177/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6852\n",
      "Epoch 00177: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.6852 - val_loss: 5.7750\n",
      "\n",
      "[20210301-2318-24] Learning rate for epoch 178 is 0.003785833017900586\n",
      "Epoch 178/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6634\n",
      "Epoch 00178: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.6634 - val_loss: 6.6876\n",
      "\n",
      "[20210301-2318-29] Learning rate for epoch 179 is 0.0036168494261801243\n",
      "Epoch 179/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9002\n",
      "Epoch 00179: val_loss did not improve from 5.54473\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.9002 - val_loss: 7.4206\n",
      "\n",
      " \n",
      " K =  1 \n",
      "\n",
      "Check cache in memory:Y True\n",
      "Check keypoints process01: Tensor(\"args_1:0\", shape=(2,), dtype=int64) (2,) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Check keypoints process02: Tensor(\"Reshape:0\", shape=(1, 2), dtype=int64) (1, 2) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Check keypoints process03: Tensor(\"PyFunc:1\", dtype=float32, device=/job:localhost/replica:0/task:0)\n",
      "Check keypoints process04: Tensor(\"Reshape_1:0\", shape=(2,), dtype=float32)\n",
      "Check augment :Y True\n",
      "Check ds cache[True] and augment[True]\n",
      "Check cache in memory:Y True\n",
      "Check augment :N False\n",
      "Check ds cache[True] and augment[False]\n",
      "best_model_name: ./20210301-225844/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "base_model.trainable :  False\n",
      "\n",
      "[20210301-2318-36] Learning rate for epoch 1 is 0.009999999776482582\n",
      "Epoch 1/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 75.4785\n",
      "Epoch 00001: val_loss improved from inf to 52.26141, saving model to ./20210301-225844/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 6s 311ms/step - loss: 74.0191 - val_loss: 52.2614\n",
      "\n",
      "[20210301-2318-52] Learning rate for epoch 2 is 0.009960000403225422\n",
      "Epoch 2/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 32.8429\n",
      "Epoch 00002: val_loss improved from 52.26141 to 22.18699, saving model to ./20210301-225844/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 32.5645 - val_loss: 22.1870\n",
      "\n",
      "[20210301-2318-56] Learning rate for epoch 3 is 0.009920000098645687\n",
      "Epoch 3/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 20.1956\n",
      "Epoch 00003: val_loss improved from 22.18699 to 10.82620, saving model to ./20210301-225844/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 9s 429ms/step - loss: 20.1956 - val_loss: 10.8262\n",
      "\n",
      "[20210301-2319-08] Learning rate for epoch 4 is 0.009879999794065952\n",
      "Epoch 4/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 18.7520\n",
      "Epoch 00004: val_loss improved from 10.82620 to 10.51050, saving model to ./20210301-225844/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 18.7536 - val_loss: 10.5105\n",
      "\n",
      "[20210301-2319-12] Learning rate for epoch 5 is 0.009840000420808792\n",
      "Epoch 5/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 18.0353\n",
      "Epoch 00005: val_loss did not improve from 10.51050\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 17.8539 - val_loss: 10.5932\n",
      "\n",
      "[20210301-2319-16] Learning rate for epoch 6 is 0.009800000116229057\n",
      "Epoch 6/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 17.0569\n",
      "Epoch 00006: val_loss improved from 10.51050 to 10.33321, saving model to ./20210301-225844/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 17.0569 - val_loss: 10.3332\n",
      "\n",
      "[20210301-2319-20] Learning rate for epoch 7 is 0.009759999811649323\n",
      "Epoch 7/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 16.3932\n",
      "Epoch 00007: val_loss did not improve from 10.33321\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.3612 - val_loss: 11.0001\n",
      "\n",
      "[20210301-2319-24] Learning rate for epoch 8 is 0.009720000438392162\n",
      "Epoch 8/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.1557\n",
      "Epoch 00008: val_loss did not improve from 10.33321\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.0909 - val_loss: 11.1961\n",
      "\n",
      "[20210301-2319-28] Learning rate for epoch 9 is 0.009680000133812428\n",
      "Epoch 9/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.7931\n",
      "Epoch 00009: val_loss did not improve from 10.33321\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.7931 - val_loss: 10.6391\n",
      "\n",
      "[20210301-2319-32] Learning rate for epoch 10 is 0.009639999829232693\n",
      "Epoch 10/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.9117\n",
      "Epoch 00010: val_loss improved from 10.33321 to 10.02570, saving model to ./20210301-225844/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 15.8915 - val_loss: 10.0257\n",
      "\n",
      "[20210301-2319-36] Learning rate for epoch 11 is 0.009600000455975533\n",
      "Epoch 11/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.4999\n",
      "Epoch 00011: val_loss improved from 10.02570 to 9.97441, saving model to ./20210301-225844/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 15.5294 - val_loss: 9.9744\n",
      "\n",
      "[20210301-2319-40] Learning rate for epoch 12 is 0.009560000151395798\n",
      "Epoch 12/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.3844\n",
      "Epoch 00012: val_loss did not improve from 9.97441\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.3844 - val_loss: 10.0418\n",
      "\n",
      "[20210301-2319-44] Learning rate for epoch 13 is 0.009519999846816063\n",
      "Epoch 13/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.3472\n",
      "Epoch 00013: val_loss did not improve from 9.97441\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 15.2928 - val_loss: 10.1898\n",
      "\n",
      "[20210301-2319-47] Learning rate for epoch 14 is 0.009479999542236328\n",
      "Epoch 14/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.3458\n",
      "Epoch 00014: val_loss did not improve from 9.97441\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.3458 - val_loss: 11.1136\n",
      "\n",
      "[20210301-2319-51] Learning rate for epoch 15 is 0.009440000168979168\n",
      "Epoch 15/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.1576\n",
      "Epoch 00015: val_loss did not improve from 9.97441\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.0088 - val_loss: 10.4418\n",
      "\n",
      "[20210301-2319-55] Learning rate for epoch 16 is 0.009399999864399433\n",
      "Epoch 16/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.4062\n",
      "Epoch 00016: val_loss did not improve from 9.97441\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.4062 - val_loss: 10.9116\n",
      "\n",
      "[20210301-2319-58] Learning rate for epoch 17 is 0.009359999559819698\n",
      "Epoch 17/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.1048\n",
      "Epoch 00017: val_loss did not improve from 9.97441\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.1483 - val_loss: 10.4003\n",
      "\n",
      "[20210301-2320-02] Learning rate for epoch 18 is 0.009320000186562538\n",
      "Epoch 18/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 14.4791\n",
      "Epoch 00018: val_loss did not improve from 9.97441\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 14.5379 - val_loss: 10.0671\n",
      "\n",
      "[20210301-2320-06] Learning rate for epoch 19 is 0.009279999881982803\n",
      "Epoch 19/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 14.4208\n",
      "Epoch 00019: val_loss did not improve from 9.97441\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 14.5358 - val_loss: 10.4629\n",
      "\n",
      "[20210301-2320-10] Learning rate for epoch 20 is 0.009239999577403069\n",
      "Epoch 20/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.6686\n",
      "Epoch 00020: val_loss did not improve from 9.97441\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 14.6686 - val_loss: 10.2898\n",
      "\n",
      "[20210301-2320-13] Learning rate for epoch 21 is 0.009200000204145908\n",
      "Epoch 21/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.5074\n",
      "Epoch 00021: val_loss did not improve from 9.97441\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 14.5074 - val_loss: 10.1266\n",
      "\n",
      "[20210301-2320-17] Learning rate for epoch 22 is 0.009159999899566174\n",
      "Epoch 22/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 14.5016\n",
      "Epoch 00022: val_loss did not improve from 9.97441\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 14.4892 - val_loss: 11.1957\n",
      "\n",
      "[20210301-2320-21] Learning rate for epoch 23 is 0.009119999594986439\n",
      "Epoch 23/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 14.6598\n",
      "Epoch 00023: val_loss did not improve from 9.97441\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 14.6034 - val_loss: 10.7537\n",
      "\n",
      "[20210301-2320-25] Learning rate for epoch 24 is 0.009080000221729279\n",
      "Epoch 24/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.6528\n",
      "Epoch 00024: val_loss improved from 9.97441 to 9.97030, saving model to ./20210301-225844/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 14.6528 - val_loss: 9.9703\n",
      "\n",
      "[20210301-2320-29] Learning rate for epoch 25 is 0.009039999917149544\n",
      "Epoch 25/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.4898\n",
      "Epoch 00025: val_loss did not improve from 9.97030\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 14.4898 - val_loss: 10.2235\n",
      "\n",
      "[20210301-2320-32] Learning rate for epoch 26 is 0.008999999612569809\n",
      "Epoch 26/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.1599\n",
      "Epoch 00026: val_loss did not improve from 9.97030\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 14.1599 - val_loss: 10.9354\n",
      "\n",
      "[20210301-2320-36] Learning rate for epoch 27 is 0.008960000239312649\n",
      "Epoch 27/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 14.3248\n",
      "Epoch 00027: val_loss did not improve from 9.97030\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 14.3232 - val_loss: 10.8585\n",
      "\n",
      "[20210301-2320-40] Learning rate for epoch 28 is 0.008919999934732914\n",
      "Epoch 28/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 14.3513\n",
      "Epoch 00028: val_loss did not improve from 9.97030\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 14.3707 - val_loss: 11.1360\n",
      "\n",
      "[20210301-2320-44] Learning rate for epoch 29 is 0.00887999963015318\n",
      "Epoch 29/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 14.2188\n",
      "Epoch 00029: val_loss did not improve from 9.97030\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 14.1419 - val_loss: 10.6411\n",
      "\n",
      "[20210301-2320-48] Learning rate for epoch 30 is 0.008840000256896019\n",
      "Epoch 30/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 14.1943\n",
      "Epoch 00030: val_loss did not improve from 9.97030\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 14.1396 - val_loss: 10.4318\n",
      "\n",
      "[20210301-2320-51] Learning rate for epoch 31 is 0.008799999952316284\n",
      "Epoch 31/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.1687\n",
      "Epoch 00031: val_loss did not improve from 9.97030\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 14.1687 - val_loss: 10.1260\n",
      "\n",
      "[20210301-2320-55] Learning rate for epoch 32 is 0.00875999964773655\n",
      "Epoch 32/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 14.3314\n",
      "Epoch 00032: val_loss did not improve from 9.97030\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 14.4279 - val_loss: 10.3101\n",
      "\n",
      "[20210301-2320-59] Learning rate for epoch 33 is 0.00872000027447939\n",
      "Epoch 33/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 14.0758\n",
      "Epoch 00033: val_loss did not improve from 9.97030\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 14.0586 - val_loss: 10.2391\n",
      "\n",
      "[20210301-2321-02] Learning rate for epoch 34 is 0.008679999969899654\n",
      "Epoch 34/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 14.6949\n",
      "Epoch 00034: val_loss did not improve from 9.97030\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 14.6712 - val_loss: 10.1252\n",
      "\n",
      "[20210301-2321-06] Learning rate for epoch 35 is 0.00863999966531992\n",
      "Epoch 35/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 13.7933\n",
      "Epoch 00035: val_loss did not improve from 9.97030\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 13.8659 - val_loss: 10.4696\n",
      "\n",
      "[20210301-2321-10] Learning rate for epoch 36 is 0.00860000029206276\n",
      "Epoch 36/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 13.9294\n",
      "Epoch 00036: val_loss did not improve from 9.97030\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 13.9869 - val_loss: 10.5320\n",
      "\n",
      "[20210301-2321-14] Learning rate for epoch 37 is 0.008559999987483025\n",
      "Epoch 37/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 14.2295\n",
      "Epoch 00037: val_loss did not improve from 9.97030\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 14.1370 - val_loss: 10.6134\n",
      "\n",
      "[20210301-2321-17] Learning rate for epoch 38 is 0.00851999968290329\n",
      "Epoch 38/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 14.0020\n",
      "Epoch 00038: val_loss did not improve from 9.97030\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 13.9227 - val_loss: 10.7047\n",
      "\n",
      "[20210301-2321-21] Learning rate for epoch 39 is 0.00848000030964613\n",
      "Epoch 39/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 13.7905\n",
      "Epoch 00039: val_loss did not improve from 9.97030\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 13.8300 - val_loss: 10.1595\n",
      "\n",
      "[20210301-2321-25] Learning rate for epoch 40 is 0.008440000005066395\n",
      "Epoch 40/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 13.8762\n",
      "Epoch 00040: val_loss did not improve from 9.97030\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 13.8471 - val_loss: 10.0831\n",
      "\n",
      "[20210301-2321-29] Learning rate for epoch 41 is 0.00839999970048666\n",
      "Epoch 41/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 13.5661\n",
      "Epoch 00041: val_loss did not improve from 9.97030\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 13.6056 - val_loss: 10.5781\n",
      "\n",
      "[20210301-2321-32] Learning rate for epoch 42 is 0.0083600003272295\n",
      "Epoch 42/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 14.0950\n",
      "Epoch 00042: val_loss did not improve from 9.97030\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 14.0737 - val_loss: 10.5304\n",
      "\n",
      "[20210301-2321-36] Learning rate for epoch 43 is 0.008320000022649765\n",
      "Epoch 43/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 13.6637\n",
      "Epoch 00043: val_loss did not improve from 9.97030\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 13.6258 - val_loss: 10.1005\n",
      "\n",
      "[20210301-2321-40] Learning rate for epoch 44 is 0.00827999971807003\n",
      "Epoch 44/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 13.4890\n",
      "Epoch 00044: val_loss did not improve from 9.97030\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 13.5540 - val_loss: 10.3483\n",
      "\n",
      "*********************************** Start fine tune ***********************************\n",
      "tt: 195, nt:46, total layers:241\n",
      "*********************************** Start fine tune ***********************************\n",
      "\n",
      "[20210301-2321-44] Learning rate for epoch 1 is 4.999999987376214e-07\n",
      "Epoch 1/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.1575\n",
      "Epoch 00001: val_loss did not improve from 9.97030\n",
      "20/20 [==============================] - 16s 785ms/step - loss: 15.1575 - val_loss: 10.2931\n",
      "\n",
      "[20210301-2322-16] Learning rate for epoch 2 is 5.044450517743826e-05\n",
      "Epoch 2/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.3004\n",
      "Epoch 00002: val_loss did not improve from 9.97030\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 15.3004 - val_loss: 10.7260\n",
      "\n",
      "[20210301-2322-20] Learning rate for epoch 3 is 0.00010028902033809572\n",
      "Epoch 3/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.7175\n",
      "Epoch 00003: val_loss did not improve from 9.97030\n",
      "20/20 [==============================] - 19s 926ms/step - loss: 14.7175 - val_loss: 11.3165\n",
      "\n",
      "[20210301-2322-42] Learning rate for epoch 4 is 0.00015003354928921908\n",
      "Epoch 4/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.3052\n",
      "Epoch 00004: val_loss did not improve from 9.97030\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 14.3052 - val_loss: 12.4686\n",
      "\n",
      "[20210301-2322-47] Learning rate for epoch 5 is 0.0001996780774788931\n",
      "Epoch 5/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.6522\n",
      "Epoch 00005: val_loss did not improve from 9.97030\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 13.6522 - val_loss: 12.2606\n",
      "\n",
      "[20210301-2322-51] Learning rate for epoch 6 is 0.000249222619459033\n",
      "Epoch 6/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.2443\n",
      "Epoch 00006: val_loss did not improve from 9.97030\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 13.2443 - val_loss: 11.2352\n",
      "\n",
      "[20210301-2322-56] Learning rate for epoch 7 is 0.0002986671752296388\n",
      "Epoch 7/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 12.5852\n",
      "Epoch 00007: val_loss improved from 9.97030 to 9.40534, saving model to ./20210301-225844/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 91ms/step - loss: 12.5852 - val_loss: 9.4053\n",
      "\n",
      "[20210301-2323-01] Learning rate for epoch 8 is 0.00034801175934262574\n",
      "Epoch 8/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 12.4635\n",
      "Epoch 00008: val_loss improved from 9.40534 to 9.28712, saving model to ./20210301-225844/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 12.4635 - val_loss: 9.2871\n",
      "\n",
      "[20210301-2323-06] Learning rate for epoch 9 is 0.00039725631359033287\n",
      "Epoch 9/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.9377\n",
      "Epoch 00009: val_loss improved from 9.28712 to 8.77004, saving model to ./20210301-225844/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 87ms/step - loss: 11.9377 - val_loss: 8.7700\n",
      "\n",
      "[20210301-2323-11] Learning rate for epoch 10 is 0.0004464008961804211\n",
      "Epoch 10/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.9207\n",
      "Epoch 00010: val_loss improved from 8.77004 to 8.46094, saving model to ./20210301-225844/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 11.9207 - val_loss: 8.4609\n",
      "\n",
      "[20210301-2323-16] Learning rate for epoch 11 is 0.0004954455071128905\n",
      "Epoch 11/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.5658\n",
      "Epoch 00011: val_loss improved from 8.46094 to 8.40245, saving model to ./20210301-225844/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 89ms/step - loss: 11.5658 - val_loss: 8.4025\n",
      "\n",
      "[20210301-2323-21] Learning rate for epoch 12 is 0.0005443901172839105\n",
      "Epoch 12/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.5490\n",
      "Epoch 00012: val_loss did not improve from 8.40245\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 11.5490 - val_loss: 8.4810\n",
      "\n",
      "[20210301-2323-26] Learning rate for epoch 13 is 0.0005932347266934812\n",
      "Epoch 13/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.2675\n",
      "Epoch 00013: val_loss did not improve from 8.40245\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 11.2675 - val_loss: 8.4737\n",
      "\n",
      "[20210301-2323-31] Learning rate for epoch 14 is 0.0006419793353416026\n",
      "Epoch 14/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.9589\n",
      "Epoch 00014: val_loss improved from 8.40245 to 7.56314, saving model to ./20210301-225844/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 89ms/step - loss: 10.9589 - val_loss: 7.5631\n",
      "\n",
      "[20210301-2323-36] Learning rate for epoch 15 is 0.0006906240014359355\n",
      "Epoch 15/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.8805\n",
      "Epoch 00015: val_loss did not improve from 7.56314\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 10.8805 - val_loss: 8.2293\n",
      "\n",
      "[20210301-2323-40] Learning rate for epoch 16 is 0.0007391686085611582\n",
      "Epoch 16/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.0946\n",
      "Epoch 00016: val_loss did not improve from 7.56314\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 11.0946 - val_loss: 8.3420\n",
      "\n",
      "[20210301-2323-45] Learning rate for epoch 17 is 0.0007876132731325924\n",
      "Epoch 17/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.4721\n",
      "Epoch 00017: val_loss did not improve from 7.56314\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 10.4721 - val_loss: 8.8741\n",
      "\n",
      "[20210301-2323-49] Learning rate for epoch 18 is 0.0008359579369425774\n",
      "Epoch 18/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.7566\n",
      "Epoch 00018: val_loss did not improve from 7.56314\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 10.7566 - val_loss: 8.4030\n",
      "\n",
      "[20210301-2323-54] Learning rate for epoch 19 is 0.000884202599991113\n",
      "Epoch 19/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.5757\n",
      "Epoch 00019: val_loss did not improve from 7.56314\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 10.5757 - val_loss: 9.3442\n",
      "\n",
      "[20210301-2323-59] Learning rate for epoch 20 is 0.0009323473204858601\n",
      "Epoch 20/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.6390\n",
      "Epoch 00020: val_loss did not improve from 7.56314\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 10.6390 - val_loss: 9.4279\n",
      "\n",
      "[20210301-2324-03] Learning rate for epoch 21 is 0.000980391982011497\n",
      "Epoch 21/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.3908\n",
      "Epoch 00021: val_loss did not improve from 7.56314\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 10.3908 - val_loss: 9.2105\n",
      "\n",
      "[20210301-2324-08] Learning rate for epoch 22 is 0.0010283367009833455\n",
      "Epoch 22/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.2499\n",
      "Epoch 00022: val_loss did not improve from 7.56314\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 10.2499 - val_loss: 8.4478\n",
      "\n",
      "[20210301-2324-12] Learning rate for epoch 23 is 0.0010761814191937447\n",
      "Epoch 23/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.9744\n",
      "Epoch 00023: val_loss did not improve from 7.56314\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 9.9744 - val_loss: 9.3437\n",
      "\n",
      "[20210301-2324-17] Learning rate for epoch 24 is 0.0011239261366426945\n",
      "Epoch 24/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.8463\n",
      "Epoch 00024: val_loss did not improve from 7.56314\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 9.8463 - val_loss: 8.8320\n",
      "\n",
      "[20210301-2324-21] Learning rate for epoch 25 is 0.001171570853330195\n",
      "Epoch 25/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.9252\n",
      "Epoch 00025: val_loss did not improve from 7.56314\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 9.9252 - val_loss: 9.3306\n",
      "\n",
      "[20210301-2324-26] Learning rate for epoch 26 is 0.001219115569256246\n",
      "Epoch 26/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.5607\n",
      "Epoch 00026: val_loss did not improve from 7.56314\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 9.5607 - val_loss: 8.7114\n",
      "\n",
      "[20210301-2324-31] Learning rate for epoch 27 is 0.0012665604008361697\n",
      "Epoch 27/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.4281\n",
      "Epoch 00027: val_loss did not improve from 7.56314\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 9.4281 - val_loss: 9.4458\n",
      "\n",
      "[20210301-2324-35] Learning rate for epoch 28 is 0.0013139051152393222\n",
      "Epoch 28/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.2599\n",
      "Epoch 00028: val_loss did not improve from 7.56314\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 9.2599 - val_loss: 9.0779\n",
      "\n",
      "[20210301-2324-40] Learning rate for epoch 29 is 0.0013611499452963471\n",
      "Epoch 29/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.5711\n",
      "Epoch 00029: val_loss did not improve from 7.56314\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 9.5711 - val_loss: 8.8174\n",
      "\n",
      "[20210301-2324-45] Learning rate for epoch 30 is 0.001408294658176601\n",
      "Epoch 30/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.1456\n",
      "Epoch 00030: val_loss did not improve from 7.56314\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 9.1456 - val_loss: 8.4745\n",
      "\n",
      "[20210301-2324-50] Learning rate for epoch 31 is 0.0014553394867107272\n",
      "Epoch 31/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.1358\n",
      "Epoch 00031: val_loss did not improve from 7.56314\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 9.1358 - val_loss: 8.9940\n",
      "\n",
      "[20210301-2324-54] Learning rate for epoch 32 is 0.0015022843144834042\n",
      "Epoch 32/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.1509\n",
      "Epoch 00032: val_loss did not improve from 7.56314\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 9.1509 - val_loss: 9.7828\n",
      "\n",
      "[20210301-2324-59] Learning rate for epoch 33 is 0.0015491291414946318\n",
      "Epoch 33/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.9958\n",
      "Epoch 00033: val_loss did not improve from 7.56314\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 8.9958 - val_loss: 8.7409\n",
      "\n",
      "[20210301-2325-03] Learning rate for epoch 34 is 0.00159587396774441\n",
      "Epoch 34/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.3033\n",
      "Epoch 00034: val_loss did not improve from 7.56314\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 9.3033 - val_loss: 8.4022\n",
      "\n",
      "[20210301-2325-08] Learning rate for epoch 35 is 0.001642518793232739\n",
      "Epoch 35/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.9107\n",
      "Epoch 00035: val_loss did not improve from 7.56314\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 8.9107 - val_loss: 8.4279\n",
      "\n",
      "[20210301-2325-13] Learning rate for epoch 36 is 0.0016890636179596186\n",
      "Epoch 36/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.8892\n",
      "Epoch 00036: val_loss did not improve from 7.56314\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 8.8892 - val_loss: 7.9116\n",
      "\n",
      "[20210301-2325-17] Learning rate for epoch 37 is 0.0017355084419250488\n",
      "Epoch 37/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.7625\n",
      "Epoch 00037: val_loss did not improve from 7.56314\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 8.7625 - val_loss: 8.5174\n",
      "\n",
      "[20210301-2325-22] Learning rate for epoch 38 is 0.0017818533815443516\n",
      "Epoch 38/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.5581\n",
      "Epoch 00038: val_loss did not improve from 7.56314\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 8.5581 - val_loss: 8.2375\n",
      "\n",
      "[20210301-2325-26] Learning rate for epoch 39 is 0.0018280982039868832\n",
      "Epoch 39/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.5161\n",
      "Epoch 00039: val_loss did not improve from 7.56314\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 8.5161 - val_loss: 8.0410\n",
      "\n",
      "[20210301-2325-31] Learning rate for epoch 40 is 0.0018742431420832872\n",
      "Epoch 40/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.3654\n",
      "Epoch 00040: val_loss did not improve from 7.56314\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 8.3654 - val_loss: 8.4728\n",
      "\n",
      "[20210301-2325-36] Learning rate for epoch 41 is 0.0019202879630029202\n",
      "Epoch 41/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.5585\n",
      "Epoch 00041: val_loss improved from 7.56314 to 7.15066, saving model to ./20210301-225844/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 91ms/step - loss: 8.5585 - val_loss: 7.1507\n",
      "\n",
      "[20210301-2325-41] Learning rate for epoch 42 is 0.0019662328995764256\n",
      "Epoch 42/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.7724\n",
      "Epoch 00042: val_loss did not improve from 7.15066\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 8.7724 - val_loss: 7.7013\n",
      "\n",
      "[20210301-2325-45] Learning rate for epoch 43 is 0.00201207771897316\n",
      "Epoch 43/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.3191\n",
      "Epoch 00043: val_loss did not improve from 7.15066\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 8.3191 - val_loss: 7.8136\n",
      "\n",
      "[20210301-2325-50] Learning rate for epoch 44 is 0.0020578226540237665\n",
      "Epoch 44/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.3328\n",
      "Epoch 00044: val_loss improved from 7.15066 to 6.87577, saving model to ./20210301-225844/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 8.3328 - val_loss: 6.8758\n",
      "\n",
      "[20210301-2325-55] Learning rate for epoch 45 is 0.0021034677047282457\n",
      "Epoch 45/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.1912\n",
      "Epoch 00045: val_loss did not improve from 6.87577\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 8.1912 - val_loss: 7.5410\n",
      "\n",
      "[20210301-2326-00] Learning rate for epoch 46 is 0.002149012638255954\n",
      "Epoch 46/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.4497\n",
      "Epoch 00046: val_loss did not improve from 6.87577\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 8.4497 - val_loss: 7.5232\n",
      "\n",
      "[20210301-2326-04] Learning rate for epoch 47 is 0.0021944576874375343\n",
      "Epoch 47/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.3863\n",
      "Epoch 00047: val_loss did not improve from 6.87577\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 8.3863 - val_loss: 7.2437\n",
      "\n",
      "[20210301-2326-09] Learning rate for epoch 48 is 0.0022398026194423437\n",
      "Epoch 48/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.2373\n",
      "Epoch 00048: val_loss improved from 6.87577 to 6.61998, saving model to ./20210301-225844/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 8.2373 - val_loss: 6.6200\n",
      "\n",
      "[20210301-2326-14] Learning rate for epoch 49 is 0.002285047434270382\n",
      "Epoch 49/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.0328\n",
      "Epoch 00049: val_loss did not improve from 6.61998\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 8.0328 - val_loss: 6.6997\n",
      "\n",
      "[20210301-2326-19] Learning rate for epoch 50 is 0.0023301925975829363\n",
      "Epoch 50/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.9788\n",
      "Epoch 00050: val_loss improved from 6.61998 to 6.44005, saving model to ./20210301-225844/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 7.9788 - val_loss: 6.4401\n",
      "\n",
      "[20210301-2326-24] Learning rate for epoch 51 is 0.002375237410888076\n",
      "Epoch 51/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.1224\n",
      "Epoch 00051: val_loss did not improve from 6.44005\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 8.1224 - val_loss: 6.8453\n",
      "\n",
      "[20210301-2326-28] Learning rate for epoch 52 is 0.0024201825726777315\n",
      "Epoch 52/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.1956\n",
      "Epoch 00052: val_loss did not improve from 6.44005\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 8.1956 - val_loss: 6.4512\n",
      "\n",
      "[20210301-2326-33] Learning rate for epoch 53 is 0.002465027617290616\n",
      "Epoch 53/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.1456\n",
      "Epoch 00053: val_loss did not improve from 6.44005\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 8.1456 - val_loss: 6.7705\n",
      "\n",
      "[20210301-2326-37] Learning rate for epoch 54 is 0.0025097725447267294\n",
      "Epoch 54/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7885\n",
      "Epoch 00054: val_loss did not improve from 6.44005\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.7885 - val_loss: 7.1402\n",
      "\n",
      "[20210301-2326-42] Learning rate for epoch 55 is 0.0025544175878167152\n",
      "Epoch 55/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8737\n",
      "Epoch 00055: val_loss did not improve from 6.44005\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.8737 - val_loss: 7.0210\n",
      "\n",
      "[20210301-2326-47] Learning rate for epoch 56 is 0.00259896251372993\n",
      "Epoch 56/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.0522\n",
      "Epoch 00056: val_loss did not improve from 6.44005\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 8.0522 - val_loss: 6.6124\n",
      "\n",
      "[20210301-2326-51] Learning rate for epoch 57 is 0.0026434077881276608\n",
      "Epoch 57/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8936\n",
      "Epoch 00057: val_loss did not improve from 6.44005\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.8936 - val_loss: 6.6821\n",
      "\n",
      "[20210301-2326-56] Learning rate for epoch 58 is 0.0026877527125179768\n",
      "Epoch 58/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.9400\n",
      "Epoch 00058: val_loss did not improve from 6.44005\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.9400 - val_loss: 6.7001\n",
      "\n",
      "[20210301-2327-00] Learning rate for epoch 59 is 0.0027319977525621653\n",
      "Epoch 59/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8772\n",
      "Epoch 00059: val_loss did not improve from 6.44005\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.8772 - val_loss: 7.7379\n",
      "\n",
      "[20210301-2327-05] Learning rate for epoch 60 is 0.0027761429082602262\n",
      "Epoch 60/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6823\n",
      "Epoch 00060: val_loss did not improve from 6.44005\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.6823 - val_loss: 6.9727\n",
      "\n",
      "[20210301-2327-10] Learning rate for epoch 61 is 0.002820187946781516\n",
      "Epoch 61/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8174\n",
      "Epoch 00061: val_loss did not improve from 6.44005\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 7.8174 - val_loss: 6.8014\n",
      "\n",
      "[20210301-2327-14] Learning rate for epoch 62 is 0.0028641331009566784\n",
      "Epoch 62/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6402\n",
      "Epoch 00062: val_loss did not improve from 6.44005\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.6402 - val_loss: 7.1333\n",
      "\n",
      "[20210301-2327-19] Learning rate for epoch 63 is 0.0029079781379550695\n",
      "Epoch 63/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7962\n",
      "Epoch 00063: val_loss did not improve from 6.44005\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 7.7962 - val_loss: 7.3174\n",
      "\n",
      "[20210301-2327-23] Learning rate for epoch 64 is 0.002951723290607333\n",
      "Epoch 64/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5765\n",
      "Epoch 00064: val_loss did not improve from 6.44005\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.5765 - val_loss: 9.3736\n",
      "\n",
      "[20210301-2327-28] Learning rate for epoch 65 is 0.0029953685589134693\n",
      "Epoch 65/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5535\n",
      "Epoch 00065: val_loss improved from 6.44005 to 6.16056, saving model to ./20210301-225844/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 7.5535 - val_loss: 6.1606\n",
      "\n",
      "[20210301-2327-33] Learning rate for epoch 66 is 0.0030389137100428343\n",
      "Epoch 66/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6850\n",
      "Epoch 00066: val_loss did not improve from 6.16056\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.6850 - val_loss: 7.7413\n",
      "\n",
      "[20210301-2327-38] Learning rate for epoch 67 is 0.003082358743995428\n",
      "Epoch 67/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7722\n",
      "Epoch 00067: val_loss did not improve from 6.16056\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 7.7722 - val_loss: 6.2864\n",
      "\n",
      "[20210301-2327-42] Learning rate for epoch 68 is 0.0031257038936018944\n",
      "Epoch 68/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6751\n",
      "Epoch 00068: val_loss did not improve from 6.16056\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.6751 - val_loss: 8.1406\n",
      "\n",
      "[20210301-2327-47] Learning rate for epoch 69 is 0.003168949158862233\n",
      "Epoch 69/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8869\n",
      "Epoch 00069: val_loss did not improve from 6.16056\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.8869 - val_loss: 6.4739\n",
      "\n",
      "[20210301-2327-52] Learning rate for epoch 70 is 0.0032120943069458008\n",
      "Epoch 70/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7716\n",
      "Epoch 00070: val_loss did not improve from 6.16056\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.7716 - val_loss: 6.3778\n",
      "\n",
      "[20210301-2327-56] Learning rate for epoch 71 is 0.003255139570683241\n",
      "Epoch 71/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8414\n",
      "Epoch 00071: val_loss did not improve from 6.16056\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.8414 - val_loss: 7.2562\n",
      "\n",
      "[20210301-2328-01] Learning rate for epoch 72 is 0.00329808471724391\n",
      "Epoch 72/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5868\n",
      "Epoch 00072: val_loss did not improve from 6.16056\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.5868 - val_loss: 6.7793\n",
      "\n",
      "[20210301-2328-05] Learning rate for epoch 73 is 0.0033409299794584513\n",
      "Epoch 73/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5234\n",
      "Epoch 00073: val_loss did not improve from 6.16056\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.5234 - val_loss: 6.3292\n",
      "\n",
      "[20210301-2328-10] Learning rate for epoch 74 is 0.0033836751244962215\n",
      "Epoch 74/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8251\n",
      "Epoch 00074: val_loss did not improve from 6.16056\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.8251 - val_loss: 7.8717\n",
      "\n",
      "[20210301-2328-15] Learning rate for epoch 75 is 0.0034263203851878643\n",
      "Epoch 75/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5089\n",
      "Epoch 00075: val_loss did not improve from 6.16056\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 7.5089 - val_loss: 6.4598\n",
      "\n",
      "[20210301-2328-19] Learning rate for epoch 76 is 0.003468865528702736\n",
      "Epoch 76/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8130\n",
      "Epoch 00076: val_loss did not improve from 6.16056\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.8130 - val_loss: 7.2207\n",
      "\n",
      "[20210301-2328-24] Learning rate for epoch 77 is 0.00351131078787148\n",
      "Epoch 77/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6557\n",
      "Epoch 00077: val_loss did not improve from 6.16056\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.6557 - val_loss: 7.0858\n",
      "\n",
      "[20210301-2328-28] Learning rate for epoch 78 is 0.0035536561626940966\n",
      "Epoch 78/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5580\n",
      "Epoch 00078: val_loss did not improve from 6.16056\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.5580 - val_loss: 7.1260\n",
      "\n",
      "[20210301-2328-33] Learning rate for epoch 79 is 0.003595901420339942\n",
      "Epoch 79/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5457\n",
      "Epoch 00079: val_loss improved from 6.16056 to 6.01205, saving model to ./20210301-225844/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 92ms/step - loss: 7.5457 - val_loss: 6.0121\n",
      "\n",
      "[20210301-2328-39] Learning rate for epoch 80 is 0.00363804679363966\n",
      "Epoch 80/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6634\n",
      "Epoch 00080: val_loss did not improve from 6.01205\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.6634 - val_loss: 6.1792\n",
      "\n",
      "[20210301-2328-43] Learning rate for epoch 81 is 0.0036800920497626066\n",
      "Epoch 81/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7965\n",
      "Epoch 00081: val_loss did not improve from 6.01205\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.7965 - val_loss: 6.1225\n",
      "\n",
      "[20210301-2328-48] Learning rate for epoch 82 is 0.003722037188708782\n",
      "Epoch 82/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4347\n",
      "Epoch 00082: val_loss did not improve from 6.01205\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.4347 - val_loss: 7.0240\n",
      "\n",
      "[20210301-2328-52] Learning rate for epoch 83 is 0.003763882676139474\n",
      "Epoch 83/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6113\n",
      "Epoch 00083: val_loss did not improve from 6.01205\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.6113 - val_loss: 6.9162\n",
      "\n",
      "[20210301-2328-57] Learning rate for epoch 84 is 0.0038056280463933945\n",
      "Epoch 84/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2550\n",
      "Epoch 00084: val_loss did not improve from 6.01205\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.2550 - val_loss: 8.8535\n",
      "\n",
      "[20210301-2329-02] Learning rate for epoch 85 is 0.003847273299470544\n",
      "Epoch 85/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5801\n",
      "Epoch 00085: val_loss did not improve from 6.01205\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.5801 - val_loss: 6.6244\n",
      "\n",
      "[20210301-2329-06] Learning rate for epoch 86 is 0.0038888186682015657\n",
      "Epoch 86/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5850\n",
      "Epoch 00086: val_loss did not improve from 6.01205\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.5850 - val_loss: 7.0671\n",
      "\n",
      "[20210301-2329-11] Learning rate for epoch 87 is 0.00393026415258646\n",
      "Epoch 87/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7395\n",
      "Epoch 00087: val_loss did not improve from 6.01205\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.7395 - val_loss: 7.0791\n",
      "\n",
      "[20210301-2329-16] Learning rate for epoch 88 is 0.00397160928696394\n",
      "Epoch 88/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3808\n",
      "Epoch 00088: val_loss did not improve from 6.01205\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.3808 - val_loss: 7.0362\n",
      "\n",
      "[20210301-2329-20] Learning rate for epoch 89 is 0.004012854769825935\n",
      "Epoch 89/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3977\n",
      "Epoch 00089: val_loss did not improve from 6.01205\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.3977 - val_loss: 7.2818\n",
      "\n",
      "[20210301-2329-25] Learning rate for epoch 90 is 0.00405400013551116\n",
      "Epoch 90/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6659\n",
      "Epoch 00090: val_loss did not improve from 6.01205\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.6659 - val_loss: 7.1535\n",
      "\n",
      "[20210301-2329-29] Learning rate for epoch 91 is 0.004095045384019613\n",
      "Epoch 91/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2421\n",
      "Epoch 00091: val_loss did not improve from 6.01205\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.2421 - val_loss: 6.5783\n",
      "\n",
      "[20210301-2329-34] Learning rate for epoch 92 is 0.004135990981012583\n",
      "Epoch 92/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5423\n",
      "Epoch 00092: val_loss did not improve from 6.01205\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.5423 - val_loss: 6.6602\n",
      "\n",
      "[20210301-2329-38] Learning rate for epoch 93 is 0.004176836460828781\n",
      "Epoch 93/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4641\n",
      "Epoch 00093: val_loss did not improve from 6.01205\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.4641 - val_loss: 6.3439\n",
      "\n",
      "[20210301-2329-43] Learning rate for epoch 94 is 0.004217581823468208\n",
      "Epoch 94/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4776\n",
      "Epoch 00094: val_loss did not improve from 6.01205\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 7.4776 - val_loss: 7.2066\n",
      "\n",
      "[20210301-2329-47] Learning rate for epoch 95 is 0.004258227068930864\n",
      "Epoch 95/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1652\n",
      "Epoch 00095: val_loss did not improve from 6.01205\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.1652 - val_loss: 6.7912\n",
      "\n",
      "[20210301-2329-52] Learning rate for epoch 96 is 0.0042987726628780365\n",
      "Epoch 96/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3997\n",
      "Epoch 00096: val_loss did not improve from 6.01205\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.3997 - val_loss: 6.9951\n",
      "\n",
      "[20210301-2329-57] Learning rate for epoch 97 is 0.0043392181396484375\n",
      "Epoch 97/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1604\n",
      "Epoch 00097: val_loss improved from 6.01205 to 5.96954, saving model to ./20210301-225844/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 93ms/step - loss: 7.1604 - val_loss: 5.9695\n",
      "\n",
      "[20210301-2330-02] Learning rate for epoch 98 is 0.004379563499242067\n",
      "Epoch 98/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1561\n",
      "Epoch 00098: val_loss did not improve from 5.96954\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.1561 - val_loss: 6.6814\n",
      "\n",
      "[20210301-2330-06] Learning rate for epoch 99 is 0.004419809207320213\n",
      "Epoch 99/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4378\n",
      "Epoch 00099: val_loss did not improve from 5.96954\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.4378 - val_loss: 7.3098\n",
      "\n",
      "[20210301-2330-11] Learning rate for epoch 100 is 0.004459954332560301\n",
      "Epoch 100/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2991\n",
      "Epoch 00100: val_loss did not improve from 5.96954\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.2991 - val_loss: 6.7947\n",
      "\n",
      "[20210301-2330-16] Learning rate for epoch 101 is 4.4999998749517545e-07\n",
      "Epoch 101/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3654\n",
      "Epoch 00101: val_loss did not improve from 5.96954\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.3654 - val_loss: 6.5070\n",
      "\n",
      "[20210301-2330-20] Learning rate for epoch 102 is 0.00018023152370005846\n",
      "Epoch 102/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1900\n",
      "Epoch 00102: val_loss did not improve from 5.96954\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.1900 - val_loss: 6.0470\n",
      "\n",
      "[20210301-2330-25] Learning rate for epoch 103 is 0.000359613070031628\n",
      "Epoch 103/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0675\n",
      "Epoch 00103: val_loss improved from 5.96954 to 5.77830, saving model to ./20210301-225844/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 87ms/step - loss: 7.0675 - val_loss: 5.7783\n",
      "\n",
      "[20210301-2330-30] Learning rate for epoch 104 is 0.0005385946715250611\n",
      "Epoch 104/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2112\n",
      "Epoch 00104: val_loss improved from 5.77830 to 5.76108, saving model to ./20210301-225844/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 89ms/step - loss: 7.2112 - val_loss: 5.7611\n",
      "\n",
      "[20210301-2330-35] Learning rate for epoch 105 is 0.0007171762990765274\n",
      "Epoch 105/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7420\n",
      "Epoch 00105: val_loss did not improve from 5.76108\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.7420 - val_loss: 5.7872\n",
      "\n",
      "[20210301-2330-40] Learning rate for epoch 106 is 0.0008953579817898571\n",
      "Epoch 106/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8122\n",
      "Epoch 00106: val_loss improved from 5.76108 to 5.73126, saving model to ./20210301-225844/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 6.8122 - val_loss: 5.7313\n",
      "\n",
      "[20210301-2330-45] Learning rate for epoch 107 is 0.0010731397196650505\n",
      "Epoch 107/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8809\n",
      "Epoch 00107: val_loss did not improve from 5.73126\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.8809 - val_loss: 5.7549\n",
      "\n",
      "[20210301-2330-50] Learning rate for epoch 108 is 0.0012505215127021074\n",
      "Epoch 108/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6294\n",
      "Epoch 00108: val_loss improved from 5.73126 to 5.44358, saving model to ./20210301-225844/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 92ms/step - loss: 6.6294 - val_loss: 5.4436\n",
      "\n",
      "[20210301-2330-55] Learning rate for epoch 109 is 0.001427503302693367\n",
      "Epoch 109/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8990\n",
      "Epoch 00109: val_loss did not improve from 5.44358\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.8990 - val_loss: 5.5131\n",
      "\n",
      "[20210301-2330-59] Learning rate for epoch 110 is 0.0016040850896388292\n",
      "Epoch 110/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7094\n",
      "Epoch 00110: val_loss did not improve from 5.44358\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.7094 - val_loss: 5.7109\n",
      "\n",
      "[20210301-2331-04] Learning rate for epoch 111 is 0.001780266989953816\n",
      "Epoch 111/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6598\n",
      "Epoch 00111: val_loss did not improve from 5.44358\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.6598 - val_loss: 5.8170\n",
      "\n",
      "[20210301-2331-09] Learning rate for epoch 112 is 0.0019560488872230053\n",
      "Epoch 112/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8780\n",
      "Epoch 00112: val_loss did not improve from 5.44358\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.8780 - val_loss: 5.6915\n",
      "\n",
      "[20210301-2331-13] Learning rate for epoch 113 is 0.002131430897861719\n",
      "Epoch 113/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8133\n",
      "Epoch 00113: val_loss did not improve from 5.44358\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.8133 - val_loss: 5.6341\n",
      "\n",
      "[20210301-2331-18] Learning rate for epoch 114 is 0.0023064129054546356\n",
      "Epoch 114/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7739\n",
      "Epoch 00114: val_loss improved from 5.44358 to 5.31718, saving model to ./20210301-225844/toe_K1_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 87ms/step - loss: 6.7739 - val_loss: 5.3172\n",
      "\n",
      "[20210301-2331-23] Learning rate for epoch 115 is 0.0024809949100017548\n",
      "Epoch 115/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8640\n",
      "Epoch 00115: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.8640 - val_loss: 6.0517\n",
      "\n",
      "[20210301-2331-28] Learning rate for epoch 116 is 0.0026551769115030766\n",
      "Epoch 116/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0313\n",
      "Epoch 00116: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.0313 - val_loss: 5.6163\n",
      "\n",
      "[20210301-2331-32] Learning rate for epoch 117 is 0.0028289591427892447\n",
      "Epoch 117/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8646\n",
      "Epoch 00117: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.8646 - val_loss: 6.0846\n",
      "\n",
      "[20210301-2331-37] Learning rate for epoch 118 is 0.0030023413710296154\n",
      "Epoch 118/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7900\n",
      "Epoch 00118: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.7900 - val_loss: 6.4430\n",
      "\n",
      "[20210301-2331-41] Learning rate for epoch 119 is 0.003175323596224189\n",
      "Epoch 119/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8738\n",
      "Epoch 00119: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.8738 - val_loss: 6.5753\n",
      "\n",
      "[20210301-2331-46] Learning rate for epoch 120 is 0.003347905818372965\n",
      "Epoch 120/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8404\n",
      "Epoch 00120: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.8404 - val_loss: 6.3160\n",
      "\n",
      "[20210301-2331-50] Learning rate for epoch 121 is 0.0035200880374759436\n",
      "Epoch 121/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0421\n",
      "Epoch 00121: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.0421 - val_loss: 6.0811\n",
      "\n",
      "[20210301-2331-55] Learning rate for epoch 122 is 0.003691870253533125\n",
      "Epoch 122/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0665\n",
      "Epoch 00122: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.0665 - val_loss: 5.8673\n",
      "\n",
      "[20210301-2332-00] Learning rate for epoch 123 is 0.0038632526993751526\n",
      "Epoch 123/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0678\n",
      "Epoch 00123: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.0678 - val_loss: 7.7223\n",
      "\n",
      "[20210301-2332-04] Learning rate for epoch 124 is 0.004034235142171383\n",
      "Epoch 124/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1487\n",
      "Epoch 00124: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.1487 - val_loss: 6.1556\n",
      "\n",
      "[20210301-2332-09] Learning rate for epoch 125 is 0.004204817581921816\n",
      "Epoch 125/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9087\n",
      "Epoch 00125: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.9087 - val_loss: 6.9255\n",
      "\n",
      "[20210301-2332-13] Learning rate for epoch 126 is 0.0043750000186264515\n",
      "Epoch 126/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0585\n",
      "Epoch 00126: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.0585 - val_loss: 6.8165\n",
      "\n",
      "[20210301-2332-18] Learning rate for epoch 127 is 0.0041952175088226795\n",
      "Epoch 127/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0905\n",
      "Epoch 00127: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.0905 - val_loss: 8.1090\n",
      "\n",
      "[20210301-2332-23] Learning rate for epoch 128 is 0.004015835002064705\n",
      "Epoch 128/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9802\n",
      "Epoch 00128: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.9802 - val_loss: 7.4689\n",
      "\n",
      "[20210301-2332-28] Learning rate for epoch 129 is 0.003836852265521884\n",
      "Epoch 129/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1135\n",
      "Epoch 00129: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.1135 - val_loss: 6.6624\n",
      "\n",
      "[20210301-2332-32] Learning rate for epoch 130 is 0.003658269764855504\n",
      "Epoch 130/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1294\n",
      "Epoch 00130: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.1294 - val_loss: 6.8061\n",
      "\n",
      "[20210301-2332-37] Learning rate for epoch 131 is 0.003480087034404278\n",
      "Epoch 131/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0443\n",
      "Epoch 00131: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.0443 - val_loss: 6.6102\n",
      "\n",
      "[20210301-2332-41] Learning rate for epoch 132 is 0.003302304306998849\n",
      "Epoch 132/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0954\n",
      "Epoch 00132: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.0954 - val_loss: 7.1974\n",
      "\n",
      "[20210301-2332-46] Learning rate for epoch 133 is 0.0031249215826392174\n",
      "Epoch 133/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9358\n",
      "Epoch 00133: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.9358 - val_loss: 6.4706\n",
      "\n",
      "[20210301-2332-51] Learning rate for epoch 134 is 0.0029479386284947395\n",
      "Epoch 134/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9430\n",
      "Epoch 00134: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.9430 - val_loss: 5.7831\n",
      "\n",
      "[20210301-2332-55] Learning rate for epoch 135 is 0.0027713559102267027\n",
      "Epoch 135/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8862\n",
      "Epoch 00135: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.8862 - val_loss: 6.6043\n",
      "\n",
      "[20210301-2333-00] Learning rate for epoch 136 is 0.0025951729621738195\n",
      "Epoch 136/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5518\n",
      "Epoch 00136: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.5518 - val_loss: 5.8311\n",
      "\n",
      "[20210301-2333-05] Learning rate for epoch 137 is 0.0024193900171667337\n",
      "Epoch 137/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6295\n",
      "Epoch 00137: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.6295 - val_loss: 6.1867\n",
      "\n",
      "[20210301-2333-09] Learning rate for epoch 138 is 0.0022440070752054453\n",
      "Epoch 138/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9306\n",
      "Epoch 00138: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.9306 - val_loss: 5.8400\n",
      "\n",
      "[20210301-2333-14] Learning rate for epoch 139 is 0.002069024136289954\n",
      "Epoch 139/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5557\n",
      "Epoch 00139: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.5557 - val_loss: 5.9851\n",
      "\n",
      "[20210301-2333-18] Learning rate for epoch 140 is 0.0018944410840049386\n",
      "Epoch 140/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5437\n",
      "Epoch 00140: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.5437 - val_loss: 5.5864\n",
      "\n",
      "[20210301-2333-23] Learning rate for epoch 141 is 0.0017202580347657204\n",
      "Epoch 141/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6740\n",
      "Epoch 00141: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.6740 - val_loss: 6.1568\n",
      "\n",
      "[20210301-2333-28] Learning rate for epoch 142 is 0.0015464748721569777\n",
      "Epoch 142/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5625\n",
      "Epoch 00142: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.5625 - val_loss: 5.9573\n",
      "\n",
      "[20210301-2333-32] Learning rate for epoch 143 is 0.0013730917125940323\n",
      "Epoch 143/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4351\n",
      "Epoch 00143: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.4351 - val_loss: 5.5258\n",
      "\n",
      "[20210301-2333-37] Learning rate for epoch 144 is 0.0012001085560768843\n",
      "Epoch 144/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4196\n",
      "Epoch 00144: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.4196 - val_loss: 5.4917\n",
      "\n",
      "[20210301-2333-41] Learning rate for epoch 145 is 0.0010275252861902118\n",
      "Epoch 145/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3250\n",
      "Epoch 00145: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.3250 - val_loss: 5.6509\n",
      "\n",
      "[20210301-2333-46] Learning rate for epoch 146 is 0.0008553420193493366\n",
      "Epoch 146/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2551\n",
      "Epoch 00146: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.2551 - val_loss: 5.8149\n",
      "\n",
      "[20210301-2333-51] Learning rate for epoch 147 is 0.0006835586973465979\n",
      "Epoch 147/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3243\n",
      "Epoch 00147: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.3243 - val_loss: 5.4477\n",
      "\n",
      "[20210301-2333-55] Learning rate for epoch 148 is 0.0005121753201819956\n",
      "Epoch 148/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3753\n",
      "Epoch 00148: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.3753 - val_loss: 5.4310\n",
      "\n",
      "[20210301-2334-00] Learning rate for epoch 149 is 0.00034119191695936024\n",
      "Epoch 149/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2164\n",
      "Epoch 00149: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.2164 - val_loss: 5.5120\n",
      "\n",
      "[20210301-2334-05] Learning rate for epoch 150 is 0.00017060847312677652\n",
      "Epoch 150/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2796\n",
      "Epoch 00150: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.2796 - val_loss: 5.5146\n",
      "\n",
      "[20210301-2334-09] Learning rate for epoch 151 is 4.249999960848072e-07\n",
      "Epoch 151/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2314\n",
      "Epoch 00151: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.2314 - val_loss: 5.5115\n",
      "\n",
      "[20210301-2334-14] Learning rate for epoch 152 is 0.00017020752420648932\n",
      "Epoch 152/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2268\n",
      "Epoch 00152: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.2268 - val_loss: 5.5141\n",
      "\n",
      "[20210301-2334-18] Learning rate for epoch 153 is 0.0003395900712348521\n",
      "Epoch 153/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1922\n",
      "Epoch 00153: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.1922 - val_loss: 5.5456\n",
      "\n",
      "[20210301-2334-23] Learning rate for epoch 154 is 0.0005085726734250784\n",
      "Epoch 154/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2553\n",
      "Epoch 00154: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.2553 - val_loss: 5.5068\n",
      "\n",
      "[20210301-2334-27] Learning rate for epoch 155 is 0.0006771553307771683\n",
      "Epoch 155/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2293\n",
      "Epoch 00155: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.2293 - val_loss: 5.6068\n",
      "\n",
      "[20210301-2334-32] Learning rate for epoch 156 is 0.0008453379850834608\n",
      "Epoch 156/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2948\n",
      "Epoch 00156: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.2948 - val_loss: 5.4906\n",
      "\n",
      "[20210301-2334-37] Learning rate for epoch 157 is 0.0010131207527592778\n",
      "Epoch 157/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8728\n",
      "Epoch 00157: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.8728 - val_loss: 5.5278\n",
      "\n",
      "[20210301-2334-41] Learning rate for epoch 158 is 0.0011805035173892975\n",
      "Epoch 158/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1625\n",
      "Epoch 00158: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.1625 - val_loss: 5.5507\n",
      "\n",
      "[20210301-2334-46] Learning rate for epoch 159 is 0.0013474862789735198\n",
      "Epoch 159/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1176\n",
      "Epoch 00159: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.1176 - val_loss: 5.6024\n",
      "\n",
      "[20210301-2334-50] Learning rate for epoch 160 is 0.0015140691539272666\n",
      "Epoch 160/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2576\n",
      "Epoch 00160: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.2576 - val_loss: 5.9694\n",
      "\n",
      "[20210301-2334-55] Learning rate for epoch 161 is 0.001680252025835216\n",
      "Epoch 161/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3946\n",
      "Epoch 00161: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.3946 - val_loss: 5.7328\n",
      "\n",
      "[20210301-2335-00] Learning rate for epoch 162 is 0.0018460348946973681\n",
      "Epoch 162/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7412\n",
      "Epoch 00162: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.7412 - val_loss: 6.0363\n",
      "\n",
      "[20210301-2335-04] Learning rate for epoch 163 is 0.0020114178769290447\n",
      "Epoch 163/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6351\n",
      "Epoch 00163: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.6351 - val_loss: 5.5520\n",
      "\n",
      "[20210301-2335-09] Learning rate for epoch 164 is 0.0021764009725302458\n",
      "Epoch 164/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1095\n",
      "Epoch 00164: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.1095 - val_loss: 5.7589\n",
      "\n",
      "[20210301-2335-13] Learning rate for epoch 165 is 0.002340983832255006\n",
      "Epoch 165/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7511\n",
      "Epoch 00165: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.7511 - val_loss: 5.9572\n",
      "\n",
      "[20210301-2335-18] Learning rate for epoch 166 is 0.002505166921764612\n",
      "Epoch 166/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2752\n",
      "Epoch 00166: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.2752 - val_loss: 6.2421\n",
      "\n",
      "[20210301-2335-22] Learning rate for epoch 167 is 0.002668950008228421\n",
      "Epoch 167/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3890\n",
      "Epoch 00167: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.3890 - val_loss: 6.2754\n",
      "\n",
      "[20210301-2335-27] Learning rate for epoch 168 is 0.0028323333244770765\n",
      "Epoch 168/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5120\n",
      "Epoch 00168: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.5120 - val_loss: 6.4226\n",
      "\n",
      "[20210301-2335-32] Learning rate for epoch 169 is 0.002995316404849291\n",
      "Epoch 169/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6889\n",
      "Epoch 00169: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.6889 - val_loss: 6.3604\n",
      "\n",
      "[20210301-2335-36] Learning rate for epoch 170 is 0.0031578997150063515\n",
      "Epoch 170/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5838\n",
      "Epoch 00170: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.5838 - val_loss: 6.1811\n",
      "\n",
      "[20210301-2335-41] Learning rate for epoch 171 is 0.0033200830221176147\n",
      "Epoch 171/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7456\n",
      "Epoch 00171: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.7456 - val_loss: 6.7423\n",
      "\n",
      "[20210301-2335-46] Learning rate for epoch 172 is 0.0034818663261830807\n",
      "Epoch 172/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4445\n",
      "Epoch 00172: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.4445 - val_loss: 6.8312\n",
      "\n",
      "[20210301-2335-50] Learning rate for epoch 173 is 0.0036432496272027493\n",
      "Epoch 173/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7909\n",
      "Epoch 00173: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.7909 - val_loss: 6.4720\n",
      "\n",
      "[20210301-2335-55] Learning rate for epoch 174 is 0.003804233158007264\n",
      "Epoch 174/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8574\n",
      "Epoch 00174: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.8574 - val_loss: 6.5415\n",
      "\n",
      "[20210301-2335-59] Learning rate for epoch 175 is 0.003964816685765982\n",
      "Epoch 175/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5211\n",
      "Epoch 00175: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.5211 - val_loss: 7.3149\n",
      "\n",
      "[20210301-2336-04] Learning rate for epoch 176 is 0.004124999977648258\n",
      "Epoch 176/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8665\n",
      "Epoch 00176: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.8665 - val_loss: 7.4358\n",
      "\n",
      "[20210301-2336-09] Learning rate for epoch 177 is 0.003955216612666845\n",
      "Epoch 177/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8696\n",
      "Epoch 00177: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.8696 - val_loss: 6.6978\n",
      "\n",
      "[20210301-2336-13] Learning rate for epoch 178 is 0.003785833017900586\n",
      "Epoch 178/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7024\n",
      "Epoch 00178: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.7024 - val_loss: 7.0806\n",
      "\n",
      "[20210301-2336-18] Learning rate for epoch 179 is 0.0036168494261801243\n",
      "Epoch 179/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5415\n",
      "Epoch 00179: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.5415 - val_loss: 6.9410\n",
      "\n",
      "[20210301-2336-23] Learning rate for epoch 180 is 0.003448265604674816\n",
      "Epoch 180/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8635\n",
      "Epoch 00180: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.8635 - val_loss: 7.0526\n",
      "\n",
      "[20210301-2336-27] Learning rate for epoch 181 is 0.003280082019045949\n",
      "Epoch 181/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5274\n",
      "Epoch 00181: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.5274 - val_loss: 6.5179\n",
      "\n",
      "[20210301-2336-32] Learning rate for epoch 182 is 0.0031122982036322355\n",
      "Epoch 182/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5431\n",
      "Epoch 00182: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.5431 - val_loss: 6.5386\n",
      "\n",
      "[20210301-2336-36] Learning rate for epoch 183 is 0.002944914624094963\n",
      "Epoch 183/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3972\n",
      "Epoch 00183: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.3972 - val_loss: 6.4595\n",
      "\n",
      "[20210301-2336-41] Learning rate for epoch 184 is 0.0027779308147728443\n",
      "Epoch 184/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3142\n",
      "Epoch 00184: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.3142 - val_loss: 6.0414\n",
      "\n",
      "[20210301-2336-46] Learning rate for epoch 185 is 0.0026113467756658792\n",
      "Epoch 185/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5105\n",
      "Epoch 00185: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.5105 - val_loss: 5.7928\n",
      "\n",
      "[20210301-2336-50] Learning rate for epoch 186 is 0.002445162972435355\n",
      "Epoch 186/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3463\n",
      "Epoch 00186: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.3463 - val_loss: 5.8974\n",
      "\n",
      "[20210301-2336-55] Learning rate for epoch 187 is 0.0022793791722506285\n",
      "Epoch 187/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3639\n",
      "Epoch 00187: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.3639 - val_loss: 6.1415\n",
      "\n",
      "[20210301-2336-59] Learning rate for epoch 188 is 0.0021139951422810555\n",
      "Epoch 188/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2847\n",
      "Epoch 00188: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.2847 - val_loss: 6.0316\n",
      "\n",
      "[20210301-2337-04] Learning rate for epoch 189 is 0.0019490111153572798\n",
      "Epoch 189/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3142\n",
      "Epoch 00189: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.3142 - val_loss: 6.0357\n",
      "\n",
      "[20210301-2337-09] Learning rate for epoch 190 is 0.0017844270914793015\n",
      "Epoch 190/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4832\n",
      "Epoch 00190: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.4832 - val_loss: 5.8151\n",
      "\n",
      "[20210301-2337-13] Learning rate for epoch 191 is 0.0016202429542317986\n",
      "Epoch 191/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1158\n",
      "Epoch 00191: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.1158 - val_loss: 5.9820\n",
      "\n",
      "[20210301-2337-18] Learning rate for epoch 192 is 0.001456458936445415\n",
      "Epoch 192/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8682\n",
      "Epoch 00192: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8682 - val_loss: 5.6481\n",
      "\n",
      "[20210301-2337-22] Learning rate for epoch 193 is 0.001293074688874185\n",
      "Epoch 193/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8751\n",
      "Epoch 00193: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.8751 - val_loss: 5.7588\n",
      "\n",
      "[20210301-2337-27] Learning rate for epoch 194 is 0.0011300905607640743\n",
      "Epoch 194/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2432\n",
      "Epoch 00194: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.2432 - val_loss: 5.8286\n",
      "\n",
      "[20210301-2337-31] Learning rate for epoch 195 is 0.0009675062610767782\n",
      "Epoch 195/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9847\n",
      "Epoch 00195: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.9847 - val_loss: 5.6996\n",
      "\n",
      "[20210301-2337-36] Learning rate for epoch 196 is 0.0008053220226429403\n",
      "Epoch 196/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0950\n",
      "Epoch 00196: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.0950 - val_loss: 5.5832\n",
      "\n",
      "[20210301-2337-41] Learning rate for epoch 197 is 0.0006435376708395779\n",
      "Epoch 197/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9811\n",
      "Epoch 00197: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.9811 - val_loss: 5.6698\n",
      "\n",
      "[20210301-2337-45] Learning rate for epoch 198 is 0.0004821533220820129\n",
      "Epoch 198/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1312\n",
      "Epoch 00198: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.1312 - val_loss: 5.5842\n",
      "\n",
      "[20210301-2337-50] Learning rate for epoch 199 is 0.0003211689181625843\n",
      "Epoch 199/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0898\n",
      "Epoch 00199: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.0898 - val_loss: 5.5795\n",
      "\n",
      "[20210301-2337-54] Learning rate for epoch 200 is 0.00016058447363320738\n",
      "Epoch 200/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7936\n",
      "Epoch 00200: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.7936 - val_loss: 5.5197\n",
      "\n",
      "[20210301-2337-59] Learning rate for epoch 201 is 4.0000000467443897e-07\n",
      "Epoch 201/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8031\n",
      "Epoch 00201: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.8031 - val_loss: 5.5082\n",
      "\n",
      "[20210301-2338-04] Learning rate for epoch 202 is 0.0001601835247129202\n",
      "Epoch 202/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8094\n",
      "Epoch 00202: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.8094 - val_loss: 5.4887\n",
      "\n",
      "[20210301-2338-08] Learning rate for epoch 203 is 0.00031956707243807614\n",
      "Epoch 203/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0778\n",
      "Epoch 00203: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.0778 - val_loss: 5.4810\n",
      "\n",
      "[20210301-2338-13] Learning rate for epoch 204 is 0.00047855067532509565\n",
      "Epoch 204/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8266\n",
      "Epoch 00204: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.8266 - val_loss: 5.5124\n",
      "\n",
      "[20210301-2338-18] Learning rate for epoch 205 is 0.0006371343042701483\n",
      "Epoch 205/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8642\n",
      "Epoch 00205: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8642 - val_loss: 5.6663\n",
      "\n",
      "[20210301-2338-22] Learning rate for epoch 206 is 0.0007953179883770645\n",
      "Epoch 206/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7100\n",
      "Epoch 00206: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.7100 - val_loss: 5.6666\n",
      "\n",
      "[20210301-2338-27] Learning rate for epoch 207 is 0.0009531017276458442\n",
      "Epoch 207/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0843\n",
      "Epoch 00207: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.0843 - val_loss: 5.6761\n",
      "\n",
      "[20210301-2338-31] Learning rate for epoch 208 is 0.0011104855220764875\n",
      "Epoch 208/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9314\n",
      "Epoch 00208: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.9314 - val_loss: 5.6595\n",
      "\n",
      "[20210301-2338-36] Learning rate for epoch 209 is 0.0012674692552536726\n",
      "Epoch 209/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0411\n",
      "Epoch 00209: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.0411 - val_loss: 5.5070\n",
      "\n",
      "[20210301-2338-41] Learning rate for epoch 210 is 0.0014240531018003821\n",
      "Epoch 210/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9226\n",
      "Epoch 00210: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.9226 - val_loss: 5.6731\n",
      "\n",
      "[20210301-2338-45] Learning rate for epoch 211 is 0.0015802369453012943\n",
      "Epoch 211/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1855\n",
      "Epoch 00211: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.1855 - val_loss: 5.4940\n",
      "\n",
      "[20210301-2338-50] Learning rate for epoch 212 is 0.001736020902171731\n",
      "Epoch 212/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1219\n",
      "Epoch 00212: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.1219 - val_loss: 5.6249\n",
      "\n",
      "[20210301-2338-54] Learning rate for epoch 213 is 0.0018914048559963703\n",
      "Epoch 213/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2411\n",
      "Epoch 00213: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.2411 - val_loss: 7.0949\n",
      "\n",
      "[20210301-2338-59] Learning rate for epoch 214 is 0.0020463888067752123\n",
      "Epoch 214/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2068\n",
      "Epoch 00214: val_loss did not improve from 5.31718\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.2068 - val_loss: 6.2315\n",
      "\n",
      " \n",
      " K =  2 \n",
      "\n",
      "Check cache in memory:Y True\n",
      "Check keypoints process01: Tensor(\"args_1:0\", shape=(2,), dtype=int64) (2,) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Check keypoints process02: Tensor(\"Reshape:0\", shape=(1, 2), dtype=int64) (1, 2) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Check keypoints process03: Tensor(\"PyFunc:1\", dtype=float32, device=/job:localhost/replica:0/task:0)\n",
      "Check keypoints process04: Tensor(\"Reshape_1:0\", shape=(2,), dtype=float32)\n",
      "Check augment :Y True\n",
      "Check ds cache[True] and augment[True]\n",
      "Check cache in memory:Y True\n",
      "Check augment :N False\n",
      "Check ds cache[True] and augment[False]\n",
      "best_model_name: ./20210301-225844/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "base_model.trainable :  False\n",
      "\n",
      "[20210301-2339-07] Learning rate for epoch 1 is 0.009999999776482582\n",
      "Epoch 1/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 74.7064\n",
      "Epoch 00001: val_loss improved from inf to 55.92173, saving model to ./20210301-225844/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 5s 275ms/step - loss: 73.8813 - val_loss: 55.9217\n",
      "\n",
      "[20210301-2339-21] Learning rate for epoch 2 is 0.009960000403225422\n",
      "Epoch 2/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 32.3091\n",
      "Epoch 00002: val_loss improved from 55.92173 to 19.86862, saving model to ./20210301-225844/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 31.9484 - val_loss: 19.8686\n",
      "\n",
      "[20210301-2339-26] Learning rate for epoch 3 is 0.009920000098645687\n",
      "Epoch 3/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 20.8006\n",
      "Epoch 00003: val_loss improved from 19.86862 to 14.56973, saving model to ./20210301-225844/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 8s 388ms/step - loss: 20.8006 - val_loss: 14.5697\n",
      "\n",
      "[20210301-2339-37] Learning rate for epoch 4 is 0.009879999794065952\n",
      "Epoch 4/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 18.5465\n",
      "Epoch 00004: val_loss improved from 14.56973 to 13.75159, saving model to ./20210301-225844/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 18.5703 - val_loss: 13.7516\n",
      "\n",
      "[20210301-2339-41] Learning rate for epoch 5 is 0.009840000420808792\n",
      "Epoch 5/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 17.5872\n",
      "Epoch 00005: val_loss improved from 13.75159 to 13.09858, saving model to ./20210301-225844/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 17.5872 - val_loss: 13.0986\n",
      "\n",
      "[20210301-2339-45] Learning rate for epoch 6 is 0.009800000116229057\n",
      "Epoch 6/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 17.2901\n",
      "Epoch 00006: val_loss did not improve from 13.09858\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 17.1494 - val_loss: 13.2918\n",
      "\n",
      "[20210301-2339-49] Learning rate for epoch 7 is 0.009759999811649323\n",
      "Epoch 7/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 16.3696\n",
      "Epoch 00007: val_loss did not improve from 13.09858\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.4842 - val_loss: 13.1721\n",
      "\n",
      "[20210301-2339-52] Learning rate for epoch 8 is 0.009720000438392162\n",
      "Epoch 8/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.5119\n",
      "Epoch 00008: val_loss improved from 13.09858 to 12.94148, saving model to ./20210301-225844/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.5241 - val_loss: 12.9415\n",
      "\n",
      "[20210301-2339-56] Learning rate for epoch 9 is 0.009680000133812428\n",
      "Epoch 9/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 16.4289\n",
      "Epoch 00009: val_loss did not improve from 12.94148\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.4076 - val_loss: 12.9856\n",
      "\n",
      "[20210301-2340-00] Learning rate for epoch 10 is 0.009639999829232693\n",
      "Epoch 10/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.0326\n",
      "Epoch 00010: val_loss improved from 12.94148 to 12.36653, saving model to ./20210301-225844/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 16.0852 - val_loss: 12.3665\n",
      "\n",
      "[20210301-2340-04] Learning rate for epoch 11 is 0.009600000455975533\n",
      "Epoch 11/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.6373\n",
      "Epoch 00011: val_loss improved from 12.36653 to 12.29632, saving model to ./20210301-225844/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 15.6976 - val_loss: 12.2963\n",
      "\n",
      "[20210301-2340-08] Learning rate for epoch 12 is 0.009560000151395798\n",
      "Epoch 12/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 14.8170\n",
      "Epoch 00012: val_loss improved from 12.29632 to 12.16606, saving model to ./20210301-225844/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 14.8344 - val_loss: 12.1661\n",
      "\n",
      "[20210301-2340-12] Learning rate for epoch 13 is 0.009519999846816063\n",
      "Epoch 13/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 14.9502\n",
      "Epoch 00013: val_loss did not improve from 12.16606\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 14.8897 - val_loss: 12.1756\n",
      "\n",
      "[20210301-2340-17] Learning rate for epoch 14 is 0.009479999542236328\n",
      "Epoch 14/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.0765\n",
      "Epoch 00014: val_loss improved from 12.16606 to 12.00850, saving model to ./20210301-225844/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 15.0677 - val_loss: 12.0085\n",
      "\n",
      "[20210301-2340-21] Learning rate for epoch 15 is 0.009440000168979168\n",
      "Epoch 15/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 14.8498\n",
      "Epoch 00015: val_loss did not improve from 12.00850\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 14.9937 - val_loss: 12.2052\n",
      "\n",
      "[20210301-2340-24] Learning rate for epoch 16 is 0.009399999864399433\n",
      "Epoch 16/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 14.6590\n",
      "Epoch 00016: val_loss improved from 12.00850 to 11.99914, saving model to ./20210301-225844/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 14.6179 - val_loss: 11.9991\n",
      "\n",
      "[20210301-2340-28] Learning rate for epoch 17 is 0.009359999559819698\n",
      "Epoch 17/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.7272\n",
      "Epoch 00017: val_loss improved from 11.99914 to 11.93910, saving model to ./20210301-225844/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 14.7272 - val_loss: 11.9391\n",
      "\n",
      "[20210301-2340-32] Learning rate for epoch 18 is 0.009320000186562538\n",
      "Epoch 18/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.1382\n",
      "Epoch 00018: val_loss improved from 11.93910 to 11.50198, saving model to ./20210301-225844/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 15.1464 - val_loss: 11.5020\n",
      "\n",
      "[20210301-2340-37] Learning rate for epoch 19 is 0.009279999881982803\n",
      "Epoch 19/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.8634\n",
      "Epoch 00019: val_loss did not improve from 11.50198\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 14.8634 - val_loss: 11.7988\n",
      "\n",
      "[20210301-2340-40] Learning rate for epoch 20 is 0.009239999577403069\n",
      "Epoch 20/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 14.5696\n",
      "Epoch 00020: val_loss improved from 11.50198 to 11.49929, saving model to ./20210301-225844/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 14.6847 - val_loss: 11.4993\n",
      "\n",
      "[20210301-2340-44] Learning rate for epoch 21 is 0.009200000204145908\n",
      "Epoch 21/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.5671\n",
      "Epoch 00021: val_loss did not improve from 11.49929\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 14.5671 - val_loss: 11.7336\n",
      "\n",
      "[20210301-2340-48] Learning rate for epoch 22 is 0.009159999899566174\n",
      "Epoch 22/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 14.0800\n",
      "Epoch 00022: val_loss did not improve from 11.49929\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 14.1617 - val_loss: 11.9570\n",
      "\n",
      "[20210301-2340-52] Learning rate for epoch 23 is 0.009119999594986439\n",
      "Epoch 23/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 14.1283\n",
      "Epoch 00023: val_loss did not improve from 11.49929\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 14.1904 - val_loss: 11.7834\n",
      "\n",
      "[20210301-2340-55] Learning rate for epoch 24 is 0.009080000221729279\n",
      "Epoch 24/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.3992\n",
      "Epoch 00024: val_loss did not improve from 11.49929\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 14.3992 - val_loss: 11.8583\n",
      "\n",
      "[20210301-2340-59] Learning rate for epoch 25 is 0.009039999917149544\n",
      "Epoch 25/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 14.5368\n",
      "Epoch 00025: val_loss did not improve from 11.49929\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 14.5306 - val_loss: 12.1269\n",
      "\n",
      "[20210301-2341-03] Learning rate for epoch 26 is 0.008999999612569809\n",
      "Epoch 26/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 14.5061\n",
      "Epoch 00026: val_loss did not improve from 11.49929\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 14.3453 - val_loss: 11.7583\n",
      "\n",
      "[20210301-2341-07] Learning rate for epoch 27 is 0.008960000239312649\n",
      "Epoch 27/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 14.3493\n",
      "Epoch 00027: val_loss did not improve from 11.49929\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 14.3803 - val_loss: 11.8921\n",
      "\n",
      "[20210301-2341-10] Learning rate for epoch 28 is 0.008919999934732914\n",
      "Epoch 28/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 14.0835\n",
      "Epoch 00028: val_loss did not improve from 11.49929\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 14.0430 - val_loss: 12.2102\n",
      "\n",
      "[20210301-2341-14] Learning rate for epoch 29 is 0.00887999963015318\n",
      "Epoch 29/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.1343\n",
      "Epoch 00029: val_loss did not improve from 11.49929\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 14.1343 - val_loss: 12.1671\n",
      "\n",
      "[20210301-2341-18] Learning rate for epoch 30 is 0.008840000256896019\n",
      "Epoch 30/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 14.0453\n",
      "Epoch 00030: val_loss did not improve from 11.49929\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 14.1176 - val_loss: 12.0566\n",
      "\n",
      "[20210301-2341-22] Learning rate for epoch 31 is 0.008799999952316284\n",
      "Epoch 31/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.3223\n",
      "Epoch 00031: val_loss did not improve from 11.49929\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 14.3223 - val_loss: 12.1909\n",
      "\n",
      "[20210301-2341-25] Learning rate for epoch 32 is 0.00875999964773655\n",
      "Epoch 32/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 13.8884\n",
      "Epoch 00032: val_loss did not improve from 11.49929\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 13.9872 - val_loss: 11.7673\n",
      "\n",
      "[20210301-2341-29] Learning rate for epoch 33 is 0.00872000027447939\n",
      "Epoch 33/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 13.8028\n",
      "Epoch 00033: val_loss did not improve from 11.49929\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 13.8192 - val_loss: 12.1868\n",
      "\n",
      "[20210301-2341-33] Learning rate for epoch 34 is 0.008679999969899654\n",
      "Epoch 34/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 14.1121\n",
      "Epoch 00034: val_loss did not improve from 11.49929\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 14.1558 - val_loss: 12.2665\n",
      "\n",
      "[20210301-2341-37] Learning rate for epoch 35 is 0.00863999966531992\n",
      "Epoch 35/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 13.9855\n",
      "Epoch 00035: val_loss did not improve from 11.49929\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 13.9304 - val_loss: 11.9728\n",
      "\n",
      "[20210301-2341-40] Learning rate for epoch 36 is 0.00860000029206276\n",
      "Epoch 36/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 14.1455\n",
      "Epoch 00036: val_loss did not improve from 11.49929\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 14.1262 - val_loss: 11.5519\n",
      "\n",
      "[20210301-2341-44] Learning rate for epoch 37 is 0.008559999987483025\n",
      "Epoch 37/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 13.9596\n",
      "Epoch 00037: val_loss did not improve from 11.49929\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 13.9327 - val_loss: 12.0141\n",
      "\n",
      "[20210301-2341-48] Learning rate for epoch 38 is 0.00851999968290329\n",
      "Epoch 38/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 14.0903\n",
      "Epoch 00038: val_loss did not improve from 11.49929\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 13.9288 - val_loss: 12.6689\n",
      "\n",
      "[20210301-2341-52] Learning rate for epoch 39 is 0.00848000030964613\n",
      "Epoch 39/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.9643\n",
      "Epoch 00039: val_loss did not improve from 11.49929\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 13.9643 - val_loss: 12.4733\n",
      "\n",
      "[20210301-2341-55] Learning rate for epoch 40 is 0.008440000005066395\n",
      "Epoch 40/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 13.6888\n",
      "Epoch 00040: val_loss did not improve from 11.49929\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 13.6691 - val_loss: 12.2816\n",
      "\n",
      "*********************************** Start fine tune ***********************************\n",
      "tt: 195, nt:46, total layers:241\n",
      "*********************************** Start fine tune ***********************************\n",
      "\n",
      "[20210301-2341-59] Learning rate for epoch 1 is 4.999999987376214e-07\n",
      "Epoch 1/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.1914\n",
      "Epoch 00001: val_loss did not improve from 11.49929\n",
      "20/20 [==============================] - 16s 779ms/step - loss: 15.1914 - val_loss: 12.4896\n",
      "\n",
      "[20210301-2342-31] Learning rate for epoch 2 is 5.044450517743826e-05\n",
      "Epoch 2/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.9363\n",
      "Epoch 00002: val_loss did not improve from 11.49929\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 14.9363 - val_loss: 12.8464\n",
      "\n",
      "[20210301-2342-36] Learning rate for epoch 3 is 0.00010028902033809572\n",
      "Epoch 3/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.6996\n",
      "Epoch 00003: val_loss did not improve from 11.49929\n",
      "20/20 [==============================] - 17s 839ms/step - loss: 14.6996 - val_loss: 12.9628\n",
      "\n",
      "[20210301-2342-56] Learning rate for epoch 4 is 0.00015003354928921908\n",
      "Epoch 4/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.2239\n",
      "Epoch 00004: val_loss did not improve from 11.49929\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 14.2239 - val_loss: 13.3885\n",
      "\n",
      "[20210301-2343-01] Learning rate for epoch 5 is 0.0001996780774788931\n",
      "Epoch 5/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.2535\n",
      "Epoch 00005: val_loss did not improve from 11.49929\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 13.2535 - val_loss: 13.1458\n",
      "\n",
      "[20210301-2343-05] Learning rate for epoch 6 is 0.000249222619459033\n",
      "Epoch 6/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 12.9531\n",
      "Epoch 00006: val_loss did not improve from 11.49929\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 12.9531 - val_loss: 11.8746\n",
      "\n",
      "[20210301-2343-10] Learning rate for epoch 7 is 0.0002986671752296388\n",
      "Epoch 7/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 12.4360\n",
      "Epoch 00007: val_loss improved from 11.49929 to 11.10202, saving model to ./20210301-225844/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 91ms/step - loss: 12.4360 - val_loss: 11.1020\n",
      "\n",
      "[20210301-2343-15] Learning rate for epoch 8 is 0.00034801175934262574\n",
      "Epoch 8/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 12.4007\n",
      "Epoch 00008: val_loss improved from 11.10202 to 10.16336, saving model to ./20210301-225844/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 89ms/step - loss: 12.4007 - val_loss: 10.1634\n",
      "\n",
      "[20210301-2343-20] Learning rate for epoch 9 is 0.00039725631359033287\n",
      "Epoch 9/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.8105\n",
      "Epoch 00009: val_loss improved from 10.16336 to 9.93043, saving model to ./20210301-225844/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 92ms/step - loss: 11.8105 - val_loss: 9.9304\n",
      "\n",
      "[20210301-2343-25] Learning rate for epoch 10 is 0.0004464008961804211\n",
      "Epoch 10/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.4003\n",
      "Epoch 00010: val_loss improved from 9.93043 to 9.85102, saving model to ./20210301-225844/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 89ms/step - loss: 11.4003 - val_loss: 9.8510\n",
      "\n",
      "[20210301-2343-30] Learning rate for epoch 11 is 0.0004954455071128905\n",
      "Epoch 11/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.7724\n",
      "Epoch 00011: val_loss improved from 9.85102 to 9.83578, saving model to ./20210301-225844/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 11.7724 - val_loss: 9.8358\n",
      "\n",
      "[20210301-2343-36] Learning rate for epoch 12 is 0.0005443901172839105\n",
      "Epoch 12/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.2917\n",
      "Epoch 00012: val_loss improved from 9.83578 to 9.64392, saving model to ./20210301-225844/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 92ms/step - loss: 11.2917 - val_loss: 9.6439\n",
      "\n",
      "[20210301-2343-41] Learning rate for epoch 13 is 0.0005932347266934812\n",
      "Epoch 13/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.7114\n",
      "Epoch 00013: val_loss improved from 9.64392 to 9.39032, saving model to ./20210301-225844/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 11.7114 - val_loss: 9.3903\n",
      "\n",
      "[20210301-2343-46] Learning rate for epoch 14 is 0.0006419793353416026\n",
      "Epoch 14/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.4404\n",
      "Epoch 00014: val_loss did not improve from 9.39032\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 11.4404 - val_loss: 9.5034\n",
      "\n",
      "[20210301-2343-50] Learning rate for epoch 15 is 0.0006906240014359355\n",
      "Epoch 15/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.2128\n",
      "Epoch 00015: val_loss did not improve from 9.39032\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 11.2128 - val_loss: 9.4107\n",
      "\n",
      "[20210301-2343-55] Learning rate for epoch 16 is 0.0007391686085611582\n",
      "Epoch 16/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.6650\n",
      "Epoch 00016: val_loss improved from 9.39032 to 8.75765, saving model to ./20210301-225844/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 10.6650 - val_loss: 8.7576\n",
      "\n",
      "[20210301-2344-00] Learning rate for epoch 17 is 0.0007876132731325924\n",
      "Epoch 17/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.0541\n",
      "Epoch 00017: val_loss improved from 8.75765 to 8.72396, saving model to ./20210301-225844/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 11.0541 - val_loss: 8.7240\n",
      "\n",
      "[20210301-2344-05] Learning rate for epoch 18 is 0.0008359579369425774\n",
      "Epoch 18/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.5431\n",
      "Epoch 00018: val_loss did not improve from 8.72396\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 10.5431 - val_loss: 9.8098\n",
      "\n",
      "[20210301-2344-10] Learning rate for epoch 19 is 0.000884202599991113\n",
      "Epoch 19/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.5717\n",
      "Epoch 00019: val_loss improved from 8.72396 to 8.36426, saving model to ./20210301-225844/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 87ms/step - loss: 10.5717 - val_loss: 8.3643\n",
      "\n",
      "[20210301-2344-15] Learning rate for epoch 20 is 0.0009323473204858601\n",
      "Epoch 20/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.3384\n",
      "Epoch 00020: val_loss did not improve from 8.36426\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 10.3384 - val_loss: 9.1311\n",
      "\n",
      "[20210301-2344-19] Learning rate for epoch 21 is 0.000980391982011497\n",
      "Epoch 21/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.1857\n",
      "Epoch 00021: val_loss did not improve from 8.36426\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 10.1857 - val_loss: 8.7008\n",
      "\n",
      "[20210301-2344-24] Learning rate for epoch 22 is 0.0010283367009833455\n",
      "Epoch 22/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.0113\n",
      "Epoch 00022: val_loss did not improve from 8.36426\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 10.0113 - val_loss: 9.1198\n",
      "\n",
      "[20210301-2344-29] Learning rate for epoch 23 is 0.0010761814191937447\n",
      "Epoch 23/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.9387\n",
      "Epoch 00023: val_loss did not improve from 8.36426\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 9.9387 - val_loss: 9.4517\n",
      "\n",
      "[20210301-2344-33] Learning rate for epoch 24 is 0.0011239261366426945\n",
      "Epoch 24/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.0340\n",
      "Epoch 00024: val_loss did not improve from 8.36426\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 10.0340 - val_loss: 9.8136\n",
      "\n",
      "[20210301-2344-38] Learning rate for epoch 25 is 0.001171570853330195\n",
      "Epoch 25/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.8150\n",
      "Epoch 00025: val_loss did not improve from 8.36426\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 9.8150 - val_loss: 9.4129\n",
      "\n",
      "[20210301-2344-42] Learning rate for epoch 26 is 0.001219115569256246\n",
      "Epoch 26/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.6011\n",
      "Epoch 00026: val_loss did not improve from 8.36426\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 9.6011 - val_loss: 10.1899\n",
      "\n",
      "[20210301-2344-47] Learning rate for epoch 27 is 0.0012665604008361697\n",
      "Epoch 27/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.4901\n",
      "Epoch 00027: val_loss did not improve from 8.36426\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 9.4901 - val_loss: 10.8426\n",
      "\n",
      "[20210301-2344-52] Learning rate for epoch 28 is 0.0013139051152393222\n",
      "Epoch 28/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.3326\n",
      "Epoch 00028: val_loss did not improve from 8.36426\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 9.3326 - val_loss: 10.1534\n",
      "\n",
      "[20210301-2344-56] Learning rate for epoch 29 is 0.0013611499452963471\n",
      "Epoch 29/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.2737\n",
      "Epoch 00029: val_loss did not improve from 8.36426\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 9.2737 - val_loss: 10.3929\n",
      "\n",
      "[20210301-2345-01] Learning rate for epoch 30 is 0.001408294658176601\n",
      "Epoch 30/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.2347\n",
      "Epoch 00030: val_loss did not improve from 8.36426\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 9.2347 - val_loss: 9.0445\n",
      "\n",
      "[20210301-2345-05] Learning rate for epoch 31 is 0.0014553394867107272\n",
      "Epoch 31/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.1443\n",
      "Epoch 00031: val_loss improved from 8.36426 to 7.37603, saving model to ./20210301-225844/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 87ms/step - loss: 9.1443 - val_loss: 7.3760\n",
      "\n",
      "[20210301-2345-11] Learning rate for epoch 32 is 0.0015022843144834042\n",
      "Epoch 32/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.1492\n",
      "Epoch 00032: val_loss did not improve from 7.37603\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 9.1492 - val_loss: 7.4730\n",
      "\n",
      "[20210301-2345-15] Learning rate for epoch 33 is 0.0015491291414946318\n",
      "Epoch 33/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.8846\n",
      "Epoch 00033: val_loss did not improve from 7.37603\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 8.8846 - val_loss: 8.3763\n",
      "\n",
      "[20210301-2345-20] Learning rate for epoch 34 is 0.00159587396774441\n",
      "Epoch 34/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.8230\n",
      "Epoch 00034: val_loss did not improve from 7.37603\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 8.8230 - val_loss: 8.3686\n",
      "\n",
      "[20210301-2345-24] Learning rate for epoch 35 is 0.001642518793232739\n",
      "Epoch 35/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.9452\n",
      "Epoch 00035: val_loss did not improve from 7.37603\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 8.9452 - val_loss: 9.7690\n",
      "\n",
      "[20210301-2345-29] Learning rate for epoch 36 is 0.0016890636179596186\n",
      "Epoch 36/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.6151\n",
      "Epoch 00036: val_loss did not improve from 7.37603\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 8.6151 - val_loss: 7.8057\n",
      "\n",
      "[20210301-2345-34] Learning rate for epoch 37 is 0.0017355084419250488\n",
      "Epoch 37/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.8216\n",
      "Epoch 00037: val_loss did not improve from 7.37603\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 8.8216 - val_loss: 7.7934\n",
      "\n",
      "[20210301-2345-38] Learning rate for epoch 38 is 0.0017818533815443516\n",
      "Epoch 38/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.6496\n",
      "Epoch 00038: val_loss did not improve from 7.37603\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 8.6496 - val_loss: 8.8662\n",
      "\n",
      "[20210301-2345-43] Learning rate for epoch 39 is 0.0018280982039868832\n",
      "Epoch 39/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.6464\n",
      "Epoch 00039: val_loss did not improve from 7.37603\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 8.6464 - val_loss: 8.4134\n",
      "\n",
      "[20210301-2345-47] Learning rate for epoch 40 is 0.0018742431420832872\n",
      "Epoch 40/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.5443\n",
      "Epoch 00040: val_loss improved from 7.37603 to 6.74899, saving model to ./20210301-225844/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 8.5443 - val_loss: 6.7490\n",
      "\n",
      "[20210301-2345-52] Learning rate for epoch 41 is 0.0019202879630029202\n",
      "Epoch 41/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.4357\n",
      "Epoch 00041: val_loss did not improve from 6.74899\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 8.4357 - val_loss: 7.0572\n",
      "\n",
      "[20210301-2345-57] Learning rate for epoch 42 is 0.0019662328995764256\n",
      "Epoch 42/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.2172\n",
      "Epoch 00042: val_loss did not improve from 6.74899\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 8.2172 - val_loss: 7.5778\n",
      "\n",
      "[20210301-2346-01] Learning rate for epoch 43 is 0.00201207771897316\n",
      "Epoch 43/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.4350\n",
      "Epoch 00043: val_loss did not improve from 6.74899\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 8.4350 - val_loss: 7.2279\n",
      "\n",
      "[20210301-2346-06] Learning rate for epoch 44 is 0.0020578226540237665\n",
      "Epoch 44/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.2778\n",
      "Epoch 00044: val_loss did not improve from 6.74899\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 8.2778 - val_loss: 7.1988\n",
      "\n",
      "[20210301-2346-11] Learning rate for epoch 45 is 0.0021034677047282457\n",
      "Epoch 45/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.4686\n",
      "Epoch 00045: val_loss did not improve from 6.74899\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 8.4686 - val_loss: 7.3406\n",
      "\n",
      "[20210301-2346-15] Learning rate for epoch 46 is 0.002149012638255954\n",
      "Epoch 46/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.2415\n",
      "Epoch 00046: val_loss did not improve from 6.74899\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 8.2415 - val_loss: 8.3870\n",
      "\n",
      "[20210301-2346-20] Learning rate for epoch 47 is 0.0021944576874375343\n",
      "Epoch 47/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.3487\n",
      "Epoch 00047: val_loss did not improve from 6.74899\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 8.3487 - val_loss: 6.7602\n",
      "\n",
      "[20210301-2346-24] Learning rate for epoch 48 is 0.0022398026194423437\n",
      "Epoch 48/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.1015\n",
      "Epoch 00048: val_loss did not improve from 6.74899\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 8.1015 - val_loss: 7.5378\n",
      "\n",
      "[20210301-2346-29] Learning rate for epoch 49 is 0.002285047434270382\n",
      "Epoch 49/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.9510\n",
      "Epoch 00049: val_loss did not improve from 6.74899\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.9510 - val_loss: 7.1996\n",
      "\n",
      "[20210301-2346-33] Learning rate for epoch 50 is 0.0023301925975829363\n",
      "Epoch 50/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.0278\n",
      "Epoch 00050: val_loss did not improve from 6.74899\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 8.0278 - val_loss: 7.2948\n",
      "\n",
      "[20210301-2346-38] Learning rate for epoch 51 is 0.002375237410888076\n",
      "Epoch 51/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.0384\n",
      "Epoch 00051: val_loss improved from 6.74899 to 6.57878, saving model to ./20210301-225844/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 8.0384 - val_loss: 6.5788\n",
      "\n",
      "[20210301-2346-43] Learning rate for epoch 52 is 0.0024201825726777315\n",
      "Epoch 52/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.2779\n",
      "Epoch 00052: val_loss improved from 6.57878 to 6.39424, saving model to ./20210301-225844/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 91ms/step - loss: 8.2779 - val_loss: 6.3942\n",
      "\n",
      "[20210301-2346-48] Learning rate for epoch 53 is 0.002465027617290616\n",
      "Epoch 53/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.0417\n",
      "Epoch 00053: val_loss improved from 6.39424 to 6.11427, saving model to ./20210301-225844/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 91ms/step - loss: 8.0417 - val_loss: 6.1143\n",
      "\n",
      "[20210301-2346-53] Learning rate for epoch 54 is 0.0025097725447267294\n",
      "Epoch 54/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.1597\n",
      "Epoch 00054: val_loss did not improve from 6.11427\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 8.1597 - val_loss: 6.5784\n",
      "\n",
      "[20210301-2346-58] Learning rate for epoch 55 is 0.0025544175878167152\n",
      "Epoch 55/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.1077\n",
      "Epoch 00055: val_loss did not improve from 6.11427\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 8.1077 - val_loss: 6.1159\n",
      "\n",
      "[20210301-2347-03] Learning rate for epoch 56 is 0.00259896251372993\n",
      "Epoch 56/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.3481\n",
      "Epoch 00056: val_loss did not improve from 6.11427\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 8.3481 - val_loss: 7.9281\n",
      "\n",
      "[20210301-2347-07] Learning rate for epoch 57 is 0.0026434077881276608\n",
      "Epoch 57/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.1180\n",
      "Epoch 00057: val_loss did not improve from 6.11427\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 8.1180 - val_loss: 8.3155\n",
      "\n",
      "[20210301-2347-12] Learning rate for epoch 58 is 0.0026877527125179768\n",
      "Epoch 58/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.9779\n",
      "Epoch 00058: val_loss did not improve from 6.11427\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.9779 - val_loss: 7.0506\n",
      "\n",
      "[20210301-2347-16] Learning rate for epoch 59 is 0.0027319977525621653\n",
      "Epoch 59/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.9823\n",
      "Epoch 00059: val_loss did not improve from 6.11427\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.9823 - val_loss: 6.6315\n",
      "\n",
      "[20210301-2347-22] Learning rate for epoch 60 is 0.0027761429082602262\n",
      "Epoch 60/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7291\n",
      "Epoch 00060: val_loss did not improve from 6.11427\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.7291 - val_loss: 6.4975\n",
      "\n",
      "[20210301-2347-26] Learning rate for epoch 61 is 0.002820187946781516\n",
      "Epoch 61/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8010\n",
      "Epoch 00061: val_loss did not improve from 6.11427\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.8010 - val_loss: 7.8501\n",
      "\n",
      "[20210301-2347-31] Learning rate for epoch 62 is 0.0028641331009566784\n",
      "Epoch 62/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8498\n",
      "Epoch 00062: val_loss did not improve from 6.11427\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.8498 - val_loss: 7.0511\n",
      "\n",
      "[20210301-2347-36] Learning rate for epoch 63 is 0.0029079781379550695\n",
      "Epoch 63/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8434\n",
      "Epoch 00063: val_loss did not improve from 6.11427\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.8434 - val_loss: 6.5953\n",
      "\n",
      "[20210301-2347-40] Learning rate for epoch 64 is 0.002951723290607333\n",
      "Epoch 64/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8795\n",
      "Epoch 00064: val_loss did not improve from 6.11427\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 7.8795 - val_loss: 6.1349\n",
      "\n",
      "[20210301-2347-45] Learning rate for epoch 65 is 0.0029953685589134693\n",
      "Epoch 65/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8460\n",
      "Epoch 00065: val_loss did not improve from 6.11427\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.8460 - val_loss: 6.7191\n",
      "\n",
      "[20210301-2347-49] Learning rate for epoch 66 is 0.0030389137100428343\n",
      "Epoch 66/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8441\n",
      "Epoch 00066: val_loss improved from 6.11427 to 6.09074, saving model to ./20210301-225844/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 92ms/step - loss: 7.8441 - val_loss: 6.0907\n",
      "\n",
      "[20210301-2347-55] Learning rate for epoch 67 is 0.003082358743995428\n",
      "Epoch 67/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.9829\n",
      "Epoch 00067: val_loss did not improve from 6.09074\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.9829 - val_loss: 6.6795\n",
      "\n",
      "[20210301-2347-59] Learning rate for epoch 68 is 0.0031257038936018944\n",
      "Epoch 68/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.9495\n",
      "Epoch 00068: val_loss did not improve from 6.09074\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.9495 - val_loss: 6.1920\n",
      "\n",
      "[20210301-2348-04] Learning rate for epoch 69 is 0.003168949158862233\n",
      "Epoch 69/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7061\n",
      "Epoch 00069: val_loss did not improve from 6.09074\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.7061 - val_loss: 6.2965\n",
      "\n",
      "[20210301-2348-08] Learning rate for epoch 70 is 0.0032120943069458008\n",
      "Epoch 70/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6601\n",
      "Epoch 00070: val_loss did not improve from 6.09074\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.6601 - val_loss: 7.2503\n",
      "\n",
      "[20210301-2348-13] Learning rate for epoch 71 is 0.003255139570683241\n",
      "Epoch 71/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6063\n",
      "Epoch 00071: val_loss did not improve from 6.09074\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.6063 - val_loss: 6.4270\n",
      "\n",
      "[20210301-2348-18] Learning rate for epoch 72 is 0.00329808471724391\n",
      "Epoch 72/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6946\n",
      "Epoch 00072: val_loss did not improve from 6.09074\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.6946 - val_loss: 6.2769\n",
      "\n",
      "[20210301-2348-22] Learning rate for epoch 73 is 0.0033409299794584513\n",
      "Epoch 73/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5942\n",
      "Epoch 00073: val_loss did not improve from 6.09074\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.5942 - val_loss: 7.1001\n",
      "\n",
      "[20210301-2348-27] Learning rate for epoch 74 is 0.0033836751244962215\n",
      "Epoch 74/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6467\n",
      "Epoch 00074: val_loss did not improve from 6.09074\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.6467 - val_loss: 6.2113\n",
      "\n",
      "[20210301-2348-31] Learning rate for epoch 75 is 0.0034263203851878643\n",
      "Epoch 75/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4805\n",
      "Epoch 00075: val_loss did not improve from 6.09074\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.4805 - val_loss: 6.4742\n",
      "\n",
      "[20210301-2348-36] Learning rate for epoch 76 is 0.003468865528702736\n",
      "Epoch 76/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6556\n",
      "Epoch 00076: val_loss did not improve from 6.09074\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.6556 - val_loss: 6.1875\n",
      "\n",
      "[20210301-2348-41] Learning rate for epoch 77 is 0.00351131078787148\n",
      "Epoch 77/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6454\n",
      "Epoch 00077: val_loss did not improve from 6.09074\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.6454 - val_loss: 6.6987\n",
      "\n",
      "[20210301-2348-46] Learning rate for epoch 78 is 0.0035536561626940966\n",
      "Epoch 78/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7484\n",
      "Epoch 00078: val_loss did not improve from 6.09074\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 7.7484 - val_loss: 6.3876\n",
      "\n",
      "[20210301-2348-50] Learning rate for epoch 79 is 0.003595901420339942\n",
      "Epoch 79/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6401\n",
      "Epoch 00079: val_loss did not improve from 6.09074\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.6401 - val_loss: 6.2084\n",
      "\n",
      "[20210301-2348-55] Learning rate for epoch 80 is 0.00363804679363966\n",
      "Epoch 80/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5137\n",
      "Epoch 00080: val_loss did not improve from 6.09074\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.5137 - val_loss: 6.8079\n",
      "\n",
      "[20210301-2348-59] Learning rate for epoch 81 is 0.0036800920497626066\n",
      "Epoch 81/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6457\n",
      "Epoch 00081: val_loss did not improve from 6.09074\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.6457 - val_loss: 7.1823\n",
      "\n",
      "[20210301-2349-04] Learning rate for epoch 82 is 0.003722037188708782\n",
      "Epoch 82/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6228\n",
      "Epoch 00082: val_loss did not improve from 6.09074\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.6228 - val_loss: 7.7636\n",
      "\n",
      "[20210301-2349-08] Learning rate for epoch 83 is 0.003763882676139474\n",
      "Epoch 83/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6850\n",
      "Epoch 00083: val_loss did not improve from 6.09074\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.6850 - val_loss: 6.6823\n",
      "\n",
      "[20210301-2349-13] Learning rate for epoch 84 is 0.0038056280463933945\n",
      "Epoch 84/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5877\n",
      "Epoch 00084: val_loss did not improve from 6.09074\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.5877 - val_loss: 6.7981\n",
      "\n",
      "[20210301-2349-17] Learning rate for epoch 85 is 0.003847273299470544\n",
      "Epoch 85/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5551\n",
      "Epoch 00085: val_loss did not improve from 6.09074\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.5551 - val_loss: 6.4437\n",
      "\n",
      "[20210301-2349-22] Learning rate for epoch 86 is 0.0038888186682015657\n",
      "Epoch 86/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4367\n",
      "Epoch 00086: val_loss did not improve from 6.09074\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.4367 - val_loss: 6.5769\n",
      "\n",
      "[20210301-2349-27] Learning rate for epoch 87 is 0.00393026415258646\n",
      "Epoch 87/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7431\n",
      "Epoch 00087: val_loss did not improve from 6.09074\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.7431 - val_loss: 7.3074\n",
      "\n",
      "[20210301-2349-31] Learning rate for epoch 88 is 0.00397160928696394\n",
      "Epoch 88/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6800\n",
      "Epoch 00088: val_loss did not improve from 6.09074\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.6800 - val_loss: 6.5290\n",
      "\n",
      "[20210301-2349-36] Learning rate for epoch 89 is 0.004012854769825935\n",
      "Epoch 89/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4558\n",
      "Epoch 00089: val_loss did not improve from 6.09074\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.4558 - val_loss: 6.8582\n",
      "\n",
      "[20210301-2349-40] Learning rate for epoch 90 is 0.00405400013551116\n",
      "Epoch 90/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2534\n",
      "Epoch 00090: val_loss did not improve from 6.09074\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.2534 - val_loss: 6.9816\n",
      "\n",
      "[20210301-2349-45] Learning rate for epoch 91 is 0.004095045384019613\n",
      "Epoch 91/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5298\n",
      "Epoch 00091: val_loss did not improve from 6.09074\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.5298 - val_loss: 7.3156\n",
      "\n",
      "[20210301-2349-50] Learning rate for epoch 92 is 0.004135990981012583\n",
      "Epoch 92/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2370\n",
      "Epoch 00092: val_loss did not improve from 6.09074\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.2370 - val_loss: 7.6746\n",
      "\n",
      "[20210301-2349-54] Learning rate for epoch 93 is 0.004176836460828781\n",
      "Epoch 93/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3080\n",
      "Epoch 00093: val_loss did not improve from 6.09074\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.3080 - val_loss: 6.5046\n",
      "\n",
      "[20210301-2349-59] Learning rate for epoch 94 is 0.004217581823468208\n",
      "Epoch 94/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4723\n",
      "Epoch 00094: val_loss did not improve from 6.09074\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.4723 - val_loss: 8.3853\n",
      "\n",
      "[20210301-2350-03] Learning rate for epoch 95 is 0.004258227068930864\n",
      "Epoch 95/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3484\n",
      "Epoch 00095: val_loss did not improve from 6.09074\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.3484 - val_loss: 85.2160\n",
      "\n",
      "[20210301-2350-08] Learning rate for epoch 96 is 0.0042987726628780365\n",
      "Epoch 96/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5636\n",
      "Epoch 00096: val_loss did not improve from 6.09074\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.5636 - val_loss: 8.6840\n",
      "\n",
      "[20210301-2350-13] Learning rate for epoch 97 is 0.0043392181396484375\n",
      "Epoch 97/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3875\n",
      "Epoch 00097: val_loss did not improve from 6.09074\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.3875 - val_loss: 6.7960\n",
      "\n",
      "[20210301-2350-17] Learning rate for epoch 98 is 0.004379563499242067\n",
      "Epoch 98/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2985\n",
      "Epoch 00098: val_loss did not improve from 6.09074\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.2985 - val_loss: 7.3639\n",
      "\n",
      "[20210301-2350-22] Learning rate for epoch 99 is 0.004419809207320213\n",
      "Epoch 99/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3336\n",
      "Epoch 00099: val_loss did not improve from 6.09074\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.3336 - val_loss: 6.5334\n",
      "\n",
      "[20210301-2350-26] Learning rate for epoch 100 is 0.004459954332560301\n",
      "Epoch 100/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1843\n",
      "Epoch 00100: val_loss did not improve from 6.09074\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.1843 - val_loss: 6.7716\n",
      "\n",
      "[20210301-2350-31] Learning rate for epoch 101 is 4.4999998749517545e-07\n",
      "Epoch 101/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2123\n",
      "Epoch 00101: val_loss did not improve from 6.09074\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.2123 - val_loss: 6.6172\n",
      "\n",
      "[20210301-2350-36] Learning rate for epoch 102 is 0.00018023152370005846\n",
      "Epoch 102/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2015\n",
      "Epoch 00102: val_loss did not improve from 6.09074\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.2015 - val_loss: 6.2718\n",
      "\n",
      "[20210301-2350-40] Learning rate for epoch 103 is 0.000359613070031628\n",
      "Epoch 103/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1072\n",
      "Epoch 00103: val_loss did not improve from 6.09074\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.1072 - val_loss: 6.1653\n",
      "\n",
      "[20210301-2350-45] Learning rate for epoch 104 is 0.0005385946715250611\n",
      "Epoch 104/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0718\n",
      "Epoch 00104: val_loss improved from 6.09074 to 6.00326, saving model to ./20210301-225844/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 89ms/step - loss: 7.0718 - val_loss: 6.0033\n",
      "\n",
      "[20210301-2350-50] Learning rate for epoch 105 is 0.0007171762990765274\n",
      "Epoch 105/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7860\n",
      "Epoch 00105: val_loss improved from 6.00326 to 5.96170, saving model to ./20210301-225844/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 6.7860 - val_loss: 5.9617\n",
      "\n",
      "[20210301-2350-55] Learning rate for epoch 106 is 0.0008953579817898571\n",
      "Epoch 106/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7262\n",
      "Epoch 00106: val_loss did not improve from 5.96170\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.7262 - val_loss: 6.0054\n",
      "\n",
      "[20210301-2351-00] Learning rate for epoch 107 is 0.0010731397196650505\n",
      "Epoch 107/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9553\n",
      "Epoch 00107: val_loss improved from 5.96170 to 5.90817, saving model to ./20210301-225844/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 6.9553 - val_loss: 5.9082\n",
      "\n",
      "[20210301-2351-05] Learning rate for epoch 108 is 0.0012505215127021074\n",
      "Epoch 108/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8155\n",
      "Epoch 00108: val_loss did not improve from 5.90817\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.8155 - val_loss: 6.1436\n",
      "\n",
      "[20210301-2351-10] Learning rate for epoch 109 is 0.001427503302693367\n",
      "Epoch 109/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8371\n",
      "Epoch 00109: val_loss did not improve from 5.90817\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.8371 - val_loss: 5.9835\n",
      "\n",
      "[20210301-2351-14] Learning rate for epoch 110 is 0.0016040850896388292\n",
      "Epoch 110/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8936\n",
      "Epoch 00110: val_loss did not improve from 5.90817\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.8936 - val_loss: 6.2371\n",
      "\n",
      "[20210301-2351-19] Learning rate for epoch 111 is 0.001780266989953816\n",
      "Epoch 111/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8414\n",
      "Epoch 00111: val_loss did not improve from 5.90817\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.8414 - val_loss: 5.9359\n",
      "\n",
      "[20210301-2351-24] Learning rate for epoch 112 is 0.0019560488872230053\n",
      "Epoch 112/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8825\n",
      "Epoch 00112: val_loss did not improve from 5.90817\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.8825 - val_loss: 6.2869\n",
      "\n",
      "[20210301-2351-28] Learning rate for epoch 113 is 0.002131430897861719\n",
      "Epoch 113/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6842\n",
      "Epoch 00113: val_loss did not improve from 5.90817\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.6842 - val_loss: 6.0895\n",
      "\n",
      "[20210301-2351-33] Learning rate for epoch 114 is 0.0023064129054546356\n",
      "Epoch 114/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8163\n",
      "Epoch 00114: val_loss did not improve from 5.90817\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.8163 - val_loss: 6.0654\n",
      "\n",
      "[20210301-2351-37] Learning rate for epoch 115 is 0.0024809949100017548\n",
      "Epoch 115/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6214\n",
      "Epoch 00115: val_loss did not improve from 5.90817\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.6214 - val_loss: 6.3212\n",
      "\n",
      "[20210301-2351-42] Learning rate for epoch 116 is 0.0026551769115030766\n",
      "Epoch 116/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9367\n",
      "Epoch 00116: val_loss did not improve from 5.90817\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.9367 - val_loss: 6.6411\n",
      "\n",
      "[20210301-2351-47] Learning rate for epoch 117 is 0.0028289591427892447\n",
      "Epoch 117/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9547\n",
      "Epoch 00117: val_loss did not improve from 5.90817\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.9547 - val_loss: 6.2855\n",
      "\n",
      "[20210301-2351-51] Learning rate for epoch 118 is 0.0030023413710296154\n",
      "Epoch 118/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6327\n",
      "Epoch 00118: val_loss did not improve from 5.90817\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.6327 - val_loss: 6.8004\n",
      "\n",
      "[20210301-2351-56] Learning rate for epoch 119 is 0.003175323596224189\n",
      "Epoch 119/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9093\n",
      "Epoch 00119: val_loss did not improve from 5.90817\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.9093 - val_loss: 6.6282\n",
      "\n",
      "[20210301-2352-01] Learning rate for epoch 120 is 0.003347905818372965\n",
      "Epoch 120/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9443\n",
      "Epoch 00120: val_loss did not improve from 5.90817\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.9443 - val_loss: 6.5674\n",
      "\n",
      "[20210301-2352-05] Learning rate for epoch 121 is 0.0035200880374759436\n",
      "Epoch 121/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8063\n",
      "Epoch 00121: val_loss did not improve from 5.90817\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.8063 - val_loss: 6.5254\n",
      "\n",
      "[20210301-2352-10] Learning rate for epoch 122 is 0.003691870253533125\n",
      "Epoch 122/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9909\n",
      "Epoch 00122: val_loss did not improve from 5.90817\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.9909 - val_loss: 7.3618\n",
      "\n",
      "[20210301-2352-14] Learning rate for epoch 123 is 0.0038632526993751526\n",
      "Epoch 123/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0561\n",
      "Epoch 00123: val_loss did not improve from 5.90817\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.0561 - val_loss: 7.6111\n",
      "\n",
      "[20210301-2352-19] Learning rate for epoch 124 is 0.004034235142171383\n",
      "Epoch 124/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0531\n",
      "Epoch 00124: val_loss did not improve from 5.90817\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.0531 - val_loss: 7.9284\n",
      "\n",
      "[20210301-2352-24] Learning rate for epoch 125 is 0.004204817581921816\n",
      "Epoch 125/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0663\n",
      "Epoch 00125: val_loss did not improve from 5.90817\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.0663 - val_loss: 7.0645\n",
      "\n",
      "[20210301-2352-28] Learning rate for epoch 126 is 0.0043750000186264515\n",
      "Epoch 126/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0327\n",
      "Epoch 00126: val_loss did not improve from 5.90817\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.0327 - val_loss: 6.9078\n",
      "\n",
      "[20210301-2352-33] Learning rate for epoch 127 is 0.0041952175088226795\n",
      "Epoch 127/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1523\n",
      "Epoch 00127: val_loss did not improve from 5.90817\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.1523 - val_loss: 8.4198\n",
      "\n",
      "[20210301-2352-37] Learning rate for epoch 128 is 0.004015835002064705\n",
      "Epoch 128/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3225\n",
      "Epoch 00128: val_loss did not improve from 5.90817\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 7.3225 - val_loss: 6.3395\n",
      "\n",
      "[20210301-2352-43] Learning rate for epoch 129 is 0.003836852265521884\n",
      "Epoch 129/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1181\n",
      "Epoch 00129: val_loss did not improve from 5.90817\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.1181 - val_loss: 6.9033\n",
      "\n",
      "[20210301-2352-47] Learning rate for epoch 130 is 0.003658269764855504\n",
      "Epoch 130/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8607\n",
      "Epoch 00130: val_loss did not improve from 5.90817\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.8607 - val_loss: 7.0174\n",
      "\n",
      "[20210301-2352-52] Learning rate for epoch 131 is 0.003480087034404278\n",
      "Epoch 131/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2081\n",
      "Epoch 00131: val_loss did not improve from 5.90817\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.2081 - val_loss: 6.9027\n",
      "\n",
      "[20210301-2352-56] Learning rate for epoch 132 is 0.003302304306998849\n",
      "Epoch 132/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1232\n",
      "Epoch 00132: val_loss did not improve from 5.90817\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.1232 - val_loss: 7.1897\n",
      "\n",
      "[20210301-2353-01] Learning rate for epoch 133 is 0.0031249215826392174\n",
      "Epoch 133/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1778\n",
      "Epoch 00133: val_loss did not improve from 5.90817\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.1778 - val_loss: 7.1890\n",
      "\n",
      "[20210301-2353-06] Learning rate for epoch 134 is 0.0029479386284947395\n",
      "Epoch 134/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9090\n",
      "Epoch 00134: val_loss did not improve from 5.90817\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.9090 - val_loss: 6.2623\n",
      "\n",
      "[20210301-2353-10] Learning rate for epoch 135 is 0.0027713559102267027\n",
      "Epoch 135/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6221\n",
      "Epoch 00135: val_loss did not improve from 5.90817\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.6221 - val_loss: 6.4764\n",
      "\n",
      "[20210301-2353-15] Learning rate for epoch 136 is 0.0025951729621738195\n",
      "Epoch 136/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6266\n",
      "Epoch 00136: val_loss did not improve from 5.90817\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.6266 - val_loss: 6.3856\n",
      "\n",
      "[20210301-2353-20] Learning rate for epoch 137 is 0.0024193900171667337\n",
      "Epoch 137/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6677\n",
      "Epoch 00137: val_loss did not improve from 5.90817\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.6677 - val_loss: 6.5284\n",
      "\n",
      "[20210301-2353-24] Learning rate for epoch 138 is 0.0022440070752054453\n",
      "Epoch 138/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0281\n",
      "Epoch 00138: val_loss did not improve from 5.90817\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.0281 - val_loss: 6.3429\n",
      "\n",
      "[20210301-2353-29] Learning rate for epoch 139 is 0.002069024136289954\n",
      "Epoch 139/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6234\n",
      "Epoch 00139: val_loss did not improve from 5.90817\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.6234 - val_loss: 5.9866\n",
      "\n",
      "[20210301-2353-34] Learning rate for epoch 140 is 0.0018944410840049386\n",
      "Epoch 140/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5951\n",
      "Epoch 00140: val_loss did not improve from 5.90817\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.5951 - val_loss: 6.0385\n",
      "\n",
      "[20210301-2353-38] Learning rate for epoch 141 is 0.0017202580347657204\n",
      "Epoch 141/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6879\n",
      "Epoch 00141: val_loss did not improve from 5.90817\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.6879 - val_loss: 6.2269\n",
      "\n",
      "[20210301-2353-43] Learning rate for epoch 142 is 0.0015464748721569777\n",
      "Epoch 142/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4979\n",
      "Epoch 00142: val_loss did not improve from 5.90817\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.4979 - val_loss: 6.4065\n",
      "\n",
      "[20210301-2353-48] Learning rate for epoch 143 is 0.0013730917125940323\n",
      "Epoch 143/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6065\n",
      "Epoch 00143: val_loss did not improve from 5.90817\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.6065 - val_loss: 6.1386\n",
      "\n",
      "[20210301-2353-52] Learning rate for epoch 144 is 0.0012001085560768843\n",
      "Epoch 144/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2961\n",
      "Epoch 00144: val_loss did not improve from 5.90817\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.2961 - val_loss: 6.3012\n",
      "\n",
      "[20210301-2353-57] Learning rate for epoch 145 is 0.0010275252861902118\n",
      "Epoch 145/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4584\n",
      "Epoch 00145: val_loss did not improve from 5.90817\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.4584 - val_loss: 6.1847\n",
      "\n",
      "[20210301-2354-01] Learning rate for epoch 146 is 0.0008553420193493366\n",
      "Epoch 146/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2525\n",
      "Epoch 00146: val_loss improved from 5.90817 to 5.89036, saving model to ./20210301-225844/toe_K2_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 6.2525 - val_loss: 5.8904\n",
      "\n",
      "[20210301-2354-07] Learning rate for epoch 147 is 0.0006835586973465979\n",
      "Epoch 147/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3260\n",
      "Epoch 00147: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.3260 - val_loss: 6.0499\n",
      "\n",
      "[20210301-2354-11] Learning rate for epoch 148 is 0.0005121753201819956\n",
      "Epoch 148/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4227\n",
      "Epoch 00148: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.4227 - val_loss: 6.1012\n",
      "\n",
      "[20210301-2354-16] Learning rate for epoch 149 is 0.00034119191695936024\n",
      "Epoch 149/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1538\n",
      "Epoch 00149: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.1538 - val_loss: 5.9959\n",
      "\n",
      "[20210301-2354-21] Learning rate for epoch 150 is 0.00017060847312677652\n",
      "Epoch 150/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2195\n",
      "Epoch 00150: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.2195 - val_loss: 6.0118\n",
      "\n",
      "[20210301-2354-25] Learning rate for epoch 151 is 4.249999960848072e-07\n",
      "Epoch 151/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2113\n",
      "Epoch 00151: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.2113 - val_loss: 6.0103\n",
      "\n",
      "[20210301-2354-30] Learning rate for epoch 152 is 0.00017020752420648932\n",
      "Epoch 152/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3052\n",
      "Epoch 00152: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.3052 - val_loss: 5.9947\n",
      "\n",
      "[20210301-2354-34] Learning rate for epoch 153 is 0.0003395900712348521\n",
      "Epoch 153/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1831\n",
      "Epoch 00153: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.1831 - val_loss: 6.0613\n",
      "\n",
      "[20210301-2354-39] Learning rate for epoch 154 is 0.0005085726734250784\n",
      "Epoch 154/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0946\n",
      "Epoch 00154: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.0946 - val_loss: 6.0068\n",
      "\n",
      "[20210301-2354-43] Learning rate for epoch 155 is 0.0006771553307771683\n",
      "Epoch 155/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2708\n",
      "Epoch 00155: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.2708 - val_loss: 6.1683\n",
      "\n",
      "[20210301-2354-48] Learning rate for epoch 156 is 0.0008453379850834608\n",
      "Epoch 156/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1246\n",
      "Epoch 00156: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.1246 - val_loss: 6.0733\n",
      "\n",
      "[20210301-2354-53] Learning rate for epoch 157 is 0.0010131207527592778\n",
      "Epoch 157/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3038\n",
      "Epoch 00157: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.3038 - val_loss: 6.1977\n",
      "\n",
      "[20210301-2354-57] Learning rate for epoch 158 is 0.0011805035173892975\n",
      "Epoch 158/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2859\n",
      "Epoch 00158: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.2859 - val_loss: 6.0768\n",
      "\n",
      "[20210301-2355-02] Learning rate for epoch 159 is 0.0013474862789735198\n",
      "Epoch 159/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3864\n",
      "Epoch 00159: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.3864 - val_loss: 6.1891\n",
      "\n",
      "[20210301-2355-06] Learning rate for epoch 160 is 0.0015140691539272666\n",
      "Epoch 160/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3721\n",
      "Epoch 00160: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.3721 - val_loss: 6.5732\n",
      "\n",
      "[20210301-2355-11] Learning rate for epoch 161 is 0.001680252025835216\n",
      "Epoch 161/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2048\n",
      "Epoch 00161: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.2048 - val_loss: 5.9363\n",
      "\n",
      "[20210301-2355-16] Learning rate for epoch 162 is 0.0018460348946973681\n",
      "Epoch 162/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2356\n",
      "Epoch 00162: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.2356 - val_loss: 6.4609\n",
      "\n",
      "[20210301-2355-20] Learning rate for epoch 163 is 0.0020114178769290447\n",
      "Epoch 163/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4038\n",
      "Epoch 00163: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.4038 - val_loss: 6.2824\n",
      "\n",
      "[20210301-2355-25] Learning rate for epoch 164 is 0.0021764009725302458\n",
      "Epoch 164/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3110\n",
      "Epoch 00164: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.3110 - val_loss: 6.4383\n",
      "\n",
      "[20210301-2355-30] Learning rate for epoch 165 is 0.002340983832255006\n",
      "Epoch 165/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1870\n",
      "Epoch 00165: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.1870 - val_loss: 6.3666\n",
      "\n",
      "[20210301-2355-34] Learning rate for epoch 166 is 0.002505166921764612\n",
      "Epoch 166/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3156\n",
      "Epoch 00166: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.3156 - val_loss: 6.6671\n",
      "\n",
      "[20210301-2355-39] Learning rate for epoch 167 is 0.002668950008228421\n",
      "Epoch 167/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5863\n",
      "Epoch 00167: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.5863 - val_loss: 6.6422\n",
      "\n",
      "[20210301-2355-43] Learning rate for epoch 168 is 0.0028323333244770765\n",
      "Epoch 168/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5203\n",
      "Epoch 00168: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.5203 - val_loss: 6.5415\n",
      "\n",
      "[20210301-2355-48] Learning rate for epoch 169 is 0.002995316404849291\n",
      "Epoch 169/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8164\n",
      "Epoch 00169: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.8164 - val_loss: 6.5180\n",
      "\n",
      "[20210301-2355-52] Learning rate for epoch 170 is 0.0031578997150063515\n",
      "Epoch 170/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4320\n",
      "Epoch 00170: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.4320 - val_loss: 6.9754\n",
      "\n",
      "[20210301-2355-57] Learning rate for epoch 171 is 0.0033200830221176147\n",
      "Epoch 171/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6665\n",
      "Epoch 00171: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.6665 - val_loss: 6.9395\n",
      "\n",
      "[20210301-2356-02] Learning rate for epoch 172 is 0.0034818663261830807\n",
      "Epoch 172/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6219\n",
      "Epoch 00172: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.6219 - val_loss: 6.7225\n",
      "\n",
      "[20210301-2356-06] Learning rate for epoch 173 is 0.0036432496272027493\n",
      "Epoch 173/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7162\n",
      "Epoch 00173: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.7162 - val_loss: 7.0189\n",
      "\n",
      "[20210301-2356-11] Learning rate for epoch 174 is 0.003804233158007264\n",
      "Epoch 174/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5885\n",
      "Epoch 00174: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.5885 - val_loss: 7.2162\n",
      "\n",
      "[20210301-2356-15] Learning rate for epoch 175 is 0.003964816685765982\n",
      "Epoch 175/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9939\n",
      "Epoch 00175: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.9939 - val_loss: 7.7277\n",
      "\n",
      "[20210301-2356-20] Learning rate for epoch 176 is 0.004124999977648258\n",
      "Epoch 176/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9089\n",
      "Epoch 00176: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.9089 - val_loss: 7.9688\n",
      "\n",
      "[20210301-2356-24] Learning rate for epoch 177 is 0.003955216612666845\n",
      "Epoch 177/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0683\n",
      "Epoch 00177: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.0683 - val_loss: 8.3617\n",
      "\n",
      "[20210301-2356-29] Learning rate for epoch 178 is 0.003785833017900586\n",
      "Epoch 178/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8084\n",
      "Epoch 00178: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.8084 - val_loss: 8.7203\n",
      "\n",
      "[20210301-2356-34] Learning rate for epoch 179 is 0.0036168494261801243\n",
      "Epoch 179/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7528\n",
      "Epoch 00179: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.7528 - val_loss: 7.0915\n",
      "\n",
      "[20210301-2356-38] Learning rate for epoch 180 is 0.003448265604674816\n",
      "Epoch 180/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7659\n",
      "Epoch 00180: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.7659 - val_loss: 7.4032\n",
      "\n",
      "[20210301-2356-43] Learning rate for epoch 181 is 0.003280082019045949\n",
      "Epoch 181/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7489\n",
      "Epoch 00181: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.7489 - val_loss: 6.6955\n",
      "\n",
      "[20210301-2356-48] Learning rate for epoch 182 is 0.0031122982036322355\n",
      "Epoch 182/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4272\n",
      "Epoch 00182: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.4272 - val_loss: 6.6645\n",
      "\n",
      "[20210301-2356-52] Learning rate for epoch 183 is 0.002944914624094963\n",
      "Epoch 183/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4750\n",
      "Epoch 00183: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.4750 - val_loss: 7.1060\n",
      "\n",
      "[20210301-2356-57] Learning rate for epoch 184 is 0.0027779308147728443\n",
      "Epoch 184/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4292\n",
      "Epoch 00184: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.4292 - val_loss: 7.1190\n",
      "\n",
      "[20210301-2357-01] Learning rate for epoch 185 is 0.0026113467756658792\n",
      "Epoch 185/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5026\n",
      "Epoch 00185: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.5026 - val_loss: 7.1169\n",
      "\n",
      "[20210301-2357-06] Learning rate for epoch 186 is 0.002445162972435355\n",
      "Epoch 186/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2904\n",
      "Epoch 00186: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.2904 - val_loss: 6.6294\n",
      "\n",
      "[20210301-2357-10] Learning rate for epoch 187 is 0.0022793791722506285\n",
      "Epoch 187/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5263\n",
      "Epoch 00187: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.5263 - val_loss: 6.5713\n",
      "\n",
      "[20210301-2357-15] Learning rate for epoch 188 is 0.0021139951422810555\n",
      "Epoch 188/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2297\n",
      "Epoch 00188: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.2297 - val_loss: 6.2661\n",
      "\n",
      "[20210301-2357-19] Learning rate for epoch 189 is 0.0019490111153572798\n",
      "Epoch 189/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4993\n",
      "Epoch 00189: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.4993 - val_loss: 6.4052\n",
      "\n",
      "[20210301-2357-24] Learning rate for epoch 190 is 0.0017844270914793015\n",
      "Epoch 190/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4060\n",
      "Epoch 00190: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 2s 93ms/step - loss: 6.4060 - val_loss: 6.1013\n",
      "\n",
      "[20210301-2357-29] Learning rate for epoch 191 is 0.0016202429542317986\n",
      "Epoch 191/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2030\n",
      "Epoch 00191: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.2030 - val_loss: 6.0236\n",
      "\n",
      "[20210301-2357-34] Learning rate for epoch 192 is 0.001456458936445415\n",
      "Epoch 192/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3787\n",
      "Epoch 00192: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 6.3787 - val_loss: 6.3538\n",
      "\n",
      "[20210301-2357-38] Learning rate for epoch 193 is 0.001293074688874185\n",
      "Epoch 193/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1679\n",
      "Epoch 00193: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.1679 - val_loss: 6.2119\n",
      "\n",
      "[20210301-2357-43] Learning rate for epoch 194 is 0.0011300905607640743\n",
      "Epoch 194/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9348\n",
      "Epoch 00194: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.9348 - val_loss: 6.4587\n",
      "\n",
      "[20210301-2357-48] Learning rate for epoch 195 is 0.0009675062610767782\n",
      "Epoch 195/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0549\n",
      "Epoch 00195: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.0549 - val_loss: 6.2155\n",
      "\n",
      "[20210301-2357-52] Learning rate for epoch 196 is 0.0008053220226429403\n",
      "Epoch 196/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0579\n",
      "Epoch 00196: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.0579 - val_loss: 6.2073\n",
      "\n",
      "[20210301-2357-57] Learning rate for epoch 197 is 0.0006435376708395779\n",
      "Epoch 197/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8530\n",
      "Epoch 00197: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.8530 - val_loss: 6.2615\n",
      "\n",
      "[20210301-2358-02] Learning rate for epoch 198 is 0.0004821533220820129\n",
      "Epoch 198/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0420\n",
      "Epoch 00198: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.0420 - val_loss: 6.2802\n",
      "\n",
      "[20210301-2358-06] Learning rate for epoch 199 is 0.0003211689181625843\n",
      "Epoch 199/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7206\n",
      "Epoch 00199: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.7206 - val_loss: 6.1639\n",
      "\n",
      "[20210301-2358-11] Learning rate for epoch 200 is 0.00016058447363320738\n",
      "Epoch 200/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8514\n",
      "Epoch 00200: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.8514 - val_loss: 6.1545\n",
      "\n",
      "[20210301-2358-15] Learning rate for epoch 201 is 4.0000000467443897e-07\n",
      "Epoch 201/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1327\n",
      "Epoch 00201: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.1327 - val_loss: 6.1517\n",
      "\n",
      "[20210301-2358-20] Learning rate for epoch 202 is 0.0001601835247129202\n",
      "Epoch 202/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9772\n",
      "Epoch 00202: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.9772 - val_loss: 6.1497\n",
      "\n",
      "[20210301-2358-25] Learning rate for epoch 203 is 0.00031956707243807614\n",
      "Epoch 203/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9977\n",
      "Epoch 00203: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 5.9977 - val_loss: 6.1316\n",
      "\n",
      "[20210301-2358-29] Learning rate for epoch 204 is 0.00047855067532509565\n",
      "Epoch 204/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9959\n",
      "Epoch 00204: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.9959 - val_loss: 6.2151\n",
      "\n",
      "[20210301-2358-34] Learning rate for epoch 205 is 0.0006371343042701483\n",
      "Epoch 205/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1336\n",
      "Epoch 00205: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.1336 - val_loss: 6.1932\n",
      "\n",
      "[20210301-2358-39] Learning rate for epoch 206 is 0.0007953179883770645\n",
      "Epoch 206/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0130\n",
      "Epoch 00206: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.0130 - val_loss: 6.1426\n",
      "\n",
      "[20210301-2358-43] Learning rate for epoch 207 is 0.0009531017276458442\n",
      "Epoch 207/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9768\n",
      "Epoch 00207: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.9768 - val_loss: 6.2388\n",
      "\n",
      "[20210301-2358-48] Learning rate for epoch 208 is 0.0011104855220764875\n",
      "Epoch 208/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9729\n",
      "Epoch 00208: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.9729 - val_loss: 6.2790\n",
      "\n",
      "[20210301-2358-52] Learning rate for epoch 209 is 0.0012674692552536726\n",
      "Epoch 209/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9665\n",
      "Epoch 00209: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.9665 - val_loss: 6.1393\n",
      "\n",
      "[20210301-2358-57] Learning rate for epoch 210 is 0.0014240531018003821\n",
      "Epoch 210/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9253\n",
      "Epoch 00210: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.9253 - val_loss: 6.3917\n",
      "\n",
      "[20210301-2359-01] Learning rate for epoch 211 is 0.0015802369453012943\n",
      "Epoch 211/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0369\n",
      "Epoch 00211: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.0369 - val_loss: 6.5261\n",
      "\n",
      "[20210301-2359-06] Learning rate for epoch 212 is 0.001736020902171731\n",
      "Epoch 212/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9374\n",
      "Epoch 00212: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.9374 - val_loss: 6.5357\n",
      "\n",
      "[20210301-2359-11] Learning rate for epoch 213 is 0.0018914048559963703\n",
      "Epoch 213/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3310\n",
      "Epoch 00213: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.3310 - val_loss: 6.2595\n",
      "\n",
      "[20210301-2359-15] Learning rate for epoch 214 is 0.0020463888067752123\n",
      "Epoch 214/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9256\n",
      "Epoch 00214: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.9256 - val_loss: 6.4501\n",
      "\n",
      "[20210301-2359-20] Learning rate for epoch 215 is 0.0022009729873389006\n",
      "Epoch 215/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3117\n",
      "Epoch 00215: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.3117 - val_loss: 6.8531\n",
      "\n",
      "[20210301-2359-25] Learning rate for epoch 216 is 0.002355156932026148\n",
      "Epoch 216/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3450\n",
      "Epoch 00216: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.3450 - val_loss: 6.5345\n",
      "\n",
      "[20210301-2359-29] Learning rate for epoch 217 is 0.0025089411064982414\n",
      "Epoch 217/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1441\n",
      "Epoch 00217: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.1441 - val_loss: 6.2196\n",
      "\n",
      "[20210301-2359-34] Learning rate for epoch 218 is 0.0026623252779245377\n",
      "Epoch 218/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2398\n",
      "Epoch 00218: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.2398 - val_loss: 6.5292\n",
      "\n",
      "[20210301-2359-38] Learning rate for epoch 219 is 0.0028153094463050365\n",
      "Epoch 219/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2791\n",
      "Epoch 00219: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.2791 - val_loss: 6.1763\n",
      "\n",
      "[20210301-2359-43] Learning rate for epoch 220 is 0.002967893611639738\n",
      "Epoch 220/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0482\n",
      "Epoch 00220: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.0482 - val_loss: 6.9425\n",
      "\n",
      "[20210301-2359-47] Learning rate for epoch 221 is 0.003120078006759286\n",
      "Epoch 221/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1700\n",
      "Epoch 00221: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.1700 - val_loss: 7.3824\n",
      "\n",
      "[20210301-2359-52] Learning rate for epoch 222 is 0.0032718623988330364\n",
      "Epoch 222/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1630\n",
      "Epoch 00222: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.1630 - val_loss: 6.8039\n",
      "\n",
      "[20210301-2359-56] Learning rate for epoch 223 is 0.0034232467878609896\n",
      "Epoch 223/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6522\n",
      "Epoch 00223: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.6522 - val_loss: 6.6353\n",
      "\n",
      "[20210302-0000-01] Learning rate for epoch 224 is 0.0035742311738431454\n",
      "Epoch 224/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5390\n",
      "Epoch 00224: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.5390 - val_loss: 7.0927\n",
      "\n",
      "[20210302-0000-06] Learning rate for epoch 225 is 0.003724815556779504\n",
      "Epoch 225/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1867\n",
      "Epoch 00225: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.1867 - val_loss: 7.4132\n",
      "\n",
      "[20210302-0000-10] Learning rate for epoch 226 is 0.003874999936670065\n",
      "Epoch 226/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7226\n",
      "Epoch 00226: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.7226 - val_loss: 6.5550\n",
      "\n",
      "[20210302-0000-15] Learning rate for epoch 227 is 0.0037152154836803675\n",
      "Epoch 227/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2595\n",
      "Epoch 00227: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.2595 - val_loss: 7.8687\n",
      "\n",
      "[20210302-0000-19] Learning rate for epoch 228 is 0.0035558310337364674\n",
      "Epoch 228/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2940\n",
      "Epoch 00228: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.2940 - val_loss: 7.1870\n",
      "\n",
      "[20210302-0000-24] Learning rate for epoch 229 is 0.003396846354007721\n",
      "Epoch 229/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4109\n",
      "Epoch 00229: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.4109 - val_loss: 7.6999\n",
      "\n",
      "[20210302-0000-29] Learning rate for epoch 230 is 0.003238261677324772\n",
      "Epoch 230/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5653\n",
      "Epoch 00230: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.5653 - val_loss: 8.2045\n",
      "\n",
      "[20210302-0000-33] Learning rate for epoch 231 is 0.00308007700368762\n",
      "Epoch 231/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5892\n",
      "Epoch 00231: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.5892 - val_loss: 8.1870\n",
      "\n",
      "[20210302-0000-38] Learning rate for epoch 232 is 0.002922292333096266\n",
      "Epoch 232/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4913\n",
      "Epoch 00232: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.4913 - val_loss: 6.6435\n",
      "\n",
      "[20210302-0000-42] Learning rate for epoch 233 is 0.002764907432720065\n",
      "Epoch 233/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3657\n",
      "Epoch 00233: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.3657 - val_loss: 7.6296\n",
      "\n",
      "[20210302-0000-47] Learning rate for epoch 234 is 0.0026079227682203054\n",
      "Epoch 234/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7217\n",
      "Epoch 00234: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.7217 - val_loss: 7.0372\n",
      "\n",
      "[20210302-0000-52] Learning rate for epoch 235 is 0.0024513378739356995\n",
      "Epoch 235/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2509\n",
      "Epoch 00235: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.2509 - val_loss: 6.8318\n",
      "\n",
      "[20210302-0000-56] Learning rate for epoch 236 is 0.002295152982696891\n",
      "Epoch 236/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1219\n",
      "Epoch 00236: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.1219 - val_loss: 6.5340\n",
      "\n",
      "[20210302-0001-01] Learning rate for epoch 237 is 0.0021393680945038795\n",
      "Epoch 237/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1572\n",
      "Epoch 00237: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.1572 - val_loss: 6.4808\n",
      "\n",
      "[20210302-0001-05] Learning rate for epoch 238 is 0.0019839832093566656\n",
      "Epoch 238/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3452\n",
      "Epoch 00238: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.3452 - val_loss: 6.3338\n",
      "\n",
      "[20210302-0001-10] Learning rate for epoch 239 is 0.0018289980944246054\n",
      "Epoch 239/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2545\n",
      "Epoch 00239: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.2545 - val_loss: 6.4192\n",
      "\n",
      "[20210302-0001-14] Learning rate for epoch 240 is 0.0016744130989536643\n",
      "Epoch 240/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0708\n",
      "Epoch 00240: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.0708 - val_loss: 6.4225\n",
      "\n",
      "[20210302-0001-19] Learning rate for epoch 241 is 0.0015202279901131988\n",
      "Epoch 241/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1240\n",
      "Epoch 00241: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.1240 - val_loss: 6.2099\n",
      "\n",
      "[20210302-0001-24] Learning rate for epoch 242 is 0.0013664428843185306\n",
      "Epoch 242/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9627\n",
      "Epoch 00242: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.9627 - val_loss: 6.1170\n",
      "\n",
      "[20210302-0001-28] Learning rate for epoch 243 is 0.0012130576651543379\n",
      "Epoch 243/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8989\n",
      "Epoch 00243: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.8989 - val_loss: 6.2899\n",
      "\n",
      "[20210302-0001-33] Learning rate for epoch 244 is 0.0010600725654512644\n",
      "Epoch 244/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7374\n",
      "Epoch 00244: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.7374 - val_loss: 6.5885\n",
      "\n",
      "[20210302-0001-37] Learning rate for epoch 245 is 0.0009074872941710055\n",
      "Epoch 245/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0158\n",
      "Epoch 00245: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.0158 - val_loss: 6.7716\n",
      "\n",
      "[20210302-0001-42] Learning rate for epoch 246 is 0.0007553020259365439\n",
      "Epoch 246/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8690\n",
      "Epoch 00246: val_loss did not improve from 5.89036\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.8690 - val_loss: 6.4572\n",
      "\n",
      " \n",
      " K =  3 \n",
      "\n",
      "Check cache in memory:Y True\n",
      "Check keypoints process01: Tensor(\"args_1:0\", shape=(2,), dtype=int64) (2,) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Check keypoints process02: Tensor(\"Reshape:0\", shape=(1, 2), dtype=int64) (1, 2) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Check keypoints process03: Tensor(\"PyFunc:1\", dtype=float32, device=/job:localhost/replica:0/task:0)\n",
      "Check keypoints process04: Tensor(\"Reshape_1:0\", shape=(2,), dtype=float32)\n",
      "Check augment :Y True\n",
      "Check ds cache[True] and augment[True]\n",
      "Check cache in memory:Y True\n",
      "Check augment :N False\n",
      "Check ds cache[True] and augment[False]\n",
      "best_model_name: ./20210301-225844/toe_K3_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "base_model.trainable :  False\n",
      "\n",
      "[20210302-0001-49] Learning rate for epoch 1 is 0.009999999776482582\n",
      "Epoch 1/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 75.6548\n",
      "Epoch 00001: val_loss improved from inf to 52.52108, saving model to ./20210301-225844/toe_K3_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 6s 280ms/step - loss: 74.1508 - val_loss: 52.5211\n",
      "\n",
      "[20210302-0002-05] Learning rate for epoch 2 is 0.009960000403225422\n",
      "Epoch 2/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 33.5848\n",
      "Epoch 00002: val_loss improved from 52.52108 to 22.97319, saving model to ./20210301-225844/toe_K3_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 33.0346 - val_loss: 22.9732\n",
      "\n",
      "[20210302-0002-09] Learning rate for epoch 3 is 0.009920000098645687\n",
      "Epoch 3/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 20.8704\n",
      "Epoch 00003: val_loss improved from 22.97319 to 14.69938, saving model to ./20210301-225844/toe_K3_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 8s 380ms/step - loss: 20.8704 - val_loss: 14.6994\n",
      "\n",
      "[20210302-0002-20] Learning rate for epoch 4 is 0.009879999794065952\n",
      "Epoch 4/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 18.7815\n",
      "Epoch 00004: val_loss improved from 14.69938 to 12.77248, saving model to ./20210301-225844/toe_K3_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 18.7815 - val_loss: 12.7725\n",
      "\n",
      "[20210302-0002-24] Learning rate for epoch 5 is 0.009840000420808792\n",
      "Epoch 5/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 17.4689\n",
      "Epoch 00005: val_loss improved from 12.77248 to 12.50337, saving model to ./20210301-225844/toe_K3_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 17.4689 - val_loss: 12.5034\n",
      "\n",
      "[20210302-0002-28] Learning rate for epoch 6 is 0.009800000116229057\n",
      "Epoch 6/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.7742\n",
      "Epoch 00006: val_loss improved from 12.50337 to 12.38205, saving model to ./20210301-225844/toe_K3_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 16.7742 - val_loss: 12.3820\n",
      "\n",
      "[20210302-0002-32] Learning rate for epoch 7 is 0.009759999811649323\n",
      "Epoch 7/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.4780\n",
      "Epoch 00007: val_loss improved from 12.38205 to 12.16724, saving model to ./20210301-225844/toe_K3_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 16.4780 - val_loss: 12.1672\n",
      "\n",
      "[20210302-0002-36] Learning rate for epoch 8 is 0.009720000438392162\n",
      "Epoch 8/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.0431\n",
      "Epoch 00008: val_loss improved from 12.16724 to 11.99645, saving model to ./20210301-225844/toe_K3_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.0929 - val_loss: 11.9965\n",
      "\n",
      "[20210302-0002-40] Learning rate for epoch 9 is 0.009680000133812428\n",
      "Epoch 9/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.8787\n",
      "Epoch 00009: val_loss improved from 11.99645 to 11.72788, saving model to ./20210301-225844/toe_K3_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 15.9733 - val_loss: 11.7279\n",
      "\n",
      "[20210302-0002-44] Learning rate for epoch 10 is 0.009639999829232693\n",
      "Epoch 10/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.4747\n",
      "Epoch 00010: val_loss did not improve from 11.72788\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.4760 - val_loss: 11.9197\n",
      "\n",
      "[20210302-0002-48] Learning rate for epoch 11 is 0.009600000455975533\n",
      "Epoch 11/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.6005\n",
      "Epoch 00011: val_loss improved from 11.72788 to 11.55708, saving model to ./20210301-225844/toe_K3_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 15.6313 - val_loss: 11.5571\n",
      "\n",
      "[20210302-0002-52] Learning rate for epoch 12 is 0.009560000151395798\n",
      "Epoch 12/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.5996\n",
      "Epoch 00012: val_loss did not improve from 11.55708\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.5996 - val_loss: 11.6858\n",
      "\n",
      "[20210302-0002-56] Learning rate for epoch 13 is 0.009519999846816063\n",
      "Epoch 13/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.0281\n",
      "Epoch 00013: val_loss improved from 11.55708 to 11.46882, saving model to ./20210301-225844/toe_K3_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 15.0526 - val_loss: 11.4688\n",
      "\n",
      "[20210302-0003-00] Learning rate for epoch 14 is 0.009479999542236328\n",
      "Epoch 14/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 15.2150\n",
      "Epoch 00014: val_loss did not improve from 11.46882\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.0837 - val_loss: 11.6286\n",
      "\n",
      "[20210302-0003-04] Learning rate for epoch 15 is 0.009440000168979168\n",
      "Epoch 15/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 14.7200\n",
      "Epoch 00015: val_loss improved from 11.46882 to 11.24242, saving model to ./20210301-225844/toe_K3_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 14.6433 - val_loss: 11.2424\n",
      "\n",
      "[20210302-0003-08] Learning rate for epoch 16 is 0.009399999864399433\n",
      "Epoch 16/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.1380\n",
      "Epoch 00016: val_loss improved from 11.24242 to 11.18996, saving model to ./20210301-225844/toe_K3_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 15.0519 - val_loss: 11.1900\n",
      "\n",
      "[20210302-0003-12] Learning rate for epoch 17 is 0.009359999559819698\n",
      "Epoch 17/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 14.8042\n",
      "Epoch 00017: val_loss did not improve from 11.18996\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 14.7558 - val_loss: 11.4668\n",
      "\n",
      "[20210302-0003-16] Learning rate for epoch 18 is 0.009320000186562538\n",
      "Epoch 18/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 14.4133\n",
      "Epoch 00018: val_loss improved from 11.18996 to 11.04097, saving model to ./20210301-225844/toe_K3_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 14.4154 - val_loss: 11.0410\n",
      "\n",
      "[20210302-0003-20] Learning rate for epoch 19 is 0.009279999881982803\n",
      "Epoch 19/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.6491\n",
      "Epoch 00019: val_loss improved from 11.04097 to 10.88591, saving model to ./20210301-225844/toe_K3_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 14.6491 - val_loss: 10.8859\n",
      "\n",
      "[20210302-0003-24] Learning rate for epoch 20 is 0.009239999577403069\n",
      "Epoch 20/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 14.3331\n",
      "Epoch 00020: val_loss did not improve from 10.88591\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 14.3662 - val_loss: 11.0471\n",
      "\n",
      "[20210302-0003-28] Learning rate for epoch 21 is 0.009200000204145908\n",
      "Epoch 21/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 14.6156\n",
      "Epoch 00021: val_loss did not improve from 10.88591\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 14.5065 - val_loss: 10.9058\n",
      "\n",
      "[20210302-0003-31] Learning rate for epoch 22 is 0.009159999899566174\n",
      "Epoch 22/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 14.2786\n",
      "Epoch 00022: val_loss did not improve from 10.88591\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 14.3109 - val_loss: 11.4102\n",
      "\n",
      "[20210302-0003-35] Learning rate for epoch 23 is 0.009119999594986439\n",
      "Epoch 23/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 14.5049\n",
      "Epoch 00023: val_loss did not improve from 10.88591\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 14.5414 - val_loss: 10.9302\n",
      "\n",
      "[20210302-0003-39] Learning rate for epoch 24 is 0.009080000221729279\n",
      "Epoch 24/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.3104\n",
      "Epoch 00024: val_loss did not improve from 10.88591\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 14.3104 - val_loss: 11.1735\n",
      "\n",
      "[20210302-0003-43] Learning rate for epoch 25 is 0.009039999917149544\n",
      "Epoch 25/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 14.6511\n",
      "Epoch 00025: val_loss did not improve from 10.88591\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 14.5485 - val_loss: 10.9136\n",
      "\n",
      "[20210302-0003-46] Learning rate for epoch 26 is 0.008999999612569809\n",
      "Epoch 26/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 14.2381\n",
      "Epoch 00026: val_loss did not improve from 10.88591\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 14.2559 - val_loss: 11.0307\n",
      "\n",
      "[20210302-0003-50] Learning rate for epoch 27 is 0.008960000239312649\n",
      "Epoch 27/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.1876\n",
      "Epoch 00027: val_loss improved from 10.88591 to 10.83873, saving model to ./20210301-225844/toe_K3_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 14.1876 - val_loss: 10.8387\n",
      "\n",
      "[20210302-0003-54] Learning rate for epoch 28 is 0.008919999934732914\n",
      "Epoch 28/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 14.0820\n",
      "Epoch 00028: val_loss did not improve from 10.83873\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 14.1838 - val_loss: 10.8643\n",
      "\n",
      "[20210302-0003-58] Learning rate for epoch 29 is 0.00887999963015318\n",
      "Epoch 29/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.3544\n",
      "Epoch 00029: val_loss did not improve from 10.83873\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 14.3544 - val_loss: 11.0580\n",
      "\n",
      "[20210302-0004-02] Learning rate for epoch 30 is 0.008840000256896019\n",
      "Epoch 30/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.4607\n",
      "Epoch 00030: val_loss improved from 10.83873 to 10.83658, saving model to ./20210301-225844/toe_K3_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 14.4607 - val_loss: 10.8366\n",
      "\n",
      "[20210302-0004-06] Learning rate for epoch 31 is 0.008799999952316284\n",
      "Epoch 31/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.4504\n",
      "Epoch 00031: val_loss did not improve from 10.83658\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 14.4504 - val_loss: 10.8980\n",
      "\n",
      "[20210302-0004-10] Learning rate for epoch 32 is 0.00875999964773655\n",
      "Epoch 32/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 14.2018\n",
      "Epoch 00032: val_loss improved from 10.83658 to 10.79795, saving model to ./20210301-225844/toe_K3_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 14.0039 - val_loss: 10.7980\n",
      "\n",
      "[20210302-0004-14] Learning rate for epoch 33 is 0.00872000027447939\n",
      "Epoch 33/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 13.8037\n",
      "Epoch 00033: val_loss did not improve from 10.79795\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 13.7772 - val_loss: 11.5329\n",
      "\n",
      "[20210302-0004-17] Learning rate for epoch 34 is 0.008679999969899654\n",
      "Epoch 34/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 13.9721\n",
      "Epoch 00034: val_loss did not improve from 10.79795\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 14.0238 - val_loss: 11.2705\n",
      "\n",
      "[20210302-0004-21] Learning rate for epoch 35 is 0.00863999966531992\n",
      "Epoch 35/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 13.9308\n",
      "Epoch 00035: val_loss did not improve from 10.79795\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 13.9386 - val_loss: 11.1320\n",
      "\n",
      "[20210302-0004-25] Learning rate for epoch 36 is 0.00860000029206276\n",
      "Epoch 36/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 13.6749\n",
      "Epoch 00036: val_loss did not improve from 10.79795\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 13.8101 - val_loss: 11.0633\n",
      "\n",
      "[20210302-0004-29] Learning rate for epoch 37 is 0.008559999987483025\n",
      "Epoch 37/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 13.9696\n",
      "Epoch 00037: val_loss did not improve from 10.79795\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 13.9434 - val_loss: 11.2470\n",
      "\n",
      "[20210302-0004-32] Learning rate for epoch 38 is 0.00851999968290329\n",
      "Epoch 38/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 13.9398\n",
      "Epoch 00038: val_loss improved from 10.79795 to 10.73114, saving model to ./20210301-225844/toe_K3_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 13.9517 - val_loss: 10.7311\n",
      "\n",
      "[20210302-0004-36] Learning rate for epoch 39 is 0.00848000030964613\n",
      "Epoch 39/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 13.9393\n",
      "Epoch 00039: val_loss did not improve from 10.73114\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 13.9790 - val_loss: 11.0446\n",
      "\n",
      "[20210302-0004-40] Learning rate for epoch 40 is 0.008440000005066395\n",
      "Epoch 40/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 13.9394\n",
      "Epoch 00040: val_loss did not improve from 10.73114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 13.8088 - val_loss: 11.2726\n",
      "\n",
      "[20210302-0004-44] Learning rate for epoch 41 is 0.00839999970048666\n",
      "Epoch 41/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 13.6595\n",
      "Epoch 00041: val_loss improved from 10.73114 to 10.70432, saving model to ./20210301-225844/toe_K3_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 13.7516 - val_loss: 10.7043\n",
      "\n",
      "[20210302-0004-48] Learning rate for epoch 42 is 0.0083600003272295\n",
      "Epoch 42/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.8057\n",
      "Epoch 00042: val_loss improved from 10.70432 to 10.64370, saving model to ./20210301-225844/toe_K3_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 13.8057 - val_loss: 10.6437\n",
      "\n",
      "[20210302-0004-52] Learning rate for epoch 43 is 0.008320000022649765\n",
      "Epoch 43/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 13.7545\n",
      "Epoch 00043: val_loss did not improve from 10.64370\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 13.7954 - val_loss: 10.8318\n",
      "\n",
      "[20210302-0004-56] Learning rate for epoch 44 is 0.00827999971807003\n",
      "Epoch 44/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.4509\n",
      "Epoch 00044: val_loss improved from 10.64370 to 10.62729, saving model to ./20210301-225844/toe_K3_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 13.4509 - val_loss: 10.6273\n",
      "\n",
      "[20210302-0005-00] Learning rate for epoch 45 is 0.00824000034481287\n",
      "Epoch 45/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.7356\n",
      "Epoch 00045: val_loss improved from 10.62729 to 10.53501, saving model to ./20210301-225844/toe_K3_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 13.7356 - val_loss: 10.5350\n",
      "\n",
      "[20210302-0005-04] Learning rate for epoch 46 is 0.008200000040233135\n",
      "Epoch 46/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 13.3860\n",
      "Epoch 00046: val_loss did not improve from 10.53501\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 13.3569 - val_loss: 10.7007\n",
      "\n",
      "[20210302-0005-08] Learning rate for epoch 47 is 0.0081599997356534\n",
      "Epoch 47/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.7569\n",
      "Epoch 00047: val_loss did not improve from 10.53501\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 13.7569 - val_loss: 10.7993\n",
      "\n",
      "[20210302-0005-11] Learning rate for epoch 48 is 0.00812000036239624\n",
      "Epoch 48/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 13.6550\n",
      "Epoch 00048: val_loss did not improve from 10.53501\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 13.6448 - val_loss: 10.7189\n",
      "\n",
      "[20210302-0005-15] Learning rate for epoch 49 is 0.008080000057816505\n",
      "Epoch 49/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.7809\n",
      "Epoch 00049: val_loss did not improve from 10.53501\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 13.7809 - val_loss: 11.4106\n",
      "\n",
      "[20210302-0005-19] Learning rate for epoch 50 is 0.00803999975323677\n",
      "Epoch 50/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 13.8927\n",
      "Epoch 00050: val_loss did not improve from 10.53501\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 13.8732 - val_loss: 11.0996\n",
      "\n",
      "[20210302-0005-23] Learning rate for epoch 51 is 0.00800000037997961\n",
      "Epoch 51/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.6936\n",
      "Epoch 00051: val_loss improved from 10.53501 to 10.52033, saving model to ./20210301-225844/toe_K3_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 71ms/step - loss: 13.6936 - val_loss: 10.5203\n",
      "\n",
      "[20210302-0005-27] Learning rate for epoch 52 is 0.007960000075399876\n",
      "Epoch 52/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 13.6554\n",
      "Epoch 00052: val_loss did not improve from 10.52033\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 13.6525 - val_loss: 10.5815\n",
      "\n",
      "[20210302-0005-31] Learning rate for epoch 53 is 0.00791999977082014\n",
      "Epoch 53/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.4199\n",
      "Epoch 00053: val_loss did not improve from 10.52033\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 13.4199 - val_loss: 10.9158\n",
      "\n",
      "[20210302-0005-35] Learning rate for epoch 54 is 0.00788000039756298\n",
      "Epoch 54/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 13.8281\n",
      "Epoch 00054: val_loss did not improve from 10.52033\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 13.8560 - val_loss: 10.6862\n",
      "\n",
      "[20210302-0005-39] Learning rate for epoch 55 is 0.007840000092983246\n",
      "Epoch 55/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 13.7894\n",
      "Epoch 00055: val_loss did not improve from 10.52033\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 13.7624 - val_loss: 10.7100\n",
      "\n",
      "[20210302-0005-42] Learning rate for epoch 56 is 0.007799999788403511\n",
      "Epoch 56/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 13.7305\n",
      "Epoch 00056: val_loss improved from 10.52033 to 10.51466, saving model to ./20210301-225844/toe_K3_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 13.7158 - val_loss: 10.5147\n",
      "\n",
      "[20210302-0005-46] Learning rate for epoch 57 is 0.0077599999494850636\n",
      "Epoch 57/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.6345\n",
      "Epoch 00057: val_loss did not improve from 10.51466\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 13.6345 - val_loss: 10.5426\n",
      "\n",
      "[20210302-0005-50] Learning rate for epoch 58 is 0.007720000110566616\n",
      "Epoch 58/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.6777\n",
      "Epoch 00058: val_loss did not improve from 10.51466\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 13.6777 - val_loss: 10.5475\n",
      "\n",
      "[20210302-0005-54] Learning rate for epoch 59 is 0.007679999805986881\n",
      "Epoch 59/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 13.7860\n",
      "Epoch 00059: val_loss did not improve from 10.51466\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 13.6927 - val_loss: 10.5403\n",
      "\n",
      "[20210302-0005-58] Learning rate for epoch 60 is 0.007639999967068434\n",
      "Epoch 60/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 13.7180\n",
      "Epoch 00060: val_loss did not improve from 10.51466\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 13.7380 - val_loss: 10.5167\n",
      "\n",
      "[20210302-0006-01] Learning rate for epoch 61 is 0.007600000128149986\n",
      "Epoch 61/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 13.6388\n",
      "Epoch 00061: val_loss did not improve from 10.51466\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 13.6030 - val_loss: 10.5977\n",
      "\n",
      "[20210302-0006-05] Learning rate for epoch 62 is 0.0075599998235702515\n",
      "Epoch 62/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.2476\n",
      "Epoch 00062: val_loss improved from 10.51466 to 10.46509, saving model to ./20210301-225844/toe_K3_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 13.2476 - val_loss: 10.4651\n",
      "\n",
      "[20210302-0006-09] Learning rate for epoch 63 is 0.007519999984651804\n",
      "Epoch 63/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 13.4565\n",
      "Epoch 00063: val_loss improved from 10.46509 to 10.26669, saving model to ./20210301-225844/toe_K3_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 13.4112 - val_loss: 10.2667\n",
      "\n",
      "[20210302-0006-13] Learning rate for epoch 64 is 0.0074800001457333565\n",
      "Epoch 64/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.3063\n",
      "Epoch 00064: val_loss did not improve from 10.26669\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 13.3063 - val_loss: 10.4812\n",
      "\n",
      "[20210302-0006-17] Learning rate for epoch 65 is 0.007439999841153622\n",
      "Epoch 65/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.4293\n",
      "Epoch 00065: val_loss did not improve from 10.26669\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 13.4293 - val_loss: 10.3846\n",
      "\n",
      "[20210302-0006-21] Learning rate for epoch 66 is 0.007400000002235174\n",
      "Epoch 66/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.7551\n",
      "Epoch 00066: val_loss did not improve from 10.26669\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 13.7551 - val_loss: 10.5745\n",
      "\n",
      "[20210302-0006-24] Learning rate for epoch 67 is 0.007360000163316727\n",
      "Epoch 67/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 13.3141\n",
      "Epoch 00067: val_loss did not improve from 10.26669\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 13.3511 - val_loss: 10.6476\n",
      "\n",
      "[20210302-0006-28] Learning rate for epoch 68 is 0.007319999858736992\n",
      "Epoch 68/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 13.6001\n",
      "Epoch 00068: val_loss did not improve from 10.26669\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 13.5765 - val_loss: 10.8557\n",
      "\n",
      "[20210302-0006-32] Learning rate for epoch 69 is 0.007280000019818544\n",
      "Epoch 69/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 13.4880\n",
      "Epoch 00069: val_loss did not improve from 10.26669\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 13.5062 - val_loss: 10.6163\n",
      "\n",
      "[20210302-0006-36] Learning rate for epoch 70 is 0.007240000180900097\n",
      "Epoch 70/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 13.5217\n",
      "Epoch 00070: val_loss did not improve from 10.26669\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 13.4636 - val_loss: 11.1668\n",
      "\n",
      "[20210302-0006-39] Learning rate for epoch 71 is 0.007199999876320362\n",
      "Epoch 71/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 13.5326\n",
      "Epoch 00071: val_loss did not improve from 10.26669\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 13.5206 - val_loss: 10.7940\n",
      "\n",
      "[20210302-0006-43] Learning rate for epoch 72 is 0.007160000037401915\n",
      "Epoch 72/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 13.4944\n",
      "Epoch 00072: val_loss did not improve from 10.26669\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 13.4254 - val_loss: 10.4791\n",
      "\n",
      "[20210302-0006-47] Learning rate for epoch 73 is 0.007120000198483467\n",
      "Epoch 73/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.3615\n",
      "Epoch 00073: val_loss did not improve from 10.26669\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 13.3615 - val_loss: 10.8302\n",
      "\n",
      "[20210302-0006-50] Learning rate for epoch 74 is 0.007079999893903732\n",
      "Epoch 74/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 13.4728\n",
      "Epoch 00074: val_loss did not improve from 10.26669\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 13.4172 - val_loss: 10.5835\n",
      "\n",
      "[20210302-0006-54] Learning rate for epoch 75 is 0.007040000054985285\n",
      "Epoch 75/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 13.5777\n",
      "Epoch 00075: val_loss did not improve from 10.26669\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 13.6301 - val_loss: 10.4329\n",
      "\n",
      "[20210302-0006-58] Learning rate for epoch 76 is 0.007000000216066837\n",
      "Epoch 76/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 13.5437\n",
      "Epoch 00076: val_loss did not improve from 10.26669\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 13.5391 - val_loss: 10.5823\n",
      "\n",
      "[20210302-0007-01] Learning rate for epoch 77 is 0.0069599999114871025\n",
      "Epoch 77/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.4971\n",
      "Epoch 00077: val_loss did not improve from 10.26669\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 13.4971 - val_loss: 10.8083\n",
      "\n",
      "[20210302-0007-05] Learning rate for epoch 78 is 0.006920000072568655\n",
      "Epoch 78/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 13.3652\n",
      "Epoch 00078: val_loss did not improve from 10.26669\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 13.3644 - val_loss: 10.8243\n",
      "\n",
      "[20210302-0007-09] Learning rate for epoch 79 is 0.00687999976798892\n",
      "Epoch 79/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.1971\n",
      "Epoch 00079: val_loss did not improve from 10.26669\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 13.1971 - val_loss: 10.4537\n",
      "\n",
      "[20210302-0007-13] Learning rate for epoch 80 is 0.006839999929070473\n",
      "Epoch 80/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 13.5766\n",
      "Epoch 00080: val_loss did not improve from 10.26669\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 13.5785 - val_loss: 10.3101\n",
      "\n",
      "[20210302-0007-16] Learning rate for epoch 81 is 0.006800000090152025\n",
      "Epoch 81/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.3562\n",
      "Epoch 00081: val_loss did not improve from 10.26669\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 13.3562 - val_loss: 10.4973\n",
      "\n",
      "[20210302-0007-20] Learning rate for epoch 82 is 0.00675999978557229\n",
      "Epoch 82/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.2969\n",
      "Epoch 00082: val_loss did not improve from 10.26669\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 13.2969 - val_loss: 10.3714\n",
      "\n",
      "[20210302-0007-24] Learning rate for epoch 83 is 0.006719999946653843\n",
      "Epoch 83/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 13.4224\n",
      "Epoch 00083: val_loss did not improve from 10.26669\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 13.3272 - val_loss: 10.4255\n",
      "\n",
      "*********************************** Start fine tune ***********************************\n",
      "tt: 195, nt:46, total layers:241\n",
      "*********************************** Start fine tune ***********************************\n",
      "\n",
      "[20210302-0007-28] Learning rate for epoch 1 is 4.999999987376214e-07\n",
      "Epoch 1/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.1083\n",
      "Epoch 00001: val_loss improved from 10.26669 to 10.17019, saving model to ./20210301-225844/toe_K3_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 16s 784ms/step - loss: 15.1083 - val_loss: 10.1702\n",
      "\n",
      "[20210302-0008-00] Learning rate for epoch 2 is 5.044450517743826e-05\n",
      "Epoch 2/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.9793\n",
      "Epoch 00002: val_loss did not improve from 10.17019\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 14.9793 - val_loss: 10.4377\n",
      "\n",
      "[20210302-0008-05] Learning rate for epoch 3 is 0.00010028902033809572\n",
      "Epoch 3/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.6091\n",
      "Epoch 00008: val_loss improved from 9.71147 to 9.31839, saving model to ./20210301-225844/toe_K3_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 89ms/step - loss: 11.6091 - val_loss: 9.3184\n",
      "\n",
      "[20210302-0008-50] Learning rate for epoch 9 is 0.00039725631359033287\n",
      "Epoch 9/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.5653\n",
      "Epoch 00009: val_loss improved from 9.31839 to 8.98250, saving model to ./20210301-225844/toe_K3_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 11.5653 - val_loss: 8.9825\n",
      "\n",
      "[20210302-0008-55] Learning rate for epoch 10 is 0.0004464008961804211\n",
      "Epoch 10/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.1231\n",
      "Epoch 00010: val_loss did not improve from 8.98250\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 11.1231 - val_loss: 9.4964\n",
      "\n",
      "[20210302-0009-00] Learning rate for epoch 11 is 0.0004954455071128905\n",
      "Epoch 11/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.9091\n",
      "Epoch 00011: val_loss improved from 8.98250 to 8.96218, saving model to ./20210301-225844/toe_K3_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 10.9091 - val_loss: 8.9622\n",
      "\n",
      "[20210302-0009-05] Learning rate for epoch 12 is 0.0005443901172839105\n",
      "Epoch 12/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.9135\n",
      "Epoch 00012: val_loss improved from 8.96218 to 8.57328, saving model to ./20210301-225844/toe_K3_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 89ms/step - loss: 10.9135 - val_loss: 8.5733\n",
      "\n",
      "[20210302-0009-10] Learning rate for epoch 13 is 0.0005932347266934812\n",
      "Epoch 13/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.8928\n",
      "Epoch 00013: val_loss improved from 8.57328 to 8.02333, saving model to ./20210301-225844/toe_K3_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 89ms/step - loss: 10.8928 - val_loss: 8.0233\n",
      "\n",
      "[20210302-0009-15] Learning rate for epoch 14 is 0.0006419793353416026\n",
      "Epoch 14/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.5777\n",
      "Epoch 00014: val_loss did not improve from 8.02333\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 10.5777 - val_loss: 9.4913\n",
      "\n",
      "[20210302-0009-19] Learning rate for epoch 15 is 0.0006906240014359355\n",
      "Epoch 15/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.4372\n",
      "Epoch 00015: val_loss did not improve from 8.02333\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 10.4372 - val_loss: 10.4729\n",
      "\n",
      "[20210302-0009-24] Learning rate for epoch 16 is 0.0007391686085611582\n",
      "Epoch 16/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.5718\n",
      "Epoch 00016: val_loss did not improve from 8.02333\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 10.5718 - val_loss: 10.9686\n",
      "\n",
      "[20210302-0009-29] Learning rate for epoch 17 is 0.0007876132731325924\n",
      "Epoch 17/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.4266\n",
      "Epoch 00017: val_loss did not improve from 8.02333\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 10.4266 - val_loss: 9.8139\n",
      "\n",
      "[20210302-0009-33] Learning rate for epoch 18 is 0.0008359579369425774\n",
      "Epoch 18/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.0440\n",
      "Epoch 00018: val_loss did not improve from 8.02333\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 10.0440 - val_loss: 9.4059\n",
      "\n",
      "[20210302-0009-38] Learning rate for epoch 19 is 0.000884202599991113\n",
      "Epoch 19/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.9560 \n",
      "Epoch 00019: val_loss did not improve from 8.02333\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 9.9560 - val_loss: 9.1918\n",
      "\n",
      "[20210302-0009-43] Learning rate for epoch 20 is 0.0009323473204858601\n",
      "Epoch 20/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.9071\n",
      "Epoch 00020: val_loss did not improve from 8.02333\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 9.9071 - val_loss: 9.3217\n",
      "\n",
      "[20210302-0009-47] Learning rate for epoch 21 is 0.000980391982011497\n",
      "Epoch 21/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.8458\n",
      "Epoch 00021: val_loss did not improve from 8.02333\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 9.8458 - val_loss: 9.4782\n",
      "\n",
      "[20210302-0009-52] Learning rate for epoch 22 is 0.0010283367009833455\n",
      "Epoch 22/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.6494\n",
      "Epoch 00022: val_loss did not improve from 8.02333\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 9.6494 - val_loss: 10.4195\n",
      "\n",
      "[20210302-0009-56] Learning rate for epoch 23 is 0.0010761814191937447\n",
      "Epoch 23/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.6223\n",
      "Epoch 00023: val_loss did not improve from 8.02333\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 9.6223 - val_loss: 8.5789\n",
      "\n",
      "[20210302-0010-01] Learning rate for epoch 24 is 0.0011239261366426945\n",
      "Epoch 24/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.4796\n",
      "Epoch 00024: val_loss did not improve from 8.02333\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 9.4796 - val_loss: 8.9896\n",
      "\n",
      "[20210302-0010-06] Learning rate for epoch 25 is 0.001171570853330195\n",
      "Epoch 25/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.3844\n",
      "Epoch 00025: val_loss did not improve from 8.02333\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 9.3844 - val_loss: 9.2925\n",
      "\n",
      "[20210302-0010-10] Learning rate for epoch 26 is 0.001219115569256246\n",
      "Epoch 26/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.0696\n",
      "Epoch 00026: val_loss did not improve from 8.02333\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 9.0696 - val_loss: 8.4297\n",
      "\n",
      "[20210302-0010-15] Learning rate for epoch 27 is 0.0012665604008361697\n",
      "Epoch 27/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.2832\n",
      "Epoch 00027: val_loss did not improve from 8.02333\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 9.2832 - val_loss: 8.9083\n",
      "\n",
      "[20210302-0010-19] Learning rate for epoch 28 is 0.0013139051152393222\n",
      "Epoch 28/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.0214\n",
      "Epoch 00028: val_loss did not improve from 8.02333\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 9.0214 - val_loss: 8.8276\n",
      "\n",
      "[20210302-0010-24] Learning rate for epoch 29 is 0.0013611499452963471\n",
      "Epoch 29/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.9897\n",
      "Epoch 00029: val_loss did not improve from 8.02333\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 8.9897 - val_loss: 9.1341\n",
      "\n",
      "[20210302-0010-29] Learning rate for epoch 30 is 0.001408294658176601\n",
      "Epoch 30/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.1911\n",
      "Epoch 00030: val_loss did not improve from 8.02333\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 9.1911 - val_loss: 9.5498\n",
      "\n",
      "[20210302-0010-33] Learning rate for epoch 31 is 0.0014553394867107272\n",
      "Epoch 31/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.7578\n",
      "Epoch 00031: val_loss did not improve from 8.02333\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 8.7578 - val_loss: 9.7557\n",
      "\n",
      "[20210302-0010-38] Learning rate for epoch 32 is 0.0015022843144834042\n",
      "Epoch 32/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.0175\n",
      "Epoch 00032: val_loss did not improve from 8.02333\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 9.0175 - val_loss: 8.5191\n",
      "\n",
      "[20210302-0010-43] Learning rate for epoch 33 is 0.0015491291414946318\n",
      "Epoch 33/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.8519\n",
      "Epoch 00033: val_loss improved from 8.02333 to 7.60367, saving model to ./20210301-225844/toe_K3_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 89ms/step - loss: 8.8519 - val_loss: 7.6037\n",
      "\n",
      "[20210302-0010-48] Learning rate for epoch 34 is 0.00159587396774441\n",
      "Epoch 34/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.7869\n",
      "Epoch 00034: val_loss did not improve from 7.60367\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 8.7869 - val_loss: 7.9945\n",
      "\n",
      "[20210302-0010-52] Learning rate for epoch 35 is 0.001642518793232739\n",
      "Epoch 35/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.6275\n",
      "Epoch 00035: val_loss did not improve from 7.60367\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 8.6275 - val_loss: 9.2420\n",
      "\n",
      "[20210302-0010-57] Learning rate for epoch 36 is 0.0016890636179596186\n",
      "Epoch 36/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.2348\n",
      "Epoch 00036: val_loss did not improve from 7.60367\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 8.2348 - val_loss: 8.5691\n",
      "\n",
      "[20210302-0011-02] Learning rate for epoch 37 is 0.0017355084419250488\n",
      "Epoch 37/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.4629\n",
      "Epoch 00037: val_loss did not improve from 7.60367\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 8.4629 - val_loss: 8.4899\n",
      "\n",
      "[20210302-0011-06] Learning rate for epoch 38 is 0.0017818533815443516\n",
      "Epoch 38/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.3639\n",
      "Epoch 00038: val_loss did not improve from 7.60367\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 8.3639 - val_loss: 9.7809\n",
      "\n",
      "[20210302-0011-11] Learning rate for epoch 39 is 0.0018280982039868832\n",
      "Epoch 39/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.2731\n",
      "Epoch 00039: val_loss did not improve from 7.60367\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 8.2731 - val_loss: 9.2492\n",
      "\n",
      "[20210302-0011-15] Learning rate for epoch 40 is 0.0018742431420832872\n",
      "Epoch 40/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.2361\n",
      "Epoch 00040: val_loss did not improve from 7.60367\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 8.2361 - val_loss: 7.9563\n",
      "\n",
      "[20210302-0011-20] Learning rate for epoch 41 is 0.0019202879630029202\n",
      "Epoch 41/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.2747\n",
      "Epoch 00041: val_loss did not improve from 7.60367\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 8.2747 - val_loss: 8.1205\n",
      "\n",
      "[20210302-0011-25] Learning rate for epoch 42 is 0.0019662328995764256\n",
      "Epoch 42/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.2110\n",
      "Epoch 00042: val_loss did not improve from 7.60367\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 8.2110 - val_loss: 8.8236\n",
      "\n",
      "[20210302-0011-29] Learning rate for epoch 43 is 0.00201207771897316\n",
      "Epoch 43/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8551\n",
      "Epoch 00043: val_loss improved from 7.60367 to 7.52528, saving model to ./20210301-225844/toe_K3_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 7.8551 - val_loss: 7.5253\n",
      "\n",
      "[20210302-0011-34] Learning rate for epoch 44 is 0.0020578226540237665\n",
      "Epoch 44/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.2884\n",
      "Epoch 00044: val_loss improved from 7.52528 to 7.08625, saving model to ./20210301-225844/toe_K3_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 92ms/step - loss: 8.2884 - val_loss: 7.0862\n",
      "\n",
      "[20210302-0011-40] Learning rate for epoch 45 is 0.0021034677047282457\n",
      "Epoch 45/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.3038\n",
      "Epoch 00045: val_loss improved from 7.08625 to 6.87174, saving model to ./20210301-225844/toe_K3_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 89ms/step - loss: 8.3038 - val_loss: 6.8717\n",
      "\n",
      "[20210302-0011-45] Learning rate for epoch 46 is 0.002149012638255954\n",
      "Epoch 46/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.0761\n",
      "Epoch 00046: val_loss did not improve from 6.87174\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 8.0761 - val_loss: 7.6164\n",
      "\n",
      "[20210302-0011-49] Learning rate for epoch 47 is 0.0021944576874375343\n",
      "Epoch 47/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.9712\n",
      "Epoch 00047: val_loss did not improve from 6.87174\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.9712 - val_loss: 8.6447\n",
      "\n",
      "[20210302-0011-54] Learning rate for epoch 48 is 0.0022398026194423437\n",
      "Epoch 48/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.0959\n",
      "Epoch 00048: val_loss did not improve from 6.87174\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 8.0959 - val_loss: 8.2796\n",
      "\n",
      "[20210302-0011-58] Learning rate for epoch 49 is 0.002285047434270382\n",
      "Epoch 49/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6866\n",
      "Epoch 00049: val_loss did not improve from 6.87174\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.6866 - val_loss: 7.5994\n",
      "\n",
      "[20210302-0012-03] Learning rate for epoch 50 is 0.0023301925975829363\n",
      "Epoch 50/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8766\n",
      "Epoch 00050: val_loss did not improve from 6.87174\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.8766 - val_loss: 7.9695\n",
      "\n",
      "[20210302-0012-08] Learning rate for epoch 51 is 0.002375237410888076\n",
      "Epoch 51/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8448\n",
      "Epoch 00051: val_loss did not improve from 6.87174\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.8448 - val_loss: 8.2625\n",
      "\n",
      "[20210302-0012-12] Learning rate for epoch 52 is 0.0024201825726777315\n",
      "Epoch 52/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8156\n",
      "Epoch 00052: val_loss did not improve from 6.87174\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.8156 - val_loss: 8.1330\n",
      "\n",
      "[20210302-0012-17] Learning rate for epoch 53 is 0.002465027617290616\n",
      "Epoch 53/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7007\n",
      "Epoch 00053: val_loss did not improve from 6.87174\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.7007 - val_loss: 7.6672\n",
      "\n",
      "[20210302-0012-21] Learning rate for epoch 54 is 0.0025097725447267294\n",
      "Epoch 54/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8060\n",
      "Epoch 00054: val_loss improved from 6.87174 to 6.61799, saving model to ./20210301-225844/toe_K3_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 91ms/step - loss: 7.8060 - val_loss: 6.6180\n",
      "\n",
      "[20210302-0012-26] Learning rate for epoch 55 is 0.0025544175878167152\n",
      "Epoch 55/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8665\n",
      "Epoch 00055: val_loss did not improve from 6.61799\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.8665 - val_loss: 7.9859\n",
      "\n",
      "[20210302-0012-31] Learning rate for epoch 56 is 0.00259896251372993\n",
      "Epoch 56/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.9142\n",
      "Epoch 00056: val_loss did not improve from 6.61799\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.9142 - val_loss: 7.1701\n",
      "\n",
      "[20210302-0012-36] Learning rate for epoch 57 is 0.0026434077881276608\n",
      "Epoch 57/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8368\n",
      "Epoch 00057: val_loss did not improve from 6.61799\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.8368 - val_loss: 6.7994\n",
      "\n",
      "[20210302-0012-40] Learning rate for epoch 58 is 0.0026877527125179768\n",
      "Epoch 58/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5003\n",
      "Epoch 00058: val_loss did not improve from 6.61799\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.5003 - val_loss: 7.8971\n",
      "\n",
      "[20210302-0012-45] Learning rate for epoch 59 is 0.0027319977525621653\n",
      "Epoch 59/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5588\n",
      "Epoch 00059: val_loss did not improve from 6.61799\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.5588 - val_loss: 7.1242\n",
      "\n",
      "[20210302-0012-49] Learning rate for epoch 60 is 0.0027761429082602262\n",
      "Epoch 60/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8746\n",
      "Epoch 00060: val_loss did not improve from 6.61799\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.8746 - val_loss: 7.3504\n",
      "\n",
      "[20210302-0012-54] Learning rate for epoch 61 is 0.002820187946781516\n",
      "Epoch 61/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4516\n",
      "Epoch 00061: val_loss did not improve from 6.61799\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.4516 - val_loss: 6.8819\n",
      "\n",
      "[20210302-0012-59] Learning rate for epoch 62 is 0.0028641331009566784\n",
      "Epoch 62/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5933\n",
      "Epoch 00062: val_loss did not improve from 6.61799\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.5933 - val_loss: 6.9700\n",
      "\n",
      "[20210302-0013-03] Learning rate for epoch 63 is 0.0029079781379550695\n",
      "Epoch 63/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6676\n",
      "Epoch 00063: val_loss did not improve from 6.61799\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.6676 - val_loss: 6.6932\n",
      "\n",
      "[20210302-0013-08] Learning rate for epoch 64 is 0.002951723290607333\n",
      "Epoch 64/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3259\n",
      "Epoch 00064: val_loss did not improve from 6.61799\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.3259 - val_loss: 8.5338\n",
      "\n",
      "[20210302-0013-12] Learning rate for epoch 65 is 0.0029953685589134693\n",
      "Epoch 65/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1888\n",
      "Epoch 00065: val_loss did not improve from 6.61799\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.1888 - val_loss: 7.5357\n",
      "\n",
      "[20210302-0013-17] Learning rate for epoch 66 is 0.0030389137100428343\n",
      "Epoch 66/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6199\n",
      "Epoch 00066: val_loss did not improve from 6.61799\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.6199 - val_loss: 8.9885\n",
      "\n",
      "[20210302-0013-22] Learning rate for epoch 67 is 0.003082358743995428\n",
      "Epoch 67/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4094\n",
      "Epoch 00067: val_loss did not improve from 6.61799\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.4094 - val_loss: 7.3015\n",
      "\n",
      "[20210302-0013-26] Learning rate for epoch 68 is 0.0031257038936018944\n",
      "Epoch 68/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7109\n",
      "Epoch 00068: val_loss improved from 6.61799 to 6.53054, saving model to ./20210301-225844/toe_K3_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 89ms/step - loss: 7.7109 - val_loss: 6.5305\n",
      "\n",
      "[20210302-0013-31] Learning rate for epoch 69 is 0.003168949158862233\n",
      "Epoch 69/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6281\n",
      "Epoch 00069: val_loss did not improve from 6.53054\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.6281 - val_loss: 7.2974\n",
      "\n",
      "[20210302-0013-36] Learning rate for epoch 70 is 0.0032120943069458008\n",
      "Epoch 70/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3958\n",
      "Epoch 00070: val_loss improved from 6.53054 to 6.43625, saving model to ./20210301-225844/toe_K3_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 7.3958 - val_loss: 6.4363\n",
      "\n",
      "[20210302-0013-41] Learning rate for epoch 71 is 0.003255139570683241\n",
      "Epoch 71/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4451\n",
      "Epoch 00071: val_loss did not improve from 6.43625\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.4451 - val_loss: 7.2886\n",
      "\n",
      "[20210302-0013-46] Learning rate for epoch 72 is 0.00329808471724391\n",
      "Epoch 72/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4392\n",
      "Epoch 00072: val_loss improved from 6.43625 to 5.80003, saving model to ./20210301-225844/toe_K3_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 7.4392 - val_loss: 5.8000\n",
      "\n",
      "[20210302-0013-51] Learning rate for epoch 73 is 0.0033409299794584513\n",
      "Epoch 73/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4271\n",
      "Epoch 00073: val_loss did not improve from 5.80003\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.4271 - val_loss: 7.1268\n",
      "\n",
      "[20210302-0013-55] Learning rate for epoch 74 is 0.0033836751244962215\n",
      "Epoch 74/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5566\n",
      "Epoch 00074: val_loss did not improve from 5.80003\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 7.5566 - val_loss: 6.3372\n",
      "\n",
      "[20210302-0014-00] Learning rate for epoch 75 is 0.0034263203851878643\n",
      "Epoch 75/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2911\n",
      "Epoch 00075: val_loss did not improve from 5.80003\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.2911 - val_loss: 7.1765\n",
      "\n",
      "[20210302-0014-05] Learning rate for epoch 76 is 0.003468865528702736\n",
      "Epoch 76/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5602\n",
      "Epoch 00076: val_loss did not improve from 5.80003\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.5602 - val_loss: 7.3868\n",
      "\n",
      "[20210302-0014-09] Learning rate for epoch 77 is 0.00351131078787148\n",
      "Epoch 77/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5958\n",
      "Epoch 00077: val_loss did not improve from 5.80003\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.5958 - val_loss: 6.2398\n",
      "\n",
      "[20210302-0014-14] Learning rate for epoch 78 is 0.0035536561626940966\n",
      "Epoch 78/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6579\n",
      "Epoch 00078: val_loss did not improve from 5.80003\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.6579 - val_loss: 6.6780\n",
      "\n",
      "[20210302-0014-18] Learning rate for epoch 79 is 0.003595901420339942\n",
      "Epoch 79/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6454\n",
      "Epoch 00079: val_loss did not improve from 5.80003\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.6454 - val_loss: 8.1153\n",
      "\n",
      "[20210302-0014-23] Learning rate for epoch 80 is 0.00363804679363966\n",
      "Epoch 80/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2078\n",
      "Epoch 00080: val_loss did not improve from 5.80003\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.2078 - val_loss: 7.4093\n",
      "\n",
      "[20210302-0014-28] Learning rate for epoch 81 is 0.0036800920497626066\n",
      "Epoch 81/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3002\n",
      "Epoch 00081: val_loss did not improve from 5.80003\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.3002 - val_loss: 6.7147\n",
      "\n",
      "[20210302-0014-32] Learning rate for epoch 82 is 0.003722037188708782\n",
      "Epoch 82/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4587\n",
      "Epoch 00082: val_loss did not improve from 5.80003\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.4587 - val_loss: 7.7246\n",
      "\n",
      "[20210302-0014-37] Learning rate for epoch 83 is 0.003763882676139474\n",
      "Epoch 83/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4214\n",
      "Epoch 00083: val_loss did not improve from 5.80003\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.4214 - val_loss: 7.2858\n",
      "\n",
      "[20210302-0014-41] Learning rate for epoch 84 is 0.0038056280463933945\n",
      "Epoch 84/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2053\n",
      "Epoch 00084: val_loss did not improve from 5.80003\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.2053 - val_loss: 7.1357\n",
      "\n",
      "[20210302-0014-46] Learning rate for epoch 85 is 0.003847273299470544\n",
      "Epoch 85/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4281\n",
      "Epoch 00085: val_loss did not improve from 5.80003\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.4281 - val_loss: 7.2028\n",
      "\n",
      "[20210302-0014-51] Learning rate for epoch 86 is 0.0038888186682015657\n",
      "Epoch 86/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2562\n",
      "Epoch 00086: val_loss did not improve from 5.80003\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.2562 - val_loss: 6.7255\n",
      "\n",
      "[20210302-0014-56] Learning rate for epoch 87 is 0.00393026415258646\n",
      "Epoch 87/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1540\n",
      "Epoch 00087: val_loss did not improve from 5.80003\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.1540 - val_loss: 6.7963\n",
      "\n",
      "[20210302-0015-00] Learning rate for epoch 88 is 0.00397160928696394\n",
      "Epoch 88/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5874\n",
      "Epoch 00088: val_loss did not improve from 5.80003\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.5874 - val_loss: 7.0330\n",
      "\n",
      "[20210302-0015-05] Learning rate for epoch 89 is 0.004012854769825935\n",
      "Epoch 89/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4444\n",
      "Epoch 00089: val_loss did not improve from 5.80003\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.4444 - val_loss: 7.0640\n",
      "\n",
      "[20210302-0015-10] Learning rate for epoch 90 is 0.00405400013551116\n",
      "Epoch 90/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2411\n",
      "Epoch 00090: val_loss did not improve from 5.80003\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.2411 - val_loss: 8.3483\n",
      "\n",
      "[20210302-0015-14] Learning rate for epoch 91 is 0.004095045384019613\n",
      "Epoch 91/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3723\n",
      "Epoch 00091: val_loss did not improve from 5.80003\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.3723 - val_loss: 8.2883\n",
      "\n",
      "[20210302-0015-19] Learning rate for epoch 92 is 0.004135990981012583\n",
      "Epoch 92/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3367\n",
      "Epoch 00092: val_loss did not improve from 5.80003\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.3367 - val_loss: 7.7948\n",
      "\n",
      "[20210302-0015-23] Learning rate for epoch 93 is 0.004176836460828781\n",
      "Epoch 93/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3050\n",
      "Epoch 00093: val_loss did not improve from 5.80003\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.3050 - val_loss: 6.2275\n",
      "\n",
      "[20210302-0015-28] Learning rate for epoch 94 is 0.004217581823468208\n",
      "Epoch 94/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2352\n",
      "Epoch 00094: val_loss did not improve from 5.80003\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.2352 - val_loss: 8.0417\n",
      "\n",
      "[20210302-0015-33] Learning rate for epoch 95 is 0.004258227068930864\n",
      "Epoch 95/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2822\n",
      "Epoch 00095: val_loss did not improve from 5.80003\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.2822 - val_loss: 7.0757\n",
      "\n",
      "[20210302-0015-37] Learning rate for epoch 96 is 0.0042987726628780365\n",
      "Epoch 96/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0747\n",
      "Epoch 00096: val_loss did not improve from 5.80003\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.0747 - val_loss: 6.4969\n",
      "\n",
      "[20210302-0015-42] Learning rate for epoch 97 is 0.0043392181396484375\n",
      "Epoch 97/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2716\n",
      "Epoch 00097: val_loss did not improve from 5.80003\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.2716 - val_loss: 7.1621\n",
      "\n",
      "[20210302-0015-46] Learning rate for epoch 98 is 0.004379563499242067\n",
      "Epoch 98/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2954\n",
      "Epoch 00098: val_loss did not improve from 5.80003\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.2954 - val_loss: 6.8351\n",
      "\n",
      "[20210302-0015-51] Learning rate for epoch 99 is 0.004419809207320213\n",
      "Epoch 99/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3393\n",
      "Epoch 00099: val_loss did not improve from 5.80003\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.3393 - val_loss: 7.1175\n",
      "\n",
      "[20210302-0015-56] Learning rate for epoch 100 is 0.004459954332560301\n",
      "Epoch 100/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4201\n",
      "Epoch 00100: val_loss did not improve from 5.80003\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.4201 - val_loss: 8.1367\n",
      "\n",
      "[20210302-0016-00] Learning rate for epoch 101 is 4.4999998749517545e-07\n",
      "Epoch 101/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7949\n",
      "Epoch 00101: val_loss did not improve from 5.80003\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.7949 - val_loss: 7.8892\n",
      "\n",
      "[20210302-0016-05] Learning rate for epoch 102 is 0.00018023152370005846\n",
      "Epoch 102/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2457\n",
      "Epoch 00102: val_loss did not improve from 5.80003\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.2457 - val_loss: 6.7040\n",
      "\n",
      "[20210302-0016-09] Learning rate for epoch 103 is 0.000359613070031628\n",
      "Epoch 103/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9755\n",
      "Epoch 00103: val_loss did not improve from 5.80003\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.9755 - val_loss: 6.1026\n",
      "\n",
      "[20210302-0016-14] Learning rate for epoch 104 is 0.0005385946715250611\n",
      "Epoch 104/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8166\n",
      "Epoch 00104: val_loss did not improve from 5.80003\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.8166 - val_loss: 6.2974\n",
      "\n",
      "[20210302-0016-19] Learning rate for epoch 105 is 0.0007171762990765274\n",
      "Epoch 105/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8663\n",
      "Epoch 00105: val_loss did not improve from 5.80003\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.8663 - val_loss: 6.1462\n",
      "\n",
      "[20210302-0016-23] Learning rate for epoch 106 is 0.0008953579817898571\n",
      "Epoch 106/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8443\n",
      "Epoch 00106: val_loss improved from 5.80003 to 5.76055, saving model to ./20210301-225844/toe_K3_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 89ms/step - loss: 6.8443 - val_loss: 5.7605\n",
      "\n",
      "[20210302-0016-28] Learning rate for epoch 107 is 0.0010731397196650505\n",
      "Epoch 107/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8413\n",
      "Epoch 00107: val_loss did not improve from 5.76055\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.8413 - val_loss: 6.0104\n",
      "\n",
      "[20210302-0016-33] Learning rate for epoch 108 is 0.0012505215127021074\n",
      "Epoch 108/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8026\n",
      "Epoch 00108: val_loss improved from 5.76055 to 5.73921, saving model to ./20210301-225844/toe_K3_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 89ms/step - loss: 6.8026 - val_loss: 5.7392\n",
      "\n",
      "[20210302-0016-38] Learning rate for epoch 109 is 0.001427503302693367\n",
      "Epoch 109/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4213\n",
      "Epoch 00109: val_loss improved from 5.73921 to 5.66072, saving model to ./20210301-225844/toe_K3_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 91ms/step - loss: 6.4213 - val_loss: 5.6607\n",
      "\n",
      "[20210302-0016-43] Learning rate for epoch 110 is 0.0016040850896388292\n",
      "Epoch 110/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7052\n",
      "Epoch 00110: val_loss did not improve from 5.66072\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.7052 - val_loss: 6.4088\n",
      "\n",
      "[20210302-0016-48] Learning rate for epoch 111 is 0.001780266989953816\n",
      "Epoch 111/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6408\n",
      "Epoch 00111: val_loss did not improve from 5.66072\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.6408 - val_loss: 6.3708\n",
      "\n",
      "[20210302-0016-52] Learning rate for epoch 112 is 0.0019560488872230053\n",
      "Epoch 112/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8112\n",
      "Epoch 00112: val_loss did not improve from 5.66072\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.8112 - val_loss: 5.8270\n",
      "\n",
      "[20210302-0016-57] Learning rate for epoch 113 is 0.002131430897861719\n",
      "Epoch 113/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6951\n",
      "Epoch 00113: val_loss did not improve from 5.66072\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.6951 - val_loss: 5.7145\n",
      "\n",
      "[20210302-0017-02] Learning rate for epoch 114 is 0.0023064129054546356\n",
      "Epoch 114/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6858\n",
      "Epoch 00114: val_loss did not improve from 5.66072\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.6858 - val_loss: 6.1068\n",
      "\n",
      "[20210302-0017-06] Learning rate for epoch 115 is 0.0024809949100017548\n",
      "Epoch 115/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5672\n",
      "Epoch 00115: val_loss did not improve from 5.66072\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.5672 - val_loss: 6.0577\n",
      "\n",
      "[20210302-0017-11] Learning rate for epoch 116 is 0.0026551769115030766\n",
      "Epoch 116/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6295\n",
      "Epoch 00116: val_loss did not improve from 5.66072\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.6295 - val_loss: 6.5878\n",
      "\n",
      "[20210302-0017-15] Learning rate for epoch 117 is 0.0028289591427892447\n",
      "Epoch 117/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8752\n",
      "Epoch 00117: val_loss did not improve from 5.66072\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.8752 - val_loss: 6.3567\n",
      "\n",
      "[20210302-0017-20] Learning rate for epoch 118 is 0.0030023413710296154\n",
      "Epoch 118/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6771\n",
      "Epoch 00118: val_loss did not improve from 5.66072\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.6771 - val_loss: 7.2455\n",
      "\n",
      "[20210302-0017-25] Learning rate for epoch 119 is 0.003175323596224189\n",
      "Epoch 119/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6932\n",
      "Epoch 00119: val_loss did not improve from 5.66072\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.6932 - val_loss: 6.6575\n",
      "\n",
      "[20210302-0017-29] Learning rate for epoch 120 is 0.003347905818372965\n",
      "Epoch 120/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7902\n",
      "Epoch 00120: val_loss did not improve from 5.66072\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.7902 - val_loss: 7.1550\n",
      "\n",
      "[20210302-0017-34] Learning rate for epoch 121 is 0.0035200880374759436\n",
      "Epoch 121/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0171\n",
      "Epoch 00121: val_loss did not improve from 5.66072\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.0171 - val_loss: 6.0298\n",
      "\n",
      "[20210302-0017-38] Learning rate for epoch 122 is 0.003691870253533125\n",
      "Epoch 122/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9867\n",
      "Epoch 00122: val_loss did not improve from 5.66072\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.9867 - val_loss: 6.5918\n",
      "\n",
      "[20210302-0017-43] Learning rate for epoch 123 is 0.0038632526993751526\n",
      "Epoch 123/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7988\n",
      "Epoch 00123: val_loss did not improve from 5.66072\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.7988 - val_loss: 7.1583\n",
      "\n",
      "[20210302-0017-48] Learning rate for epoch 124 is 0.004034235142171383\n",
      "Epoch 124/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6711\n",
      "Epoch 00124: val_loss did not improve from 5.66072\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.6711 - val_loss: 6.7286\n",
      "\n",
      "[20210302-0017-52] Learning rate for epoch 125 is 0.004204817581921816\n",
      "Epoch 125/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1878\n",
      "Epoch 00125: val_loss did not improve from 5.66072\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.1878 - val_loss: 7.7642\n",
      "\n",
      "[20210302-0017-57] Learning rate for epoch 126 is 0.0043750000186264515\n",
      "Epoch 126/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0181\n",
      "Epoch 00126: val_loss did not improve from 5.66072\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.0181 - val_loss: 7.8080\n",
      "\n",
      "[20210302-0018-02] Learning rate for epoch 127 is 0.0041952175088226795\n",
      "Epoch 127/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9669\n",
      "Epoch 00127: val_loss did not improve from 5.66072\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.9669 - val_loss: 6.1533\n",
      "\n",
      "[20210302-0018-06] Learning rate for epoch 128 is 0.004015835002064705\n",
      "Epoch 128/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7739\n",
      "Epoch 00128: val_loss did not improve from 5.66072\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.7739 - val_loss: 6.0986\n",
      "\n",
      "[20210302-0018-11] Learning rate for epoch 129 is 0.003836852265521884\n",
      "Epoch 129/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9551\n",
      "Epoch 00129: val_loss did not improve from 5.66072\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.9551 - val_loss: 6.0713\n",
      "\n",
      "[20210302-0018-15] Learning rate for epoch 130 is 0.003658269764855504\n",
      "Epoch 130/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6172\n",
      "Epoch 00130: val_loss did not improve from 5.66072\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.6172 - val_loss: 6.0949\n",
      "\n",
      "[20210302-0018-20] Learning rate for epoch 131 is 0.003480087034404278\n",
      "Epoch 131/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0977\n",
      "Epoch 00131: val_loss did not improve from 5.66072\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.0977 - val_loss: 5.8873\n",
      "\n",
      "[20210302-0018-25] Learning rate for epoch 132 is 0.003302304306998849\n",
      "Epoch 132/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6782\n",
      "Epoch 00132: val_loss did not improve from 5.66072\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.6782 - val_loss: 6.6323\n",
      "\n",
      "[20210302-0018-29] Learning rate for epoch 133 is 0.0031249215826392174\n",
      "Epoch 133/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7108\n",
      "Epoch 00133: val_loss did not improve from 5.66072\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.7108 - val_loss: 6.8747\n",
      "\n",
      "[20210302-0018-34] Learning rate for epoch 134 is 0.0029479386284947395\n",
      "Epoch 134/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5674\n",
      "Epoch 00134: val_loss did not improve from 5.66072\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.5674 - val_loss: 6.0947\n",
      "\n",
      "[20210302-0018-38] Learning rate for epoch 135 is 0.0027713559102267027\n",
      "Epoch 135/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6257\n",
      "Epoch 00135: val_loss did not improve from 5.66072\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.6257 - val_loss: 6.4708\n",
      "\n",
      "[20210302-0018-43] Learning rate for epoch 136 is 0.0025951729621738195\n",
      "Epoch 136/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6144\n",
      "Epoch 00136: val_loss did not improve from 5.66072\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.6144 - val_loss: 7.1776\n",
      "\n",
      "[20210302-0018-48] Learning rate for epoch 137 is 0.0024193900171667337\n",
      "Epoch 137/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5013\n",
      "Epoch 00137: val_loss did not improve from 5.66072\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.5013 - val_loss: 6.1072\n",
      "\n",
      "[20210302-0018-52] Learning rate for epoch 138 is 0.0022440070752054453\n",
      "Epoch 138/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6646\n",
      "Epoch 00138: val_loss did not improve from 5.66072\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.6646 - val_loss: 6.2991\n",
      "\n",
      "[20210302-0018-57] Learning rate for epoch 139 is 0.002069024136289954\n",
      "Epoch 139/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5413\n",
      "Epoch 00139: val_loss did not improve from 5.66072\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.5413 - val_loss: 7.0485\n",
      "\n",
      "[20210302-0019-01] Learning rate for epoch 140 is 0.0018944410840049386\n",
      "Epoch 140/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4093\n",
      "Epoch 00140: val_loss did not improve from 5.66072\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.4093 - val_loss: 5.8216\n",
      "\n",
      "[20210302-0019-06] Learning rate for epoch 141 is 0.0017202580347657204\n",
      "Epoch 141/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4050\n",
      "Epoch 00141: val_loss did not improve from 5.66072\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.4050 - val_loss: 6.1540\n",
      "\n",
      "[20210302-0019-11] Learning rate for epoch 142 is 0.0015464748721569777\n",
      "Epoch 142/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4156\n",
      "Epoch 00142: val_loss did not improve from 5.66072\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.4156 - val_loss: 6.2125\n",
      "\n",
      "[20210302-0019-15] Learning rate for epoch 143 is 0.0013730917125940323\n",
      "Epoch 143/1000\n",
      " 9/20 [============>.................] - ETA: 0s - loss: 6.2361"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - ETA: 0s - loss: 6.1322\n",
      "Epoch 00287: val_loss did not improve from 4.38208\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.1322 - val_loss: 5.3321\n",
      "\n",
      "[20210302-0100-30] Learning rate for epoch 288 is 0.001853971160016954\n",
      "Epoch 288/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0114\n",
      "Epoch 00288: val_loss did not improve from 4.38208\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.0114 - val_loss: 5.2678\n",
      "\n",
      "[20210302-0100-35] Learning rate for epoch 289 is 0.001708985073491931\n",
      "Epoch 289/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9987\n",
      "Epoch 00289: val_loss did not improve from 4.38208\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.9987 - val_loss: 5.6480\n",
      "\n",
      "[20210302-0100-39] Learning rate for epoch 290 is 0.0015643991064280272\n",
      "Epoch 290/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8787\n",
      "Epoch 00290: val_loss did not improve from 4.38208\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.8787 - val_loss: 5.3578\n",
      "\n",
      "[20210302-0100-44] Learning rate for epoch 291 is 0.0014202130259945989\n",
      "Epoch 291/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9036\n",
      "Epoch 00291: val_loss did not improve from 4.38208\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.9036 - val_loss: 5.2773\n",
      "\n",
      "[20210302-0100-49] Learning rate for epoch 292 is 0.001276426832191646\n",
      "Epoch 292/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8010\n",
      "Epoch 00292: val_loss did not improve from 4.38208\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.8010 - val_loss: 5.2453\n",
      "\n",
      "[20210302-0100-53] Learning rate for epoch 293 is 0.0011330407578498125\n",
      "Epoch 293/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6952\n",
      "Epoch 00293: val_loss did not improve from 4.38208\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.6952 - val_loss: 4.8905\n",
      "\n",
      "[20210302-0100-58] Learning rate for epoch 294 is 0.0009900545701384544\n",
      "Epoch 294/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8630\n",
      "Epoch 00294: val_loss did not improve from 4.38208\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.8630 - val_loss: 5.5120\n",
      "\n",
      "[20210302-0101-03] Learning rate for epoch 295 is 0.0008474682690575719\n",
      "Epoch 295/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6927\n",
      "Epoch 00295: val_loss did not improve from 4.38208\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6927 - val_loss: 5.6036\n",
      "\n",
      "[20210302-0101-07] Learning rate for epoch 296 is 0.0007052819710224867\n",
      "Epoch 296/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7574\n",
      "Epoch 00296: val_loss did not improve from 4.38208\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.7574 - val_loss: 5.4200\n",
      "\n",
      "[20210302-0101-12] Learning rate for epoch 297 is 0.0005634956760331988\n",
      "Epoch 297/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5944\n",
      "Epoch 00297: val_loss did not improve from 4.38208\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.5944 - val_loss: 5.4654\n",
      "\n",
      "[20210302-0101-17] Learning rate for epoch 298 is 0.0004221093258820474\n",
      "Epoch 298/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5419\n",
      "Epoch 00298: val_loss did not improve from 4.38208\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.5419 - val_loss: 5.3503\n",
      "\n",
      "[20210302-0101-21] Learning rate for epoch 299 is 0.00028112292056903243\n",
      "Epoch 299/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5003\n",
      "Epoch 00299: val_loss did not improve from 4.38208\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.5003 - val_loss: 5.2117\n",
      "\n",
      "[20210302-0101-26] Learning rate for epoch 300 is 0.0001405364746460691\n",
      "Epoch 300/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4114\n",
      "Epoch 00300: val_loss did not improve from 4.38208\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.4114 - val_loss: 5.1906\n",
      "\n",
      "[20210302-0101-30] Learning rate for epoch 301 is 3.4999999343199306e-07\n",
      "Epoch 301/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6113\n",
      "Epoch 00301: val_loss did not improve from 4.38208\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.6113 - val_loss: 5.1642\n",
      "\n",
      "[20210302-0101-35] Learning rate for epoch 302 is 0.00014013552572578192\n",
      "Epoch 302/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6652\n",
      "Epoch 00302: val_loss did not improve from 4.38208\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.6652 - val_loss: 5.1586\n",
      "\n",
      "[20210302-0101-40] Learning rate for epoch 303 is 0.00027952107484452426\n",
      "Epoch 303/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5545\n",
      "Epoch 00303: val_loss did not improve from 4.38208\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5545 - val_loss: 5.0949\n",
      "\n",
      "[20210302-0101-44] Learning rate for epoch 304 is 0.0004185066791251302\n",
      "Epoch 304/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5552\n",
      "Epoch 00304: val_loss did not improve from 4.38208\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.5552 - val_loss: 5.2050\n",
      "\n",
      " \n",
      " K =  5 \n",
      "\n",
      "Check cache in memory:Y True\n",
      "Check keypoints process01: Tensor(\"args_1:0\", shape=(2,), dtype=int64) (2,) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Check keypoints process02: Tensor(\"Reshape:0\", shape=(1, 2), dtype=int64) (1, 2) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Check keypoints process03: Tensor(\"PyFunc:1\", dtype=float32, device=/job:localhost/replica:0/task:0)\n",
      "Check keypoints process04: Tensor(\"Reshape_1:0\", shape=(2,), dtype=float32)\n",
      "Check augment :Y True\n",
      "Check ds cache[True] and augment[True]\n",
      "Check cache in memory:Y True\n",
      "Check augment :N False\n",
      "Check ds cache[True] and augment[False]\n",
      "best_model_name: ./20210301-225844/toe_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "base_model.trainable :  False\n",
      "\n",
      "[20210302-0101-51] Learning rate for epoch 1 is 0.009999999776482582\n",
      "Epoch 1/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 75.3271\n",
      "Epoch 00001: val_loss improved from inf to 55.64592, saving model to ./20210301-225844/toe_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 5s 259ms/step - loss: 73.8772 - val_loss: 55.6459\n",
      "\n",
      "[20210302-0102-06] Learning rate for epoch 2 is 0.009960000403225422\n",
      "Epoch 2/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 32.4091\n",
      "Epoch 00002: val_loss improved from 55.64592 to 21.80605, saving model to ./20210301-225844/toe_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 32.4091 - val_loss: 21.8061\n",
      "\n",
      "[20210302-0102-10] Learning rate for epoch 3 is 0.009920000098645687\n",
      "Epoch 3/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 21.1768\n",
      "Epoch 00003: val_loss improved from 21.80605 to 13.68128, saving model to ./20210301-225844/toe_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 7s 373ms/step - loss: 21.1768 - val_loss: 13.6813\n",
      "\n",
      "[20210302-0102-21] Learning rate for epoch 4 is 0.009879999794065952\n",
      "Epoch 4/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 18.5056\n",
      "Epoch 00004: val_loss improved from 13.68128 to 11.59294, saving model to ./20210301-225844/toe_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 18.5057 - val_loss: 11.5929\n",
      "\n",
      "[20210302-0102-25] Learning rate for epoch 5 is 0.009840000420808792\n",
      "Epoch 5/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 18.0559\n",
      "Epoch 00005: val_loss improved from 11.59294 to 11.54141, saving model to ./20210301-225844/toe_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 18.0329 - val_loss: 11.5414\n",
      "\n",
      "[20210302-0102-29] Learning rate for epoch 6 is 0.009800000116229057\n",
      "Epoch 6/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 17.0099\n",
      "Epoch 00006: val_loss improved from 11.54141 to 11.13343, saving model to ./20210301-225844/toe_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 17.0519 - val_loss: 11.1334\n",
      "\n",
      "[20210302-0102-33] Learning rate for epoch 7 is 0.009759999811649323\n",
      "Epoch 7/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 17.1990\n",
      "Epoch 00007: val_loss did not improve from 11.13343\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 17.1103 - val_loss: 11.1515\n",
      "\n",
      "[20210302-0102-37] Learning rate for epoch 8 is 0.009720000438392162\n",
      "Epoch 8/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.6466\n",
      "Epoch 00008: val_loss improved from 11.13343 to 10.83632, saving model to ./20210301-225844/toe_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.6466 - val_loss: 10.8363\n",
      "\n",
      "[20210302-0102-41] Learning rate for epoch 9 is 0.009680000133812428\n",
      "Epoch 9/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.8848\n",
      "Epoch 00009: val_loss improved from 10.83632 to 10.69240, saving model to ./20210301-225844/toe_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 15.8848 - val_loss: 10.6924\n",
      "\n",
      "[20210302-0102-45] Learning rate for epoch 10 is 0.009639999829232693\n",
      "Epoch 10/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.4473\n",
      "Epoch 00010: val_loss improved from 10.69240 to 10.65301, saving model to ./20210301-225844/toe_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 15.3291 - val_loss: 10.6530\n",
      "\n",
      "[20210302-0102-49] Learning rate for epoch 11 is 0.009600000455975533\n",
      "Epoch 11/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.9698\n",
      "Epoch 00011: val_loss improved from 10.65301 to 10.61751, saving model to ./20210301-225844/toe_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 15.8943 - val_loss: 10.6175\n",
      "\n",
      "[20210302-0102-53] Learning rate for epoch 12 is 0.009560000151395798\n",
      "Epoch 12/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.0234\n",
      "Epoch 00012: val_loss did not improve from 10.61751\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.0573 - val_loss: 11.3496\n",
      "\n",
      "[20210302-0102-57] Learning rate for epoch 13 is 0.009519999846816063\n",
      "Epoch 13/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.5144\n",
      "Epoch 00013: val_loss improved from 10.61751 to 10.51821, saving model to ./20210301-225844/toe_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 15.5144 - val_loss: 10.5182\n",
      "\n",
      "[20210302-0103-02] Learning rate for epoch 14 is 0.009479999542236328\n",
      "Epoch 14/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 14.8286\n",
      "Epoch 00014: val_loss improved from 10.51821 to 10.23738, saving model to ./20210301-225844/toe_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 14.8959 - val_loss: 10.2374\n",
      "\n",
      "[20210302-0103-06] Learning rate for epoch 15 is 0.009440000168979168\n",
      "Epoch 15/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.9360\n",
      "Epoch 00015: val_loss did not improve from 10.23738\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 14.9360 - val_loss: 10.5088\n",
      "\n",
      "[20210302-0103-10] Learning rate for epoch 16 is 0.009399999864399433\n",
      "Epoch 16/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.7178\n",
      "Epoch 00016: val_loss did not improve from 10.23738\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 14.7178 - val_loss: 10.4152\n",
      "\n",
      "[20210302-0103-14] Learning rate for epoch 17 is 0.009359999559819698\n",
      "Epoch 17/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 14.7908\n",
      "Epoch 00017: val_loss did not improve from 10.23738\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 14.8141 - val_loss: 10.3559\n",
      "\n",
      "[20210302-0103-18] Learning rate for epoch 18 is 0.009320000186562538\n",
      "Epoch 18/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.2056\n",
      "Epoch 00018: val_loss did not improve from 10.23738\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.1033 - val_loss: 10.5262\n",
      "\n",
      "[20210302-0103-22] Learning rate for epoch 19 is 0.009279999881982803\n",
      "Epoch 19/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 14.4463\n",
      "Epoch 00019: val_loss did not improve from 10.23738\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 14.4327 - val_loss: 11.0556\n",
      "\n",
      "[20210302-0103-25] Learning rate for epoch 20 is 0.009239999577403069\n",
      "Epoch 20/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.7992\n",
      "Epoch 00020: val_loss did not improve from 10.23738\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 14.7992 - val_loss: 10.2649\n",
      "\n",
      "[20210302-0103-29] Learning rate for epoch 21 is 0.009200000204145908\n",
      "Epoch 21/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 14.6365\n",
      "Epoch 00021: val_loss did not improve from 10.23738\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 14.5938 - val_loss: 10.6576\n",
      "\n",
      "[20210302-0103-33] Learning rate for epoch 22 is 0.009159999899566174\n",
      "Epoch 22/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 14.7056\n",
      "Epoch 00022: val_loss did not improve from 10.23738\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 14.6214 - val_loss: 10.4641\n",
      "\n",
      "[20210302-0103-37] Learning rate for epoch 23 is 0.009119999594986439\n",
      "Epoch 23/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 14.4560\n",
      "Epoch 00023: val_loss improved from 10.23738 to 10.06030, saving model to ./20210301-225844/toe_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 14.4279 - val_loss: 10.0603\n",
      "\n",
      "[20210302-0103-41] Learning rate for epoch 24 is 0.009080000221729279\n",
      "Epoch 24/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.5463\n",
      "Epoch 00024: val_loss improved from 10.06030 to 9.98075, saving model to ./20210301-225844/toe_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 14.5463 - val_loss: 9.9808\n",
      "\n",
      "[20210302-0103-45] Learning rate for epoch 25 is 0.009039999917149544\n",
      "Epoch 25/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 14.4197\n",
      "Epoch 00025: val_loss improved from 9.98075 to 9.92578, saving model to ./20210301-225844/toe_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 14.2714 - val_loss: 9.9258\n",
      "\n",
      "[20210302-0103-49] Learning rate for epoch 26 is 0.008999999612569809\n",
      "Epoch 26/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 13.7540\n",
      "Epoch 00026: val_loss did not improve from 9.92578\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 13.9708 - val_loss: 10.0604\n",
      "\n",
      "[20210302-0103-53] Learning rate for epoch 27 is 0.008960000239312649\n",
      "Epoch 27/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 14.1643\n",
      "Epoch 00027: val_loss did not improve from 9.92578\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 14.0881 - val_loss: 10.3554\n",
      "\n",
      "[20210302-0103-56] Learning rate for epoch 28 is 0.008919999934732914\n",
      "Epoch 28/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 14.7077\n",
      "Epoch 00028: val_loss did not improve from 9.92578\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 14.6539 - val_loss: 10.2543\n",
      "\n",
      "[20210302-0104-00] Learning rate for epoch 29 is 0.00887999963015318\n",
      "Epoch 29/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 13.9787\n",
      "Epoch 00029: val_loss did not improve from 9.92578\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 14.0312 - val_loss: 9.9353\n",
      "\n",
      "[20210302-0104-04] Learning rate for epoch 30 is 0.008840000256896019\n",
      "Epoch 30/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 14.3760\n",
      "Epoch 00030: val_loss did not improve from 9.92578\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 14.3578 - val_loss: 9.9659\n",
      "\n",
      "[20210302-0104-08] Learning rate for epoch 31 is 0.008799999952316284\n",
      "Epoch 31/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 14.1220\n",
      "Epoch 00031: val_loss did not improve from 9.92578\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 14.1455 - val_loss: 10.0315\n",
      "\n",
      "[20210302-0104-11] Learning rate for epoch 32 is 0.00875999964773655\n",
      "Epoch 32/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 13.8393\n",
      "Epoch 00032: val_loss did not improve from 9.92578\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 13.8944 - val_loss: 10.0364\n",
      "\n",
      "[20210302-0104-15] Learning rate for epoch 33 is 0.00872000027447939\n",
      "Epoch 33/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.3175\n",
      "Epoch 00033: val_loss did not improve from 9.92578\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 14.3175 - val_loss: 10.1297\n",
      "\n",
      "[20210302-0104-19] Learning rate for epoch 34 is 0.008679999969899654\n",
      "Epoch 34/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 13.7020\n",
      "Epoch 00034: val_loss improved from 9.92578 to 9.88977, saving model to ./20210301-225844/toe_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 13.7025 - val_loss: 9.8898\n",
      "\n",
      "[20210302-0104-23] Learning rate for epoch 35 is 0.00863999966531992\n",
      "Epoch 35/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.1454\n",
      "Epoch 00035: val_loss did not improve from 9.88977\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 14.1454 - val_loss: 9.9361\n",
      "\n",
      "[20210302-0104-27] Learning rate for epoch 36 is 0.00860000029206276\n",
      "Epoch 36/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 14.2034\n",
      "Epoch 00036: val_loss did not improve from 9.88977\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 14.2154 - val_loss: 10.2109\n",
      "\n",
      "[20210302-0104-30] Learning rate for epoch 37 is 0.008559999987483025\n",
      "Epoch 37/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.0934\n",
      "Epoch 00037: val_loss did not improve from 9.88977\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 14.0934 - val_loss: 10.5340\n",
      "\n",
      "[20210302-0104-34] Learning rate for epoch 38 is 0.00851999968290329\n",
      "Epoch 38/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 14.2571\n",
      "Epoch 00038: val_loss did not improve from 9.88977\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 14.2112 - val_loss: 10.5083\n",
      "\n",
      "[20210302-0104-38] Learning rate for epoch 39 is 0.00848000030964613\n",
      "Epoch 39/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 14.1914\n",
      "Epoch 00039: val_loss did not improve from 9.88977\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 14.0844 - val_loss: 10.1272\n",
      "\n",
      "[20210302-0104-42] Learning rate for epoch 40 is 0.008440000005066395\n",
      "Epoch 40/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 14.2774\n",
      "Epoch 00040: val_loss did not improve from 9.88977\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 14.3706 - val_loss: 10.0412\n",
      "\n",
      "[20210302-0104-45] Learning rate for epoch 41 is 0.00839999970048666\n",
      "Epoch 41/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 14.1410\n",
      "Epoch 00041: val_loss did not improve from 9.88977\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 14.0386 - val_loss: 10.1731\n",
      "\n",
      "[20210302-0104-49] Learning rate for epoch 42 is 0.0083600003272295\n",
      "Epoch 42/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 14.0944\n",
      "Epoch 00042: val_loss did not improve from 9.88977\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 13.9905 - val_loss: 10.3711\n",
      "\n",
      "[20210302-0104-53] Learning rate for epoch 43 is 0.008320000022649765\n",
      "Epoch 43/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 13.9073\n",
      "Epoch 00043: val_loss did not improve from 9.88977\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 13.8674 - val_loss: 10.6534\n",
      "\n",
      "[20210302-0104-56] Learning rate for epoch 44 is 0.00827999971807003\n",
      "Epoch 44/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.5817\n",
      "Epoch 00044: val_loss did not improve from 9.88977\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 13.5817 - val_loss: 10.1579\n",
      "\n",
      "[20210302-0105-00] Learning rate for epoch 45 is 0.00824000034481287\n",
      "Epoch 45/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 13.6498\n",
      "Epoch 00045: val_loss did not improve from 9.88977\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 13.6469 - val_loss: 10.1691\n",
      "\n",
      "[20210302-0105-04] Learning rate for epoch 46 is 0.008200000040233135\n",
      "Epoch 46/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 13.6282\n",
      "Epoch 00046: val_loss did not improve from 9.88977\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 13.5330 - val_loss: 10.3105\n",
      "\n",
      "[20210302-0105-08] Learning rate for epoch 47 is 0.0081599997356534\n",
      "Epoch 47/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.6061\n",
      "Epoch 00047: val_loss did not improve from 9.88977\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 13.6061 - val_loss: 10.0480\n",
      "\n",
      "[20210302-0105-11] Learning rate for epoch 48 is 0.00812000036239624\n",
      "Epoch 48/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 13.7045\n",
      "Epoch 00048: val_loss did not improve from 9.88977\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 13.6719 - val_loss: 10.1036\n",
      "\n",
      "[20210302-0105-15] Learning rate for epoch 49 is 0.008080000057816505\n",
      "Epoch 49/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 13.4004\n",
      "Epoch 00049: val_loss did not improve from 9.88977\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 13.4264 - val_loss: 10.1729\n",
      "\n",
      "[20210302-0105-19] Learning rate for epoch 50 is 0.00803999975323677\n",
      "Epoch 50/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 13.9835\n",
      "Epoch 00050: val_loss did not improve from 9.88977\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 13.9149 - val_loss: 10.2296\n",
      "\n",
      "[20210302-0105-23] Learning rate for epoch 51 is 0.00800000037997961\n",
      "Epoch 51/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 13.6824\n",
      "Epoch 00051: val_loss did not improve from 9.88977\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 13.7317 - val_loss: 10.1284\n",
      "\n",
      "[20210302-0105-26] Learning rate for epoch 52 is 0.007960000075399876\n",
      "Epoch 52/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 13.2429\n",
      "Epoch 00052: val_loss did not improve from 9.88977\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 13.2847 - val_loss: 10.5373\n",
      "\n",
      "[20210302-0105-30] Learning rate for epoch 53 is 0.00791999977082014\n",
      "Epoch 53/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 13.6467\n",
      "Epoch 00053: val_loss did not improve from 9.88977\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 13.7286 - val_loss: 10.3724\n",
      "\n",
      "[20210302-0105-34] Learning rate for epoch 54 is 0.00788000039756298\n",
      "Epoch 54/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.6857\n",
      "Epoch 00054: val_loss did not improve from 9.88977\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 13.6857 - val_loss: 10.3532\n",
      "\n",
      "*********************************** Start fine tune ***********************************\n",
      "tt: 195, nt:46, total layers:241\n",
      "*********************************** Start fine tune ***********************************\n",
      "\n",
      "[20210302-0105-38] Learning rate for epoch 1 is 4.999999987376214e-07\n",
      "Epoch 1/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.1431\n",
      "Epoch 00001: val_loss did not improve from 9.88977\n",
      "20/20 [==============================] - 14s 704ms/step - loss: 15.1431 - val_loss: 10.0858\n",
      "\n",
      "[20210302-0106-08] Learning rate for epoch 2 is 5.044450517743826e-05\n",
      "Epoch 2/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.0841\n",
      "Epoch 00002: val_loss did not improve from 9.88977\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 15.0841 - val_loss: 9.9169\n",
      "\n",
      "[20210302-0106-13] Learning rate for epoch 3 is 0.00010028902033809572\n",
      "Epoch 3/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.6763\n",
      "Epoch 00003: val_loss improved from 9.88977 to 9.60725, saving model to ./20210301-225844/toe_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 16s 823ms/step - loss: 14.6763 - val_loss: 9.6072\n",
      "\n",
      "[20210302-0106-33] Learning rate for epoch 4 is 0.00015003354928921908\n",
      "Epoch 4/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.0607\n",
      "Epoch 00004: val_loss improved from 9.60725 to 9.44121, saving model to ./20210301-225844/toe_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 91ms/step - loss: 14.0607 - val_loss: 9.4412\n",
      "\n",
      "[20210302-0106-38] Learning rate for epoch 5 is 0.0001996780774788931\n",
      "Epoch 5/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.1586\n",
      "Epoch 00005: val_loss did not improve from 9.44121\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 13.1586 - val_loss: 9.8197\n",
      "\n",
      "[20210302-0106-42] Learning rate for epoch 6 is 0.000249222619459033\n",
      "Epoch 6/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 12.5770\n",
      "Epoch 00006: val_loss did not improve from 9.44121\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 12.5770 - val_loss: 10.3887\n",
      "\n",
      "[20210302-0106-47] Learning rate for epoch 7 is 0.0002986671752296388\n",
      "Epoch 7/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 12.7025\n",
      "Epoch 00007: val_loss did not improve from 9.44121\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 12.7025 - val_loss: 9.5278\n",
      "\n",
      "[20210302-0106-52] Learning rate for epoch 8 is 0.00034801175934262574\n",
      "Epoch 8/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 12.0379\n",
      "Epoch 00008: val_loss did not improve from 9.44121\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 12.0379 - val_loss: 9.6413\n",
      "\n",
      "[20210302-0106-56] Learning rate for epoch 9 is 0.00039725631359033287\n",
      "Epoch 9/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.9892\n",
      "Epoch 00009: val_loss did not improve from 9.44121\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 11.9892 - val_loss: 9.7223\n",
      "\n",
      "[20210302-0107-01] Learning rate for epoch 10 is 0.0004464008961804211\n",
      "Epoch 10/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.3310\n",
      "Epoch 00010: val_loss did not improve from 9.44121\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 11.3310 - val_loss: 9.5749\n",
      "\n",
      "[20210302-0107-06] Learning rate for epoch 11 is 0.0004954455071128905\n",
      "Epoch 11/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.5534\n",
      "Epoch 00011: val_loss improved from 9.44121 to 8.48372, saving model to ./20210301-225844/toe_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 92ms/step - loss: 11.5534 - val_loss: 8.4837\n",
      "\n",
      "[20210302-0107-11] Learning rate for epoch 12 is 0.0005443901172839105\n",
      "Epoch 12/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.3024\n",
      "Epoch 00012: val_loss did not improve from 8.48372\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 11.3024 - val_loss: 8.5339\n",
      "\n",
      "[20210302-0107-15] Learning rate for epoch 13 is 0.0005932347266934812\n",
      "Epoch 13/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.1863\n",
      "Epoch 00013: val_loss improved from 8.48372 to 7.86829, saving model to ./20210301-225844/toe_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 89ms/step - loss: 11.1863 - val_loss: 7.8683\n",
      "\n",
      "[20210302-0107-20] Learning rate for epoch 14 is 0.0006419793353416026\n",
      "Epoch 14/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.8903\n",
      "Epoch 00014: val_loss did not improve from 7.86829\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 10.8903 - val_loss: 8.0972\n",
      "\n",
      "[20210302-0107-25] Learning rate for epoch 15 is 0.0006906240014359355\n",
      "Epoch 15/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.8705\n",
      "Epoch 00015: val_loss did not improve from 7.86829\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 10.8705 - val_loss: 8.5200\n",
      "\n",
      "[20210302-0107-30] Learning rate for epoch 16 is 0.0007391686085611582\n",
      "Epoch 16/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.7730\n",
      "Epoch 00016: val_loss did not improve from 7.86829\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 10.7730 - val_loss: 7.9625\n",
      "\n",
      "[20210302-0107-34] Learning rate for epoch 17 is 0.0007876132731325924\n",
      "Epoch 17/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.7592\n",
      "Epoch 00017: val_loss did not improve from 7.86829\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 10.7592 - val_loss: 9.7205\n",
      "\n",
      "[20210302-0107-39] Learning rate for epoch 18 is 0.0008359579369425774\n",
      "Epoch 18/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.7823\n",
      "Epoch 00018: val_loss did not improve from 7.86829\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 10.7823 - val_loss: 9.5091\n",
      "\n",
      "[20210302-0107-44] Learning rate for epoch 19 is 0.000884202599991113\n",
      "Epoch 19/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.4730\n",
      "Epoch 00019: val_loss did not improve from 7.86829\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 10.4730 - val_loss: 10.0580\n",
      "\n",
      "[20210302-0107-48] Learning rate for epoch 20 is 0.0009323473204858601\n",
      "Epoch 20/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.5455\n",
      "Epoch 00020: val_loss did not improve from 7.86829\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 10.5455 - val_loss: 8.6253\n",
      "\n",
      "[20210302-0107-53] Learning rate for epoch 21 is 0.000980391982011497\n",
      "Epoch 21/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.8602\n",
      "Epoch 00021: val_loss did not improve from 7.86829\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 9.8602 - val_loss: 8.0717\n",
      "\n",
      "[20210302-0107-57] Learning rate for epoch 22 is 0.0010283367009833455\n",
      "Epoch 22/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.0217\n",
      "Epoch 00022: val_loss did not improve from 7.86829\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 10.0217 - val_loss: 9.7235\n",
      "\n",
      "[20210302-0108-02] Learning rate for epoch 23 is 0.0010761814191937447\n",
      "Epoch 23/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.8066\n",
      "Epoch 00023: val_loss did not improve from 7.86829\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 9.8066 - val_loss: 9.5742\n",
      "\n",
      "[20210302-0108-08] Learning rate for epoch 24 is 0.0011239261366426945\n",
      "Epoch 24/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.9946\n",
      "Epoch 00024: val_loss did not improve from 7.86829\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 9.9946 - val_loss: 9.3669\n",
      "\n",
      "[20210302-0108-12] Learning rate for epoch 25 is 0.001171570853330195\n",
      "Epoch 25/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.9721\n",
      "Epoch 00025: val_loss did not improve from 7.86829\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 9.9721 - val_loss: 8.5614\n",
      "\n",
      "[20210302-0108-17] Learning rate for epoch 26 is 0.001219115569256246\n",
      "Epoch 26/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.3283\n",
      "Epoch 00026: val_loss did not improve from 7.86829\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 9.3283 - val_loss: 8.0956\n",
      "\n",
      "[20210302-0108-22] Learning rate for epoch 27 is 0.0012665604008361697\n",
      "Epoch 27/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.4495\n",
      "Epoch 00027: val_loss improved from 7.86829 to 7.80092, saving model to ./20210301-225844/toe_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 92ms/step - loss: 9.4495 - val_loss: 7.8009\n",
      "\n",
      "[20210302-0108-27] Learning rate for epoch 28 is 0.0013139051152393222\n",
      "Epoch 28/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.4068\n",
      "Epoch 00028: val_loss improved from 7.80092 to 7.52930, saving model to ./20210301-225844/toe_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 92ms/step - loss: 9.4068 - val_loss: 7.5293\n",
      "\n",
      "[20210302-0108-32] Learning rate for epoch 29 is 0.0013611499452963471\n",
      "Epoch 29/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.3655\n",
      "Epoch 00029: val_loss did not improve from 7.52930\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 9.3655 - val_loss: 8.4094\n",
      "\n",
      "[20210302-0108-37] Learning rate for epoch 30 is 0.001408294658176601\n",
      "Epoch 30/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.2620\n",
      "Epoch 00030: val_loss did not improve from 7.52930\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 9.2620 - val_loss: 8.3120\n",
      "\n",
      "[20210302-0108-41] Learning rate for epoch 31 is 0.0014553394867107272\n",
      "Epoch 31/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.3219\n",
      "Epoch 00031: val_loss did not improve from 7.52930\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 9.3219 - val_loss: 8.3110\n",
      "\n",
      "[20210302-0108-46] Learning rate for epoch 32 is 0.0015022843144834042\n",
      "Epoch 32/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.2042\n",
      "Epoch 00032: val_loss improved from 7.52930 to 7.05758, saving model to ./20210301-225844/toe_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 9.2042 - val_loss: 7.0576\n",
      "\n",
      "[20210302-0108-51] Learning rate for epoch 33 is 0.0015491291414946318\n",
      "Epoch 33/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.9147\n",
      "Epoch 00033: val_loss did not improve from 7.05758\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 8.9147 - val_loss: 7.5285\n",
      "\n",
      "[20210302-0108-56] Learning rate for epoch 34 is 0.00159587396774441\n",
      "Epoch 34/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.9745\n",
      "Epoch 00034: val_loss did not improve from 7.05758\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 8.9745 - val_loss: 7.3652\n",
      "\n",
      "[20210302-0109-00] Learning rate for epoch 35 is 0.001642518793232739\n",
      "Epoch 35/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.5202\n",
      "Epoch 00035: val_loss did not improve from 7.05758\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 8.5202 - val_loss: 7.0655\n",
      "\n",
      "[20210302-0109-05] Learning rate for epoch 36 is 0.0016890636179596186\n",
      "Epoch 36/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.8131\n",
      "Epoch 00036: val_loss improved from 7.05758 to 6.96395, saving model to ./20210301-225844/toe_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 8.8131 - val_loss: 6.9640\n",
      "\n",
      "[20210302-0109-10] Learning rate for epoch 37 is 0.0017355084419250488\n",
      "Epoch 37/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.5533\n",
      "Epoch 00037: val_loss did not improve from 6.96395\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 8.5533 - val_loss: 8.4070\n",
      "\n",
      "[20210302-0109-15] Learning rate for epoch 38 is 0.0017818533815443516\n",
      "Epoch 38/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.5571\n",
      "Epoch 00038: val_loss improved from 6.96395 to 6.40472, saving model to ./20210301-225844/toe_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 8.5571 - val_loss: 6.4047\n",
      "\n",
      "[20210302-0109-20] Learning rate for epoch 39 is 0.0018280982039868832\n",
      "Epoch 39/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.5734\n",
      "Epoch 00039: val_loss improved from 6.40472 to 6.22876, saving model to ./20210301-225844/toe_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 89ms/step - loss: 8.5734 - val_loss: 6.2288\n",
      "\n",
      "[20210302-0109-25] Learning rate for epoch 40 is 0.0018742431420832872\n",
      "Epoch 40/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.4614\n",
      "Epoch 00040: val_loss did not improve from 6.22876\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 8.4614 - val_loss: 6.4990\n",
      "\n",
      "[20210302-0109-30] Learning rate for epoch 41 is 0.0019202879630029202\n",
      "Epoch 41/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.4697\n",
      "Epoch 00041: val_loss did not improve from 6.22876\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 8.4697 - val_loss: 6.4758\n",
      "\n",
      "[20210302-0109-34] Learning rate for epoch 42 is 0.0019662328995764256\n",
      "Epoch 42/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.6084\n",
      "Epoch 00042: val_loss did not improve from 6.22876\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 8.6084 - val_loss: 7.7105\n",
      "\n",
      "[20210302-0109-39] Learning rate for epoch 43 is 0.00201207771897316\n",
      "Epoch 43/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.3276\n",
      "Epoch 00043: val_loss did not improve from 6.22876\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 8.3276 - val_loss: 7.4085\n",
      "\n",
      "[20210302-0109-44] Learning rate for epoch 44 is 0.0020578226540237665\n",
      "Epoch 44/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.5331\n",
      "Epoch 00044: val_loss did not improve from 6.22876\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 8.5331 - val_loss: 6.9719\n",
      "\n",
      "[20210302-0109-48] Learning rate for epoch 45 is 0.0021034677047282457\n",
      "Epoch 45/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.2713\n",
      "Epoch 00045: val_loss did not improve from 6.22876\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 8.2713 - val_loss: 6.5830\n",
      "\n",
      "[20210302-0109-53] Learning rate for epoch 46 is 0.002149012638255954\n",
      "Epoch 46/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.2223\n",
      "Epoch 00046: val_loss improved from 6.22876 to 5.84252, saving model to ./20210301-225844/toe_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 93ms/step - loss: 8.2223 - val_loss: 5.8425\n",
      "\n",
      "[20210302-0109-58] Learning rate for epoch 47 is 0.0021944576874375343\n",
      "Epoch 47/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.2651\n",
      "Epoch 00047: val_loss improved from 5.84252 to 5.69782, saving model to ./20210301-225844/toe_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 8.2651 - val_loss: 5.6978\n",
      "\n",
      "[20210302-0110-03] Learning rate for epoch 48 is 0.0022398026194423437\n",
      "Epoch 48/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.0143\n",
      "Epoch 00048: val_loss did not improve from 5.69782\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 8.0143 - val_loss: 6.8948\n",
      "\n",
      "[20210302-0110-08] Learning rate for epoch 49 is 0.002285047434270382\n",
      "Epoch 49/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.1058\n",
      "Epoch 00049: val_loss did not improve from 5.69782\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 8.1058 - val_loss: 7.9220\n",
      "\n",
      "[20210302-0110-13] Learning rate for epoch 50 is 0.0023301925975829363\n",
      "Epoch 50/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.1537\n",
      "Epoch 00050: val_loss did not improve from 5.69782\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 8.1537 - val_loss: 6.7887\n",
      "\n",
      "[20210302-0110-17] Learning rate for epoch 51 is 0.002375237410888076\n",
      "Epoch 51/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.1703\n",
      "Epoch 00051: val_loss did not improve from 5.69782\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 8.1703 - val_loss: 6.6675\n",
      "\n",
      "[20210302-0110-22] Learning rate for epoch 52 is 0.0024201825726777315\n",
      "Epoch 52/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.9322\n",
      "Epoch 00052: val_loss did not improve from 5.69782\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.9322 - val_loss: 6.4076\n",
      "\n",
      "[20210302-0110-26] Learning rate for epoch 53 is 0.002465027617290616\n",
      "Epoch 53/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.0100\n",
      "Epoch 00053: val_loss improved from 5.69782 to 5.41492, saving model to ./20210301-225844/toe_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 89ms/step - loss: 8.0100 - val_loss: 5.4149\n",
      "\n",
      "[20210302-0110-32] Learning rate for epoch 54 is 0.0025097725447267294\n",
      "Epoch 54/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.9502\n",
      "Epoch 00054: val_loss did not improve from 5.41492\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.9502 - val_loss: 5.9984\n",
      "\n",
      "[20210302-0110-36] Learning rate for epoch 55 is 0.0025544175878167152\n",
      "Epoch 55/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.1141\n",
      "Epoch 00055: val_loss did not improve from 5.41492\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 8.1141 - val_loss: 5.4669\n",
      "\n",
      "[20210302-0110-41] Learning rate for epoch 56 is 0.00259896251372993\n",
      "Epoch 56/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.9650\n",
      "Epoch 00056: val_loss did not improve from 5.41492\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.9650 - val_loss: 5.5668\n",
      "\n",
      "[20210302-0110-45] Learning rate for epoch 57 is 0.0026434077881276608\n",
      "Epoch 57/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.9910\n",
      "Epoch 00057: val_loss did not improve from 5.41492\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.9910 - val_loss: 5.9981\n",
      "\n",
      "[20210302-0110-50] Learning rate for epoch 58 is 0.0026877527125179768\n",
      "Epoch 58/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8496\n",
      "Epoch 00058: val_loss improved from 5.41492 to 5.30976, saving model to ./20210301-225844/toe_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 7.8496 - val_loss: 5.3098\n",
      "\n",
      "[20210302-0110-55] Learning rate for epoch 59 is 0.0027319977525621653\n",
      "Epoch 59/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.9160\n",
      "Epoch 00059: val_loss did not improve from 5.30976\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.9160 - val_loss: 6.2116\n",
      "\n",
      "[20210302-0111-00] Learning rate for epoch 60 is 0.0027761429082602262\n",
      "Epoch 60/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8681\n",
      "Epoch 00060: val_loss did not improve from 5.30976\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.8681 - val_loss: 5.9738\n",
      "\n",
      "[20210302-0111-04] Learning rate for epoch 61 is 0.002820187946781516\n",
      "Epoch 61/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7277\n",
      "Epoch 00061: val_loss improved from 5.30976 to 5.25801, saving model to ./20210301-225844/toe_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 93ms/step - loss: 7.7277 - val_loss: 5.2580\n",
      "\n",
      "[20210302-0111-09] Learning rate for epoch 62 is 0.0028641331009566784\n",
      "Epoch 62/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8556\n",
      "Epoch 00062: val_loss did not improve from 5.25801\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.8556 - val_loss: 5.3221\n",
      "\n",
      "[20210302-0111-14] Learning rate for epoch 63 is 0.0029079781379550695\n",
      "Epoch 63/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.9102\n",
      "Epoch 00063: val_loss did not improve from 5.25801\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.9102 - val_loss: 7.1229\n",
      "\n",
      "[20210302-0111-19] Learning rate for epoch 64 is 0.002951723290607333\n",
      "Epoch 64/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7568\n",
      "Epoch 00064: val_loss did not improve from 5.25801\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.7568 - val_loss: 8.0088\n",
      "\n",
      "[20210302-0111-23] Learning rate for epoch 65 is 0.0029953685589134693\n",
      "Epoch 65/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.9982\n",
      "Epoch 00065: val_loss did not improve from 5.25801\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.9982 - val_loss: 7.2108\n",
      "\n",
      "[20210302-0111-28] Learning rate for epoch 66 is 0.0030389137100428343\n",
      "Epoch 66/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8411\n",
      "Epoch 00066: val_loss did not improve from 5.25801\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.8411 - val_loss: 5.9017\n",
      "\n",
      "[20210302-0111-33] Learning rate for epoch 67 is 0.003082358743995428\n",
      "Epoch 67/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.9429\n",
      "Epoch 00067: val_loss did not improve from 5.25801\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.9429 - val_loss: 5.5962\n",
      "\n",
      "[20210302-0111-37] Learning rate for epoch 68 is 0.0031257038936018944\n",
      "Epoch 68/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5585\n",
      "Epoch 00068: val_loss did not improve from 5.25801\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.5585 - val_loss: 6.9745\n",
      "\n",
      "[20210302-0111-42] Learning rate for epoch 69 is 0.003168949158862233\n",
      "Epoch 69/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7434\n",
      "Epoch 00069: val_loss did not improve from 5.25801\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.7434 - val_loss: 5.3585\n",
      "\n",
      "[20210302-0111-47] Learning rate for epoch 70 is 0.0032120943069458008\n",
      "Epoch 70/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5981\n",
      "Epoch 00070: val_loss did not improve from 5.25801\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.5981 - val_loss: 5.8572\n",
      "\n",
      "[20210302-0111-51] Learning rate for epoch 71 is 0.003255139570683241\n",
      "Epoch 71/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7472\n",
      "Epoch 00071: val_loss did not improve from 5.25801\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 7.7472 - val_loss: 5.4111\n",
      "\n",
      "[20210302-0111-56] Learning rate for epoch 72 is 0.00329808471724391\n",
      "Epoch 72/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4631\n",
      "Epoch 00072: val_loss improved from 5.25801 to 5.22550, saving model to ./20210301-225844/toe_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 89ms/step - loss: 7.4631 - val_loss: 5.2255\n",
      "\n",
      "[20210302-0112-01] Learning rate for epoch 73 is 0.0033409299794584513\n",
      "Epoch 73/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6089\n",
      "Epoch 00073: val_loss did not improve from 5.22550\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.6089 - val_loss: 6.5508\n",
      "\n",
      "[20210302-0112-06] Learning rate for epoch 74 is 0.0033836751244962215\n",
      "Epoch 74/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5374\n",
      "Epoch 00074: val_loss improved from 5.22550 to 5.05946, saving model to ./20210301-225844/toe_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 7.5374 - val_loss: 5.0595\n",
      "\n",
      "[20210302-0112-11] Learning rate for epoch 75 is 0.0034263203851878643\n",
      "Epoch 75/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6729\n",
      "Epoch 00075: val_loss did not improve from 5.05946\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.6729 - val_loss: 5.2033\n",
      "\n",
      "[20210302-0112-15] Learning rate for epoch 76 is 0.003468865528702736\n",
      "Epoch 76/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4810\n",
      "Epoch 00076: val_loss did not improve from 5.05946\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 7.4810 - val_loss: 5.8320\n",
      "\n",
      "[20210302-0112-20] Learning rate for epoch 77 is 0.00351131078787148\n",
      "Epoch 77/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5086\n",
      "Epoch 00077: val_loss did not improve from 5.05946\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.5086 - val_loss: 5.8784\n",
      "\n",
      "[20210302-0112-25] Learning rate for epoch 78 is 0.0035536561626940966\n",
      "Epoch 78/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7170\n",
      "Epoch 00078: val_loss did not improve from 5.05946\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.7170 - val_loss: 5.6086\n",
      "\n",
      "[20210302-0112-29] Learning rate for epoch 79 is 0.003595901420339942\n",
      "Epoch 79/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4022\n",
      "Epoch 00079: val_loss did not improve from 5.05946\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 7.4022 - val_loss: 5.2682\n",
      "\n",
      "[20210302-0112-34] Learning rate for epoch 80 is 0.00363804679363966\n",
      "Epoch 80/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6554\n",
      "Epoch 00080: val_loss did not improve from 5.05946\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.6554 - val_loss: 5.6006\n",
      "\n",
      "[20210302-0112-39] Learning rate for epoch 81 is 0.0036800920497626066\n",
      "Epoch 81/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7125\n",
      "Epoch 00081: val_loss did not improve from 5.05946\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.7125 - val_loss: 6.2124\n",
      "\n",
      "[20210302-0112-43] Learning rate for epoch 82 is 0.003722037188708782\n",
      "Epoch 82/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5587\n",
      "Epoch 00082: val_loss did not improve from 5.05946\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 7.5587 - val_loss: 5.0734\n",
      "\n",
      "[20210302-0112-48] Learning rate for epoch 83 is 0.003763882676139474\n",
      "Epoch 83/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4183\n",
      "Epoch 00083: val_loss did not improve from 5.05946\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.4183 - val_loss: 6.0515\n",
      "\n",
      "[20210302-0112-52] Learning rate for epoch 84 is 0.0038056280463933945\n",
      "Epoch 84/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6176\n",
      "Epoch 00084: val_loss did not improve from 5.05946\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.6176 - val_loss: 5.1954\n",
      "\n",
      "[20210302-0112-57] Learning rate for epoch 85 is 0.003847273299470544\n",
      "Epoch 85/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3572\n",
      "Epoch 00085: val_loss did not improve from 5.05946\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.3572 - val_loss: 6.9524\n",
      "\n",
      "[20210302-0113-02] Learning rate for epoch 86 is 0.0038888186682015657\n",
      "Epoch 86/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8449\n",
      "Epoch 00086: val_loss did not improve from 5.05946\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.8449 - val_loss: 5.7008\n",
      "\n",
      "[20210302-0113-06] Learning rate for epoch 87 is 0.00393026415258646\n",
      "Epoch 87/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2874\n",
      "Epoch 00087: val_loss did not improve from 5.05946\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.2874 - val_loss: 6.0613\n",
      "\n",
      "[20210302-0113-11] Learning rate for epoch 88 is 0.00397160928696394\n",
      "Epoch 88/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3528\n",
      "Epoch 00088: val_loss did not improve from 5.05946\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.3528 - val_loss: 6.2317\n",
      "\n",
      "[20210302-0113-15] Learning rate for epoch 89 is 0.004012854769825935\n",
      "Epoch 89/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5189\n",
      "Epoch 00089: val_loss did not improve from 5.05946\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.5189 - val_loss: 6.5225\n",
      "\n",
      "[20210302-0113-20] Learning rate for epoch 90 is 0.00405400013551116\n",
      "Epoch 90/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6756\n",
      "Epoch 00090: val_loss did not improve from 5.05946\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.6756 - val_loss: 5.2781\n",
      "\n",
      "[20210302-0113-25] Learning rate for epoch 91 is 0.004095045384019613\n",
      "Epoch 91/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4713\n",
      "Epoch 00091: val_loss did not improve from 5.05946\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.4713 - val_loss: 6.1312\n",
      "\n",
      "[20210302-0113-29] Learning rate for epoch 92 is 0.004135990981012583\n",
      "Epoch 92/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3019\n",
      "Epoch 00092: val_loss did not improve from 5.05946\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.3019 - val_loss: 5.7254\n",
      "\n",
      "[20210302-0113-34] Learning rate for epoch 93 is 0.004176836460828781\n",
      "Epoch 93/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6274\n",
      "Epoch 00093: val_loss did not improve from 5.05946\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 7.6274 - val_loss: 6.2600\n",
      "\n",
      "[20210302-0113-39] Learning rate for epoch 94 is 0.004217581823468208\n",
      "Epoch 94/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6696\n",
      "Epoch 00094: val_loss did not improve from 5.05946\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.6696 - val_loss: 7.4408\n",
      "\n",
      "[20210302-0113-43] Learning rate for epoch 95 is 0.004258227068930864\n",
      "Epoch 95/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5994\n",
      "Epoch 00095: val_loss did not improve from 5.05946\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.5994 - val_loss: 6.4334\n",
      "\n",
      "[20210302-0113-48] Learning rate for epoch 96 is 0.0042987726628780365\n",
      "Epoch 96/1000\n",
      " 5/20 [======>.......................] - ETA: 0s - loss: 7.7718"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[20210302-0201-07] Learning rate for epoch 125 is 0.004204817581921816\n",
      "Epoch 125/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1767\n",
      "Epoch 00125: val_loss did not improve from 5.81879\n",
      "20/20 [==============================] - 1s 69ms/step - loss: 7.1767 - val_loss: 6.8695\n",
      "\n",
      "[20210302-0201-11] Learning rate for epoch 126 is 0.0043750000186264515\n",
      "Epoch 126/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0653\n",
      "Epoch 00126: val_loss did not improve from 5.81879\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.0653 - val_loss: 7.5198\n",
      "\n",
      "[20210302-0201-16] Learning rate for epoch 127 is 0.0041952175088226795\n",
      "Epoch 127/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1422\n",
      "Epoch 00127: val_loss did not improve from 5.81879\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.1422 - val_loss: 7.7601\n",
      "\n",
      "[20210302-0201-21] Learning rate for epoch 128 is 0.004015835002064705\n",
      "Epoch 128/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9171\n",
      "Epoch 00128: val_loss did not improve from 5.81879\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.9171 - val_loss: 7.4951\n",
      "\n",
      "[20210302-0201-25] Learning rate for epoch 129 is 0.003836852265521884\n",
      "Epoch 129/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0977\n",
      "Epoch 00129: val_loss did not improve from 5.81879\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.0977 - val_loss: 6.6036\n",
      "\n",
      "[20210302-0201-30] Learning rate for epoch 130 is 0.003658269764855504\n",
      "Epoch 130/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1765\n",
      "Epoch 00130: val_loss did not improve from 5.81879\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.1765 - val_loss: 7.2064\n",
      "\n",
      "[20210302-0201-35] Learning rate for epoch 131 is 0.003480087034404278\n",
      "Epoch 131/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8234\n",
      "Epoch 00131: val_loss did not improve from 5.81879\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.8234 - val_loss: 6.9403\n",
      "\n",
      "[20210302-0201-39] Learning rate for epoch 132 is 0.003302304306998849\n",
      "Epoch 132/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9158\n",
      "Epoch 00132: val_loss did not improve from 5.81879\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.9158 - val_loss: 7.5028\n",
      "\n",
      "[20210302-0201-44] Learning rate for epoch 133 is 0.0031249215826392174\n",
      "Epoch 133/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8799\n",
      "Epoch 00133: val_loss did not improve from 5.81879\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.8799 - val_loss: 6.5364\n",
      "\n",
      "[20210302-0201-48] Learning rate for epoch 134 is 0.0029479386284947395\n",
      "Epoch 134/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7632\n",
      "Epoch 00134: val_loss did not improve from 5.81879\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.7632 - val_loss: 8.4648\n",
      "\n",
      "[20210302-0201-53] Learning rate for epoch 135 is 0.0027713559102267027\n",
      "Epoch 135/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9411\n",
      "Epoch 00135: val_loss did not improve from 5.81879\n",
      "20/20 [==============================] - 1s 69ms/step - loss: 6.9411 - val_loss: 6.7103\n",
      "\n",
      "[20210302-0201-58] Learning rate for epoch 136 is 0.0025951729621738195\n",
      "Epoch 136/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4529\n",
      "Epoch 00136: val_loss did not improve from 5.81879\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.4529 - val_loss: 7.1225\n",
      "\n",
      "[20210302-0202-03] Learning rate for epoch 137 is 0.0024193900171667337\n",
      "Epoch 137/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5959\n",
      "Epoch 00137: val_loss did not improve from 5.81879\n",
      "20/20 [==============================] - 1s 71ms/step - loss: 6.5959 - val_loss: 6.7349\n",
      "\n",
      "[20210302-0202-07] Learning rate for epoch 138 is 0.0022440070752054453\n",
      "Epoch 138/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7508\n",
      "Epoch 00138: val_loss did not improve from 5.81879\n",
      "20/20 [==============================] - 1s 68ms/step - loss: 6.7508 - val_loss: 6.3211\n",
      "\n",
      "[20210302-0202-12] Learning rate for epoch 139 is 0.002069024136289954\n",
      "Epoch 139/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6814\n",
      "Epoch 00139: val_loss did not improve from 5.81879\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.6814 - val_loss: 6.3019\n",
      "\n",
      "[20210302-0202-17] Learning rate for epoch 140 is 0.0018944410840049386\n",
      "Epoch 140/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6720\n",
      "Epoch 00140: val_loss did not improve from 5.81879\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.6720 - val_loss: 6.0084\n",
      "\n",
      "[20210302-0202-21] Learning rate for epoch 141 is 0.0017202580347657204\n",
      "Epoch 141/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3672\n",
      "Epoch 00141: val_loss did not improve from 5.81879\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.3672 - val_loss: 6.4952\n",
      "\n",
      "[20210302-0202-26] Learning rate for epoch 142 is 0.0015464748721569777\n",
      "Epoch 142/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2883\n",
      "Epoch 00142: val_loss did not improve from 5.81879\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.2883 - val_loss: 6.1488\n",
      "\n",
      "[20210302-0202-31] Learning rate for epoch 143 is 0.0013730917125940323\n",
      "Epoch 143/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5802\n",
      "Epoch 00143: val_loss did not improve from 5.81879\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.5802 - val_loss: 6.5691\n",
      "\n",
      "[20210302-0202-35] Learning rate for epoch 144 is 0.0012001085560768843\n",
      "Epoch 144/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5134\n",
      "Epoch 00144: val_loss did not improve from 5.81879\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.5134 - val_loss: 6.4669\n",
      "\n",
      "[20210302-0202-40] Learning rate for epoch 145 is 0.0010275252861902118\n",
      "Epoch 145/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4146\n",
      "Epoch 00145: val_loss did not improve from 5.81879\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.4146 - val_loss: 5.9831\n",
      "\n",
      "[20210302-0202-44] Learning rate for epoch 146 is 0.0008553420193493366\n",
      "Epoch 146/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3600\n",
      "Epoch 00146: val_loss did not improve from 5.81879\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.3600 - val_loss: 6.2971\n",
      "\n",
      "[20210302-0202-49] Learning rate for epoch 147 is 0.0006835586973465979\n",
      "Epoch 147/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2570\n",
      "Epoch 00147: val_loss did not improve from 5.81879\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.2570 - val_loss: 6.2826\n",
      "\n",
      "[20210302-0202-54] Learning rate for epoch 148 is 0.0005121753201819956\n",
      "Epoch 148/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0960\n",
      "Epoch 00148: val_loss did not improve from 5.81879\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.0960 - val_loss: 5.9531\n",
      "\n",
      "[20210302-0202-58] Learning rate for epoch 149 is 0.00034119191695936024\n",
      "Epoch 149/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1392\n",
      "Epoch 00149: val_loss did not improve from 5.81879\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.1392 - val_loss: 5.9138\n",
      "\n",
      "[20210302-0203-03] Learning rate for epoch 150 is 0.00017060847312677652\n",
      "Epoch 150/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1875\n",
      "Epoch 00150: val_loss did not improve from 5.81879\n",
      "20/20 [==============================] - 1s 71ms/step - loss: 6.1875 - val_loss: 5.9189\n",
      "\n",
      "[20210302-0203-08] Learning rate for epoch 151 is 4.249999960848072e-07\n",
      "Epoch 151/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2374\n",
      "Epoch 00151: val_loss did not improve from 5.81879\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.2374 - val_loss: 5.9201\n",
      "\n",
      "[20210302-0203-13] Learning rate for epoch 152 is 0.00017020752420648932\n",
      "Epoch 152/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1862\n",
      "Epoch 00152: val_loss did not improve from 5.81879\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.1862 - val_loss: 5.9100\n",
      "\n",
      "[20210302-0203-17] Learning rate for epoch 153 is 0.0003395900712348521\n",
      "Epoch 153/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1511\n",
      "Epoch 00153: val_loss did not improve from 5.81879\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.1511 - val_loss: 5.9277\n",
      "\n",
      "[20210302-0203-22] Learning rate for epoch 154 is 0.0005085726734250784\n",
      "Epoch 154/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1256\n",
      "Epoch 00154: val_loss did not improve from 5.81879\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.1256 - val_loss: 6.0057\n",
      "\n",
      "[20210302-0203-26] Learning rate for epoch 155 is 0.0006771553307771683\n",
      "Epoch 155/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1728\n",
      "Epoch 00155: val_loss did not improve from 5.81879\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.1728 - val_loss: 6.2223\n",
      "\n",
      "[20210302-0203-31] Learning rate for epoch 156 is 0.0008453379850834608\n",
      "Epoch 156/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2040\n",
      "Epoch 00156: val_loss did not improve from 5.81879\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.2040 - val_loss: 5.8545\n",
      "\n",
      "[20210302-0203-36] Learning rate for epoch 157 is 0.0010131207527592778\n",
      "Epoch 157/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0633\n",
      "Epoch 00157: val_loss did not improve from 5.81879\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.0633 - val_loss: 5.9943\n",
      "\n",
      "[20210302-0203-40] Learning rate for epoch 158 is 0.0011805035173892975\n",
      "Epoch 158/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2153\n",
      "Epoch 00158: val_loss did not improve from 5.81879\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.2153 - val_loss: 6.0714\n",
      "\n",
      "[20210302-0203-45] Learning rate for epoch 159 is 0.0013474862789735198\n",
      "Epoch 159/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0584\n",
      "Epoch 00159: val_loss did not improve from 5.81879\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.0584 - val_loss: 6.4286\n",
      "\n",
      "[20210302-0203-50] Learning rate for epoch 160 is 0.0015140691539272666\n",
      "Epoch 160/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1173\n",
      "Epoch 00160: val_loss did not improve from 5.81879\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.1173 - val_loss: 6.1469\n",
      "\n",
      "[20210302-0203-54] Learning rate for epoch 161 is 0.001680252025835216\n",
      "Epoch 161/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1209\n",
      "Epoch 00161: val_loss did not improve from 5.81879\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.1209 - val_loss: 6.3696\n",
      "\n",
      "[20210302-0203-59] Learning rate for epoch 162 is 0.0018460348946973681\n",
      "Epoch 162/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1744\n",
      "Epoch 00162: val_loss did not improve from 5.81879\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.1744 - val_loss: 6.2699\n",
      "\n",
      "[20210302-0204-04] Learning rate for epoch 163 is 0.0020114178769290447\n",
      "Epoch 163/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4520\n",
      "Epoch 00163: val_loss did not improve from 5.81879\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.4520 - val_loss: 6.3084\n",
      "\n",
      "[20210302-0204-08] Learning rate for epoch 164 is 0.0021764009725302458\n",
      "Epoch 164/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3127\n",
      "Epoch 00164: val_loss did not improve from 5.81879\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.3127 - val_loss: 6.3809\n",
      "\n",
      "[20210302-0204-13] Learning rate for epoch 165 is 0.002340983832255006\n",
      "Epoch 165/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5991\n",
      "Epoch 00165: val_loss did not improve from 5.81879\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.5991 - val_loss: 6.0375\n",
      "\n",
      "[20210302-0204-17] Learning rate for epoch 166 is 0.002505166921764612\n",
      "Epoch 166/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4484\n",
      "Epoch 00166: val_loss improved from 5.81879 to 5.62886, saving model to ./20210301-225844/toe_K7_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 89ms/step - loss: 6.4484 - val_loss: 5.6289\n",
      "\n",
      "[20210302-0204-23] Learning rate for epoch 167 is 0.002668950008228421\n",
      "Epoch 167/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5782\n",
      "Epoch 00167: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 6.5782 - val_loss: 6.4929\n",
      "\n",
      "[20210302-0204-27] Learning rate for epoch 168 is 0.0028323333244770765\n",
      "Epoch 168/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4003\n",
      "Epoch 00168: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.4003 - val_loss: 7.7433\n",
      "\n",
      "[20210302-0204-32] Learning rate for epoch 169 is 0.002995316404849291\n",
      "Epoch 169/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4621\n",
      "Epoch 00169: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.4621 - val_loss: 6.2961\n",
      "\n",
      "[20210302-0204-37] Learning rate for epoch 170 is 0.0031578997150063515\n",
      "Epoch 170/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7171\n",
      "Epoch 00170: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.7171 - val_loss: 6.9199\n",
      "\n",
      "[20210302-0204-41] Learning rate for epoch 171 is 0.0033200830221176147\n",
      "Epoch 171/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4727\n",
      "Epoch 00171: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 69ms/step - loss: 6.4727 - val_loss: 6.3564\n",
      "\n",
      "[20210302-0204-46] Learning rate for epoch 172 is 0.0034818663261830807\n",
      "Epoch 172/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6986\n",
      "Epoch 00172: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.6986 - val_loss: 5.9900\n",
      "\n",
      "[20210302-0204-51] Learning rate for epoch 173 is 0.0036432496272027493\n",
      "Epoch 173/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6965\n",
      "Epoch 00173: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.6965 - val_loss: 6.5086\n",
      "\n",
      "[20210302-0204-55] Learning rate for epoch 174 is 0.003804233158007264\n",
      "Epoch 174/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4856\n",
      "Epoch 00174: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 6.4856 - val_loss: 6.5164\n",
      "\n",
      "[20210302-0205-00] Learning rate for epoch 175 is 0.003964816685765982\n",
      "Epoch 175/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8358\n",
      "Epoch 00175: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.8358 - val_loss: 7.7737\n",
      "\n",
      "[20210302-0205-04] Learning rate for epoch 176 is 0.004124999977648258\n",
      "Epoch 176/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4568\n",
      "Epoch 00176: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.4568 - val_loss: 6.3855\n",
      "\n",
      "[20210302-0205-09] Learning rate for epoch 177 is 0.003955216612666845\n",
      "Epoch 177/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8683\n",
      "Epoch 00177: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.8683 - val_loss: 8.0794\n",
      "\n",
      "[20210302-0205-14] Learning rate for epoch 178 is 0.003785833017900586\n",
      "Epoch 178/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6194\n",
      "Epoch 00178: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.6194 - val_loss: 6.6898\n",
      "\n",
      "[20210302-0205-18] Learning rate for epoch 179 is 0.0036168494261801243\n",
      "Epoch 179/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7640\n",
      "Epoch 00179: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.7640 - val_loss: 6.6042\n",
      "\n",
      "[20210302-0205-23] Learning rate for epoch 180 is 0.003448265604674816\n",
      "Epoch 180/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6474\n",
      "Epoch 00180: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.6474 - val_loss: 8.0399\n",
      "\n",
      "[20210302-0205-27] Learning rate for epoch 181 is 0.003280082019045949\n",
      "Epoch 181/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5732\n",
      "Epoch 00181: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.5732 - val_loss: 7.1697\n",
      "\n",
      "[20210302-0205-32] Learning rate for epoch 182 is 0.0031122982036322355\n",
      "Epoch 182/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7043\n",
      "Epoch 00182: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 6.7043 - val_loss: 7.2298\n",
      "\n",
      "[20210302-0205-37] Learning rate for epoch 183 is 0.002944914624094963\n",
      "Epoch 183/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4283\n",
      "Epoch 00183: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.4283 - val_loss: 6.4063\n",
      "\n",
      "[20210302-0205-42] Learning rate for epoch 184 is 0.0027779308147728443\n",
      "Epoch 184/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6363\n",
      "Epoch 00184: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.6363 - val_loss: 7.4699\n",
      "\n",
      "[20210302-0205-46] Learning rate for epoch 185 is 0.0026113467756658792\n",
      "Epoch 185/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4414\n",
      "Epoch 00185: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.4414 - val_loss: 6.2028\n",
      "\n",
      "[20210302-0205-51] Learning rate for epoch 186 is 0.002445162972435355\n",
      "Epoch 186/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2021\n",
      "Epoch 00186: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 71ms/step - loss: 6.2021 - val_loss: 6.6434\n",
      "\n",
      "[20210302-0205-56] Learning rate for epoch 187 is 0.0022793791722506285\n",
      "Epoch 187/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4473\n",
      "Epoch 00187: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.4473 - val_loss: 6.5669\n",
      "\n",
      "[20210302-0206-00] Learning rate for epoch 188 is 0.0021139951422810555\n",
      "Epoch 188/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4054\n",
      "Epoch 00188: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.4054 - val_loss: 6.3987\n",
      "\n",
      "[20210302-0206-05] Learning rate for epoch 189 is 0.0019490111153572798\n",
      "Epoch 189/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4732\n",
      "Epoch 00189: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 6.4732 - val_loss: 6.6226\n",
      "\n",
      "[20210302-0206-09] Learning rate for epoch 190 is 0.0017844270914793015\n",
      "Epoch 190/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0556\n",
      "Epoch 00190: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 6.0556 - val_loss: 6.4487\n",
      "\n",
      "[20210302-0206-14] Learning rate for epoch 191 is 0.0016202429542317986\n",
      "Epoch 191/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2649\n",
      "Epoch 00191: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.2649 - val_loss: 6.2489\n",
      "\n",
      "[20210302-0206-19] Learning rate for epoch 192 is 0.001456458936445415\n",
      "Epoch 192/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2788\n",
      "Epoch 00192: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.2788 - val_loss: 6.0574\n",
      "\n",
      "[20210302-0206-23] Learning rate for epoch 193 is 0.001293074688874185\n",
      "Epoch 193/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4736\n",
      "Epoch 00193: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.4736 - val_loss: 6.2984\n",
      "\n",
      "[20210302-0206-28] Learning rate for epoch 194 is 0.0011300905607640743\n",
      "Epoch 194/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3260\n",
      "Epoch 00194: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.3260 - val_loss: 6.0474\n",
      "\n",
      "[20210302-0206-32] Learning rate for epoch 195 is 0.0009675062610767782\n",
      "Epoch 195/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0901\n",
      "Epoch 00195: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.0901 - val_loss: 6.0406\n",
      "\n",
      "[20210302-0206-37] Learning rate for epoch 196 is 0.0008053220226429403\n",
      "Epoch 196/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8649\n",
      "Epoch 00196: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.8649 - val_loss: 5.8595\n",
      "\n",
      "[20210302-0206-42] Learning rate for epoch 197 is 0.0006435376708395779\n",
      "Epoch 197/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9932\n",
      "Epoch 00197: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.9932 - val_loss: 6.3676\n",
      "\n",
      "[20210302-0206-46] Learning rate for epoch 198 is 0.0004821533220820129\n",
      "Epoch 198/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8739\n",
      "Epoch 00198: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.8739 - val_loss: 5.9385\n",
      "\n",
      "[20210302-0206-51] Learning rate for epoch 199 is 0.0003211689181625843\n",
      "Epoch 199/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1526\n",
      "Epoch 00199: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.1526 - val_loss: 5.9715\n",
      "\n",
      "[20210302-0206-56] Learning rate for epoch 200 is 0.00016058447363320738\n",
      "Epoch 200/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8502\n",
      "Epoch 00200: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.8502 - val_loss: 5.9704\n",
      "\n",
      "[20210302-0207-00] Learning rate for epoch 201 is 4.0000000467443897e-07\n",
      "Epoch 201/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7478\n",
      "Epoch 00201: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.7478 - val_loss: 5.9784\n",
      "\n",
      "[20210302-0207-05] Learning rate for epoch 202 is 0.0001601835247129202\n",
      "Epoch 202/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8970\n",
      "Epoch 00202: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.8970 - val_loss: 5.9282\n",
      "\n",
      "[20210302-0207-09] Learning rate for epoch 203 is 0.00031956707243807614\n",
      "Epoch 203/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9205\n",
      "Epoch 00203: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.9205 - val_loss: 5.9424\n",
      "\n",
      "[20210302-0207-14] Learning rate for epoch 204 is 0.00047855067532509565\n",
      "Epoch 204/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6992\n",
      "Epoch 00204: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 71ms/step - loss: 5.6992 - val_loss: 6.0878\n",
      "\n",
      "[20210302-0207-19] Learning rate for epoch 205 is 0.0006371343042701483\n",
      "Epoch 205/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7591\n",
      "Epoch 00205: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.7591 - val_loss: 6.0497\n",
      "\n",
      "[20210302-0207-23] Learning rate for epoch 206 is 0.0007953179883770645\n",
      "Epoch 206/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8977\n",
      "Epoch 00206: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 71ms/step - loss: 5.8977 - val_loss: 5.8257\n",
      "\n",
      "[20210302-0207-28] Learning rate for epoch 207 is 0.0009531017276458442\n",
      "Epoch 207/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9656\n",
      "Epoch 00207: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.9656 - val_loss: 5.9026\n",
      "\n",
      "[20210302-0207-33] Learning rate for epoch 208 is 0.0011104855220764875\n",
      "Epoch 208/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7961\n",
      "Epoch 00208: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.7961 - val_loss: 6.1889\n",
      "\n",
      "[20210302-0207-38] Learning rate for epoch 209 is 0.0012674692552536726\n",
      "Epoch 209/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0305\n",
      "Epoch 00209: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.0305 - val_loss: 6.0852\n",
      "\n",
      "[20210302-0207-42] Learning rate for epoch 210 is 0.0014240531018003821\n",
      "Epoch 210/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8071\n",
      "Epoch 00210: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.8071 - val_loss: 6.3453\n",
      "\n",
      "[20210302-0207-47] Learning rate for epoch 211 is 0.0015802369453012943\n",
      "Epoch 211/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0214\n",
      "Epoch 00211: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.0214 - val_loss: 6.2467\n",
      "\n",
      "[20210302-0207-52] Learning rate for epoch 212 is 0.001736020902171731\n",
      "Epoch 212/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0719\n",
      "Epoch 00212: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.0719 - val_loss: 6.2189\n",
      "\n",
      "[20210302-0207-56] Learning rate for epoch 213 is 0.0018914048559963703\n",
      "Epoch 213/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1118\n",
      "Epoch 00213: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.1118 - val_loss: 6.3472\n",
      "\n",
      "[20210302-0208-01] Learning rate for epoch 214 is 0.0020463888067752123\n",
      "Epoch 214/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1192\n",
      "Epoch 00214: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.1192 - val_loss: 6.1327\n",
      "\n",
      "[20210302-0208-05] Learning rate for epoch 215 is 0.0022009729873389006\n",
      "Epoch 215/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9991\n",
      "Epoch 00215: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.9991 - val_loss: 6.4263\n",
      "\n",
      "[20210302-0208-10] Learning rate for epoch 216 is 0.002355156932026148\n",
      "Epoch 216/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0998\n",
      "Epoch 00216: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 6.0998 - val_loss: 6.6174\n",
      "\n",
      "[20210302-0208-15] Learning rate for epoch 217 is 0.0025089411064982414\n",
      "Epoch 217/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3151\n",
      "Epoch 00217: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.3151 - val_loss: 6.3008\n",
      "\n",
      "[20210302-0208-19] Learning rate for epoch 218 is 0.0026623252779245377\n",
      "Epoch 218/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2523\n",
      "Epoch 00218: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 68ms/step - loss: 6.2523 - val_loss: 6.4752\n",
      "\n",
      "[20210302-0208-24] Learning rate for epoch 219 is 0.0028153094463050365\n",
      "Epoch 219/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3391\n",
      "Epoch 00219: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 71ms/step - loss: 6.3391 - val_loss: 6.3800\n",
      "\n",
      "[20210302-0208-29] Learning rate for epoch 220 is 0.002967893611639738\n",
      "Epoch 220/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3083\n",
      "Epoch 00220: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.3083 - val_loss: 6.3466\n",
      "\n",
      "[20210302-0208-34] Learning rate for epoch 221 is 0.003120078006759286\n",
      "Epoch 221/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3125\n",
      "Epoch 00221: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.3125 - val_loss: 6.2002\n",
      "\n",
      "[20210302-0208-38] Learning rate for epoch 222 is 0.0032718623988330364\n",
      "Epoch 222/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2323\n",
      "Epoch 00222: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.2323 - val_loss: 5.9268\n",
      "\n",
      "[20210302-0208-43] Learning rate for epoch 223 is 0.0034232467878609896\n",
      "Epoch 223/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5394\n",
      "Epoch 00223: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.5394 - val_loss: 6.5301\n",
      "\n",
      "[20210302-0208-47] Learning rate for epoch 224 is 0.0035742311738431454\n",
      "Epoch 224/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6795\n",
      "Epoch 00224: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.6795 - val_loss: 6.3484\n",
      "\n",
      "[20210302-0208-52] Learning rate for epoch 225 is 0.003724815556779504\n",
      "Epoch 225/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4810\n",
      "Epoch 00225: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.4810 - val_loss: 6.7318\n",
      "\n",
      "[20210302-0208-57] Learning rate for epoch 226 is 0.003874999936670065\n",
      "Epoch 226/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3189\n",
      "Epoch 00226: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.3189 - val_loss: 7.1954\n",
      "\n",
      "[20210302-0209-01] Learning rate for epoch 227 is 0.0037152154836803675\n",
      "Epoch 227/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5024\n",
      "Epoch 00227: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.5024 - val_loss: 7.2067\n",
      "\n",
      "[20210302-0209-06] Learning rate for epoch 228 is 0.0035558310337364674\n",
      "Epoch 228/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4992\n",
      "Epoch 00228: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.4992 - val_loss: 9.2824\n",
      "\n",
      "[20210302-0209-10] Learning rate for epoch 229 is 0.003396846354007721\n",
      "Epoch 229/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3946\n",
      "Epoch 00229: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.3946 - val_loss: 6.9574\n",
      "\n",
      "[20210302-0209-15] Learning rate for epoch 230 is 0.003238261677324772\n",
      "Epoch 230/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6405\n",
      "Epoch 00230: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.6405 - val_loss: 7.1688\n",
      "\n",
      "[20210302-0209-20] Learning rate for epoch 231 is 0.00308007700368762\n",
      "Epoch 231/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3544\n",
      "Epoch 00231: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.3544 - val_loss: 8.2827\n",
      "\n",
      "[20210302-0209-24] Learning rate for epoch 232 is 0.002922292333096266\n",
      "Epoch 232/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2642\n",
      "Epoch 00232: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.2642 - val_loss: 7.7378\n",
      "\n",
      "[20210302-0209-29] Learning rate for epoch 233 is 0.002764907432720065\n",
      "Epoch 233/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3091\n",
      "Epoch 00233: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.3091 - val_loss: 7.1721\n",
      "\n",
      "[20210302-0209-34] Learning rate for epoch 234 is 0.0026079227682203054\n",
      "Epoch 234/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1293\n",
      "Epoch 00234: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.1293 - val_loss: 7.0006\n",
      "\n",
      "[20210302-0209-38] Learning rate for epoch 235 is 0.0024513378739356995\n",
      "Epoch 235/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0714\n",
      "Epoch 00235: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.0714 - val_loss: 7.2954\n",
      "\n",
      "[20210302-0209-43] Learning rate for epoch 236 is 0.002295152982696891\n",
      "Epoch 236/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0196\n",
      "Epoch 00236: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.0196 - val_loss: 7.0206\n",
      "\n",
      "[20210302-0209-48] Learning rate for epoch 237 is 0.0021393680945038795\n",
      "Epoch 237/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2605\n",
      "Epoch 00237: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.2605 - val_loss: 6.4815\n",
      "\n",
      "[20210302-0209-52] Learning rate for epoch 238 is 0.0019839832093566656\n",
      "Epoch 238/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1025\n",
      "Epoch 00238: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.1025 - val_loss: 6.7040\n",
      "\n",
      "[20210302-0209-57] Learning rate for epoch 239 is 0.0018289980944246054\n",
      "Epoch 239/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9836\n",
      "Epoch 00239: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.9836 - val_loss: 6.1367\n",
      "\n",
      "[20210302-0210-01] Learning rate for epoch 240 is 0.0016744130989536643\n",
      "Epoch 240/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9558\n",
      "Epoch 00240: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.9558 - val_loss: 6.4578\n",
      "\n",
      "[20210302-0210-06] Learning rate for epoch 241 is 0.0015202279901131988\n",
      "Epoch 241/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8866\n",
      "Epoch 00241: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 72ms/step - loss: 5.8866 - val_loss: 5.9384\n",
      "\n",
      "[20210302-0210-11] Learning rate for epoch 242 is 0.0013664428843185306\n",
      "Epoch 242/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9664\n",
      "Epoch 00242: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.9664 - val_loss: 6.8383\n",
      "\n",
      "[20210302-0210-16] Learning rate for epoch 243 is 0.0012130576651543379\n",
      "Epoch 243/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0738\n",
      "Epoch 00243: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.0738 - val_loss: 6.1749\n",
      "\n",
      "[20210302-0210-20] Learning rate for epoch 244 is 0.0010600725654512644\n",
      "Epoch 244/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1127\n",
      "Epoch 00244: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.1127 - val_loss: 6.6037\n",
      "\n",
      "[20210302-0210-25] Learning rate for epoch 245 is 0.0009074872941710055\n",
      "Epoch 245/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9774\n",
      "Epoch 00245: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.9774 - val_loss: 6.1492\n",
      "\n",
      "[20210302-0210-29] Learning rate for epoch 246 is 0.0007553020259365439\n",
      "Epoch 246/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7375\n",
      "Epoch 00246: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.7375 - val_loss: 5.9813\n",
      "\n",
      "[20210302-0210-34] Learning rate for epoch 247 is 0.0006035167025402188\n",
      "Epoch 247/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7625\n",
      "Epoch 00247: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.7625 - val_loss: 6.0222\n",
      "\n",
      "[20210302-0210-39] Learning rate for epoch 248 is 0.00045213132398203015\n",
      "Epoch 248/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5920\n",
      "Epoch 00248: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.5920 - val_loss: 6.1228\n",
      "\n",
      "[20210302-0210-43] Learning rate for epoch 249 is 0.00030114591936580837\n",
      "Epoch 249/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7898\n",
      "Epoch 00249: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.7898 - val_loss: 5.8536\n",
      "\n",
      "[20210302-0210-48] Learning rate for epoch 250 is 0.00015056047413963825\n",
      "Epoch 250/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5832\n",
      "Epoch 00250: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 69ms/step - loss: 5.5832 - val_loss: 5.8922\n",
      "\n",
      "[20210302-0210-53] Learning rate for epoch 251 is 3.7500001326407073e-07\n",
      "Epoch 251/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5630\n",
      "Epoch 00251: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 69ms/step - loss: 5.5630 - val_loss: 5.8937\n",
      "\n",
      "[20210302-0210-57] Learning rate for epoch 252 is 0.00015015952521935105\n",
      "Epoch 252/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6869\n",
      "Epoch 00252: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 5.6869 - val_loss: 5.8924\n",
      "\n",
      "[20210302-0211-02] Learning rate for epoch 253 is 0.0002995440736413002\n",
      "Epoch 253/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7307\n",
      "Epoch 00253: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.7307 - val_loss: 5.8945\n",
      "\n",
      "[20210302-0211-07] Learning rate for epoch 254 is 0.0004485286772251129\n",
      "Epoch 254/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6750\n",
      "Epoch 00254: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.6750 - val_loss: 5.9526\n",
      "\n",
      "[20210302-0211-12] Learning rate for epoch 255 is 0.0005971133359707892\n",
      "Epoch 255/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6241\n",
      "Epoch 00255: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.6241 - val_loss: 6.0763\n",
      "\n",
      "[20210302-0211-16] Learning rate for epoch 256 is 0.0007452979916706681\n",
      "Epoch 256/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8306\n",
      "Epoch 00256: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 70ms/step - loss: 5.8306 - val_loss: 5.8120\n",
      "\n",
      "[20210302-0211-21] Learning rate for epoch 257 is 0.0008930827025324106\n",
      "Epoch 257/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9177\n",
      "Epoch 00257: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.9177 - val_loss: 6.1029\n",
      "\n",
      "[20210302-0211-26] Learning rate for epoch 258 is 0.0010404675267636776\n",
      "Epoch 258/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6689\n",
      "Epoch 00258: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6689 - val_loss: 6.1157\n",
      "\n",
      "[20210302-0211-30] Learning rate for epoch 259 is 0.0011874522315338254\n",
      "Epoch 259/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5908\n",
      "Epoch 00259: val_loss did not improve from 5.62886\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5908 - val_loss: 6.4133\n",
      "\n",
      "[20210302-0211-35] Learning rate for epoch 260 is 0.0013340371660888195\n",
      "Epoch 260/1000\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 5.5950"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00007: val_loss did not improve from 9.69430\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 12.1932 - val_loss: 11.0200\n",
      "\n",
      "[20210302-0302-53] Learning rate for epoch 8 is 0.00034801175934262574\n",
      "Epoch 8/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.9639\n",
      "Epoch 00008: val_loss improved from 9.69430 to 9.16300, saving model to ./20210301-225844/toe_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 93ms/step - loss: 11.9639 - val_loss: 9.1630\n",
      "\n",
      "[20210302-0302-58] Learning rate for epoch 9 is 0.00039725631359033287\n",
      "Epoch 9/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.5523\n",
      "Epoch 00009: val_loss did not improve from 9.16300\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 11.5523 - val_loss: 9.1736\n",
      "\n",
      "[20210302-0303-03] Learning rate for epoch 10 is 0.0004464008961804211\n",
      "Epoch 10/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.4932\n",
      "Epoch 00010: val_loss did not improve from 9.16300\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 11.4932 - val_loss: 9.2251\n",
      "\n",
      "[20210302-0303-07] Learning rate for epoch 11 is 0.0004954455071128905\n",
      "Epoch 11/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.3163\n",
      "Epoch 00011: val_loss did not improve from 9.16300\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 11.3163 - val_loss: 9.7476\n",
      "\n",
      "[20210302-0303-12] Learning rate for epoch 12 is 0.0005443901172839105\n",
      "Epoch 12/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.3376\n",
      "Epoch 00012: val_loss improved from 9.16300 to 8.22400, saving model to ./20210301-225844/toe_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 11.3376 - val_loss: 8.2240\n",
      "\n",
      "[20210302-0303-17] Learning rate for epoch 13 is 0.0005932347266934812\n",
      "Epoch 13/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.9228\n",
      "Epoch 00013: val_loss improved from 8.22400 to 7.84230, saving model to ./20210301-225844/toe_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 92ms/step - loss: 10.9228 - val_loss: 7.8423\n",
      "\n",
      "[20210302-0303-22] Learning rate for epoch 14 is 0.0006419793353416026\n",
      "Epoch 14/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.8706\n",
      "Epoch 00014: val_loss did not improve from 7.84230\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 10.8706 - val_loss: 8.1459\n",
      "\n",
      "[20210302-0303-27] Learning rate for epoch 15 is 0.0006906240014359355\n",
      "Epoch 15/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.9331\n",
      "Epoch 00015: val_loss did not improve from 7.84230\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 10.9331 - val_loss: 8.1545\n",
      "\n",
      "[20210302-0303-32] Learning rate for epoch 16 is 0.0007391686085611582\n",
      "Epoch 16/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.4118\n",
      "Epoch 00016: val_loss did not improve from 7.84230\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 10.4118 - val_loss: 8.4291\n",
      "\n",
      "[20210302-0303-36] Learning rate for epoch 17 is 0.0007876132731325924\n",
      "Epoch 17/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.3756\n",
      "Epoch 00017: val_loss did not improve from 7.84230\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 10.3756 - val_loss: 8.5764\n",
      "\n",
      "[20210302-0303-41] Learning rate for epoch 18 is 0.0008359579369425774\n",
      "Epoch 18/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.4609\n",
      "Epoch 00018: val_loss did not improve from 7.84230\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 10.4609 - val_loss: 8.1449\n",
      "\n",
      "[20210302-0303-45] Learning rate for epoch 19 is 0.000884202599991113\n",
      "Epoch 19/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.3230\n",
      "Epoch 00019: val_loss did not improve from 7.84230\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 10.3230 - val_loss: 7.9835\n",
      "\n",
      "[20210302-0303-50] Learning rate for epoch 20 is 0.0009323473204858601\n",
      "Epoch 20/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.0347\n",
      "Epoch 00020: val_loss did not improve from 7.84230\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 10.0347 - val_loss: 9.7860\n",
      "\n",
      "[20210302-0303-55] Learning rate for epoch 21 is 0.000980391982011497\n",
      "Epoch 21/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.8621\n",
      "Epoch 00021: val_loss did not improve from 7.84230\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 9.8621 - val_loss: 8.5598\n",
      "\n",
      "[20210302-0303-59] Learning rate for epoch 22 is 0.0010283367009833455\n",
      "Epoch 22/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.4855\n",
      "Epoch 00022: val_loss improved from 7.84230 to 7.58082, saving model to ./20210301-225844/toe_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 9.4855 - val_loss: 7.5808\n",
      "\n",
      "[20210302-0304-04] Learning rate for epoch 23 is 0.0010761814191937447\n",
      "Epoch 23/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.5944\n",
      "Epoch 00023: val_loss did not improve from 7.58082\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 9.5944 - val_loss: 7.8379\n",
      "\n",
      "[20210302-0304-09] Learning rate for epoch 24 is 0.0011239261366426945\n",
      "Epoch 24/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.5951\n",
      "Epoch 00024: val_loss did not improve from 7.58082\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 9.5951 - val_loss: 7.6770\n",
      "\n",
      "[20210302-0304-14] Learning rate for epoch 25 is 0.001171570853330195\n",
      "Epoch 25/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.6799\n",
      "Epoch 00025: val_loss did not improve from 7.58082\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 9.6799 - val_loss: 8.3875\n",
      "\n",
      "[20210302-0304-18] Learning rate for epoch 26 is 0.001219115569256246\n",
      "Epoch 26/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.6238\n",
      "Epoch 00026: val_loss improved from 7.58082 to 7.55952, saving model to ./20210301-225844/toe_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 89ms/step - loss: 9.6238 - val_loss: 7.5595\n",
      "\n",
      "[20210302-0304-23] Learning rate for epoch 27 is 0.0012665604008361697\n",
      "Epoch 27/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.4486\n",
      "Epoch 00027: val_loss did not improve from 7.55952\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 9.4486 - val_loss: 9.6918\n",
      "\n",
      "[20210302-0304-28] Learning rate for epoch 28 is 0.0013139051152393222\n",
      "Epoch 28/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.2998\n",
      "Epoch 00028: val_loss did not improve from 7.55952\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 9.2998 - val_loss: 9.9944\n",
      "\n",
      "[20210302-0304-33] Learning rate for epoch 29 is 0.0013611499452963471\n",
      "Epoch 29/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.6503\n",
      "Epoch 00029: val_loss did not improve from 7.55952\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 9.6503 - val_loss: 9.2994\n",
      "\n",
      "[20210302-0304-37] Learning rate for epoch 30 is 0.001408294658176601\n",
      "Epoch 30/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.1479\n",
      "Epoch 00030: val_loss did not improve from 7.55952\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 9.1479 - val_loss: 9.8975\n",
      "\n",
      "[20210302-0304-42] Learning rate for epoch 31 is 0.0014553394867107272\n",
      "Epoch 31/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.0104\n",
      "Epoch 00031: val_loss did not improve from 7.55952\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 9.0104 - val_loss: 9.3538\n",
      "\n",
      "[20210302-0304-47] Learning rate for epoch 32 is 0.0015022843144834042\n",
      "Epoch 32/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.7510\n",
      "Epoch 00032: val_loss did not improve from 7.55952\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 8.7510 - val_loss: 10.4460\n",
      "\n",
      "[20210302-0304-51] Learning rate for epoch 33 is 0.0015491291414946318\n",
      "Epoch 33/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.7739\n",
      "Epoch 00033: val_loss did not improve from 7.55952\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 8.7739 - val_loss: 8.0643\n",
      "\n",
      "[20210302-0304-58] Learning rate for epoch 34 is 0.00159587396774441\n",
      "Epoch 34/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.6987\n",
      "Epoch 00034: val_loss did not improve from 7.55952\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 8.6987 - val_loss: 7.9117\n",
      "\n",
      "[20210302-0305-02] Learning rate for epoch 35 is 0.001642518793232739\n",
      "Epoch 35/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.6003\n",
      "Epoch 00035: val_loss did not improve from 7.55952\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 8.6003 - val_loss: 7.8477\n",
      "\n",
      "[20210302-0305-07] Learning rate for epoch 36 is 0.0016890636179596186\n",
      "Epoch 36/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.6681\n",
      "Epoch 00036: val_loss did not improve from 7.55952\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 8.6681 - val_loss: 7.9176\n",
      "\n",
      "[20210302-0305-12] Learning rate for epoch 37 is 0.0017355084419250488\n",
      "Epoch 37/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.5297\n",
      "Epoch 00037: val_loss improved from 7.55952 to 6.49902, saving model to ./20210301-225844/toe_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 8.5297 - val_loss: 6.4990\n",
      "\n",
      "[20210302-0305-17] Learning rate for epoch 38 is 0.0017818533815443516\n",
      "Epoch 38/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.4692\n",
      "Epoch 00038: val_loss did not improve from 6.49902\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 8.4692 - val_loss: 9.5701\n",
      "\n",
      "[20210302-0305-21] Learning rate for epoch 39 is 0.0018280982039868832\n",
      "Epoch 39/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.3797\n",
      "Epoch 00039: val_loss did not improve from 6.49902\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 8.3797 - val_loss: 8.9988\n",
      "\n",
      "[20210302-0305-26] Learning rate for epoch 40 is 0.0018742431420832872\n",
      "Epoch 40/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.3654\n",
      "Epoch 00040: val_loss did not improve from 6.49902\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 8.3654 - val_loss: 7.2854\n",
      "\n",
      "[20210302-0305-31] Learning rate for epoch 41 is 0.0019202879630029202\n",
      "Epoch 41/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.4656\n",
      "Epoch 00041: val_loss improved from 6.49902 to 6.36452, saving model to ./20210301-225844/toe_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 92ms/step - loss: 8.4656 - val_loss: 6.3645\n",
      "\n",
      "[20210302-0305-36] Learning rate for epoch 42 is 0.0019662328995764256\n",
      "Epoch 42/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.2276\n",
      "Epoch 00042: val_loss did not improve from 6.36452\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 8.2276 - val_loss: 6.6578\n",
      "\n",
      "[20210302-0305-41] Learning rate for epoch 43 is 0.00201207771897316\n",
      "Epoch 43/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.5183\n",
      "Epoch 00043: val_loss did not improve from 6.36452\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 8.5183 - val_loss: 6.5696\n",
      "\n",
      "[20210302-0305-45] Learning rate for epoch 44 is 0.0020578226540237665\n",
      "Epoch 44/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.2045\n",
      "Epoch 00044: val_loss did not improve from 6.36452\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 8.2045 - val_loss: 7.0025\n",
      "\n",
      "[20210302-0305-50] Learning rate for epoch 45 is 0.0021034677047282457\n",
      "Epoch 45/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.1675\n",
      "Epoch 00045: val_loss improved from 6.36452 to 5.76270, saving model to ./20210301-225844/toe_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 8.1675 - val_loss: 5.7627\n",
      "\n",
      "[20210302-0305-55] Learning rate for epoch 46 is 0.002149012638255954\n",
      "Epoch 46/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.4111\n",
      "Epoch 00046: val_loss did not improve from 5.76270\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 8.4111 - val_loss: 7.0363\n",
      "\n",
      "[20210302-0306-00] Learning rate for epoch 47 is 0.0021944576874375343\n",
      "Epoch 47/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.9955\n",
      "Epoch 00047: val_loss did not improve from 5.76270\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.9955 - val_loss: 7.2490\n",
      "\n",
      "[20210302-0306-04] Learning rate for epoch 48 is 0.0022398026194423437\n",
      "Epoch 48/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.9687\n",
      "Epoch 00048: val_loss did not improve from 5.76270\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.9687 - val_loss: 7.0453\n",
      "\n",
      "[20210302-0306-09] Learning rate for epoch 49 is 0.002285047434270382\n",
      "Epoch 49/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.1086\n",
      "Epoch 00049: val_loss did not improve from 5.76270\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 8.1086 - val_loss: 7.7547\n",
      "\n",
      "[20210302-0306-13] Learning rate for epoch 50 is 0.0023301925975829363\n",
      "Epoch 50/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7886\n",
      "Epoch 00050: val_loss did not improve from 5.76270\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 7.7886 - val_loss: 6.9183\n",
      "\n",
      "[20210302-0306-18] Learning rate for epoch 51 is 0.002375237410888076\n",
      "Epoch 51/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7936\n",
      "Epoch 00051: val_loss did not improve from 5.76270\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.7936 - val_loss: 6.4370\n",
      "\n",
      "[20210302-0306-23] Learning rate for epoch 52 is 0.0024201825726777315\n",
      "Epoch 52/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8007\n",
      "Epoch 00052: val_loss did not improve from 5.76270\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.8007 - val_loss: 8.5338\n",
      "\n",
      "[20210302-0306-27] Learning rate for epoch 53 is 0.002465027617290616\n",
      "Epoch 53/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8566\n",
      "Epoch 00053: val_loss did not improve from 5.76270\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.8566 - val_loss: 6.1654\n",
      "\n",
      "[20210302-0306-32] Learning rate for epoch 54 is 0.0025097725447267294\n",
      "Epoch 54/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.9232\n",
      "Epoch 00054: val_loss did not improve from 5.76270\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.9232 - val_loss: 8.0686\n",
      "\n",
      "[20210302-0306-36] Learning rate for epoch 55 is 0.0025544175878167152\n",
      "Epoch 55/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7393\n",
      "Epoch 00055: val_loss did not improve from 5.76270\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 7.7393 - val_loss: 6.6375\n",
      "\n",
      "[20210302-0306-41] Learning rate for epoch 56 is 0.00259896251372993\n",
      "Epoch 56/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.9933\n",
      "Epoch 00056: val_loss did not improve from 5.76270\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.9933 - val_loss: 7.3371\n",
      "\n",
      "[20210302-0306-46] Learning rate for epoch 57 is 0.0026434077881276608\n",
      "Epoch 57/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8929\n",
      "Epoch 00057: val_loss did not improve from 5.76270\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.8929 - val_loss: 5.9903\n",
      "\n",
      "[20210302-0306-50] Learning rate for epoch 58 is 0.0026877527125179768\n",
      "Epoch 58/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7530\n",
      "Epoch 00058: val_loss did not improve from 5.76270\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.7530 - val_loss: 6.0545\n",
      "\n",
      "[20210302-0306-55] Learning rate for epoch 59 is 0.0027319977525621653\n",
      "Epoch 59/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.1039\n",
      "Epoch 00059: val_loss did not improve from 5.76270\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 8.1039 - val_loss: 6.3466\n",
      "\n",
      "[20210302-0306-59] Learning rate for epoch 60 is 0.0027761429082602262\n",
      "Epoch 60/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8810\n",
      "Epoch 00060: val_loss did not improve from 5.76270\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.8810 - val_loss: 6.4776\n",
      "\n",
      "[20210302-0307-04] Learning rate for epoch 61 is 0.002820187946781516\n",
      "Epoch 61/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.0692\n",
      "Epoch 00061: val_loss improved from 5.76270 to 5.55992, saving model to ./20210301-225844/toe_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 91ms/step - loss: 8.0692 - val_loss: 5.5599\n",
      "\n",
      "[20210302-0307-09] Learning rate for epoch 62 is 0.0028641331009566784\n",
      "Epoch 62/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6928\n",
      "Epoch 00062: val_loss did not improve from 5.55992\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.6928 - val_loss: 6.1656\n",
      "\n",
      "[20210302-0307-14] Learning rate for epoch 63 is 0.0029079781379550695\n",
      "Epoch 63/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8300\n",
      "Epoch 00063: val_loss did not improve from 5.55992\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.8300 - val_loss: 5.9257\n",
      "\n",
      "[20210302-0307-18] Learning rate for epoch 64 is 0.002951723290607333\n",
      "Epoch 64/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8383\n",
      "Epoch 00064: val_loss did not improve from 5.55992\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.8383 - val_loss: 6.7811\n",
      "\n",
      "[20210302-0307-23] Learning rate for epoch 65 is 0.0029953685589134693\n",
      "Epoch 65/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8939\n",
      "Epoch 00065: val_loss did not improve from 5.55992\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.8939 - val_loss: 6.6181\n",
      "\n",
      "[20210302-0307-28] Learning rate for epoch 66 is 0.0030389137100428343\n",
      "Epoch 66/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5242\n",
      "Epoch 00066: val_loss did not improve from 5.55992\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.5242 - val_loss: 6.7406\n",
      "\n",
      "[20210302-0307-32] Learning rate for epoch 67 is 0.003082358743995428\n",
      "Epoch 67/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6153\n",
      "Epoch 00067: val_loss did not improve from 5.55992\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.6153 - val_loss: 6.0678\n",
      "\n",
      "[20210302-0307-37] Learning rate for epoch 68 is 0.0031257038936018944\n",
      "Epoch 68/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8938\n",
      "Epoch 00068: val_loss did not improve from 5.55992\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.8938 - val_loss: 6.3611\n",
      "\n",
      "[20210302-0307-42] Learning rate for epoch 69 is 0.003168949158862233\n",
      "Epoch 69/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5886\n",
      "Epoch 00069: val_loss did not improve from 5.55992\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.5886 - val_loss: 6.7802\n",
      "\n",
      "[20210302-0307-46] Learning rate for epoch 70 is 0.0032120943069458008\n",
      "Epoch 70/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4061\n",
      "Epoch 00070: val_loss did not improve from 5.55992\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.4061 - val_loss: 5.7544\n",
      "\n",
      "[20210302-0307-51] Learning rate for epoch 71 is 0.003255139570683241\n",
      "Epoch 71/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3148\n",
      "Epoch 00071: val_loss did not improve from 5.55992\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.3148 - val_loss: 6.3881\n",
      "\n",
      "[20210302-0307-55] Learning rate for epoch 72 is 0.00329808471724391\n",
      "Epoch 72/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7600\n",
      "Epoch 00072: val_loss did not improve from 5.55992\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.7600 - val_loss: 6.6815\n",
      "\n",
      "[20210302-0308-00] Learning rate for epoch 73 is 0.0033409299794584513\n",
      "Epoch 73/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6490\n",
      "Epoch 00073: val_loss did not improve from 5.55992\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.6490 - val_loss: 6.1263\n",
      "\n",
      "[20210302-0308-05] Learning rate for epoch 74 is 0.0033836751244962215\n",
      "Epoch 74/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4160\n",
      "Epoch 00074: val_loss did not improve from 5.55992\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.4160 - val_loss: 7.0657\n",
      "\n",
      "[20210302-0308-09] Learning rate for epoch 75 is 0.0034263203851878643\n",
      "Epoch 75/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4536\n",
      "Epoch 00075: val_loss did not improve from 5.55992\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.4536 - val_loss: 6.1286\n",
      "\n",
      "[20210302-0308-14] Learning rate for epoch 76 is 0.003468865528702736\n",
      "Epoch 76/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7514\n",
      "Epoch 00076: val_loss did not improve from 5.55992\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.7514 - val_loss: 9.0718\n",
      "\n",
      "[20210302-0308-19] Learning rate for epoch 77 is 0.00351131078787148\n",
      "Epoch 77/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3479\n",
      "Epoch 00077: val_loss did not improve from 5.55992\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.3479 - val_loss: 6.5168\n",
      "\n",
      "[20210302-0308-23] Learning rate for epoch 78 is 0.0035536561626940966\n",
      "Epoch 78/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4193\n",
      "Epoch 00078: val_loss did not improve from 5.55992\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.4193 - val_loss: 6.4344\n",
      "\n",
      "[20210302-0308-28] Learning rate for epoch 79 is 0.003595901420339942\n",
      "Epoch 79/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4514\n",
      "Epoch 00079: val_loss did not improve from 5.55992\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.4514 - val_loss: 6.6161\n",
      "\n",
      "[20210302-0308-33] Learning rate for epoch 80 is 0.00363804679363966\n",
      "Epoch 80/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2534\n",
      "Epoch 00080: val_loss did not improve from 5.55992\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.2534 - val_loss: 6.3755\n",
      "\n",
      "[20210302-0308-37] Learning rate for epoch 81 is 0.0036800920497626066\n",
      "Epoch 81/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4439\n",
      "Epoch 00081: val_loss did not improve from 5.55992\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.4439 - val_loss: 6.1307\n",
      "\n",
      "[20210302-0308-42] Learning rate for epoch 82 is 0.003722037188708782\n",
      "Epoch 82/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4174\n",
      "Epoch 00082: val_loss improved from 5.55992 to 5.51150, saving model to ./20210301-225844/toe_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 91ms/step - loss: 7.4174 - val_loss: 5.5115\n",
      "\n",
      "[20210302-0308-47] Learning rate for epoch 83 is 0.003763882676139474\n",
      "Epoch 83/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5902\n",
      "Epoch 00083: val_loss did not improve from 5.51150\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.5902 - val_loss: 6.8070\n",
      "\n",
      "[20210302-0308-52] Learning rate for epoch 84 is 0.0038056280463933945\n",
      "Epoch 84/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6369\n",
      "Epoch 00084: val_loss did not improve from 5.51150\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.6369 - val_loss: 5.8513\n",
      "\n",
      "[20210302-0308-56] Learning rate for epoch 85 is 0.003847273299470544\n",
      "Epoch 85/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5016\n",
      "Epoch 00085: val_loss did not improve from 5.51150\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.5016 - val_loss: 7.4993\n",
      "\n",
      "[20210302-0309-01] Learning rate for epoch 86 is 0.0038888186682015657\n",
      "Epoch 86/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6476\n",
      "Epoch 00086: val_loss did not improve from 5.51150\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.6476 - val_loss: 6.6328\n",
      "\n",
      "[20210302-0309-06] Learning rate for epoch 87 is 0.00393026415258646\n",
      "Epoch 87/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5280\n",
      "Epoch 00087: val_loss did not improve from 5.51150\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.5280 - val_loss: 8.6468\n",
      "\n",
      "[20210302-0309-10] Learning rate for epoch 88 is 0.00397160928696394\n",
      "Epoch 88/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6194\n",
      "Epoch 00088: val_loss did not improve from 5.51150\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.6194 - val_loss: 6.7301\n",
      "\n",
      "[20210302-0309-15] Learning rate for epoch 89 is 0.004012854769825935\n",
      "Epoch 89/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.9391\n",
      "Epoch 00089: val_loss did not improve from 5.51150\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.9391 - val_loss: 7.1596\n",
      "\n",
      "[20210302-0309-20] Learning rate for epoch 90 is 0.00405400013551116\n",
      "Epoch 90/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5606\n",
      "Epoch 00090: val_loss did not improve from 5.51150\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.5606 - val_loss: 6.1659\n",
      "\n",
      "[20210302-0309-24] Learning rate for epoch 91 is 0.004095045384019613\n",
      "Epoch 91/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3742\n",
      "Epoch 00091: val_loss did not improve from 5.51150\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.3742 - val_loss: 6.6985\n",
      "\n",
      "[20210302-0309-29] Learning rate for epoch 92 is 0.004135990981012583\n",
      "Epoch 92/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0721\n",
      "Epoch 00092: val_loss did not improve from 5.51150\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.0721 - val_loss: 6.6356\n",
      "\n",
      "[20210302-0309-33] Learning rate for epoch 93 is 0.004176836460828781\n",
      "Epoch 93/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3052\n",
      "Epoch 00093: val_loss did not improve from 5.51150\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.3052 - val_loss: 8.7992\n",
      "\n",
      "[20210302-0309-38] Learning rate for epoch 94 is 0.004217581823468208\n",
      "Epoch 94/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3294\n",
      "Epoch 00094: val_loss did not improve from 5.51150\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.3294 - val_loss: 7.3897\n",
      "\n",
      "[20210302-0309-43] Learning rate for epoch 95 is 0.004258227068930864\n",
      "Epoch 95/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4493\n",
      "Epoch 00095: val_loss did not improve from 5.51150\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.4493 - val_loss: 6.5268\n",
      "\n",
      "[20210302-0309-47] Learning rate for epoch 96 is 0.0042987726628780365\n",
      "Epoch 96/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4955\n",
      "Epoch 00096: val_loss did not improve from 5.51150\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.4955 - val_loss: 7.2464\n",
      "\n",
      "[20210302-0309-52] Learning rate for epoch 97 is 0.0043392181396484375\n",
      "Epoch 97/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.9425\n",
      "Epoch 00097: val_loss did not improve from 5.51150\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.9425 - val_loss: 8.8208\n",
      "\n",
      "[20210302-0309-57] Learning rate for epoch 98 is 0.004379563499242067\n",
      "Epoch 98/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4609\n",
      "Epoch 00098: val_loss did not improve from 5.51150\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.4609 - val_loss: 6.0628\n",
      "\n",
      "[20210302-0310-01] Learning rate for epoch 99 is 0.004419809207320213\n",
      "Epoch 99/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3307\n",
      "Epoch 00099: val_loss did not improve from 5.51150\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.3307 - val_loss: 6.6604\n",
      "\n",
      "[20210302-0310-06] Learning rate for epoch 100 is 0.004459954332560301\n",
      "Epoch 100/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1967\n",
      "Epoch 00100: val_loss did not improve from 5.51150\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.1967 - val_loss: 6.6890\n",
      "\n",
      "[20210302-0310-11] Learning rate for epoch 101 is 4.4999998749517545e-07\n",
      "Epoch 101/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2287\n",
      "Epoch 00101: val_loss did not improve from 5.51150\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.2287 - val_loss: 6.4016\n",
      "\n",
      "[20210302-0310-15] Learning rate for epoch 102 is 0.00018023152370005846\n",
      "Epoch 102/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2768\n",
      "Epoch 00102: val_loss did not improve from 5.51150\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.2768 - val_loss: 6.2082\n",
      "\n",
      "[20210302-0310-20] Learning rate for epoch 103 is 0.000359613070031628\n",
      "Epoch 103/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0175\n",
      "Epoch 00103: val_loss did not improve from 5.51150\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.0175 - val_loss: 6.0007\n",
      "\n",
      "[20210302-0310-24] Learning rate for epoch 104 is 0.0005385946715250611\n",
      "Epoch 104/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8085\n",
      "Epoch 00104: val_loss did not improve from 5.51150\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.8085 - val_loss: 5.9539\n",
      "\n",
      "[20210302-0310-29] Learning rate for epoch 105 is 0.0007171762990765274\n",
      "Epoch 105/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7622\n",
      "Epoch 00105: val_loss did not improve from 5.51150\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.7622 - val_loss: 6.1624\n",
      "\n",
      "[20210302-0310-34] Learning rate for epoch 106 is 0.0008953579817898571\n",
      "Epoch 106/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9332\n",
      "Epoch 00106: val_loss did not improve from 5.51150\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.9332 - val_loss: 6.0770\n",
      "\n",
      "[20210302-0310-38] Learning rate for epoch 107 is 0.0010731397196650505\n",
      "Epoch 107/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8009\n",
      "Epoch 00107: val_loss did not improve from 5.51150\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.8009 - val_loss: 5.7281\n",
      "\n",
      "[20210302-0310-43] Learning rate for epoch 108 is 0.0012505215127021074\n",
      "Epoch 108/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8495\n",
      "Epoch 00108: val_loss improved from 5.51150 to 5.37255, saving model to ./20210301-225844/toe_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 87ms/step - loss: 6.8495 - val_loss: 5.3726\n",
      "\n",
      "[20210302-0310-48] Learning rate for epoch 109 is 0.001427503302693367\n",
      "Epoch 109/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6820\n",
      "Epoch 00109: val_loss did not improve from 5.37255\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.6820 - val_loss: 6.4159\n",
      "\n",
      "[20210302-0310-53] Learning rate for epoch 110 is 0.0016040850896388292\n",
      "Epoch 110/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7502\n",
      "Epoch 00110: val_loss did not improve from 5.37255\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.7502 - val_loss: 5.5453\n",
      "\n",
      "[20210302-0310-57] Learning rate for epoch 111 is 0.001780266989953816\n",
      "Epoch 111/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7106\n",
      "Epoch 00111: val_loss did not improve from 5.37255\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.7106 - val_loss: 5.6695\n",
      "\n",
      "[20210302-0311-02] Learning rate for epoch 112 is 0.0019560488872230053\n",
      "Epoch 112/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8494\n",
      "Epoch 00112: val_loss did not improve from 5.37255\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.8494 - val_loss: 5.9030\n",
      "\n",
      "[20210302-0311-07] Learning rate for epoch 113 is 0.002131430897861719\n",
      "Epoch 113/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7951\n",
      "Epoch 00113: val_loss did not improve from 5.37255\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.7951 - val_loss: 5.9557\n",
      "\n",
      "[20210302-0311-11] Learning rate for epoch 114 is 0.0023064129054546356\n",
      "Epoch 114/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8154\n",
      "Epoch 00114: val_loss did not improve from 5.37255\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.8154 - val_loss: 5.8379\n",
      "\n",
      "[20210302-0311-16] Learning rate for epoch 115 is 0.0024809949100017548\n",
      "Epoch 115/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5114\n",
      "Epoch 00115: val_loss did not improve from 5.37255\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.5114 - val_loss: 5.7802\n",
      "\n",
      "[20210302-0311-21] Learning rate for epoch 116 is 0.0026551769115030766\n",
      "Epoch 116/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6659\n",
      "Epoch 00116: val_loss did not improve from 5.37255\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.6659 - val_loss: 5.8009\n",
      "\n",
      "[20210302-0311-25] Learning rate for epoch 117 is 0.0028289591427892447\n",
      "Epoch 117/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7257\n",
      "Epoch 00117: val_loss did not improve from 5.37255\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.7257 - val_loss: 6.2279\n",
      "\n",
      "[20210302-0311-30] Learning rate for epoch 118 is 0.0030023413710296154\n",
      "Epoch 118/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0153\n",
      "Epoch 00118: val_loss did not improve from 5.37255\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.0153 - val_loss: 6.7166\n",
      "\n",
      "[20210302-0311-34] Learning rate for epoch 119 is 0.003175323596224189\n",
      "Epoch 119/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8993\n",
      "Epoch 00119: val_loss did not improve from 5.37255\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.8993 - val_loss: 6.1420\n",
      "\n",
      "[20210302-0311-39] Learning rate for epoch 120 is 0.003347905818372965\n",
      "Epoch 120/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7881\n",
      "Epoch 00120: val_loss did not improve from 5.37255\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.7881 - val_loss: 7.0716\n",
      "\n",
      "[20210302-0311-44] Learning rate for epoch 121 is 0.0035200880374759436\n",
      "Epoch 121/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9003\n",
      "Epoch 00121: val_loss did not improve from 5.37255\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.9003 - val_loss: 6.0914\n",
      "\n",
      "[20210302-0311-48] Learning rate for epoch 122 is 0.003691870253533125\n",
      "Epoch 122/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8501\n",
      "Epoch 00122: val_loss did not improve from 5.37255\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.8501 - val_loss: 6.8787\n",
      "\n",
      "[20210302-0311-53] Learning rate for epoch 123 is 0.0038632526993751526\n",
      "Epoch 123/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0354\n",
      "Epoch 00123: val_loss did not improve from 5.37255\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.0354 - val_loss: 5.9635\n",
      "\n",
      "[20210302-0311-58] Learning rate for epoch 124 is 0.004034235142171383\n",
      "Epoch 124/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8198\n",
      "Epoch 00124: val_loss did not improve from 5.37255\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.8198 - val_loss: 5.8352\n",
      "\n",
      "[20210302-0312-02] Learning rate for epoch 125 is 0.004204817581921816\n",
      "Epoch 125/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9897\n",
      "Epoch 00125: val_loss did not improve from 5.37255\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.9897 - val_loss: 6.1082\n",
      "\n",
      "[20210302-0312-07] Learning rate for epoch 126 is 0.0043750000186264515\n",
      "Epoch 126/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0121\n",
      "Epoch 00126: val_loss did not improve from 5.37255\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.0121 - val_loss: 7.5797\n",
      "\n",
      "[20210302-0312-11] Learning rate for epoch 127 is 0.0041952175088226795\n",
      "Epoch 127/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9957\n",
      "Epoch 00127: val_loss did not improve from 5.37255\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.9957 - val_loss: 6.5303\n",
      "\n",
      "[20210302-0312-16] Learning rate for epoch 128 is 0.004015835002064705\n",
      "Epoch 128/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9416\n",
      "Epoch 00128: val_loss did not improve from 5.37255\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.9416 - val_loss: 6.8688\n",
      "\n",
      "[20210302-0312-21] Learning rate for epoch 129 is 0.003836852265521884\n",
      "Epoch 129/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9712\n",
      "Epoch 00129: val_loss did not improve from 5.37255\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.9712 - val_loss: 8.2216\n",
      "\n",
      "[20210302-0312-25] Learning rate for epoch 130 is 0.003658269764855504\n",
      "Epoch 130/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0643\n",
      "Epoch 00130: val_loss did not improve from 5.37255\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.0643 - val_loss: 8.1740\n",
      "\n",
      "[20210302-0312-30] Learning rate for epoch 131 is 0.003480087034404278\n",
      "Epoch 131/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9564\n",
      "Epoch 00131: val_loss did not improve from 5.37255\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.9564 - val_loss: 7.5610\n",
      "\n",
      "[20210302-0312-35] Learning rate for epoch 132 is 0.003302304306998849\n",
      "Epoch 132/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8880\n",
      "Epoch 00132: val_loss did not improve from 5.37255\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.8880 - val_loss: 6.9583\n",
      "\n",
      "[20210302-0312-39] Learning rate for epoch 133 is 0.0031249215826392174\n",
      "Epoch 133/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6055\n",
      "Epoch 00133: val_loss did not improve from 5.37255\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.6055 - val_loss: 6.6859\n",
      "\n",
      "[20210302-0312-44] Learning rate for epoch 134 is 0.0029479386284947395\n",
      "Epoch 134/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7158\n",
      "Epoch 00134: val_loss did not improve from 5.37255\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.7158 - val_loss: 6.5357\n",
      "\n",
      "[20210302-0312-49] Learning rate for epoch 135 is 0.0027713559102267027\n",
      "Epoch 135/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9431\n",
      "Epoch 00135: val_loss did not improve from 5.37255\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.9431 - val_loss: 6.2969\n",
      "\n",
      "[20210302-0312-53] Learning rate for epoch 136 is 0.0025951729621738195\n",
      "Epoch 136/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9018\n",
      "Epoch 00136: val_loss did not improve from 5.37255\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.9018 - val_loss: 6.6427\n",
      "\n",
      "[20210302-0312-58] Learning rate for epoch 137 is 0.0024193900171667337\n",
      "Epoch 137/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8602\n",
      "Epoch 00137: val_loss did not improve from 5.37255\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.8602 - val_loss: 6.4984\n",
      "\n",
      "[20210302-0313-02] Learning rate for epoch 138 is 0.0022440070752054453\n",
      "Epoch 138/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6586\n",
      "Epoch 00138: val_loss did not improve from 5.37255\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.6586 - val_loss: 6.2165\n",
      "\n",
      "[20210302-0313-07] Learning rate for epoch 139 is 0.002069024136289954\n",
      "Epoch 139/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5627\n",
      "Epoch 00139: val_loss did not improve from 5.37255\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.5627 - val_loss: 5.6186\n",
      "\n",
      "[20210302-0313-12] Learning rate for epoch 140 is 0.0018944410840049386\n",
      "Epoch 140/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5947\n",
      "Epoch 00140: val_loss did not improve from 5.37255\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.5947 - val_loss: 5.7507\n",
      "\n",
      "[20210302-0313-16] Learning rate for epoch 141 is 0.0017202580347657204\n",
      "Epoch 141/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8115\n",
      "Epoch 00141: val_loss did not improve from 5.37255\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.8115 - val_loss: 5.9499\n",
      "\n",
      "[20210302-0313-21] Learning rate for epoch 142 is 0.0015464748721569777\n",
      "Epoch 142/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5067\n",
      "Epoch 00142: val_loss did not improve from 5.37255\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.5067 - val_loss: 5.9158\n",
      "\n",
      "[20210302-0313-26] Learning rate for epoch 143 is 0.0013730917125940323\n",
      "Epoch 143/1000\n",
      "14/20 [====================>.........] - ETA: 0s - loss: 6.4472"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - ETA: 0s - loss: 10.8321\n",
      "Epoch 00017: val_loss did not improve from 6.51702\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 10.8321 - val_loss: 7.5280\n",
      "\n",
      "[20210302-0404-17] Learning rate for epoch 18 is 0.0008359579369425774\n",
      "Epoch 18/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.6658\n",
      "Epoch 00018: val_loss did not improve from 6.51702\n",
      "20/20 [==============================] - 1s 59ms/step - loss: 10.6658 - val_loss: 6.7769\n",
      "\n",
      "[20210302-0404-21] Learning rate for epoch 19 is 0.000884202599991113\n",
      "Epoch 19/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.4346\n",
      "Epoch 00019: val_loss did not improve from 6.51702\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 10.4346 - val_loss: 7.4717\n",
      "\n",
      "[20210302-0404-26] Learning rate for epoch 20 is 0.0009323473204858601\n",
      "Epoch 20/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.4036\n",
      "Epoch 00020: val_loss did not improve from 6.51702\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 10.4036 - val_loss: 9.0103\n",
      "\n",
      "[20210302-0404-30] Learning rate for epoch 21 is 0.000980391982011497\n",
      "Epoch 21/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.4718\n",
      "Epoch 00021: val_loss did not improve from 6.51702\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 10.4718 - val_loss: 7.6036\n",
      "\n",
      "[20210302-0404-35] Learning rate for epoch 22 is 0.0010283367009833455\n",
      "Epoch 22/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.1325\n",
      "Epoch 00022: val_loss did not improve from 6.51702\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 10.1325 - val_loss: 10.8378\n",
      "\n",
      "[20210302-0404-40] Learning rate for epoch 23 is 0.0010761814191937447\n",
      "Epoch 23/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.9695\n",
      "Epoch 00023: val_loss did not improve from 6.51702\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 9.9695 - val_loss: 7.3979\n",
      "\n",
      "[20210302-0404-44] Learning rate for epoch 24 is 0.0011239261366426945\n",
      "Epoch 24/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.9906\n",
      "Epoch 00024: val_loss did not improve from 6.51702\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 9.9906 - val_loss: 9.4544\n",
      "\n",
      "[20210302-0404-49] Learning rate for epoch 25 is 0.001171570853330195\n",
      "Epoch 25/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.6923\n",
      "Epoch 00025: val_loss did not improve from 6.51702\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 9.6923 - val_loss: 11.1365\n",
      "\n",
      "[20210302-0404-53] Learning rate for epoch 26 is 0.001219115569256246\n",
      "Epoch 26/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.8412\n",
      "Epoch 00026: val_loss did not improve from 6.51702\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 9.8412 - val_loss: 7.2879\n",
      "\n",
      "[20210302-0404-58] Learning rate for epoch 27 is 0.0012665604008361697\n",
      "Epoch 27/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.5233\n",
      "Epoch 00027: val_loss did not improve from 6.51702\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 9.5233 - val_loss: 6.9318\n",
      "\n",
      "[20210302-0405-03] Learning rate for epoch 28 is 0.0013139051152393222\n",
      "Epoch 28/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.5700\n",
      "Epoch 00028: val_loss did not improve from 6.51702\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 9.5700 - val_loss: 10.3710\n",
      "\n",
      "[20210302-0405-07] Learning rate for epoch 29 is 0.0013611499452963471\n",
      "Epoch 29/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.4172\n",
      "Epoch 00029: val_loss did not improve from 6.51702\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 9.4172 - val_loss: 10.6204\n",
      "\n",
      "[20210302-0405-12] Learning rate for epoch 30 is 0.001408294658176601\n",
      "Epoch 30/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.0513\n",
      "Epoch 00030: val_loss did not improve from 6.51702\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 9.0513 - val_loss: 8.1386\n",
      "\n",
      "[20210302-0405-16] Learning rate for epoch 31 is 0.0014553394867107272\n",
      "Epoch 31/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.3533\n",
      "Epoch 00031: val_loss did not improve from 6.51702\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 9.3533 - val_loss: 7.6941\n",
      "\n",
      "[20210302-0405-21] Learning rate for epoch 32 is 0.0015022843144834042\n",
      "Epoch 32/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.8935\n",
      "Epoch 00032: val_loss did not improve from 6.51702\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 8.8935 - val_loss: 7.2520\n",
      "\n",
      "[20210302-0405-25] Learning rate for epoch 33 is 0.0015491291414946318\n",
      "Epoch 33/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.0378\n",
      "Epoch 00033: val_loss did not improve from 6.51702\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 9.0378 - val_loss: 9.8633\n",
      "\n",
      "[20210302-0405-30] Learning rate for epoch 34 is 0.00159587396774441\n",
      "Epoch 34/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.8069\n",
      "Epoch 00034: val_loss did not improve from 6.51702\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 8.8069 - val_loss: 7.6659\n",
      "\n",
      "[20210302-0405-35] Learning rate for epoch 35 is 0.001642518793232739\n",
      "Epoch 35/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.9987\n",
      "Epoch 00035: val_loss did not improve from 6.51702\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 8.9987 - val_loss: 8.1604\n",
      "\n",
      "[20210302-0405-39] Learning rate for epoch 36 is 0.0016890636179596186\n",
      "Epoch 36/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.6125\n",
      "Epoch 00036: val_loss did not improve from 6.51702\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 8.6125 - val_loss: 8.1869\n",
      "\n",
      "[20210302-0405-44] Learning rate for epoch 37 is 0.0017355084419250488\n",
      "Epoch 37/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.7026\n",
      "Epoch 00037: val_loss did not improve from 6.51702\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 8.7026 - val_loss: 9.4857\n",
      "\n",
      "[20210302-0405-48] Learning rate for epoch 38 is 0.0017818533815443516\n",
      "Epoch 38/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.6775\n",
      "Epoch 00038: val_loss did not improve from 6.51702\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 8.6775 - val_loss: 7.6028\n",
      "\n",
      "[20210302-0405-53] Learning rate for epoch 39 is 0.0018280982039868832\n",
      "Epoch 39/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.5081\n",
      "Epoch 00039: val_loss did not improve from 6.51702\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 8.5081 - val_loss: 6.7762\n",
      "\n",
      "[20210302-0405-58] Learning rate for epoch 40 is 0.0018742431420832872\n",
      "Epoch 40/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.7196\n",
      "Epoch 00040: val_loss did not improve from 6.51702\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 8.7196 - val_loss: 6.9370\n",
      "\n",
      "[20210302-0406-02] Learning rate for epoch 41 is 0.0019202879630029202\n",
      "Epoch 41/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.5478\n",
      "Epoch 00041: val_loss did not improve from 6.51702\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 8.5478 - val_loss: 6.6020\n",
      "\n",
      "[20210302-0406-07] Learning rate for epoch 42 is 0.0019662328995764256\n",
      "Epoch 42/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.4597\n",
      "Epoch 00042: val_loss improved from 6.51702 to 5.69540, saving model to ./20210301-225844/toe_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 89ms/step - loss: 8.4597 - val_loss: 5.6954\n",
      "\n",
      "[20210302-0406-12] Learning rate for epoch 43 is 0.00201207771897316\n",
      "Epoch 43/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.3665\n",
      "Epoch 00043: val_loss did not improve from 5.69540\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 8.3665 - val_loss: 6.5634\n",
      "\n",
      "[20210302-0406-16] Learning rate for epoch 44 is 0.0020578226540237665\n",
      "Epoch 44/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.1460\n",
      "Epoch 00044: val_loss did not improve from 5.69540\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 8.1460 - val_loss: 6.3174\n",
      "\n",
      "[20210302-0406-21] Learning rate for epoch 45 is 0.0021034677047282457\n",
      "Epoch 45/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.2348\n",
      "Epoch 00045: val_loss did not improve from 5.69540\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 8.2348 - val_loss: 7.5834\n",
      "\n",
      "[20210302-0406-26] Learning rate for epoch 46 is 0.002149012638255954\n",
      "Epoch 46/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.1509\n",
      "Epoch 00046: val_loss did not improve from 5.69540\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 8.1509 - val_loss: 6.4061\n",
      "\n",
      "[20210302-0406-30] Learning rate for epoch 47 is 0.0021944576874375343\n",
      "Epoch 47/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.3622\n",
      "Epoch 00047: val_loss did not improve from 5.69540\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 8.3622 - val_loss: 5.8627\n",
      "\n",
      "[20210302-0406-35] Learning rate for epoch 48 is 0.0022398026194423437\n",
      "Epoch 48/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.2662\n",
      "Epoch 00048: val_loss improved from 5.69540 to 5.57834, saving model to ./20210301-225844/toe_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 8.2662 - val_loss: 5.5783\n",
      "\n",
      "[20210302-0406-40] Learning rate for epoch 49 is 0.002285047434270382\n",
      "Epoch 49/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.3725\n",
      "Epoch 00049: val_loss did not improve from 5.57834\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 8.3725 - val_loss: 5.6675\n",
      "\n",
      "[20210302-0406-44] Learning rate for epoch 50 is 0.0023301925975829363\n",
      "Epoch 50/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.3397\n",
      "Epoch 00050: val_loss improved from 5.57834 to 5.50849, saving model to ./20210301-225844/toe_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 8.3397 - val_loss: 5.5085\n",
      "\n",
      "[20210302-0406-49] Learning rate for epoch 51 is 0.002375237410888076\n",
      "Epoch 51/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.3710\n",
      "Epoch 00051: val_loss did not improve from 5.50849\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 8.3710 - val_loss: 5.9646\n",
      "\n",
      "[20210302-0406-54] Learning rate for epoch 52 is 0.0024201825726777315\n",
      "Epoch 52/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.3884\n",
      "Epoch 00052: val_loss improved from 5.50849 to 5.19111, saving model to ./20210301-225844/toe_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 2s 87ms/step - loss: 8.3884 - val_loss: 5.1911\n",
      "\n",
      "[20210302-0406-59] Learning rate for epoch 53 is 0.002465027617290616\n",
      "Epoch 53/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.1454\n",
      "Epoch 00053: val_loss did not improve from 5.19111\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 8.1454 - val_loss: 6.3729\n",
      "\n",
      "[20210302-0407-04] Learning rate for epoch 54 is 0.0025097725447267294\n",
      "Epoch 54/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.1275\n",
      "Epoch 00054: val_loss did not improve from 5.19111\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 8.1275 - val_loss: 5.7605\n",
      "\n",
      "[20210302-0407-08] Learning rate for epoch 55 is 0.0025544175878167152\n",
      "Epoch 55/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7983\n",
      "Epoch 00055: val_loss did not improve from 5.19111\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.7983 - val_loss: 6.2670\n",
      "\n",
      "[20210302-0407-13] Learning rate for epoch 56 is 0.00259896251372993\n",
      "Epoch 56/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7870\n",
      "Epoch 00056: val_loss did not improve from 5.19111\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.7870 - val_loss: 6.2562\n",
      "\n",
      "[20210302-0407-18] Learning rate for epoch 57 is 0.0026434077881276608\n",
      "Epoch 57/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8601\n",
      "Epoch 00057: val_loss did not improve from 5.19111\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 7.8601 - val_loss: 6.6051\n",
      "\n",
      "[20210302-0407-22] Learning rate for epoch 58 is 0.0026877527125179768\n",
      "Epoch 58/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7303\n",
      "Epoch 00058: val_loss did not improve from 5.19111\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.7303 - val_loss: 8.1666\n",
      "\n",
      "[20210302-0407-27] Learning rate for epoch 59 is 0.0027319977525621653\n",
      "Epoch 59/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.1697\n",
      "Epoch 00059: val_loss did not improve from 5.19111\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 8.1697 - val_loss: 6.2532\n",
      "\n",
      "[20210302-0407-31] Learning rate for epoch 60 is 0.0027761429082602262\n",
      "Epoch 60/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.0144\n",
      "Epoch 00060: val_loss did not improve from 5.19111\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 8.0144 - val_loss: 5.8495\n",
      "\n",
      "[20210302-0407-36] Learning rate for epoch 61 is 0.002820187946781516\n",
      "Epoch 61/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8324\n",
      "Epoch 00061: val_loss did not improve from 5.19111\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 7.8324 - val_loss: 5.7030\n",
      "\n",
      "[20210302-0407-41] Learning rate for epoch 62 is 0.0028641331009566784\n",
      "Epoch 62/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6624\n",
      "Epoch 00062: val_loss did not improve from 5.19111\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.6624 - val_loss: 6.4541\n",
      "\n",
      "[20210302-0407-45] Learning rate for epoch 63 is 0.0029079781379550695\n",
      "Epoch 63/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.9905\n",
      "Epoch 00063: val_loss did not improve from 5.19111\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 7.9905 - val_loss: 5.8854\n",
      "\n",
      "[20210302-0407-50] Learning rate for epoch 64 is 0.002951723290607333\n",
      "Epoch 64/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6114\n",
      "Epoch 00064: val_loss did not improve from 5.19111\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.6114 - val_loss: 6.3714\n",
      "\n",
      "[20210302-0407-54] Learning rate for epoch 65 is 0.0029953685589134693\n",
      "Epoch 65/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8127\n",
      "Epoch 00065: val_loss did not improve from 5.19111\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.8127 - val_loss: 5.4753\n",
      "\n",
      "[20210302-0407-59] Learning rate for epoch 66 is 0.0030389137100428343\n",
      "Epoch 66/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8530\n",
      "Epoch 00066: val_loss did not improve from 5.19111\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 7.8530 - val_loss: 6.7494\n",
      "\n",
      "[20210302-0408-03] Learning rate for epoch 67 is 0.003082358743995428\n",
      "Epoch 67/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.0084\n",
      "Epoch 00067: val_loss did not improve from 5.19111\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 8.0084 - val_loss: 6.0446\n",
      "\n",
      "[20210302-0408-08] Learning rate for epoch 68 is 0.0031257038936018944\n",
      "Epoch 68/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7246\n",
      "Epoch 00068: val_loss did not improve from 5.19111\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 7.7246 - val_loss: 5.6612\n",
      "\n",
      "[20210302-0408-13] Learning rate for epoch 69 is 0.003168949158862233\n",
      "Epoch 69/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8778\n",
      "Epoch 00069: val_loss did not improve from 5.19111\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 7.8778 - val_loss: 6.0282\n",
      "\n",
      "[20210302-0408-17] Learning rate for epoch 70 is 0.0032120943069458008\n",
      "Epoch 70/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8688\n",
      "Epoch 00070: val_loss did not improve from 5.19111\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.8688 - val_loss: 7.4156\n",
      "\n",
      "[20210302-0408-22] Learning rate for epoch 71 is 0.003255139570683241\n",
      "Epoch 71/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6860\n",
      "Epoch 00071: val_loss did not improve from 5.19111\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.6860 - val_loss: 6.1286\n",
      "\n",
      "[20210302-0408-26] Learning rate for epoch 72 is 0.00329808471724391\n",
      "Epoch 72/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6539\n",
      "Epoch 00072: val_loss did not improve from 5.19111\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 7.6539 - val_loss: 5.4099\n",
      "\n",
      "[20210302-0408-31] Learning rate for epoch 73 is 0.0033409299794584513\n",
      "Epoch 73/1000\n",
      "12/20 [=================>............] - ETA: 0s - loss: 7.4151"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# big K = 5 (fold 0 ~ 4) \n",
    "KFlodNum = 20\n",
    "\n",
    "\n",
    "\n",
    "history_toe = []\n",
    "history_toe_finetune = []\n",
    "\n",
    "#above until 'train_ds_map_toe now' to 'train_ds_map_toe_s', 'valid_ds_map_toe_s'\n",
    "for k in range(KFlodNum):\n",
    "    \n",
    "    \n",
    "    # Split data to train/valid with K-Fold #\n",
    "    print(\"\\n \\n K = \", k, \"\\n\")\n",
    "    # Toe split\n",
    "    train_ds_map_toe_s, valid_ds_map_toe_s = get_KFold_ds(train_ds_map_toe, K=k)\n",
    "    \n",
    "    # Toe ds_pre\n",
    "    train_ds_pre_toe_s = configure_for_performance_cache_train(train_ds_map_toe_s, augment=True)\n",
    "    valid_ds_pre_toe_s = configure_for_performance_cache_val(valid_ds_map_toe_s)\n",
    "    \n",
    "    \n",
    "#     # heel split\n",
    "#     train_ds_map_heel_s, valid_ds_map_heel_s = get_KFold_ds(train_ds_map_heel, K=k)\n",
    "#     # Heel ds_pre\n",
    "#     train_ds_pre_heel_s = configure_for_performance_cache_train(train_ds_map_heel_s, augment=True)\n",
    "#     valid_ds_pre_heel_s = configure_for_performance_cache_val(valid_ds_map_heel_s)\n",
    "    \n",
    "    \n",
    "    # Train K-Model with transfer learnling #\n",
    "    \n",
    "    # Toe model, TL\n",
    "    th = 'toe'\n",
    "    # th = 'heel'\n",
    "    best_model_name = get_best_model_name(th, K=str(k))\n",
    "    best_model_save = tf.keras.callbacks.ModelCheckpoint(filepath=best_model_name, \n",
    "                                 save_best_only = True, \n",
    "                                 save_weights_only = False,\n",
    "                                 monitor = monitor, \n",
    "                                 mode = 'auto', verbose = 1)\n",
    "    callbacks_toe_tl = [\n",
    "                    #     tensorboard_callback,\n",
    "                        best_model_save,\n",
    "                        tf.keras.callbacks.EarlyStopping(monitor=monitor, patience=20), #patience=step_size or ep_num\n",
    "                    #     lr_reduceonplateau,\n",
    "                        tf.keras.callbacks.LearningRateScheduler(lrdump),#lrdump, decay or lrfn or lrfn2. clr\n",
    "                        PrintLRtoe()\n",
    "                        ]\n",
    "    callbacks_toe_fn = [\n",
    "                    #     tensorboard_callback,\n",
    "                        best_model_save,\n",
    "                        tf.keras.callbacks.EarlyStopping(monitor=monitor, patience=100), #patience=step_size or ep_num\n",
    "                    #     lr_reduceonplateau,\n",
    "                        tf.keras.callbacks.LearningRateScheduler(clr3),#lrdump, decay or lrfn or lrfn2. clr, CosineDecayCLRWarmUp, CosineDecayCLRWarmUpLSW\n",
    "                        PrintLRtoe()\n",
    "                    ]\n",
    "    print('best_model_name:', best_model_name)\n",
    "\n",
    "\n",
    "    top_dropout_rate = 0.8 #less dp rate, say 0.1, train_loss will lower than val_loss\n",
    "    drop_connect_rate = 0.9 #for efnet This parameter serves as a toggle for extra regularization in finetuning, but does not affect loaded weights.\n",
    "    outputnum = 2\n",
    "    with strategy.scope():\n",
    "        model_toe = build_efn_model(outputnum, top_dropout_rate, drop_connect_rate)\n",
    "#         model_toe = load_pretrained_efn_model() # from 20210224-200728 ed5.3\n",
    "#         count_model_trainOrNot_layers(model_toe)\n",
    "        \n",
    "    # fit the model on all data\n",
    "    hist = model_toe.fit(train_ds_pre_toe_s, \n",
    "                          verbose=1, \n",
    "                          epochs=ep_num_transf, \n",
    "                          validation_data=valid_ds_pre_toe_s, \n",
    "                          callbacks=callbacks_toe_tl)#, validation_split=0.1)\n",
    "    history_toe.append(hist)\n",
    "    \n",
    "      \n",
    "    # Train K-Model with fine tune #\n",
    "    \n",
    "    # Toe model, FT\n",
    "    unfreeze_model(model_toe)\n",
    "    count_model_trainOrNot_layers(model_toe)\n",
    "    # fit the model on all data\n",
    "    hist = model_toe.fit(train_ds_pre_toe_s, \n",
    "                          verbose=1, \n",
    "                          epochs=ep_num, \n",
    "                          validation_data=valid_ds_pre_toe_s, \n",
    "                          callbacks=callbacks_toe_fn)#, validation_split=0.1)\n",
    "    history_toe_finetune.append(hist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ED sum\n",
    "def get_valloss(his_v_l):  \n",
    "    return np.min(his_v_l), np.argmin(his_v_l)\n",
    "\n",
    "t_vl = []\n",
    "# h_vl = []\n",
    "for k in range(KFlodNum):\n",
    "    print(f'K:{k}')\n",
    "    t_v, _ = get_valloss(history_toe_finetune[k].history['val_loss'])\n",
    "#     h_v, _ = get_valloss(history_heel_finetune[k].history['val_loss'])\n",
    "    \n",
    "    t_vl.append(t_v)\n",
    "#     h_vl.append(h_v)\n",
    "\n",
    "# t_vl = np.mean(t_vl, axis=0)\n",
    "# h_vl = np.mean(h_vl, axis=0)\n",
    "# print(f'{round(t_vl,5)} + {round(h_vl,5)} = {round(t_vl + h_vl,5)}')\n",
    "\n",
    "t_vl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(f'{log_dir_name}/toe_FNED.txt', t_vl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum plot losses toe-tl\n",
    "\n",
    "plt.figure(figsize=(25, 10))\n",
    "\n",
    "for k in range(KFlodNum):\n",
    "\n",
    "    plt.plot(history_toe[k].history['loss'])\n",
    "    plt.plot(history_toe[k].history['val_loss'])\n",
    "\n",
    "    \n",
    "plt.title('K-model ed loss toe-TL')\n",
    "plt.ylabel('ed loss'), plt.ylim(5, 80)# for too large loss\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['k0 loss', 'k0 val_loss', 'k1 loss', 'k1 val_loss', 'k2 loss', 'k2 val_loss', \n",
    "            'k3 loss', 'k3 val_loss', 'k4 loss', 'k4 val_loss'], loc='upper left') \n",
    "# plt.show()\n",
    "\n",
    "# save plot : comment plo.show in jupyter notebook.\n",
    "plt.savefig(f'{log_dir_name}/{log_dir_name}_toe_Ksum_TL.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Single plot loss toe-tl\n",
    "\n",
    "for k in range(KFlodNum):\n",
    "    plt.figure(figsize=(25, 10))\n",
    "    plt.plot(history_toe[k].history['loss'])\n",
    "    plt.plot(history_toe[k].history['val_loss'])\n",
    "    plt.title('K-model ed loss toe-TL')\n",
    "    plt.ylabel('ed loss'), plt.ylim(5, 20)# for too large loss\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['loss', 'val_loss'], loc='upper left') \n",
    "    # plt.show()\n",
    "\n",
    "    # save plot : comment plo.show in jupyter notebook.\n",
    "    def get_valloss(his_v_l):   \n",
    "        return np.min(his_v_l), np.argmin(his_v_l)\n",
    "\n",
    "    vl, ep = get_valloss(history_toe[k].history['val_loss'])\n",
    "    plt.savefig(f'{log_dir_name}/{log_dir_name}_toe_K{k}_TL_clr_ed{round(vl,4)}@{ep}.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the loos the model trained.\n",
    "\n",
    "def get_valloss(his_v_l):  \n",
    "    return np.min(his_v_l), np.argmin(his_v_l)\n",
    "\n",
    "t_vl = []\n",
    "# h_vl = []\n",
    "for k in range(KFlodNum):\n",
    "    print(f'K:{k}')\n",
    "    t_v, _ = get_valloss(history_toe_finetune[k].history['val_loss'])\n",
    "#     h_v, _ = get_valloss(history_heel_finetune[k].history['val_loss'])\n",
    "    \n",
    "    t_vl.append(t_v)\n",
    "    \n",
    "# for different scales (different Y-axes)\n",
    "# fig, ax1 = plt.subplots()\n",
    "fig, ax1 = plt.subplots(figsize=(25, 10))\n",
    "\n",
    "# nice to have this colorful tip.\n",
    "color = 'tab:red'\n",
    "\n",
    "ax1.set_title('[ toe_finetune ] \\n ED loss')\n",
    "\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.set_ylabel('ed loss', color=color)\n",
    "\n",
    "for k in range(KFlodNum):\n",
    "\n",
    "    plt.plot(history_toe_finetune[k].history['loss'])\n",
    "    plt.plot(history_toe_finetune[k].history['val_loss'])\n",
    "\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "# ax1.legend(['loss', 'val_loss'], loc='upper center') # legend may ocvered by next ax ploting. moved to end.\n",
    "ax1.legend(['k0 loss', 'k0 val_loss', 'k1 loss', 'k1 val_loss', 'k2 loss', 'k2 val_loss', \n",
    "            'k3 loss', 'k3 val_loss', 'k4 loss', 'k4 val_loss'], loc='upper left') \n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('learning rate', color=color)\n",
    "ax2.plot(history_toe_finetune[0].history['lr'], color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.legend(['lr'], loc='upper right') \n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# # save plot : comment plo.show in jupyter notebook.\n",
    "# def get_valloss(his_v_l):   \n",
    "#     return np.min(his_v_l), np.argmin(his_v_l)\n",
    "\n",
    "# vl, ep = get_valloss(history_toe_finetune.history['val_loss'])\n",
    "\n",
    "\n",
    "t_vl = np.mean(t_vl, axis=0)\n",
    "plt.savefig(f'{log_dir_name}/{log_dir_name}_toe_ft_Ksum-clr_ed{round(t_vl,4)}.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum plot losses toe-ft\n",
    "\n",
    "plt.figure(figsize=(25, 10))\n",
    "\n",
    "for k in range(KFlodNum):\n",
    "\n",
    "    plt.plot(history_toe_finetune[k].history['loss'])\n",
    "    plt.plot(history_toe_finetune[k].history['val_loss'])\n",
    "\n",
    "    \n",
    "plt.title('K-model ed loss toe-FT')\n",
    "plt.ylabel('ed loss'), plt.ylim(4, 20)# for too large loss\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['k0 loss', 'k0 val_loss', 'k1 loss', 'k1 val_loss', 'k2 loss', 'k2 val_loss', \n",
    "            'k3 loss', 'k3 val_loss', 'k4 loss', 'k4 val_loss'], loc='upper left') \n",
    "# plt.show()\n",
    "\n",
    "# save plot : comment plo.show in jupyter notebook.\n",
    "plt.savefig(f'{log_dir_name}/{log_dir_name}_toe_Ksum_FT.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Single plot loss toe-FT\n",
    "\n",
    "for k in range(KFlodNum):\n",
    "    plt.figure(figsize=(25, 10))\n",
    "    plt.plot(history_toe_finetune[k].history['loss'])\n",
    "    plt.plot(history_toe_finetune[k].history['val_loss'])\n",
    "    plt.title('K-model ed loss toe-FT')\n",
    "    plt.ylabel('ed loss'), plt.ylim(4, 20)# for too large loss\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['loss', 'val_loss'], loc='upper left') \n",
    "    # plt.show()\n",
    "\n",
    "    # save plot : comment plo.show in jupyter notebook.\n",
    "    def get_valloss(his_v_l):   \n",
    "        return np.min(his_v_l), np.argmin(his_v_l)\n",
    "\n",
    "    vl, ep = get_valloss(history_toe_finetune[k].history['val_loss'])\n",
    "    plt.savefig(f'{log_dir_name}/{log_dir_name}_toe_K{k}_FT_clr_ed{round(vl,4)}@{ep}.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heel K-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/20 [===========================>..] - ETA: 0s - loss: 15.4802\n",
      "Epoch 00038: val_loss did not improve from 10.32340\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.4324 - val_loss: 10.3941\n",
      "\n",
      "[20210302-0906-02] Learning rate for epoch 39 is 0.00047855067532509565\n",
      "Epoch 39/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 16.0903\n",
      "Epoch 00039: val_loss improved from 10.32340 to 9.83745, saving model to ./20210301-225844/heel_K4_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 15.9604 - val_loss: 9.8375\n",
      "\n",
      "[20210302-0906-06] Learning rate for epoch 40 is 0.00047855067532509565\n",
      "Epoch 40/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.6851\n",
      "Epoch 00040: val_loss did not improve from 9.83745\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.6851 - val_loss: 9.9312\n",
      "\n",
      "[20210302-0906-10] Learning rate for epoch 41 is 0.00047855067532509565\n",
      "Epoch 41/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.0907\n",
      "Epoch 00041: val_loss improved from 9.83745 to 9.22291, saving model to ./20210301-225844/heel_K4_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 16.1056 - val_loss: 9.2229\n",
      "\n",
      "[20210302-0906-15] Learning rate for epoch 42 is 0.00047855067532509565\n",
      "Epoch 42/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.9955\n",
      "Epoch 00042: val_loss did not improve from 9.22291\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.9352 - val_loss: 10.0153\n",
      "\n",
      "[20210302-0906-19] Learning rate for epoch 43 is 0.00047855067532509565\n",
      "Epoch 43/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.6915\n",
      "Epoch 00043: val_loss did not improve from 9.22291\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.5801 - val_loss: 9.6603\n",
      "\n",
      "[20210302-0906-23] Learning rate for epoch 44 is 0.00047855067532509565\n",
      "Epoch 44/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.4180\n",
      "Epoch 00044: val_loss did not improve from 9.22291\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.4832 - val_loss: 9.6815\n",
      "\n",
      "[20210302-0906-27] Learning rate for epoch 45 is 0.00047855067532509565\n",
      "Epoch 45/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.1035\n",
      "Epoch 00045: val_loss did not improve from 9.22291\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.1035 - val_loss: 9.9245\n",
      "\n",
      "[20210302-0906-31] Learning rate for epoch 46 is 0.00047855067532509565\n",
      "Epoch 46/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.1903\n",
      "Epoch 00046: val_loss did not improve from 9.22291\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.1903 - val_loss: 10.0724\n",
      "\n",
      "[20210302-0906-35] Learning rate for epoch 47 is 0.00047855067532509565\n",
      "Epoch 47/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.6132\n",
      "Epoch 00047: val_loss did not improve from 9.22291\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.7338 - val_loss: 9.9967\n",
      "\n",
      "[20210302-0906-38] Learning rate for epoch 48 is 0.00047855067532509565\n",
      "Epoch 48/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.8215\n",
      "Epoch 00048: val_loss did not improve from 9.22291\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.7479 - val_loss: 10.8402\n",
      "\n",
      "[20210302-0906-42] Learning rate for epoch 49 is 0.00047855067532509565\n",
      "Epoch 49/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.0408\n",
      "Epoch 00049: val_loss did not improve from 9.22291\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.0428 - val_loss: 10.8055\n",
      "\n",
      "[20210302-0906-46] Learning rate for epoch 50 is 0.00047855067532509565\n",
      "Epoch 50/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 16.1877\n",
      "Epoch 00050: val_loss did not improve from 9.22291\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.1219 - val_loss: 10.2739\n",
      "\n",
      "[20210302-0906-50] Learning rate for epoch 51 is 0.00047855067532509565\n",
      "Epoch 51/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.3646\n",
      "Epoch 00051: val_loss did not improve from 9.22291\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.3646 - val_loss: 10.3230\n",
      "\n",
      "[20210302-0906-54] Learning rate for epoch 52 is 0.00047855067532509565\n",
      "Epoch 52/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 16.4267\n",
      "Epoch 00052: val_loss did not improve from 9.22291\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.4134 - val_loss: 10.0973\n",
      "\n",
      "[20210302-0906-58] Learning rate for epoch 53 is 0.00047855067532509565\n",
      "Epoch 53/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.3186\n",
      "Epoch 00053: val_loss did not improve from 9.22291\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.3898 - val_loss: 10.0004\n",
      "\n",
      "[20210302-0907-02] Learning rate for epoch 54 is 0.00047855067532509565\n",
      "Epoch 54/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 16.4494\n",
      "Epoch 00054: val_loss did not improve from 9.22291\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.4306 - val_loss: 10.1974\n",
      "\n",
      "[20210302-0907-06] Learning rate for epoch 55 is 0.00047855067532509565\n",
      "Epoch 55/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.8051\n",
      "Epoch 00055: val_loss did not improve from 9.22291\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.7851 - val_loss: 9.7749\n",
      "\n",
      "[20210302-0907-10] Learning rate for epoch 56 is 0.00047855067532509565\n",
      "Epoch 56/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.5318\n",
      "Epoch 00056: val_loss did not improve from 9.22291\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.5318 - val_loss: 9.7361\n",
      "\n",
      "[20210302-0907-14] Learning rate for epoch 57 is 0.00047855067532509565\n",
      "Epoch 57/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.4246\n",
      "Epoch 00057: val_loss did not improve from 9.22291\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.4903 - val_loss: 9.5356\n",
      "\n",
      "[20210302-0907-18] Learning rate for epoch 58 is 0.00047855067532509565\n",
      "Epoch 58/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.6924\n",
      "Epoch 00058: val_loss did not improve from 9.22291\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.6924 - val_loss: 9.4821\n",
      "\n",
      "[20210302-0907-22] Learning rate for epoch 59 is 0.00047855067532509565\n",
      "Epoch 59/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.0781\n",
      "Epoch 00059: val_loss did not improve from 9.22291\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.0781 - val_loss: 9.5436\n",
      "\n",
      "[20210302-0907-26] Learning rate for epoch 60 is 0.00047855067532509565\n",
      "Epoch 60/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.5907\n",
      "Epoch 00060: val_loss did not improve from 9.22291\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.5907 - val_loss: 9.4900\n",
      "\n",
      "[20210302-0907-30] Learning rate for epoch 61 is 0.00047855067532509565\n",
      "Epoch 61/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.5509\n",
      "Epoch 00061: val_loss did not improve from 9.22291\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.5509 - val_loss: 9.9043\n",
      "\n",
      "*********************************** Start fine tune ***********************************\n",
      "tt: 195, nt:46, total layers:241\n",
      "*********************************** Start fine tune ***********************************\n",
      "Epoch 1/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 25.3241\n",
      "Epoch 00001: val_loss did not improve from 9.22291\n",
      "\n",
      "[20210302-0908-08] Learning rate for epoch 1 is 4.999999987376214e-07\n",
      "20/20 [==============================] - 14s 688ms/step - loss: 25.3241 - val_loss: 15.5902\n",
      "Epoch 2/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 22.8987\n",
      "Epoch 00002: val_loss did not improve from 9.22291\n",
      "\n",
      "[20210302-0908-13] Learning rate for epoch 2 is 5.044450517743826e-05\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 22.8987 - val_loss: 15.9105\n",
      "Epoch 3/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 17.7170\n",
      "Epoch 00003: val_loss did not improve from 9.22291\n",
      "\n",
      "[20210302-0908-34] Learning rate for epoch 3 is 0.00010028902033809572\n",
      "20/20 [==============================] - 17s 832ms/step - loss: 17.7170 - val_loss: 11.6162\n",
      "Epoch 4/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.3073\n",
      "Epoch 00004: val_loss did not improve from 9.22291\n",
      "\n",
      "[20210302-0908-38] Learning rate for epoch 4 is 0.00015003354928921908\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 15.3073 - val_loss: 12.1696\n",
      "Epoch 5/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.4044\n",
      "Epoch 00005: val_loss did not improve from 9.22291\n",
      "\n",
      "[20210302-0908-43] Learning rate for epoch 5 is 0.0001996780774788931\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 14.4044 - val_loss: 12.5605\n",
      "Epoch 6/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.3103\n",
      "Epoch 00006: val_loss improved from 9.22291 to 9.13521, saving model to ./20210301-225844/heel_K4_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-0908-49] Learning rate for epoch 6 is 0.000249222619459033\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 14.3103 - val_loss: 9.1352\n",
      "Epoch 7/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.4923\n",
      "Epoch 00007: val_loss improved from 9.13521 to 8.04657, saving model to ./20210301-225844/heel_K4_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-0908-54] Learning rate for epoch 7 is 0.0002986671752296388\n",
      "20/20 [==============================] - 2s 92ms/step - loss: 13.4923 - val_loss: 8.0466\n",
      "Epoch 8/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.4251\n",
      "Epoch 00008: val_loss improved from 8.04657 to 7.87664, saving model to ./20210301-225844/heel_K4_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-0908-59] Learning rate for epoch 8 is 0.00034801175934262574\n",
      "20/20 [==============================] - 2s 91ms/step - loss: 13.4251 - val_loss: 7.8766\n",
      "Epoch 9/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 12.2883\n",
      "Epoch 00009: val_loss did not improve from 7.87664\n",
      "\n",
      "[20210302-0909-04] Learning rate for epoch 9 is 0.00039725631359033287\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 12.2883 - val_loss: 9.3672\n",
      "Epoch 10/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 12.2082\n",
      "Epoch 00010: val_loss did not improve from 7.87664\n",
      "\n",
      "[20210302-0909-09] Learning rate for epoch 10 is 0.0004464008961804211\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 12.2082 - val_loss: 8.0467\n",
      "Epoch 11/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 12.0483\n",
      "Epoch 00011: val_loss improved from 7.87664 to 7.15356, saving model to ./20210301-225844/heel_K4_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-0909-15] Learning rate for epoch 11 is 0.0004954455071128905\n",
      "20/20 [==============================] - 2s 92ms/step - loss: 12.0483 - val_loss: 7.1536\n",
      "Epoch 12/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 12.4096\n",
      "Epoch 00012: val_loss improved from 7.15356 to 6.51427, saving model to ./20210301-225844/heel_K4_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-0909-20] Learning rate for epoch 12 is 0.0005443901172839105\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 12.4096 - val_loss: 6.5143\n",
      "Epoch 13/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.4763\n",
      "Epoch 00013: val_loss did not improve from 6.51427\n",
      "\n",
      "[20210302-0909-25] Learning rate for epoch 13 is 0.0005932347266934812\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 11.4763 - val_loss: 7.6072\n",
      "Epoch 14/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.3862\n",
      "Epoch 00014: val_loss did not improve from 6.51427\n",
      "\n",
      "[20210302-0909-30] Learning rate for epoch 14 is 0.0006419793353416026\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 11.3862 - val_loss: 6.8889\n",
      "Epoch 15/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.8879\n",
      "Epoch 00015: val_loss did not improve from 6.51427\n",
      "\n",
      "[20210302-0909-34] Learning rate for epoch 15 is 0.0006906240014359355\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 10.8879 - val_loss: 6.5902\n",
      "Epoch 16/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.0402\n",
      "Epoch 00016: val_loss did not improve from 6.51427\n",
      "\n",
      "[20210302-0909-39] Learning rate for epoch 16 is 0.0007391686085611582\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 11.0402 - val_loss: 6.9121\n",
      "Epoch 17/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.6083\n",
      "Epoch 00017: val_loss improved from 6.51427 to 4.61006, saving model to ./20210301-225844/heel_K4_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-0909-45] Learning rate for epoch 17 is 0.0007876132731325924\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 9.6083 - val_loss: 4.6101\n",
      "Epoch 18/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.7771\n",
      "Epoch 00018: val_loss did not improve from 4.61006\n",
      "\n",
      "[20210302-0909-49] Learning rate for epoch 18 is 0.0008359579369425774\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 8.7771 - val_loss: 4.7085\n",
      "Epoch 19/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.5378\n",
      "Epoch 00019: val_loss did not improve from 4.61006\n",
      "\n",
      "[20210302-0909-54] Learning rate for epoch 19 is 0.000884202599991113\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 8.5378 - val_loss: 5.6551\n",
      "Epoch 20/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.6145\n",
      "Epoch 00020: val_loss did not improve from 4.61006\n",
      "\n",
      "[20210302-0909-59] Learning rate for epoch 20 is 0.0009323473204858601\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 8.6145 - val_loss: 7.2438\n",
      "Epoch 21/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.2289\n",
      "Epoch 00021: val_loss improved from 4.61006 to 3.85500, saving model to ./20210301-225844/heel_K4_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-0910-04] Learning rate for epoch 21 is 0.000980391982011497\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 8.2289 - val_loss: 3.8550\n",
      "Epoch 22/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.2255\n",
      "Epoch 00022: val_loss did not improve from 3.85500\n",
      "\n",
      "[20210302-0910-09] Learning rate for epoch 22 is 0.0010283367009833455\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 8.2255 - val_loss: 4.5297\n",
      "Epoch 23/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.1509\n",
      "Epoch 00023: val_loss did not improve from 3.85500\n",
      "\n",
      "[20210302-0910-14] Learning rate for epoch 23 is 0.0010761814191937447\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 8.1509 - val_loss: 6.4536\n",
      "Epoch 24/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.6133\n",
      "Epoch 00024: val_loss improved from 3.85500 to 3.74041, saving model to ./20210301-225844/heel_K4_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-0910-19] Learning rate for epoch 24 is 0.0011239261366426945\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 8.6133 - val_loss: 3.7404\n",
      "Epoch 25/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.6857\n",
      "Epoch 00025: val_loss did not improve from 3.74041\n",
      "\n",
      "[20210302-0910-24] Learning rate for epoch 25 is 0.001171570853330195\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 8.6857 - val_loss: 4.5529\n",
      "Epoch 26/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8924\n",
      "Epoch 00026: val_loss did not improve from 3.74041\n",
      "\n",
      "[20210302-0910-29] Learning rate for epoch 26 is 0.001219115569256246\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 7.8924 - val_loss: 3.8481\n",
      "Epoch 27/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7054\n",
      "Epoch 00027: val_loss did not improve from 3.74041\n",
      "\n",
      "[20210302-0910-34] Learning rate for epoch 27 is 0.0012665604008361697\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.7054 - val_loss: 16.7916\n",
      "Epoch 28/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4070\n",
      "Epoch 00028: val_loss did not improve from 3.74041\n",
      "\n",
      "[20210302-0910-39] Learning rate for epoch 28 is 0.0013139051152393222\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.4070 - val_loss: 9.9244\n",
      "Epoch 29/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.9932\n",
      "Epoch 00029: val_loss did not improve from 3.74041\n",
      "\n",
      "[20210302-0910-43] Learning rate for epoch 29 is 0.0013611499452963471\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.9932 - val_loss: 6.6924\n",
      "Epoch 30/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.1032\n",
      "Epoch 00030: val_loss did not improve from 3.74041\n",
      "\n",
      "[20210302-0910-48] Learning rate for epoch 30 is 0.001408294658176601\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 8.1032 - val_loss: 48.4307\n",
      "Epoch 31/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.9387\n",
      "Epoch 00031: val_loss did not improve from 3.74041\n",
      "\n",
      "[20210302-0910-53] Learning rate for epoch 31 is 0.0014553394867107272\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.9387 - val_loss: 7.6433\n",
      "Epoch 32/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6329\n",
      "Epoch 00032: val_loss improved from 3.74041 to 3.65295, saving model to ./20210301-225844/heel_K4_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-0910-59] Learning rate for epoch 32 is 0.0015022843144834042\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 7.6329 - val_loss: 3.6530\n",
      "Epoch 33/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3115\n",
      "Epoch 00033: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0911-03] Learning rate for epoch 33 is 0.0015491291414946318\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.3115 - val_loss: 7.6741\n",
      "Epoch 34/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5590\n",
      "Epoch 00034: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0911-08] Learning rate for epoch 34 is 0.00159587396774441\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.5590 - val_loss: 5.6633\n",
      "Epoch 35/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3821\n",
      "Epoch 00035: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0911-13] Learning rate for epoch 35 is 0.001642518793232739\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.3821 - val_loss: 11.5268\n",
      "Epoch 36/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0557\n",
      "Epoch 00036: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0911-18] Learning rate for epoch 36 is 0.0016890636179596186\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.0557 - val_loss: 9.8788\n",
      "Epoch 37/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9784\n",
      "Epoch 00037: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0911-23] Learning rate for epoch 37 is 0.0017355084419250488\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.9784 - val_loss: 9.9707\n",
      "Epoch 38/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9653\n",
      "Epoch 00038: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0911-27] Learning rate for epoch 38 is 0.0017818533815443516\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.9653 - val_loss: 6.0981\n",
      "Epoch 39/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5739\n",
      "Epoch 00039: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0911-32] Learning rate for epoch 39 is 0.0018280982039868832\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.5739 - val_loss: 5.3744\n",
      "Epoch 40/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1411\n",
      "Epoch 00040: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0911-37] Learning rate for epoch 40 is 0.0018742431420832872\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.1411 - val_loss: 4.0583\n",
      "Epoch 41/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9257\n",
      "Epoch 00041: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0911-42] Learning rate for epoch 41 is 0.0019202879630029202\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.9257 - val_loss: 6.2410\n",
      "Epoch 42/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7910\n",
      "Epoch 00042: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0911-47] Learning rate for epoch 42 is 0.0019662328995764256\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.7910 - val_loss: 7.5578\n",
      "Epoch 43/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3020\n",
      "Epoch 00043: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0911-52] Learning rate for epoch 43 is 0.00201207771897316\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.3020 - val_loss: 6.7003\n",
      "Epoch 44/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1438\n",
      "Epoch 00044: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0911-57] Learning rate for epoch 44 is 0.0020578226540237665\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.1438 - val_loss: 6.3472\n",
      "Epoch 45/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1506\n",
      "Epoch 00045: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0912-01] Learning rate for epoch 45 is 0.0021034677047282457\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.1506 - val_loss: 6.2255\n",
      "Epoch 46/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1368\n",
      "Epoch 00046: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0912-06] Learning rate for epoch 46 is 0.002149012638255954\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.1368 - val_loss: 7.0154\n",
      "Epoch 47/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1624\n",
      "Epoch 00047: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0912-11] Learning rate for epoch 47 is 0.0021944576874375343\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.1624 - val_loss: 5.9829\n",
      "Epoch 48/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8523\n",
      "Epoch 00048: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0912-16] Learning rate for epoch 48 is 0.0022398026194423437\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.8523 - val_loss: 4.0649\n",
      "Epoch 49/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9549\n",
      "Epoch 00049: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0912-20] Learning rate for epoch 49 is 0.002285047434270382\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.9549 - val_loss: 4.3083\n",
      "Epoch 50/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9482\n",
      "Epoch 00050: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0912-25] Learning rate for epoch 50 is 0.0023301925975829363\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.9482 - val_loss: 4.7463\n",
      "Epoch 51/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5675\n",
      "Epoch 00051: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0912-30] Learning rate for epoch 51 is 0.002375237410888076\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.5675 - val_loss: 4.7831\n",
      "Epoch 52/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1333\n",
      "Epoch 00052: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0912-35] Learning rate for epoch 52 is 0.0024201825726777315\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.1333 - val_loss: 19.5238\n",
      "Epoch 53/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3411\n",
      "Epoch 00053: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0912-40] Learning rate for epoch 53 is 0.002465027617290616\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 7.3411 - val_loss: 20.4808\n",
      "Epoch 54/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8772\n",
      "Epoch 00054: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0912-45] Learning rate for epoch 54 is 0.0025097725447267294\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.8772 - val_loss: 2483.8503\n",
      "Epoch 55/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0229\n",
      "Epoch 00055: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0912-49] Learning rate for epoch 55 is 0.0025544175878167152\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.0229 - val_loss: 19.9260\n",
      "Epoch 56/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4831\n",
      "Epoch 00056: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0912-54] Learning rate for epoch 56 is 0.00259896251372993\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.4831 - val_loss: 10.7043\n",
      "Epoch 57/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9477\n",
      "Epoch 00057: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0912-59] Learning rate for epoch 57 is 0.0026434077881276608\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.9477 - val_loss: 13.4708\n",
      "Epoch 58/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2528\n",
      "Epoch 00058: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0913-04] Learning rate for epoch 58 is 0.0026877527125179768\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.2528 - val_loss: 11.6475\n",
      "Epoch 59/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.2179\n",
      "Epoch 00059: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0913-09] Learning rate for epoch 59 is 0.0027319977525621653\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 8.2179 - val_loss: 5.6138\n",
      "Epoch 60/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1566\n",
      "Epoch 00060: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0913-14] Learning rate for epoch 60 is 0.0027761429082602262\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.1566 - val_loss: 216.4022\n",
      "Epoch 61/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1758\n",
      "Epoch 00061: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0913-19] Learning rate for epoch 61 is 0.002820187946781516\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.1758 - val_loss: 8.6848\n",
      "Epoch 62/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0112\n",
      "Epoch 00062: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0913-23] Learning rate for epoch 62 is 0.0028641331009566784\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 7.0112 - val_loss: 10.3262\n",
      "Epoch 63/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8381\n",
      "Epoch 00063: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0913-28] Learning rate for epoch 63 is 0.0029079781379550695\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.8381 - val_loss: 4.2325\n",
      "Epoch 64/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3527\n",
      "Epoch 00064: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0913-33] Learning rate for epoch 64 is 0.002951723290607333\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.3527 - val_loss: 3.9143\n",
      "Epoch 65/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8251\n",
      "Epoch 00065: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0913-38] Learning rate for epoch 65 is 0.0029953685589134693\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.8251 - val_loss: 4.0411\n",
      "Epoch 66/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8091\n",
      "Epoch 00066: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0913-42] Learning rate for epoch 66 is 0.0030389137100428343\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.8091 - val_loss: 4.0416\n",
      "Epoch 67/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7754\n",
      "Epoch 00067: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0913-47] Learning rate for epoch 67 is 0.003082358743995428\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.7754 - val_loss: 6.8570\n",
      "Epoch 68/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4179\n",
      "Epoch 00068: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0913-52] Learning rate for epoch 68 is 0.0031257038936018944\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.4179 - val_loss: 4.1355\n",
      "Epoch 69/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3775\n",
      "Epoch 00069: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0913-57] Learning rate for epoch 69 is 0.003168949158862233\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.3775 - val_loss: 6.6582\n",
      "Epoch 70/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9820\n",
      "Epoch 00070: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0914-02] Learning rate for epoch 70 is 0.0032120943069458008\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.9820 - val_loss: 7.3450\n",
      "Epoch 71/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7039\n",
      "Epoch 00071: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0914-07] Learning rate for epoch 71 is 0.003255139570683241\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.7039 - val_loss: 5.5848\n",
      "Epoch 72/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8520\n",
      "Epoch 00072: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0914-11] Learning rate for epoch 72 is 0.00329808471724391\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.8520 - val_loss: 110.8110\n",
      "Epoch 73/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5838\n",
      "Epoch 00073: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0914-16] Learning rate for epoch 73 is 0.0033409299794584513\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.5838 - val_loss: 6.9084\n",
      "Epoch 74/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6964\n",
      "Epoch 00074: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0914-21] Learning rate for epoch 74 is 0.0033836751244962215\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.6964 - val_loss: 7.8728\n",
      "Epoch 75/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7163\n",
      "Epoch 00075: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0914-26] Learning rate for epoch 75 is 0.0034263203851878643\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.7163 - val_loss: 9.4544\n",
      "Epoch 76/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7158\n",
      "Epoch 00076: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0914-31] Learning rate for epoch 76 is 0.003468865528702736\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.7158 - val_loss: 4.7751\n",
      "Epoch 77/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8122\n",
      "Epoch 00077: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0914-35] Learning rate for epoch 77 is 0.00351131078787148\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.8122 - val_loss: 629.0225\n",
      "Epoch 78/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5436\n",
      "Epoch 00078: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0914-40] Learning rate for epoch 78 is 0.0035536561626940966\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.5436 - val_loss: 300.5654\n",
      "Epoch 79/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4860\n",
      "Epoch 00079: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0914-45] Learning rate for epoch 79 is 0.003595901420339942\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.4860 - val_loss: 280.7935\n",
      "Epoch 80/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9626\n",
      "Epoch 00080: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0914-50] Learning rate for epoch 80 is 0.00363804679363966\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.9626 - val_loss: 131.2147\n",
      "Epoch 81/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7060\n",
      "Epoch 00081: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0914-55] Learning rate for epoch 81 is 0.0036800920497626066\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.7060 - val_loss: 189.5260\n",
      "Epoch 82/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7127\n",
      "Epoch 00082: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0915-00] Learning rate for epoch 82 is 0.003722037188708782\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.7127 - val_loss: 35.9394\n",
      "Epoch 83/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4881\n",
      "Epoch 00083: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0915-04] Learning rate for epoch 83 is 0.003763882676139474\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.4881 - val_loss: 9.2386\n",
      "Epoch 84/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6522\n",
      "Epoch 00084: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0915-09] Learning rate for epoch 84 is 0.0038056280463933945\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.6522 - val_loss: 4.5090\n",
      "Epoch 85/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8009\n",
      "Epoch 00085: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0915-14] Learning rate for epoch 85 is 0.003847273299470544\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.8009 - val_loss: 6.2805\n",
      "Epoch 86/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3840\n",
      "Epoch 00086: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0915-19] Learning rate for epoch 86 is 0.0038888186682015657\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.3840 - val_loss: 5.3016\n",
      "Epoch 87/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7856\n",
      "Epoch 00087: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0915-24] Learning rate for epoch 87 is 0.00393026415258646\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.7856 - val_loss: 6.4628\n",
      "Epoch 88/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2025\n",
      "Epoch 00088: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0915-29] Learning rate for epoch 88 is 0.00397160928696394\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.2025 - val_loss: 6.2201\n",
      "Epoch 89/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0031\n",
      "Epoch 00089: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0915-34] Learning rate for epoch 89 is 0.004012854769825935\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.0031 - val_loss: 12.7206\n",
      "Epoch 90/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5850\n",
      "Epoch 00090: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0915-38] Learning rate for epoch 90 is 0.00405400013551116\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.5850 - val_loss: 5.5967\n",
      "Epoch 91/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4776\n",
      "Epoch 00091: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0915-43] Learning rate for epoch 91 is 0.004095045384019613\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.4776 - val_loss: 4.5934\n",
      "Epoch 92/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4229\n",
      "Epoch 00092: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0915-48] Learning rate for epoch 92 is 0.004135990981012583\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.4229 - val_loss: 9.8451\n",
      "Epoch 93/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3122\n",
      "Epoch 00093: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0915-53] Learning rate for epoch 93 is 0.004176836460828781\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.3122 - val_loss: 315.2686\n",
      "Epoch 94/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1628\n",
      "Epoch 00094: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0915-57] Learning rate for epoch 94 is 0.004217581823468208\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.1628 - val_loss: 122.8685\n",
      "Epoch 95/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4783\n",
      "Epoch 00095: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0916-02] Learning rate for epoch 95 is 0.004258227068930864\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.4783 - val_loss: 31.3066\n",
      "Epoch 96/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6646\n",
      "Epoch 00096: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0916-07] Learning rate for epoch 96 is 0.0042987726628780365\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.6646 - val_loss: 105.2406\n",
      "Epoch 97/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5997\n",
      "Epoch 00097: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0916-12] Learning rate for epoch 97 is 0.0043392181396484375\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.5997 - val_loss: 120.4076\n",
      "Epoch 98/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3749\n",
      "Epoch 00098: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0916-17] Learning rate for epoch 98 is 0.004379563499242067\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.3749 - val_loss: 606.2499\n",
      "Epoch 99/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5411\n",
      "Epoch 00099: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0916-22] Learning rate for epoch 99 is 0.004419809207320213\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.5411 - val_loss: 421.2924\n",
      "Epoch 100/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5653\n",
      "Epoch 00100: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0916-27] Learning rate for epoch 100 is 0.004459954332560301\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.5653 - val_loss: 325.5143\n",
      "Epoch 101/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7306\n",
      "Epoch 00101: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0916-31] Learning rate for epoch 101 is 4.4999998749517545e-07\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.7306 - val_loss: 141.7032\n",
      "Epoch 102/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0341\n",
      "Epoch 00102: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0916-36] Learning rate for epoch 102 is 0.00018023152370005846\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.0341 - val_loss: 50.9730\n",
      "Epoch 103/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3311\n",
      "Epoch 00103: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0916-41] Learning rate for epoch 103 is 0.000359613070031628\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.3311 - val_loss: 18.2473\n",
      "Epoch 104/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1024\n",
      "Epoch 00104: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0916-46] Learning rate for epoch 104 is 0.0005385946715250611\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.1024 - val_loss: 5.2117\n",
      "Epoch 105/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3912\n",
      "Epoch 00105: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0916-51] Learning rate for epoch 105 is 0.0007171762990765274\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.3912 - val_loss: 3.6650\n",
      "Epoch 106/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2264\n",
      "Epoch 00106: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0916-56] Learning rate for epoch 106 is 0.0008953579817898571\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.2264 - val_loss: 3.9803\n",
      "Epoch 107/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8658\n",
      "Epoch 00107: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0917-01] Learning rate for epoch 107 is 0.0010731397196650505\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8658 - val_loss: 3.7705\n",
      "Epoch 108/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7189\n",
      "Epoch 00108: val_loss did not improve from 3.65295\n",
      "\n",
      "[20210302-0917-05] Learning rate for epoch 108 is 0.0012505215127021074\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.7189 - val_loss: 3.8227\n",
      "Epoch 109/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7499\n",
      "Epoch 00109: val_loss improved from 3.65295 to 3.52540, saving model to ./20210301-225844/heel_K4_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-0917-11] Learning rate for epoch 109 is 0.001427503302693367\n",
      "20/20 [==============================] - 2s 87ms/step - loss: 5.7499 - val_loss: 3.5254\n",
      "Epoch 110/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7183\n",
      "Epoch 00110: val_loss did not improve from 3.52540\n",
      "\n",
      "[20210302-0917-16] Learning rate for epoch 110 is 0.0016040850896388292\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.7183 - val_loss: 4.0102\n",
      "Epoch 111/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1649\n",
      "Epoch 00111: val_loss improved from 3.52540 to 3.45599, saving model to ./20210301-225844/heel_K4_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-0917-21] Learning rate for epoch 111 is 0.001780266989953816\n",
      "20/20 [==============================] - 2s 87ms/step - loss: 6.1649 - val_loss: 3.4560\n",
      "Epoch 112/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8944\n",
      "Epoch 00112: val_loss did not improve from 3.45599\n",
      "\n",
      "[20210302-0917-26] Learning rate for epoch 112 is 0.0019560488872230053\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.8944 - val_loss: 4.0951\n",
      "Epoch 113/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6002\n",
      "Epoch 00113: val_loss did not improve from 3.45599\n",
      "\n",
      "[20210302-0917-31] Learning rate for epoch 113 is 0.002131430897861719\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.6002 - val_loss: 4.0358\n",
      "Epoch 114/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0309\n",
      "Epoch 00114: val_loss did not improve from 3.45599\n",
      "\n",
      "[20210302-0917-35] Learning rate for epoch 114 is 0.0023064129054546356\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.0309 - val_loss: 3.6592\n",
      "Epoch 115/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0579\n",
      "Epoch 00115: val_loss did not improve from 3.45599\n",
      "\n",
      "[20210302-0917-40] Learning rate for epoch 115 is 0.0024809949100017548\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.0579 - val_loss: 4.2969\n",
      "Epoch 116/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1249\n",
      "Epoch 00116: val_loss did not improve from 3.45599\n",
      "\n",
      "[20210302-0917-45] Learning rate for epoch 116 is 0.0026551769115030766\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.1249 - val_loss: 4.2586\n",
      "Epoch 117/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5074\n",
      "Epoch 00117: val_loss did not improve from 3.45599\n",
      "\n",
      "[20210302-0917-50] Learning rate for epoch 117 is 0.0028289591427892447\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.5074 - val_loss: 11.8996\n",
      "Epoch 118/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1433\n",
      "Epoch 00118: val_loss did not improve from 3.45599\n",
      "\n",
      "[20210302-0917-55] Learning rate for epoch 118 is 0.0030023413710296154\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.1433 - val_loss: 4.2771\n",
      "Epoch 119/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9282\n",
      "Epoch 00119: val_loss did not improve from 3.45599\n",
      "\n",
      "[20210302-0917-59] Learning rate for epoch 119 is 0.003175323596224189\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.9282 - val_loss: 3.5480\n",
      "Epoch 120/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1357\n",
      "Epoch 00120: val_loss did not improve from 3.45599\n",
      "\n",
      "[20210302-0918-04] Learning rate for epoch 120 is 0.003347905818372965\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.1357 - val_loss: 4.4778\n",
      "Epoch 121/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8525\n",
      "Epoch 00121: val_loss did not improve from 3.45599\n",
      "\n",
      "[20210302-0918-09] Learning rate for epoch 121 is 0.0035200880374759436\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.8525 - val_loss: 3.6719\n",
      "Epoch 122/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0226\n",
      "Epoch 00122: val_loss did not improve from 3.45599\n",
      "\n",
      "[20210302-0918-14] Learning rate for epoch 122 is 0.003691870253533125\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.0226 - val_loss: 3.9990\n",
      "Epoch 123/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8125\n",
      "Epoch 00123: val_loss did not improve from 3.45599\n",
      "\n",
      "[20210302-0918-19] Learning rate for epoch 123 is 0.0038632526993751526\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8125 - val_loss: 4.0266\n",
      "Epoch 124/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7208\n",
      "Epoch 00124: val_loss did not improve from 3.45599\n",
      "\n",
      "[20210302-0918-24] Learning rate for epoch 124 is 0.004034235142171383\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.7208 - val_loss: 4.0198\n",
      "Epoch 125/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6874\n",
      "Epoch 00125: val_loss did not improve from 3.45599\n",
      "\n",
      "[20210302-0918-28] Learning rate for epoch 125 is 0.004204817581921816\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.6874 - val_loss: 5.3155\n",
      "Epoch 126/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3050\n",
      "Epoch 00126: val_loss did not improve from 3.45599\n",
      "\n",
      "[20210302-0918-33] Learning rate for epoch 126 is 0.0043750000186264515\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.3050 - val_loss: 3.7747\n",
      "Epoch 127/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9256\n",
      "Epoch 00127: val_loss did not improve from 3.45599\n",
      "\n",
      "[20210302-0918-38] Learning rate for epoch 127 is 0.0041952175088226795\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.9256 - val_loss: 3.7211\n",
      "Epoch 128/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1987\n",
      "Epoch 00128: val_loss did not improve from 3.45599\n",
      "\n",
      "[20210302-0918-43] Learning rate for epoch 128 is 0.004015835002064705\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.1987 - val_loss: 5.7191\n",
      "Epoch 129/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1391\n",
      "Epoch 00129: val_loss did not improve from 3.45599\n",
      "\n",
      "[20210302-0918-48] Learning rate for epoch 129 is 0.003836852265521884\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.1391 - val_loss: 4.8115\n",
      "Epoch 130/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3836\n",
      "Epoch 00130: val_loss did not improve from 3.45599\n",
      "\n",
      "[20210302-0918-52] Learning rate for epoch 130 is 0.003658269764855504\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.3836 - val_loss: 5.8919\n",
      "Epoch 131/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8936\n",
      "Epoch 00131: val_loss did not improve from 3.45599\n",
      "\n",
      "[20210302-0918-57] Learning rate for epoch 131 is 0.003480087034404278\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.8936 - val_loss: 3.7340\n",
      "Epoch 132/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6762\n",
      "Epoch 00132: val_loss did not improve from 3.45599\n",
      "\n",
      "[20210302-0919-02] Learning rate for epoch 132 is 0.003302304306998849\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.6762 - val_loss: 5.0157\n",
      "Epoch 133/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4801\n",
      "Epoch 00133: val_loss did not improve from 3.45599\n",
      "\n",
      "[20210302-0919-07] Learning rate for epoch 133 is 0.0031249215826392174\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.4801 - val_loss: 4.1049\n",
      "Epoch 134/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8688\n",
      "Epoch 00134: val_loss did not improve from 3.45599\n",
      "\n",
      "[20210302-0919-12] Learning rate for epoch 134 is 0.0029479386284947395\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 5.8688 - val_loss: 4.0594\n",
      "Epoch 135/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6607\n",
      "Epoch 00135: val_loss did not improve from 3.45599\n",
      "\n",
      "[20210302-0919-17] Learning rate for epoch 135 is 0.0027713559102267027\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.6607 - val_loss: 4.2825\n",
      "Epoch 136/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6740\n",
      "Epoch 00136: val_loss improved from 3.45599 to 3.19824, saving model to ./20210301-225844/heel_K4_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-0919-22] Learning rate for epoch 136 is 0.0025951729621738195\n",
      "20/20 [==============================] - 2s 92ms/step - loss: 5.6740 - val_loss: 3.1982\n",
      "Epoch 137/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7395\n",
      "Epoch 00137: val_loss did not improve from 3.19824\n",
      "\n",
      "[20210302-0919-27] Learning rate for epoch 137 is 0.0024193900171667337\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.7395 - val_loss: 3.2526\n",
      "Epoch 138/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8510\n",
      "Epoch 00138: val_loss did not improve from 3.19824\n",
      "\n",
      "[20210302-0919-32] Learning rate for epoch 138 is 0.0022440070752054453\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.8510 - val_loss: 3.7404\n",
      "Epoch 139/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6265\n",
      "Epoch 00139: val_loss did not improve from 3.19824\n",
      "\n",
      "[20210302-0919-36] Learning rate for epoch 139 is 0.002069024136289954\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.6265 - val_loss: 3.5142\n",
      "Epoch 140/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4720\n",
      "Epoch 00140: val_loss did not improve from 3.19824\n",
      "\n",
      "[20210302-0919-41] Learning rate for epoch 140 is 0.0018944410840049386\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.4720 - val_loss: 3.5791\n",
      "Epoch 141/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6922\n",
      "Epoch 00141: val_loss did not improve from 3.19824\n",
      "\n",
      "[20210302-0919-46] Learning rate for epoch 141 is 0.0017202580347657204\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.6922 - val_loss: 32.1132\n",
      "Epoch 142/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8620\n",
      "Epoch 00142: val_loss did not improve from 3.19824\n",
      "\n",
      "[20210302-0919-51] Learning rate for epoch 142 is 0.0015464748721569777\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.8620 - val_loss: 4.0523\n",
      "Epoch 143/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6812\n",
      "Epoch 00143: val_loss did not improve from 3.19824\n",
      "\n",
      "[20210302-0919-56] Learning rate for epoch 143 is 0.0013730917125940323\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.6812 - val_loss: 3.3744\n",
      "Epoch 144/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6528\n",
      "Epoch 00144: val_loss did not improve from 3.19824\n",
      "\n",
      "[20210302-0920-01] Learning rate for epoch 144 is 0.0012001085560768843\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6528 - val_loss: 3.5213\n",
      "Epoch 145/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5967\n",
      "Epoch 00145: val_loss improved from 3.19824 to 3.11684, saving model to ./20210301-225844/heel_K4_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-0920-06] Learning rate for epoch 145 is 0.0010275252861902118\n",
      "20/20 [==============================] - 2s 87ms/step - loss: 5.5967 - val_loss: 3.1168\n",
      "Epoch 146/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5658\n",
      "Epoch 00146: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0920-11] Learning rate for epoch 146 is 0.0008553420193493366\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5658 - val_loss: 3.3799\n",
      "Epoch 147/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5759\n",
      "Epoch 00147: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0920-16] Learning rate for epoch 147 is 0.0006835586973465979\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.5759 - val_loss: 3.4736\n",
      "Epoch 148/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5910\n",
      "Epoch 00148: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0920-20] Learning rate for epoch 148 is 0.0005121753201819956\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5910 - val_loss: 3.5302\n",
      "Epoch 149/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3525\n",
      "Epoch 00149: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0920-25] Learning rate for epoch 149 is 0.00034119191695936024\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.3525 - val_loss: 3.7491\n",
      "Epoch 150/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4268\n",
      "Epoch 00150: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0920-30] Learning rate for epoch 150 is 0.00017060847312677652\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.4268 - val_loss: 3.6337\n",
      "Epoch 151/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4110\n",
      "Epoch 00151: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0920-35] Learning rate for epoch 151 is 4.249999960848072e-07\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.4110 - val_loss: 3.5750\n",
      "Epoch 152/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5995\n",
      "Epoch 00152: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0920-40] Learning rate for epoch 152 is 0.00017020752420648932\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5995 - val_loss: 3.4959\n",
      "Epoch 153/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8061\n",
      "Epoch 00153: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0920-45] Learning rate for epoch 153 is 0.0003395900712348521\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8061 - val_loss: 3.2035\n",
      "Epoch 154/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4020\n",
      "Epoch 00154: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0920-49] Learning rate for epoch 154 is 0.0005085726734250784\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.4020 - val_loss: 3.5439\n",
      "Epoch 155/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3203\n",
      "Epoch 00155: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0920-54] Learning rate for epoch 155 is 0.0006771553307771683\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.3203 - val_loss: 3.6529\n",
      "Epoch 156/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5942\n",
      "Epoch 00156: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0920-59] Learning rate for epoch 156 is 0.0008453379850834608\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5942 - val_loss: 3.4812\n",
      "Epoch 157/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9216\n",
      "Epoch 00157: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0921-04] Learning rate for epoch 157 is 0.0010131207527592778\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.9216 - val_loss: 3.8382\n",
      "Epoch 158/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7413\n",
      "Epoch 00158: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0921-09] Learning rate for epoch 158 is 0.0011805035173892975\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.7413 - val_loss: 3.1616\n",
      "Epoch 159/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2023\n",
      "Epoch 00159: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0921-13] Learning rate for epoch 159 is 0.0013474862789735198\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.2023 - val_loss: 4.1347\n",
      "Epoch 160/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5295\n",
      "Epoch 00160: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0921-18] Learning rate for epoch 160 is 0.0015140691539272666\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.5295 - val_loss: 3.2649\n",
      "Epoch 161/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8844\n",
      "Epoch 00161: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0921-23] Learning rate for epoch 161 is 0.001680252025835216\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.8844 - val_loss: 3.5228\n",
      "Epoch 162/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8752\n",
      "Epoch 00162: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0921-28] Learning rate for epoch 162 is 0.0018460348946973681\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8752 - val_loss: 3.8959\n",
      "Epoch 163/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8050\n",
      "Epoch 00163: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0921-33] Learning rate for epoch 163 is 0.0020114178769290447\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.8050 - val_loss: 4.3925\n",
      "Epoch 164/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9577\n",
      "Epoch 00164: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0921-38] Learning rate for epoch 164 is 0.0021764009725302458\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.9577 - val_loss: 4.9970\n",
      "Epoch 165/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7268\n",
      "Epoch 00165: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0921-42] Learning rate for epoch 165 is 0.002340983832255006\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.7268 - val_loss: 6.7351\n",
      "Epoch 166/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8871\n",
      "Epoch 00166: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0921-47] Learning rate for epoch 166 is 0.002505166921764612\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.8871 - val_loss: 3.4867\n",
      "Epoch 167/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8821\n",
      "Epoch 00167: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0921-52] Learning rate for epoch 167 is 0.002668950008228421\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8821 - val_loss: 5.0152\n",
      "Epoch 168/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7761\n",
      "Epoch 00168: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0921-57] Learning rate for epoch 168 is 0.0028323333244770765\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.7761 - val_loss: 3.6613\n",
      "Epoch 169/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8683\n",
      "Epoch 00169: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0922-01] Learning rate for epoch 169 is 0.002995316404849291\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.8683 - val_loss: 3.9381\n",
      "Epoch 170/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2476\n",
      "Epoch 00170: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0922-06] Learning rate for epoch 170 is 0.0031578997150063515\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.2476 - val_loss: 3.3889\n",
      "Epoch 171/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0340\n",
      "Epoch 00171: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0922-11] Learning rate for epoch 171 is 0.0033200830221176147\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.0340 - val_loss: 6.5722\n",
      "Epoch 172/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9736\n",
      "Epoch 00172: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0922-16] Learning rate for epoch 172 is 0.0034818663261830807\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.9736 - val_loss: 44.2110\n",
      "Epoch 173/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6271\n",
      "Epoch 00173: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0922-21] Learning rate for epoch 173 is 0.0036432496272027493\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6271 - val_loss: 34.9260\n",
      "Epoch 174/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7502\n",
      "Epoch 00174: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0922-26] Learning rate for epoch 174 is 0.003804233158007264\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.7502 - val_loss: 34.1369\n",
      "Epoch 175/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0926\n",
      "Epoch 00175: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0922-30] Learning rate for epoch 175 is 0.003964816685765982\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.0926 - val_loss: 5.0541\n",
      "Epoch 176/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3158\n",
      "Epoch 00176: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0922-35] Learning rate for epoch 176 is 0.004124999977648258\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.3158 - val_loss: 5.5986\n",
      "Epoch 177/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0622\n",
      "Epoch 00177: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0922-40] Learning rate for epoch 177 is 0.003955216612666845\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.0622 - val_loss: 7.5199\n",
      "Epoch 178/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8895\n",
      "Epoch 00178: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0922-45] Learning rate for epoch 178 is 0.003785833017900586\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.8895 - val_loss: 3.7930\n",
      "Epoch 179/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7145\n",
      "Epoch 00179: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0922-50] Learning rate for epoch 179 is 0.0036168494261801243\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.7145 - val_loss: 6.4394\n",
      "Epoch 180/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9715\n",
      "Epoch 00180: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0922-55] Learning rate for epoch 180 is 0.003448265604674816\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.9715 - val_loss: 4.5444\n",
      "Epoch 181/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0238\n",
      "Epoch 00181: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0923-00] Learning rate for epoch 181 is 0.003280082019045949\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.0238 - val_loss: 3.7380\n",
      "Epoch 182/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4985\n",
      "Epoch 00182: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0923-04] Learning rate for epoch 182 is 0.0031122982036322355\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.4985 - val_loss: 4.0583\n",
      "Epoch 183/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2250\n",
      "Epoch 00183: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0923-09] Learning rate for epoch 183 is 0.002944914624094963\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.2250 - val_loss: 5.0649\n",
      "Epoch 184/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6740\n",
      "Epoch 00184: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0923-14] Learning rate for epoch 184 is 0.0027779308147728443\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.6740 - val_loss: 4.0201\n",
      "Epoch 185/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5892\n",
      "Epoch 00185: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0923-19] Learning rate for epoch 185 is 0.0026113467756658792\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.5892 - val_loss: 3.9913\n",
      "Epoch 186/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6611\n",
      "Epoch 00186: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0923-24] Learning rate for epoch 186 is 0.002445162972435355\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.6611 - val_loss: 3.8913\n",
      "Epoch 187/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3567\n",
      "Epoch 00187: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0923-29] Learning rate for epoch 187 is 0.0022793791722506285\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.3567 - val_loss: 6.3540\n",
      "Epoch 188/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1034\n",
      "Epoch 00188: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0923-34] Learning rate for epoch 188 is 0.0021139951422810555\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.1034 - val_loss: 5.1163\n",
      "Epoch 189/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1263\n",
      "Epoch 00189: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0923-38] Learning rate for epoch 189 is 0.0019490111153572798\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.1263 - val_loss: 8.2034\n",
      "Epoch 190/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6938\n",
      "Epoch 00190: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0923-43] Learning rate for epoch 190 is 0.0017844270914793015\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.6938 - val_loss: 5.3618\n",
      "Epoch 191/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9444\n",
      "Epoch 00191: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0923-48] Learning rate for epoch 191 is 0.0016202429542317986\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.9444 - val_loss: 3.4956\n",
      "Epoch 192/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7421\n",
      "Epoch 00192: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0923-53] Learning rate for epoch 192 is 0.001456458936445415\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.7421 - val_loss: 3.9399\n",
      "Epoch 193/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7717\n",
      "Epoch 00193: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0923-58] Learning rate for epoch 193 is 0.001293074688874185\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.7717 - val_loss: 4.5494\n",
      "Epoch 194/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4797\n",
      "Epoch 00194: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0924-03] Learning rate for epoch 194 is 0.0011300905607640743\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.4797 - val_loss: 4.1294\n",
      "Epoch 195/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2226\n",
      "Epoch 00195: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0924-08] Learning rate for epoch 195 is 0.0009675062610767782\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.2226 - val_loss: 4.2527\n",
      "Epoch 196/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5673\n",
      "Epoch 00196: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0924-12] Learning rate for epoch 196 is 0.0008053220226429403\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.5673 - val_loss: 3.2573\n",
      "Epoch 197/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3408\n",
      "Epoch 00197: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0924-17] Learning rate for epoch 197 is 0.0006435376708395779\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.3408 - val_loss: 3.1682\n",
      "Epoch 198/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5015\n",
      "Epoch 00198: val_loss did not improve from 3.11684\n",
      "\n",
      "[20210302-0924-22] Learning rate for epoch 198 is 0.0004821533220820129\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.5015 - val_loss: 3.1351\n",
      "Epoch 199/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1314\n",
      "Epoch 00199: val_loss improved from 3.11684 to 3.01827, saving model to ./20210301-225844/heel_K4_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-0924-28] Learning rate for epoch 199 is 0.0003211689181625843\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 5.1314 - val_loss: 3.0183\n",
      "Epoch 200/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4658\n",
      "Epoch 00200: val_loss improved from 3.01827 to 2.99663, saving model to ./20210301-225844/heel_K4_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-0924-33] Learning rate for epoch 200 is 0.00016058447363320738\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 5.4658 - val_loss: 2.9966\n",
      "Epoch 201/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4682\n",
      "Epoch 00201: val_loss improved from 2.99663 to 2.98057, saving model to ./20210301-225844/heel_K4_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-0924-38] Learning rate for epoch 201 is 4.0000000467443897e-07\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 5.4682 - val_loss: 2.9806\n",
      "Epoch 202/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3135\n",
      "Epoch 00202: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0924-43] Learning rate for epoch 202 is 0.0001601835247129202\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.3135 - val_loss: 3.1067\n",
      "Epoch 203/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6481\n",
      "Epoch 00203: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0924-48] Learning rate for epoch 203 is 0.00031956707243807614\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.6481 - val_loss: 3.0451\n",
      "Epoch 204/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3045\n",
      "Epoch 00204: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0924-53] Learning rate for epoch 204 is 0.00047855067532509565\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.3045 - val_loss: 3.1362\n",
      "Epoch 205/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4155\n",
      "Epoch 00205: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0924-58] Learning rate for epoch 205 is 0.0006371343042701483\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.4155 - val_loss: 3.0965\n",
      "Epoch 206/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2239\n",
      "Epoch 00206: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0925-03] Learning rate for epoch 206 is 0.0007953179883770645\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.2239 - val_loss: 3.3099\n",
      "Epoch 207/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3289\n",
      "Epoch 00207: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0925-07] Learning rate for epoch 207 is 0.0009531017276458442\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.3289 - val_loss: 3.5593\n",
      "Epoch 208/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3440\n",
      "Epoch 00208: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0925-12] Learning rate for epoch 208 is 0.0011104855220764875\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.3440 - val_loss: 3.1621\n",
      "Epoch 209/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3637\n",
      "Epoch 00209: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0925-17] Learning rate for epoch 209 is 0.0012674692552536726\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.3637 - val_loss: 3.2364\n",
      "Epoch 210/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1789\n",
      "Epoch 00210: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0925-22] Learning rate for epoch 210 is 0.0014240531018003821\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.1789 - val_loss: 4.5622\n",
      "Epoch 211/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0703\n",
      "Epoch 00211: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0925-27] Learning rate for epoch 211 is 0.0015802369453012943\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.0703 - val_loss: 3.3317\n",
      "Epoch 212/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8359\n",
      "Epoch 00212: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0925-32] Learning rate for epoch 212 is 0.001736020902171731\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8359 - val_loss: 3.9791\n",
      "Epoch 213/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6297\n",
      "Epoch 00213: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0925-36] Learning rate for epoch 213 is 0.0018914048559963703\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.6297 - val_loss: 3.8016\n",
      "Epoch 214/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6279\n",
      "Epoch 00214: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0925-41] Learning rate for epoch 214 is 0.0020463888067752123\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6279 - val_loss: 3.3382\n",
      "Epoch 215/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2011\n",
      "Epoch 00215: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0925-46] Learning rate for epoch 215 is 0.0022009729873389006\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.2011 - val_loss: 4.3287\n",
      "Epoch 216/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5079\n",
      "Epoch 00216: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0925-51] Learning rate for epoch 216 is 0.002355156932026148\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.5079 - val_loss: 3.5278\n",
      "Epoch 217/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1105\n",
      "Epoch 00217: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0925-56] Learning rate for epoch 217 is 0.0025089411064982414\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.1105 - val_loss: 3.3325\n",
      "Epoch 218/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6946\n",
      "Epoch 00218: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0926-00] Learning rate for epoch 218 is 0.0026623252779245377\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6946 - val_loss: 39.4315\n",
      "Epoch 219/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8266\n",
      "Epoch 00219: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0926-05] Learning rate for epoch 219 is 0.0028153094463050365\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.8266 - val_loss: 279.4312\n",
      "Epoch 220/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6226\n",
      "Epoch 00220: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0926-10] Learning rate for epoch 220 is 0.002967893611639738\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.6226 - val_loss: 64.0410\n",
      "Epoch 221/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2981\n",
      "Epoch 00221: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0926-15] Learning rate for epoch 221 is 0.003120078006759286\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 6.2981 - val_loss: 7.5787\n",
      "Epoch 222/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0724\n",
      "Epoch 00222: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0926-20] Learning rate for epoch 222 is 0.0032718623988330364\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.0724 - val_loss: 8.7031\n",
      "Epoch 223/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3672\n",
      "Epoch 00223: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0926-25] Learning rate for epoch 223 is 0.0034232467878609896\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.3672 - val_loss: 3.2834\n",
      "Epoch 224/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1054\n",
      "Epoch 00224: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0926-29] Learning rate for epoch 224 is 0.0035742311738431454\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.1054 - val_loss: 3.9173\n",
      "Epoch 225/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7885\n",
      "Epoch 00225: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0926-34] Learning rate for epoch 225 is 0.003724815556779504\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.7885 - val_loss: 5.5166\n",
      "Epoch 226/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0220\n",
      "Epoch 00226: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0926-39] Learning rate for epoch 226 is 0.003874999936670065\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.0220 - val_loss: 4.1324\n",
      "Epoch 227/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0839\n",
      "Epoch 00227: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0926-44] Learning rate for epoch 227 is 0.0037152154836803675\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.0839 - val_loss: 4.5991\n",
      "Epoch 228/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7115\n",
      "Epoch 00228: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0926-49] Learning rate for epoch 228 is 0.0035558310337364674\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.7115 - val_loss: 5.6078\n",
      "Epoch 229/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7151\n",
      "Epoch 00229: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0926-53] Learning rate for epoch 229 is 0.003396846354007721\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.7151 - val_loss: 6.6119\n",
      "Epoch 230/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8961\n",
      "Epoch 00230: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0926-58] Learning rate for epoch 230 is 0.003238261677324772\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.8961 - val_loss: 4.6422\n",
      "Epoch 231/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8254\n",
      "Epoch 00231: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0927-03] Learning rate for epoch 231 is 0.00308007700368762\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8254 - val_loss: 3.6014\n",
      "Epoch 232/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1898\n",
      "Epoch 00232: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0927-08] Learning rate for epoch 232 is 0.002922292333096266\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.1898 - val_loss: 4.8996\n",
      "Epoch 233/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6162\n",
      "Epoch 00233: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0927-13] Learning rate for epoch 233 is 0.002764907432720065\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6162 - val_loss: 3.8033\n",
      "Epoch 234/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3955\n",
      "Epoch 00234: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0927-17] Learning rate for epoch 234 is 0.0026079227682203054\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.3955 - val_loss: 4.6789\n",
      "Epoch 235/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1471\n",
      "Epoch 00235: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0927-22] Learning rate for epoch 235 is 0.0024513378739356995\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.1471 - val_loss: 3.7438\n",
      "Epoch 236/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5749\n",
      "Epoch 00236: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0927-27] Learning rate for epoch 236 is 0.002295152982696891\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5749 - val_loss: 4.3539\n",
      "Epoch 237/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5615\n",
      "Epoch 00237: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0927-32] Learning rate for epoch 237 is 0.0021393680945038795\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.5615 - val_loss: 4.0459\n",
      "Epoch 238/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9057\n",
      "Epoch 00238: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0927-37] Learning rate for epoch 238 is 0.0019839832093566656\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.9057 - val_loss: 4.6877\n",
      "Epoch 239/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6330\n",
      "Epoch 00239: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0927-42] Learning rate for epoch 239 is 0.0018289980944246054\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.6330 - val_loss: 3.5557\n",
      "Epoch 240/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6758\n",
      "Epoch 00240: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0927-46] Learning rate for epoch 240 is 0.0016744130989536643\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.6758 - val_loss: 3.3684\n",
      "Epoch 241/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1862\n",
      "Epoch 00241: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0927-51] Learning rate for epoch 241 is 0.0015202279901131988\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.1862 - val_loss: 3.1120\n",
      "Epoch 242/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8272\n",
      "Epoch 00242: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0927-56] Learning rate for epoch 242 is 0.0013664428843185306\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8272 - val_loss: 3.2375\n",
      "Epoch 243/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8580\n",
      "Epoch 00243: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0928-01] Learning rate for epoch 243 is 0.0012130576651543379\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.8580 - val_loss: 3.1985\n",
      "Epoch 244/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5181\n",
      "Epoch 00244: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0928-06] Learning rate for epoch 244 is 0.0010600725654512644\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5181 - val_loss: 3.2034\n",
      "Epoch 245/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1918\n",
      "Epoch 00245: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0928-10] Learning rate for epoch 245 is 0.0009074872941710055\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.1918 - val_loss: 3.6736\n",
      "Epoch 246/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4371\n",
      "Epoch 00246: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0928-15] Learning rate for epoch 246 is 0.0007553020259365439\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.4371 - val_loss: 3.5150\n",
      "Epoch 247/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4941\n",
      "Epoch 00247: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0928-20] Learning rate for epoch 247 is 0.0006035167025402188\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.4941 - val_loss: 3.2912\n",
      "Epoch 248/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4425\n",
      "Epoch 00248: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0928-25] Learning rate for epoch 248 is 0.00045213132398203015\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.4425 - val_loss: 3.2415\n",
      "Epoch 249/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.0712\n",
      "Epoch 00249: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0928-30] Learning rate for epoch 249 is 0.00030114591936580837\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.0712 - val_loss: 3.2907\n",
      "Epoch 250/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6264\n",
      "Epoch 00250: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0928-34] Learning rate for epoch 250 is 0.00015056047413963825\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.6264 - val_loss: 3.2162\n",
      "Epoch 251/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 4.9852\n",
      "Epoch 00251: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0928-39] Learning rate for epoch 251 is 3.7500001326407073e-07\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 4.9852 - val_loss: 3.1970\n",
      "Epoch 252/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4099\n",
      "Epoch 00252: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0928-44] Learning rate for epoch 252 is 0.00015015952521935105\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.4099 - val_loss: 3.1515\n",
      "Epoch 253/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2493\n",
      "Epoch 00253: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0928-49] Learning rate for epoch 253 is 0.0002995440736413002\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.2493 - val_loss: 3.1302\n",
      "Epoch 254/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1656\n",
      "Epoch 00254: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0928-53] Learning rate for epoch 254 is 0.0004485286772251129\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.1656 - val_loss: 3.1812\n",
      "Epoch 255/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5860\n",
      "Epoch 00255: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0928-58] Learning rate for epoch 255 is 0.0005971133359707892\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.5860 - val_loss: 3.1554\n",
      "Epoch 256/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 4.9732\n",
      "Epoch 00256: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0929-03] Learning rate for epoch 256 is 0.0007452979916706681\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 4.9732 - val_loss: 3.0744\n",
      "Epoch 257/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 4.9947\n",
      "Epoch 00257: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0929-08] Learning rate for epoch 257 is 0.0008930827025324106\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 4.9947 - val_loss: 3.3868\n",
      "Epoch 258/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4691\n",
      "Epoch 00258: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0929-13] Learning rate for epoch 258 is 0.0010404675267636776\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.4691 - val_loss: 3.6301\n",
      "Epoch 259/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5114\n",
      "Epoch 00259: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0929-18] Learning rate for epoch 259 is 0.0011874522315338254\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.5114 - val_loss: 4.1763\n",
      "Epoch 260/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8959\n",
      "Epoch 00260: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0929-23] Learning rate for epoch 260 is 0.0013340371660888195\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.8959 - val_loss: 5.6068\n",
      "Epoch 261/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3142\n",
      "Epoch 00261: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0929-27] Learning rate for epoch 261 is 0.0014802219811826944\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.3142 - val_loss: 4.7930\n",
      "Epoch 262/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6176\n",
      "Epoch 00262: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0929-32] Learning rate for epoch 262 is 0.0016260069096460938\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.6176 - val_loss: 3.7299\n",
      "Epoch 263/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1558\n",
      "Epoch 00263: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0929-37] Learning rate for epoch 263 is 0.001771391835063696\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.1558 - val_loss: 3.8465\n",
      "Epoch 264/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2902\n",
      "Epoch 00264: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0929-42] Learning rate for epoch 264 is 0.0019163768738508224\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.2902 - val_loss: 3.6487\n",
      "Epoch 265/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5066\n",
      "Epoch 00265: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0929-47] Learning rate for epoch 265 is 0.0020609619095921516\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.5066 - val_loss: 3.3376\n",
      "Epoch 266/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8255\n",
      "Epoch 00266: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0929-51] Learning rate for epoch 266 is 0.0022051469422876835\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8255 - val_loss: 3.1717\n",
      "Epoch 267/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5827\n",
      "Epoch 00267: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0929-56] Learning rate for epoch 267 is 0.0023489322047680616\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5827 - val_loss: 3.2959\n",
      "Epoch 268/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2527\n",
      "Epoch 00268: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0930-01] Learning rate for epoch 268 is 0.002492317231371999\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.2527 - val_loss: 3.6457\n",
      "Epoch 269/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5121\n",
      "Epoch 00269: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0930-06] Learning rate for epoch 269 is 0.0026353024877607822\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5121 - val_loss: 4.7209\n",
      "Epoch 270/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6002\n",
      "Epoch 00270: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0930-11] Learning rate for epoch 270 is 0.0027778877411037683\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.6002 - val_loss: 3.9933\n",
      "Epoch 271/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2275\n",
      "Epoch 00271: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0930-15] Learning rate for epoch 271 is 0.002920072991400957\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.2275 - val_loss: 3.6516\n",
      "Epoch 272/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8173\n",
      "Epoch 00272: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0930-20] Learning rate for epoch 272 is 0.0030618582386523485\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8173 - val_loss: 3.4606\n",
      "Epoch 273/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6499\n",
      "Epoch 00273: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0930-25] Learning rate for epoch 273 is 0.0032032437156885862\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6499 - val_loss: 5.1541\n",
      "Epoch 274/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6424\n",
      "Epoch 00274: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0930-30] Learning rate for epoch 274 is 0.0033442291896790266\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.6424 - val_loss: 5.2526\n",
      "Epoch 275/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1954\n",
      "Epoch 00275: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0930-35] Learning rate for epoch 275 is 0.003484814427793026\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.1954 - val_loss: 3.2565\n",
      "Epoch 276/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2893\n",
      "Epoch 00276: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0930-39] Learning rate for epoch 276 is 0.0036249998956918716\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.2893 - val_loss: 3.2012\n",
      "Epoch 277/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8007\n",
      "Epoch 00277: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0930-44] Learning rate for epoch 277 is 0.0034752145875245333\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.8007 - val_loss: 3.4126\n",
      "Epoch 278/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9085\n",
      "Epoch 00278: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0930-49] Learning rate for epoch 278 is 0.003325828816741705\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.9085 - val_loss: 4.8938\n",
      "Epoch 279/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2108\n",
      "Epoch 00279: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0930-54] Learning rate for epoch 279 is 0.0031768432818353176\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.2108 - val_loss: 3.2129\n",
      "Epoch 280/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8131\n",
      "Epoch 00280: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0930-59] Learning rate for epoch 280 is 0.0030282577499747276\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.8131 - val_loss: 4.3629\n",
      "Epoch 281/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6439\n",
      "Epoch 00281: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0931-03] Learning rate for epoch 281 is 0.0028800719883292913\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.6439 - val_loss: 5.3421\n",
      "Epoch 282/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4136\n",
      "Epoch 00282: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0931-08] Learning rate for epoch 282 is 0.0027322862297296524\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.4136 - val_loss: 3.3517\n",
      "Epoch 283/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2785\n",
      "Epoch 00283: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0931-13] Learning rate for epoch 283 is 0.002584900474175811\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.2785 - val_loss: 4.1275\n",
      "Epoch 284/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3298\n",
      "Epoch 00284: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0931-18] Learning rate for epoch 284 is 0.0024379147216677666\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.3298 - val_loss: 3.2638\n",
      "Epoch 285/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7209\n",
      "Epoch 00285: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0931-22] Learning rate for epoch 285 is 0.0022913289722055197\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.7209 - val_loss: 3.2714\n",
      "Epoch 286/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1874\n",
      "Epoch 00286: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0931-27] Learning rate for epoch 286 is 0.0021451429929584265\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.1874 - val_loss: 3.3957\n",
      "Epoch 287/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5851\n",
      "Epoch 00287: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0931-32] Learning rate for epoch 287 is 0.0019993570167571306\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.5851 - val_loss: 4.4151\n",
      "Epoch 288/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2928\n",
      "Epoch 00288: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0931-37] Learning rate for epoch 288 is 0.001853971160016954\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.2928 - val_loss: 4.0850\n",
      "Epoch 289/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.0157\n",
      "Epoch 00289: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0931-42] Learning rate for epoch 289 is 0.001708985073491931\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.0157 - val_loss: 3.7463\n",
      "Epoch 290/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4396\n",
      "Epoch 00290: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0931-47] Learning rate for epoch 290 is 0.0015643991064280272\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.4396 - val_loss: 3.4372\n",
      "Epoch 291/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3524\n",
      "Epoch 00291: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0931-52] Learning rate for epoch 291 is 0.0014202130259945989\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.3524 - val_loss: 3.1564\n",
      "Epoch 292/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4254\n",
      "Epoch 00292: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0931-56] Learning rate for epoch 292 is 0.001276426832191646\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.4254 - val_loss: 3.4249\n",
      "Epoch 293/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3729\n",
      "Epoch 00293: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0932-01] Learning rate for epoch 293 is 0.0011330407578498125\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.3729 - val_loss: 3.0425\n",
      "Epoch 294/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 4.9056\n",
      "Epoch 00294: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0932-06] Learning rate for epoch 294 is 0.0009900545701384544\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 4.9056 - val_loss: 3.0612\n",
      "Epoch 295/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4716\n",
      "Epoch 00295: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0932-11] Learning rate for epoch 295 is 0.0008474682690575719\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.4716 - val_loss: 3.2430\n",
      "Epoch 296/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.0509\n",
      "Epoch 00296: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0932-16] Learning rate for epoch 296 is 0.0007052819710224867\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.0509 - val_loss: 3.2306\n",
      "Epoch 297/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1242\n",
      "Epoch 00297: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0932-21] Learning rate for epoch 297 is 0.0005634956760331988\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.1242 - val_loss: 3.2180\n",
      "Epoch 298/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.0727\n",
      "Epoch 00298: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0932-25] Learning rate for epoch 298 is 0.0004221093258820474\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.0727 - val_loss: 3.2276\n",
      "Epoch 299/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 4.8515\n",
      "Epoch 00299: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0932-30] Learning rate for epoch 299 is 0.00028112292056903243\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 4.8515 - val_loss: 3.0780\n",
      "Epoch 300/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3581\n",
      "Epoch 00300: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0932-35] Learning rate for epoch 300 is 0.0001405364746460691\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.3581 - val_loss: 3.1475\n",
      "Epoch 301/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4412\n",
      "Epoch 00301: val_loss did not improve from 2.98057\n",
      "\n",
      "[20210302-0932-40] Learning rate for epoch 301 is 3.4999999343199306e-07\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.4412 - val_loss: 3.1179\n",
      "K= 5\n",
      "Check cache in memory:Y True\n",
      "Check keypoints process01: Tensor(\"args_1:0\", shape=(2,), dtype=int64) (2,) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Check keypoints process02: Tensor(\"Reshape:0\", shape=(1, 2), dtype=int64) (1, 2) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Check keypoints process03: Tensor(\"PyFunc:1\", dtype=float32, device=/job:localhost/replica:0/task:0)\n",
      "Check keypoints process04: Tensor(\"Reshape_1:0\", shape=(2,), dtype=float32)\n",
      "Check augment :Y True\n",
      "Check ds cache[True] and augment[True]\n",
      "Check cache in memory:Y True\n",
      "Check augment :N False\n",
      "Check ds cache[True] and augment[False]\n",
      "best_model_name: ./20210301-225844/heel_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "base_model.trainable :  False\n",
      "\n",
      "[20210302-0932-42] Learning rate for epoch 1 is 0.00047855067532509565\n",
      "Epoch 1/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 106.3063\n",
      "Epoch 00001: val_loss improved from inf to 75.22166, saving model to ./20210301-225844/heel_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 5s 260ms/step - loss: 105.1742 - val_loss: 75.2217\n",
      "\n",
      "[20210302-0932-57] Learning rate for epoch 2 is 0.00047855067532509565\n",
      "Epoch 2/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 41.6044\n",
      "Epoch 00002: val_loss improved from 75.22166 to 28.15586, saving model to ./20210301-225844/heel_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 41.6044 - val_loss: 28.1559\n",
      "\n",
      "[20210302-0933-02] Learning rate for epoch 3 is 0.00047855067532509565\n",
      "Epoch 3/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 23.7071\n",
      "Epoch 00003: val_loss improved from 28.15586 to 22.19780, saving model to ./20210301-225844/heel_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 8s 394ms/step - loss: 23.7071 - val_loss: 22.1978\n",
      "\n",
      "[20210302-0933-13] Learning rate for epoch 4 is 0.00047855067532509565\n",
      "Epoch 4/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 20.9839\n",
      "Epoch 00004: val_loss improved from 22.19780 to 21.27287, saving model to ./20210301-225844/heel_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 20.7948 - val_loss: 21.2729\n",
      "\n",
      "[20210302-0933-18] Learning rate for epoch 5 is 0.00047855067532509565\n",
      "Epoch 5/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 20.1841\n",
      "Epoch 00005: val_loss improved from 21.27287 to 19.32101, saving model to ./20210301-225844/heel_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 19.8755 - val_loss: 19.3210\n",
      "\n",
      "[20210302-0933-22] Learning rate for epoch 6 is 0.00047855067532509565\n",
      "Epoch 6/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 18.6319\n",
      "Epoch 00006: val_loss improved from 19.32101 to 18.35066, saving model to ./20210301-225844/heel_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 18.6319 - val_loss: 18.3507\n",
      "\n",
      "[20210302-0933-26] Learning rate for epoch 7 is 0.00047855067532509565\n",
      "Epoch 7/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 18.7596\n",
      "Epoch 00007: val_loss did not improve from 18.35066\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 18.7596 - val_loss: 18.8362\n",
      "\n",
      "[20210302-0933-30] Learning rate for epoch 8 is 0.00047855067532509565\n",
      "Epoch 8/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 17.7304\n",
      "Epoch 00008: val_loss improved from 18.35066 to 17.35711, saving model to ./20210301-225844/heel_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 17.6828 - val_loss: 17.3571\n",
      "\n",
      "[20210302-0933-35] Learning rate for epoch 9 is 0.00047855067532509565\n",
      "Epoch 9/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 17.9544\n",
      "Epoch 00009: val_loss improved from 17.35711 to 16.80360, saving model to ./20210301-225844/heel_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 17.9544 - val_loss: 16.8036\n",
      "\n",
      "[20210302-0933-39] Learning rate for epoch 10 is 0.00047855067532509565\n",
      "Epoch 10/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 17.7446\n",
      "Epoch 00010: val_loss improved from 16.80360 to 16.05729, saving model to ./20210301-225844/heel_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 17.7446 - val_loss: 16.0573\n",
      "\n",
      "[20210302-0933-43] Learning rate for epoch 11 is 0.00047855067532509565\n",
      "Epoch 11/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 17.7894\n",
      "Epoch 00011: val_loss improved from 16.05729 to 15.75999, saving model to ./20210301-225844/heel_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 17.6717 - val_loss: 15.7600\n",
      "\n",
      "[20210302-0933-48] Learning rate for epoch 12 is 0.00047855067532509565\n",
      "Epoch 12/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 17.5784\n",
      "Epoch 00012: val_loss improved from 15.75999 to 15.24001, saving model to ./20210301-225844/heel_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 17.5487 - val_loss: 15.2400\n",
      "\n",
      "[20210302-0933-52] Learning rate for epoch 13 is 0.00047855067532509565\n",
      "Epoch 13/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 17.0957\n",
      "Epoch 00013: val_loss did not improve from 15.24001\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 17.1237 - val_loss: 15.4079\n",
      "\n",
      "[20210302-0933-56] Learning rate for epoch 14 is 0.00047855067532509565\n",
      "Epoch 14/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 17.2973\n",
      "Epoch 00014: val_loss improved from 15.24001 to 14.86711, saving model to ./20210301-225844/heel_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 17.2973 - val_loss: 14.8671\n",
      "\n",
      "[20210302-0934-00] Learning rate for epoch 15 is 0.00047855067532509565\n",
      "Epoch 15/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 17.0517\n",
      "Epoch 00015: val_loss improved from 14.86711 to 14.10712, saving model to ./20210301-225844/heel_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 16.9608 - val_loss: 14.1071\n",
      "\n",
      "[20210302-0934-05] Learning rate for epoch 16 is 0.00047855067532509565\n",
      "Epoch 16/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.7400\n",
      "Epoch 00016: val_loss improved from 14.10712 to 13.84642, saving model to ./20210301-225844/heel_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.7400 - val_loss: 13.8464\n",
      "\n",
      "[20210302-0934-09] Learning rate for epoch 17 is 0.00047855067532509565\n",
      "Epoch 17/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.6940\n",
      "Epoch 00017: val_loss improved from 13.84642 to 13.39954, saving model to ./20210301-225844/heel_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.6649 - val_loss: 13.3995\n",
      "\n",
      "[20210302-0934-13] Learning rate for epoch 18 is 0.00047855067532509565\n",
      "Epoch 18/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.3425\n",
      "Epoch 00018: val_loss improved from 13.39954 to 13.09867, saving model to ./20210301-225844/heel_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.3425 - val_loss: 13.0987\n",
      "\n",
      "[20210302-0934-18] Learning rate for epoch 19 is 0.00047855067532509565\n",
      "Epoch 19/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.8970\n",
      "Epoch 00019: val_loss improved from 13.09867 to 11.70033, saving model to ./20210301-225844/heel_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.8218 - val_loss: 11.7003\n",
      "\n",
      "[20210302-0934-22] Learning rate for epoch 20 is 0.00047855067532509565\n",
      "Epoch 20/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.6831\n",
      "Epoch 00020: val_loss did not improve from 11.70033\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 16.5807 - val_loss: 12.4690\n",
      "\n",
      "[20210302-0934-26] Learning rate for epoch 21 is 0.00047855067532509565\n",
      "Epoch 21/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 16.8342\n",
      "Epoch 00021: val_loss did not improve from 11.70033\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.7437 - val_loss: 11.8685\n",
      "\n",
      "[20210302-0934-30] Learning rate for epoch 22 is 0.00047855067532509565\n",
      "Epoch 22/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.3895\n",
      "Epoch 00022: val_loss improved from 11.70033 to 11.64196, saving model to ./20210301-225844/heel_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.3856 - val_loss: 11.6420\n",
      "\n",
      "[20210302-0934-35] Learning rate for epoch 23 is 0.00047855067532509565\n",
      "Epoch 23/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.1529\n",
      "Epoch 00023: val_loss did not improve from 11.64196\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.1365 - val_loss: 12.0151\n",
      "\n",
      "[20210302-0934-39] Learning rate for epoch 24 is 0.00047855067532509565\n",
      "Epoch 24/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.7811\n",
      "Epoch 00024: val_loss improved from 11.64196 to 10.23285, saving model to ./20210301-225844/heel_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.7132 - val_loss: 10.2328\n",
      "\n",
      "[20210302-0934-43] Learning rate for epoch 25 is 0.00047855067532509565\n",
      "Epoch 25/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.3169\n",
      "Epoch 00025: val_loss improved from 10.23285 to 10.15011, saving model to ./20210301-225844/heel_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 16.3788 - val_loss: 10.1501\n",
      "\n",
      "[20210302-0934-47] Learning rate for epoch 26 is 0.00047855067532509565\n",
      "Epoch 26/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.5084\n",
      "Epoch 00026: val_loss improved from 10.15011 to 10.01558, saving model to ./20210301-225844/heel_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 16.5084 - val_loss: 10.0156\n",
      "\n",
      "[20210302-0934-52] Learning rate for epoch 27 is 0.00047855067532509565\n",
      "Epoch 27/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.9565\n",
      "Epoch 00027: val_loss improved from 10.01558 to 9.27687, saving model to ./20210301-225844/heel_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 16.0004 - val_loss: 9.2769\n",
      "\n",
      "[20210302-0934-56] Learning rate for epoch 28 is 0.00047855067532509565\n",
      "Epoch 28/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.8032\n",
      "Epoch 00028: val_loss did not improve from 9.27687\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.8032 - val_loss: 10.1721\n",
      "\n",
      "[20210302-0935-00] Learning rate for epoch 29 is 0.00047855067532509565\n",
      "Epoch 29/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 17.0191\n",
      "Epoch 00029: val_loss did not improve from 9.27687\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.7809 - val_loss: 10.1612\n",
      "\n",
      "[20210302-0935-04] Learning rate for epoch 30 is 0.00047855067532509565\n",
      "Epoch 30/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.5605\n",
      "Epoch 00030: val_loss improved from 9.27687 to 9.06629, saving model to ./20210301-225844/heel_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 16.5879 - val_loss: 9.0663\n",
      "\n",
      "[20210302-0935-08] Learning rate for epoch 31 is 0.00047855067532509565\n",
      "Epoch 31/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.3236\n",
      "Epoch 00031: val_loss did not improve from 9.06629\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.3236 - val_loss: 10.1307\n",
      "\n",
      "[20210302-0935-12] Learning rate for epoch 32 is 0.00047855067532509565\n",
      "Epoch 32/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 16.3379\n",
      "Epoch 00032: val_loss did not improve from 9.06629\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.3373 - val_loss: 9.1514\n",
      "\n",
      "[20210302-0935-16] Learning rate for epoch 33 is 0.00047855067532509565\n",
      "Epoch 33/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.9044\n",
      "Epoch 00033: val_loss did not improve from 9.06629\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.9811 - val_loss: 9.0941\n",
      "\n",
      "[20210302-0935-20] Learning rate for epoch 34 is 0.00047855067532509565\n",
      "Epoch 34/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.9831\n",
      "Epoch 00034: val_loss improved from 9.06629 to 8.33689, saving model to ./20210301-225844/heel_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 15.9343 - val_loss: 8.3369\n",
      "\n",
      "[20210302-0935-24] Learning rate for epoch 35 is 0.00047855067532509565\n",
      "Epoch 35/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.2774\n",
      "Epoch 00035: val_loss did not improve from 8.33689\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.3054 - val_loss: 9.1642\n",
      "\n",
      "[20210302-0935-28] Learning rate for epoch 36 is 0.00047855067532509565\n",
      "Epoch 36/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 16.6764\n",
      "Epoch 00036: val_loss improved from 8.33689 to 8.16269, saving model to ./20210301-225844/heel_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 16.5902 - val_loss: 8.1627\n",
      "\n",
      "[20210302-0935-33] Learning rate for epoch 37 is 0.00047855067532509565\n",
      "Epoch 37/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.8147\n",
      "Epoch 00037: val_loss did not improve from 8.16269\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.6323 - val_loss: 8.2868\n",
      "\n",
      "[20210302-0935-37] Learning rate for epoch 38 is 0.00047855067532509565\n",
      "Epoch 38/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.6406\n",
      "Epoch 00038: val_loss did not improve from 8.16269\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.6406 - val_loss: 8.4172\n",
      "\n",
      "[20210302-0935-41] Learning rate for epoch 39 is 0.00047855067532509565\n",
      "Epoch 39/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.7250\n",
      "Epoch 00039: val_loss did not improve from 8.16269\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.8053 - val_loss: 9.2166\n",
      "\n",
      "[20210302-0935-45] Learning rate for epoch 40 is 0.00047855067532509565\n",
      "Epoch 40/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.1753\n",
      "Epoch 00040: val_loss did not improve from 8.16269\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.1753 - val_loss: 8.2550\n",
      "\n",
      "[20210302-0935-49] Learning rate for epoch 41 is 0.00047855067532509565\n",
      "Epoch 41/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.9295\n",
      "Epoch 00041: val_loss did not improve from 8.16269\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.8744 - val_loss: 8.7472\n",
      "\n",
      "[20210302-0935-52] Learning rate for epoch 42 is 0.00047855067532509565\n",
      "Epoch 42/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.9411\n",
      "Epoch 00042: val_loss did not improve from 8.16269\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.9411 - val_loss: 8.5310\n",
      "\n",
      "[20210302-0935-56] Learning rate for epoch 43 is 0.00047855067532509565\n",
      "Epoch 43/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.2450\n",
      "Epoch 00043: val_loss did not improve from 8.16269\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 16.3311 - val_loss: 8.5621\n",
      "\n",
      "[20210302-0936-00] Learning rate for epoch 44 is 0.00047855067532509565\n",
      "Epoch 44/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.2555\n",
      "Epoch 00044: val_loss improved from 8.16269 to 8.08707, saving model to ./20210301-225844/heel_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.2008 - val_loss: 8.0871\n",
      "\n",
      "[20210302-0936-05] Learning rate for epoch 45 is 0.00047855067532509565\n",
      "Epoch 45/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.1999\n",
      "Epoch 00045: val_loss did not improve from 8.08707\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.1913 - val_loss: 8.9869\n",
      "\n",
      "[20210302-0936-09] Learning rate for epoch 46 is 0.00047855067532509565\n",
      "Epoch 46/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.8262\n",
      "Epoch 00046: val_loss did not improve from 8.08707\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.8262 - val_loss: 9.4640\n",
      "\n",
      "[20210302-0936-13] Learning rate for epoch 47 is 0.00047855067532509565\n",
      "Epoch 47/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.8618\n",
      "Epoch 00047: val_loss did not improve from 8.08707\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.8994 - val_loss: 8.7020\n",
      "\n",
      "[20210302-0936-17] Learning rate for epoch 48 is 0.00047855067532509565\n",
      "Epoch 48/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 16.4477\n",
      "Epoch 00048: val_loss did not improve from 8.08707\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.3573 - val_loss: 8.7211\n",
      "\n",
      "[20210302-0936-21] Learning rate for epoch 49 is 0.00047855067532509565\n",
      "Epoch 49/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.5832\n",
      "Epoch 00049: val_loss did not improve from 8.08707\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.6586 - val_loss: 9.6132\n",
      "\n",
      "[20210302-0936-25] Learning rate for epoch 50 is 0.00047855067532509565\n",
      "Epoch 50/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.6684\n",
      "Epoch 00050: val_loss did not improve from 8.08707\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.7285 - val_loss: 8.8293\n",
      "\n",
      "[20210302-0936-29] Learning rate for epoch 51 is 0.00047855067532509565\n",
      "Epoch 51/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.7381\n",
      "Epoch 00051: val_loss improved from 8.08707 to 7.95355, saving model to ./20210301-225844/heel_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 15.6594 - val_loss: 7.9536\n",
      "\n",
      "[20210302-0936-33] Learning rate for epoch 52 is 0.00047855067532509565\n",
      "Epoch 52/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.7595\n",
      "Epoch 00052: val_loss did not improve from 7.95355\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.5591 - val_loss: 8.8308\n",
      "\n",
      "[20210302-0936-37] Learning rate for epoch 53 is 0.00047855067532509565\n",
      "Epoch 53/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.3968\n",
      "Epoch 00053: val_loss did not improve from 7.95355\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 16.5108 - val_loss: 10.0919\n",
      "\n",
      "[20210302-0936-41] Learning rate for epoch 54 is 0.00047855067532509565\n",
      "Epoch 54/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.7752\n",
      "Epoch 00054: val_loss improved from 7.95355 to 7.72852, saving model to ./20210301-225844/heel_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 15.7752 - val_loss: 7.7285\n",
      "\n",
      "[20210302-0936-45] Learning rate for epoch 55 is 0.00047855067532509565\n",
      "Epoch 55/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.9326\n",
      "Epoch 00055: val_loss did not improve from 7.72852\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.9184 - val_loss: 8.4677\n",
      "\n",
      "[20210302-0936-49] Learning rate for epoch 56 is 0.00047855067532509565\n",
      "Epoch 56/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.7075\n",
      "Epoch 00056: val_loss did not improve from 7.72852\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.7075 - val_loss: 8.4143\n",
      "\n",
      "[20210302-0936-53] Learning rate for epoch 57 is 0.00047855067532509565\n",
      "Epoch 57/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.5811\n",
      "Epoch 00057: val_loss did not improve from 7.72852\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 15.5324 - val_loss: 8.4565\n",
      "\n",
      "[20210302-0936-57] Learning rate for epoch 58 is 0.00047855067532509565\n",
      "Epoch 58/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.9651\n",
      "Epoch 00058: val_loss did not improve from 7.72852\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.8653 - val_loss: 8.1568\n",
      "\n",
      "[20210302-0937-01] Learning rate for epoch 59 is 0.00047855067532509565\n",
      "Epoch 59/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.6405\n",
      "Epoch 00059: val_loss did not improve from 7.72852\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.6859 - val_loss: 8.3353\n",
      "\n",
      "[20210302-0937-05] Learning rate for epoch 60 is 0.00047855067532509565\n",
      "Epoch 60/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.7905\n",
      "Epoch 00060: val_loss did not improve from 7.72852\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.8386 - val_loss: 8.9909\n",
      "\n",
      "[20210302-0937-09] Learning rate for epoch 61 is 0.00047855067532509565\n",
      "Epoch 61/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.2633\n",
      "Epoch 00061: val_loss did not improve from 7.72852\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.2633 - val_loss: 8.9156\n",
      "\n",
      "[20210302-0937-13] Learning rate for epoch 62 is 0.00047855067532509565\n",
      "Epoch 62/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.8545\n",
      "Epoch 00062: val_loss did not improve from 7.72852\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.8106 - val_loss: 9.4514\n",
      "\n",
      "[20210302-0937-17] Learning rate for epoch 63 is 0.00047855067532509565\n",
      "Epoch 63/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.9074\n",
      "Epoch 00063: val_loss did not improve from 7.72852\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 14.9074 - val_loss: 9.0405\n",
      "\n",
      "[20210302-0937-21] Learning rate for epoch 64 is 0.00047855067532509565\n",
      "Epoch 64/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.8970\n",
      "Epoch 00064: val_loss did not improve from 7.72852\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.8970 - val_loss: 9.2783\n",
      "\n",
      "[20210302-0937-25] Learning rate for epoch 65 is 0.00047855067532509565\n",
      "Epoch 65/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.7445\n",
      "Epoch 00065: val_loss did not improve from 7.72852\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.9247 - val_loss: 9.4266\n",
      "\n",
      "[20210302-0937-29] Learning rate for epoch 66 is 0.00047855067532509565\n",
      "Epoch 66/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.4894\n",
      "Epoch 00066: val_loss did not improve from 7.72852\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.5179 - val_loss: 8.6718\n",
      "\n",
      "[20210302-0937-33] Learning rate for epoch 67 is 0.00047855067532509565\n",
      "Epoch 67/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.3935\n",
      "Epoch 00067: val_loss did not improve from 7.72852\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.3935 - val_loss: 9.1876\n",
      "\n",
      "[20210302-0937-37] Learning rate for epoch 68 is 0.00047855067532509565\n",
      "Epoch 68/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.8298\n",
      "Epoch 00068: val_loss did not improve from 7.72852\n",
      "20/20 [==============================] - 0s 24ms/step - loss: 15.8486 - val_loss: 9.0869\n",
      "\n",
      "[20210302-0937-41] Learning rate for epoch 69 is 0.00047855067532509565\n",
      "Epoch 69/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.5026\n",
      "Epoch 00069: val_loss did not improve from 7.72852\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.5026 - val_loss: 8.9356\n",
      "\n",
      "[20210302-0937-45] Learning rate for epoch 70 is 0.00047855067532509565\n",
      "Epoch 70/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 15.5408\n",
      "Epoch 00070: val_loss did not improve from 7.72852\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.5902 - val_loss: 9.1381\n",
      "\n",
      "[20210302-0937-49] Learning rate for epoch 71 is 0.00047855067532509565\n",
      "Epoch 71/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.8276\n",
      "Epoch 00071: val_loss did not improve from 7.72852\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.8577 - val_loss: 7.9729\n",
      "\n",
      "[20210302-0937-53] Learning rate for epoch 72 is 0.00047855067532509565\n",
      "Epoch 72/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.0948\n",
      "Epoch 00072: val_loss did not improve from 7.72852\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 16.1110 - val_loss: 9.4680\n",
      "\n",
      "[20210302-0937-57] Learning rate for epoch 73 is 0.00047855067532509565\n",
      "Epoch 73/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.1090\n",
      "Epoch 00073: val_loss did not improve from 7.72852\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 16.1090 - val_loss: 9.0019\n",
      "\n",
      "[20210302-0938-01] Learning rate for epoch 74 is 0.00047855067532509565\n",
      "Epoch 74/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.5629\n",
      "Epoch 00074: val_loss did not improve from 7.72852\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 15.5933 - val_loss: 8.7605\n",
      "\n",
      "*********************************** Start fine tune ***********************************\n",
      "tt: 195, nt:46, total layers:241\n",
      "*********************************** Start fine tune ***********************************\n",
      "Epoch 1/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 25.8444\n",
      "Epoch 00001: val_loss did not improve from 7.72852\n",
      "\n",
      "[20210302-0938-36] Learning rate for epoch 1 is 4.999999987376214e-07\n",
      "20/20 [==============================] - 14s 718ms/step - loss: 25.8444 - val_loss: 13.4073\n",
      "Epoch 2/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 22.7161\n",
      "Epoch 00002: val_loss did not improve from 7.72852\n",
      "\n",
      "[20210302-0938-41] Learning rate for epoch 2 is 5.044450517743826e-05\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 22.7161 - val_loss: 16.0541\n",
      "Epoch 3/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 18.3590\n",
      "Epoch 00003: val_loss did not improve from 7.72852\n",
      "\n",
      "[20210302-0939-01] Learning rate for epoch 3 is 0.00010028902033809572\n",
      "20/20 [==============================] - 17s 837ms/step - loss: 18.3590 - val_loss: 13.4966\n",
      "Epoch 4/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.0101\n",
      "Epoch 00004: val_loss did not improve from 7.72852\n",
      "\n",
      "[20210302-0939-06] Learning rate for epoch 4 is 0.00015003354928921908\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 16.0101 - val_loss: 11.5644\n",
      "Epoch 5/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.9124\n",
      "Epoch 00005: val_loss did not improve from 7.72852\n",
      "\n",
      "[20210302-0939-11] Learning rate for epoch 5 is 0.0001996780774788931\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 14.9124 - val_loss: 15.1980\n",
      "Epoch 6/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.2169\n",
      "Epoch 00006: val_loss did not improve from 7.72852\n",
      "\n",
      "[20210302-0939-16] Learning rate for epoch 6 is 0.000249222619459033\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 14.2169 - val_loss: 12.1753\n",
      "Epoch 7/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.3976\n",
      "Epoch 00007: val_loss did not improve from 7.72852\n",
      "\n",
      "[20210302-0939-21] Learning rate for epoch 7 is 0.0002986671752296388\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 13.3976 - val_loss: 10.9062\n",
      "Epoch 8/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 12.5353\n",
      "Epoch 00008: val_loss did not improve from 7.72852\n",
      "\n",
      "[20210302-0939-26] Learning rate for epoch 8 is 0.00034801175934262574\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 12.5353 - val_loss: 8.2018\n",
      "Epoch 9/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.3645\n",
      "Epoch 00009: val_loss did not improve from 7.72852\n",
      "\n",
      "[20210302-0939-35] Learning rate for epoch 9 is 0.00039725631359033287\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 13.3645 - val_loss: 11.1970\n",
      "Epoch 10/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 12.8611\n",
      "Epoch 00010: val_loss improved from 7.72852 to 7.61656, saving model to ./20210301-225844/heel_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-0939-41] Learning rate for epoch 10 is 0.0004464008961804211\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 12.8611 - val_loss: 7.6166\n",
      "Epoch 11/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 12.1902\n",
      "Epoch 00011: val_loss improved from 7.61656 to 5.94031, saving model to ./20210301-225844/heel_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-0939-46] Learning rate for epoch 11 is 0.0004954455071128905\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 12.1902 - val_loss: 5.9403\n",
      "Epoch 12/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.9522\n",
      "Epoch 00012: val_loss did not improve from 5.94031\n",
      "\n",
      "[20210302-0939-51] Learning rate for epoch 12 is 0.0005443901172839105\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 11.9522 - val_loss: 6.0660\n",
      "Epoch 13/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.8904\n",
      "Epoch 00013: val_loss improved from 5.94031 to 5.59582, saving model to ./20210301-225844/heel_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-0939-56] Learning rate for epoch 13 is 0.0005932347266934812\n",
      "20/20 [==============================] - 2s 89ms/step - loss: 11.8904 - val_loss: 5.5958\n",
      "Epoch 14/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.0383\n",
      "Epoch 00014: val_loss did not improve from 5.59582\n",
      "\n",
      "[20210302-0940-01] Learning rate for epoch 14 is 0.0006419793353416026\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 11.0383 - val_loss: 7.4184\n",
      "Epoch 15/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.1952\n",
      "Epoch 00015: val_loss did not improve from 5.59582\n",
      "\n",
      "[20210302-0940-06] Learning rate for epoch 15 is 0.0006906240014359355\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 10.1952 - val_loss: 5.8216\n",
      "Epoch 16/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.7867\n",
      "Epoch 00016: val_loss improved from 5.59582 to 5.47918, saving model to ./20210301-225844/heel_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-0940-11] Learning rate for epoch 16 is 0.0007391686085611582\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 9.7867 - val_loss: 5.4792\n",
      "Epoch 17/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.5469\n",
      "Epoch 00017: val_loss did not improve from 5.47918\n",
      "\n",
      "[20210302-0940-16] Learning rate for epoch 17 is 0.0007876132731325924\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 9.5469 - val_loss: 6.5221\n",
      "Epoch 18/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.5243\n",
      "Epoch 00018: val_loss improved from 5.47918 to 5.47016, saving model to ./20210301-225844/heel_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-0940-22] Learning rate for epoch 18 is 0.0008359579369425774\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 8.5243 - val_loss: 5.4702\n",
      "Epoch 19/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.0695\n",
      "Epoch 00019: val_loss did not improve from 5.47016\n",
      "\n",
      "[20210302-0940-27] Learning rate for epoch 19 is 0.000884202599991113\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 9.0695 - val_loss: 5.5655\n",
      "Epoch 20/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.4481\n",
      "Epoch 00020: val_loss improved from 5.47016 to 4.34692, saving model to ./20210301-225844/heel_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-0940-32] Learning rate for epoch 20 is 0.0009323473204858601\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 8.4481 - val_loss: 4.3469\n",
      "Epoch 21/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.6263\n",
      "Epoch 00021: val_loss did not improve from 4.34692\n",
      "\n",
      "[20210302-0940-37] Learning rate for epoch 21 is 0.000980391982011497\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 8.6263 - val_loss: 5.0946\n",
      "Epoch 22/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.0127\n",
      "Epoch 00022: val_loss did not improve from 4.34692\n",
      "\n",
      "[20210302-0940-42] Learning rate for epoch 22 is 0.0010283367009833455\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 8.0127 - val_loss: 21.8119\n",
      "Epoch 23/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.2289\n",
      "Epoch 00023: val_loss did not improve from 4.34692\n",
      "\n",
      "[20210302-0940-47] Learning rate for epoch 23 is 0.0010761814191937447\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 8.2289 - val_loss: 31.8571\n",
      "Epoch 24/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.2147\n",
      "Epoch 00024: val_loss did not improve from 4.34692\n",
      "\n",
      "[20210302-0940-52] Learning rate for epoch 24 is 0.0011239261366426945\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 8.2147 - val_loss: 25.7807\n",
      "Epoch 25/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.0524\n",
      "Epoch 00025: val_loss did not improve from 4.34692\n",
      "\n",
      "[20210302-0940-56] Learning rate for epoch 25 is 0.001171570853330195\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 8.0524 - val_loss: 22.1715\n",
      "Epoch 26/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.2355\n",
      "Epoch 00026: val_loss did not improve from 4.34692\n",
      "\n",
      "[20210302-0941-01] Learning rate for epoch 26 is 0.001219115569256246\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 8.2355 - val_loss: 21.5437\n",
      "Epoch 27/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.1049\n",
      "Epoch 00027: val_loss did not improve from 4.34692\n",
      "\n",
      "[20210302-0941-06] Learning rate for epoch 27 is 0.0012665604008361697\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 8.1049 - val_loss: 16.7565\n",
      "Epoch 28/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5905\n",
      "Epoch 00028: val_loss did not improve from 4.34692\n",
      "\n",
      "[20210302-0941-11] Learning rate for epoch 28 is 0.0013139051152393222\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.5905 - val_loss: 13.7387\n",
      "Epoch 29/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.9772\n",
      "Epoch 00029: val_loss did not improve from 4.34692\n",
      "\n",
      "[20210302-0941-16] Learning rate for epoch 29 is 0.0013611499452963471\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.9772 - val_loss: 10.4515\n",
      "Epoch 30/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.0285\n",
      "Epoch 00030: val_loss did not improve from 4.34692\n",
      "\n",
      "[20210302-0941-21] Learning rate for epoch 30 is 0.001408294658176601\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 8.0285 - val_loss: 9.3203\n",
      "Epoch 31/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6510\n",
      "Epoch 00031: val_loss did not improve from 4.34692\n",
      "\n",
      "[20210302-0941-26] Learning rate for epoch 31 is 0.0014553394867107272\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.6510 - val_loss: 8.0234\n",
      "Epoch 32/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5882\n",
      "Epoch 00032: val_loss did not improve from 4.34692\n",
      "\n",
      "[20210302-0941-31] Learning rate for epoch 32 is 0.0015022843144834042\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.5882 - val_loss: 5.0279\n",
      "Epoch 33/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5085\n",
      "Epoch 00033: val_loss did not improve from 4.34692\n",
      "\n",
      "[20210302-0941-36] Learning rate for epoch 33 is 0.0015491291414946318\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 7.5085 - val_loss: 5.6266\n",
      "Epoch 34/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3780\n",
      "Epoch 00034: val_loss did not improve from 4.34692\n",
      "\n",
      "[20210302-0941-41] Learning rate for epoch 34 is 0.00159587396774441\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.3780 - val_loss: 4.3751\n",
      "Epoch 35/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1901\n",
      "Epoch 00035: val_loss did not improve from 4.34692\n",
      "\n",
      "[20210302-0941-46] Learning rate for epoch 35 is 0.001642518793232739\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.1901 - val_loss: 4.8909\n",
      "Epoch 36/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8708\n",
      "Epoch 00036: val_loss did not improve from 4.34692\n",
      "\n",
      "[20210302-0941-51] Learning rate for epoch 36 is 0.0016890636179596186\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.8708 - val_loss: 4.6303\n",
      "Epoch 37/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3462\n",
      "Epoch 00037: val_loss did not improve from 4.34692\n",
      "\n",
      "[20210302-0941-55] Learning rate for epoch 37 is 0.0017355084419250488\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.3462 - val_loss: 5.7545\n",
      "Epoch 38/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3665\n",
      "Epoch 00038: val_loss did not improve from 4.34692\n",
      "\n",
      "[20210302-0942-00] Learning rate for epoch 38 is 0.0017818533815443516\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.3665 - val_loss: 5.8715\n",
      "Epoch 39/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3255\n",
      "Epoch 00039: val_loss did not improve from 4.34692\n",
      "\n",
      "[20210302-0942-05] Learning rate for epoch 39 is 0.0018280982039868832\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.3255 - val_loss: 7.6444\n",
      "Epoch 40/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6305\n",
      "Epoch 00040: val_loss did not improve from 4.34692\n",
      "\n",
      "[20210302-0942-10] Learning rate for epoch 40 is 0.0018742431420832872\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 7.6305 - val_loss: 8.8329\n",
      "Epoch 41/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6158\n",
      "Epoch 00041: val_loss did not improve from 4.34692\n",
      "\n",
      "[20210302-0942-15] Learning rate for epoch 41 is 0.0019202879630029202\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.6158 - val_loss: 7.5564\n",
      "Epoch 42/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0064\n",
      "Epoch 00042: val_loss did not improve from 4.34692\n",
      "\n",
      "[20210302-0942-20] Learning rate for epoch 42 is 0.0019662328995764256\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.0064 - val_loss: 7.6266\n",
      "Epoch 43/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5459\n",
      "Epoch 00043: val_loss did not improve from 4.34692\n",
      "\n",
      "[20210302-0942-25] Learning rate for epoch 43 is 0.00201207771897316\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.5459 - val_loss: 6.9079\n",
      "Epoch 44/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9087\n",
      "Epoch 00044: val_loss did not improve from 4.34692\n",
      "\n",
      "[20210302-0942-30] Learning rate for epoch 44 is 0.0020578226540237665\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.9087 - val_loss: 6.9457\n",
      "Epoch 45/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1913\n",
      "Epoch 00045: val_loss did not improve from 4.34692\n",
      "\n",
      "[20210302-0942-35] Learning rate for epoch 45 is 0.0021034677047282457\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.1913 - val_loss: 6.0670\n",
      "Epoch 46/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0907\n",
      "Epoch 00046: val_loss did not improve from 4.34692\n",
      "\n",
      "[20210302-0942-39] Learning rate for epoch 46 is 0.002149012638255954\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 7.0907 - val_loss: 5.0823\n",
      "Epoch 47/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3718\n",
      "Epoch 00047: val_loss did not improve from 4.34692\n",
      "\n",
      "[20210302-0942-44] Learning rate for epoch 47 is 0.0021944576874375343\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 7.3718 - val_loss: 6.8916\n",
      "Epoch 48/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7083\n",
      "Epoch 00048: val_loss did not improve from 4.34692\n",
      "\n",
      "[20210302-0942-49] Learning rate for epoch 48 is 0.0022398026194423437\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.7083 - val_loss: 9.8070\n",
      "Epoch 49/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7066\n",
      "Epoch 00049: val_loss did not improve from 4.34692\n",
      "\n",
      "[20210302-0942-54] Learning rate for epoch 49 is 0.002285047434270382\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.7066 - val_loss: 7.6828\n",
      "Epoch 50/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0869\n",
      "Epoch 00050: val_loss did not improve from 4.34692\n",
      "\n",
      "[20210302-0942-59] Learning rate for epoch 50 is 0.0023301925975829363\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.0869 - val_loss: 7.8878\n",
      "Epoch 51/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7786\n",
      "Epoch 00051: val_loss did not improve from 4.34692\n",
      "\n",
      "[20210302-0943-04] Learning rate for epoch 51 is 0.002375237410888076\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.7786 - val_loss: 16.5703\n",
      "Epoch 52/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5456\n",
      "Epoch 00052: val_loss did not improve from 4.34692\n",
      "\n",
      "[20210302-0943-09] Learning rate for epoch 52 is 0.0024201825726777315\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.5456 - val_loss: 53.6316\n",
      "Epoch 53/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9253\n",
      "Epoch 00053: val_loss did not improve from 4.34692\n",
      "\n",
      "[20210302-0943-14] Learning rate for epoch 53 is 0.002465027617290616\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.9253 - val_loss: 14.2023\n",
      "Epoch 54/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3791\n",
      "Epoch 00054: val_loss did not improve from 4.34692\n",
      "\n",
      "[20210302-0943-19] Learning rate for epoch 54 is 0.0025097725447267294\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 7.3791 - val_loss: 6.9853\n",
      "Epoch 55/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9216\n",
      "Epoch 00055: val_loss did not improve from 4.34692\n",
      "\n",
      "[20210302-0943-23] Learning rate for epoch 55 is 0.0025544175878167152\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.9216 - val_loss: 6.2820\n",
      "Epoch 56/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9109\n",
      "Epoch 00056: val_loss did not improve from 4.34692\n",
      "\n",
      "[20210302-0943-28] Learning rate for epoch 56 is 0.00259896251372993\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.9109 - val_loss: 4.9223\n",
      "Epoch 57/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6115\n",
      "Epoch 00057: val_loss did not improve from 4.34692\n",
      "\n",
      "[20210302-0943-33] Learning rate for epoch 57 is 0.0026434077881276608\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.6115 - val_loss: 5.1626\n",
      "Epoch 58/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0990\n",
      "Epoch 00058: val_loss did not improve from 4.34692\n",
      "\n",
      "[20210302-0943-38] Learning rate for epoch 58 is 0.0026877527125179768\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.0990 - val_loss: 5.1755\n",
      "Epoch 59/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7312\n",
      "Epoch 00059: val_loss did not improve from 4.34692\n",
      "\n",
      "[20210302-0943-43] Learning rate for epoch 59 is 0.0027319977525621653\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.7312 - val_loss: 8.5826\n",
      "Epoch 60/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5117\n",
      "Epoch 00060: val_loss did not improve from 4.34692\n",
      "\n",
      "[20210302-0943-48] Learning rate for epoch 60 is 0.0027761429082602262\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.5117 - val_loss: 6.2988\n",
      "Epoch 61/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9617\n",
      "Epoch 00061: val_loss did not improve from 4.34692\n",
      "\n",
      "[20210302-0943-53] Learning rate for epoch 61 is 0.002820187946781516\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.9617 - val_loss: 5.6855\n",
      "Epoch 62/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7669\n",
      "Epoch 00062: val_loss did not improve from 4.34692\n",
      "\n",
      "[20210302-0943-58] Learning rate for epoch 62 is 0.0028641331009566784\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.7669 - val_loss: 5.3232\n",
      "Epoch 63/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7775\n",
      "Epoch 00063: val_loss did not improve from 4.34692\n",
      "\n",
      "[20210302-0944-03] Learning rate for epoch 63 is 0.0029079781379550695\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.7775 - val_loss: 5.6439\n",
      "Epoch 64/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9528\n",
      "Epoch 00064: val_loss improved from 4.34692 to 4.03563, saving model to ./20210301-225844/heel_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-0944-08] Learning rate for epoch 64 is 0.002951723290607333\n",
      "20/20 [==============================] - 2s 93ms/step - loss: 6.9528 - val_loss: 4.0356\n",
      "Epoch 65/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9463\n",
      "Epoch 00065: val_loss improved from 4.03563 to 3.79365, saving model to ./20210301-225844/heel_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-0944-14] Learning rate for epoch 65 is 0.0029953685589134693\n",
      "20/20 [==============================] - 2s 92ms/step - loss: 6.9463 - val_loss: 3.7936\n",
      "Epoch 66/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7676\n",
      "Epoch 00066: val_loss did not improve from 3.79365\n",
      "\n",
      "[20210302-0944-18] Learning rate for epoch 66 is 0.0030389137100428343\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.7676 - val_loss: 5.0320\n",
      "Epoch 67/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0113\n",
      "Epoch 00067: val_loss did not improve from 3.79365\n",
      "\n",
      "[20210302-0944-23] Learning rate for epoch 67 is 0.003082358743995428\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.0113 - val_loss: 4.2013\n",
      "Epoch 68/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6697\n",
      "Epoch 00068: val_loss did not improve from 3.79365\n",
      "\n",
      "[20210302-0944-28] Learning rate for epoch 68 is 0.0031257038936018944\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.6697 - val_loss: 4.4737\n",
      "Epoch 69/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3035\n",
      "Epoch 00069: val_loss improved from 3.79365 to 3.62986, saving model to ./20210301-225844/heel_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-0944-34] Learning rate for epoch 69 is 0.003168949158862233\n",
      "20/20 [==============================] - 2s 89ms/step - loss: 6.3035 - val_loss: 3.6299\n",
      "Epoch 70/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1182\n",
      "Epoch 00070: val_loss did not improve from 3.62986\n",
      "\n",
      "[20210302-0944-39] Learning rate for epoch 70 is 0.0032120943069458008\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.1182 - val_loss: 5.4381\n",
      "Epoch 71/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6765\n",
      "Epoch 00071: val_loss did not improve from 3.62986\n",
      "\n",
      "[20210302-0944-43] Learning rate for epoch 71 is 0.003255139570683241\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.6765 - val_loss: 7.3568\n",
      "Epoch 72/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9808\n",
      "Epoch 00072: val_loss did not improve from 3.62986\n",
      "\n",
      "[20210302-0944-48] Learning rate for epoch 72 is 0.00329808471724391\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.9808 - val_loss: 5.8526\n",
      "Epoch 73/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6943\n",
      "Epoch 00073: val_loss did not improve from 3.62986\n",
      "\n",
      "[20210302-0944-53] Learning rate for epoch 73 is 0.0033409299794584513\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.6943 - val_loss: 3.7422\n",
      "Epoch 74/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6088\n",
      "Epoch 00074: val_loss did not improve from 3.62986\n",
      "\n",
      "[20210302-0944-58] Learning rate for epoch 74 is 0.0033836751244962215\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.6088 - val_loss: 4.8154\n",
      "Epoch 75/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8129\n",
      "Epoch 00075: val_loss improved from 3.62986 to 3.58017, saving model to ./20210301-225844/heel_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-0945-03] Learning rate for epoch 75 is 0.0034263203851878643\n",
      "20/20 [==============================] - 2s 91ms/step - loss: 6.8129 - val_loss: 3.5802\n",
      "Epoch 76/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5788\n",
      "Epoch 00076: val_loss did not improve from 3.58017\n",
      "\n",
      "[20210302-0945-08] Learning rate for epoch 76 is 0.003468865528702736\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.5788 - val_loss: 4.7281\n",
      "Epoch 77/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7364\n",
      "Epoch 00077: val_loss did not improve from 3.58017\n",
      "\n",
      "[20210302-0945-13] Learning rate for epoch 77 is 0.00351131078787148\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.7364 - val_loss: 4.8206\n",
      "Epoch 78/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6836\n",
      "Epoch 00078: val_loss did not improve from 3.58017\n",
      "\n",
      "[20210302-0945-18] Learning rate for epoch 78 is 0.0035536561626940966\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.6836 - val_loss: 4.0540\n",
      "Epoch 79/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7362\n",
      "Epoch 00079: val_loss did not improve from 3.58017\n",
      "\n",
      "[20210302-0945-23] Learning rate for epoch 79 is 0.003595901420339942\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.7362 - val_loss: 4.1840\n",
      "Epoch 80/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6621\n",
      "Epoch 00080: val_loss did not improve from 3.58017\n",
      "\n",
      "[20210302-0945-28] Learning rate for epoch 80 is 0.00363804679363966\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.6621 - val_loss: 6.1008\n",
      "Epoch 81/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3607\n",
      "Epoch 00081: val_loss did not improve from 3.58017\n",
      "\n",
      "[20210302-0945-33] Learning rate for epoch 81 is 0.0036800920497626066\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.3607 - val_loss: 4.7133\n",
      "Epoch 82/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6441\n",
      "Epoch 00082: val_loss did not improve from 3.58017\n",
      "\n",
      "[20210302-0945-38] Learning rate for epoch 82 is 0.003722037188708782\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.6441 - val_loss: 814.1603\n",
      "Epoch 83/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3017\n",
      "Epoch 00083: val_loss did not improve from 3.58017\n",
      "\n",
      "[20210302-0945-42] Learning rate for epoch 83 is 0.003763882676139474\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.3017 - val_loss: 1392.8036\n",
      "Epoch 84/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4865\n",
      "Epoch 00084: val_loss did not improve from 3.58017\n",
      "\n",
      "[20210302-0945-47] Learning rate for epoch 84 is 0.0038056280463933945\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.4865 - val_loss: 120.0126\n",
      "Epoch 85/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8824\n",
      "Epoch 00085: val_loss did not improve from 3.58017\n",
      "\n",
      "[20210302-0945-52] Learning rate for epoch 85 is 0.003847273299470544\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 6.8824 - val_loss: 8.2413\n",
      "Epoch 86/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2706\n",
      "Epoch 00086: val_loss did not improve from 3.58017\n",
      "\n",
      "[20210302-0945-57] Learning rate for epoch 86 is 0.0038888186682015657\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.2706 - val_loss: 4.3035\n",
      "Epoch 87/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3112\n",
      "Epoch 00087: val_loss did not improve from 3.58017\n",
      "\n",
      "[20210302-0946-02] Learning rate for epoch 87 is 0.00393026415258646\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.3112 - val_loss: 3.8177\n",
      "Epoch 88/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5684\n",
      "Epoch 00088: val_loss did not improve from 3.58017\n",
      "\n",
      "[20210302-0946-07] Learning rate for epoch 88 is 0.00397160928696394\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.5684 - val_loss: 3.9861\n",
      "Epoch 89/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6134\n",
      "Epoch 00089: val_loss did not improve from 3.58017\n",
      "\n",
      "[20210302-0946-12] Learning rate for epoch 89 is 0.004012854769825935\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.6134 - val_loss: 7.1755\n",
      "Epoch 90/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4046\n",
      "Epoch 00090: val_loss did not improve from 3.58017\n",
      "\n",
      "[20210302-0946-17] Learning rate for epoch 90 is 0.00405400013551116\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.4046 - val_loss: 4.8550\n",
      "Epoch 91/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5417\n",
      "Epoch 00091: val_loss did not improve from 3.58017\n",
      "\n",
      "[20210302-0946-22] Learning rate for epoch 91 is 0.004095045384019613\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.5417 - val_loss: 62.9343\n",
      "Epoch 92/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3700\n",
      "Epoch 00092: val_loss did not improve from 3.58017\n",
      "\n",
      "[20210302-0946-26] Learning rate for epoch 92 is 0.004135990981012583\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.3700 - val_loss: 343.1233\n",
      "Epoch 93/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5024\n",
      "Epoch 00093: val_loss did not improve from 3.58017\n",
      "\n",
      "[20210302-0946-31] Learning rate for epoch 93 is 0.004176836460828781\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.5024 - val_loss: 79.7140\n",
      "Epoch 94/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1385\n",
      "Epoch 00094: val_loss did not improve from 3.58017\n",
      "\n",
      "[20210302-0946-36] Learning rate for epoch 94 is 0.004217581823468208\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.1385 - val_loss: 109.0107\n",
      "Epoch 95/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4405\n",
      "Epoch 00095: val_loss did not improve from 3.58017\n",
      "\n",
      "[20210302-0946-41] Learning rate for epoch 95 is 0.004258227068930864\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.4405 - val_loss: 5.2013\n",
      "Epoch 96/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3245\n",
      "Epoch 00096: val_loss did not improve from 3.58017\n",
      "\n",
      "[20210302-0946-46] Learning rate for epoch 96 is 0.0042987726628780365\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 7.3245 - val_loss: 4.3608\n",
      "Epoch 97/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6602\n",
      "Epoch 00097: val_loss did not improve from 3.58017\n",
      "\n",
      "[20210302-0946-51] Learning rate for epoch 97 is 0.0043392181396484375\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.6602 - val_loss: 14.0964\n",
      "Epoch 98/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3907\n",
      "Epoch 00098: val_loss did not improve from 3.58017\n",
      "\n",
      "[20210302-0946-55] Learning rate for epoch 98 is 0.004379563499242067\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.3907 - val_loss: 23.1210\n",
      "Epoch 99/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5724\n",
      "Epoch 00099: val_loss did not improve from 3.58017\n",
      "\n",
      "[20210302-0947-00] Learning rate for epoch 99 is 0.004419809207320213\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.5724 - val_loss: 12.8365\n",
      "Epoch 100/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9361\n",
      "Epoch 00100: val_loss did not improve from 3.58017\n",
      "\n",
      "[20210302-0947-05] Learning rate for epoch 100 is 0.004459954332560301\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.9361 - val_loss: 4.6553\n",
      "Epoch 101/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6081\n",
      "Epoch 00101: val_loss did not improve from 3.58017\n",
      "\n",
      "[20210302-0947-10] Learning rate for epoch 101 is 4.4999998749517545e-07\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.6081 - val_loss: 4.0557\n",
      "Epoch 102/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3461\n",
      "Epoch 00102: val_loss improved from 3.58017 to 3.47255, saving model to ./20210301-225844/heel_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-0947-15] Learning rate for epoch 102 is 0.00018023152370005846\n",
      "20/20 [==============================] - 2s 89ms/step - loss: 7.3461 - val_loss: 3.4725\n",
      "Epoch 103/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9508\n",
      "Epoch 00103: val_loss did not improve from 3.47255\n",
      "\n",
      "[20210302-0947-20] Learning rate for epoch 103 is 0.000359613070031628\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.9508 - val_loss: 3.9269\n",
      "Epoch 104/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7640\n",
      "Epoch 00104: val_loss did not improve from 3.47255\n",
      "\n",
      "[20210302-0947-25] Learning rate for epoch 104 is 0.0005385946715250611\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.7640 - val_loss: 3.7775\n",
      "Epoch 105/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3290\n",
      "Epoch 00105: val_loss did not improve from 3.47255\n",
      "\n",
      "[20210302-0947-30] Learning rate for epoch 105 is 0.0007171762990765274\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.3290 - val_loss: 3.9493\n",
      "Epoch 106/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9818\n",
      "Epoch 00106: val_loss did not improve from 3.47255\n",
      "\n",
      "[20210302-0947-35] Learning rate for epoch 106 is 0.0008953579817898571\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.9818 - val_loss: 3.8029\n",
      "Epoch 107/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2603\n",
      "Epoch 00107: val_loss did not improve from 3.47255\n",
      "\n",
      "[20210302-0947-40] Learning rate for epoch 107 is 0.0010731397196650505\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.2603 - val_loss: 4.0008\n",
      "Epoch 108/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8925\n",
      "Epoch 00108: val_loss improved from 3.47255 to 3.44314, saving model to ./20210301-225844/heel_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-0947-45] Learning rate for epoch 108 is 0.0012505215127021074\n",
      "20/20 [==============================] - 2s 91ms/step - loss: 5.8925 - val_loss: 3.4431\n",
      "Epoch 109/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7405\n",
      "Epoch 00109: val_loss did not improve from 3.44314\n",
      "\n",
      "[20210302-0947-50] Learning rate for epoch 109 is 0.001427503302693367\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.7405 - val_loss: 4.0225\n",
      "Epoch 110/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3125\n",
      "Epoch 00110: val_loss did not improve from 3.44314\n",
      "\n",
      "[20210302-0947-55] Learning rate for epoch 110 is 0.0016040850896388292\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.3125 - val_loss: 4.4297\n",
      "Epoch 111/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3581\n",
      "Epoch 00111: val_loss did not improve from 3.44314\n",
      "\n",
      "[20210302-0948-00] Learning rate for epoch 111 is 0.001780266989953816\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.3581 - val_loss: 4.0091\n",
      "Epoch 112/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4007\n",
      "Epoch 00112: val_loss did not improve from 3.44314\n",
      "\n",
      "[20210302-0948-04] Learning rate for epoch 112 is 0.0019560488872230053\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.4007 - val_loss: 3.4610\n",
      "Epoch 113/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3289\n",
      "Epoch 00113: val_loss improved from 3.44314 to 3.13163, saving model to ./20210301-225844/heel_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-0948-10] Learning rate for epoch 113 is 0.002131430897861719\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 6.3289 - val_loss: 3.1316\n",
      "Epoch 114/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2012\n",
      "Epoch 00114: val_loss did not improve from 3.13163\n",
      "\n",
      "[20210302-0948-15] Learning rate for epoch 114 is 0.0023064129054546356\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.2012 - val_loss: 4.3078\n",
      "Epoch 115/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2476\n",
      "Epoch 00115: val_loss did not improve from 3.13163\n",
      "\n",
      "[20210302-0948-20] Learning rate for epoch 115 is 0.0024809949100017548\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.2476 - val_loss: 3.4280\n",
      "Epoch 116/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3737\n",
      "Epoch 00116: val_loss improved from 3.13163 to 3.11969, saving model to ./20210301-225844/heel_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-0948-25] Learning rate for epoch 116 is 0.0026551769115030766\n",
      "20/20 [==============================] - 2s 87ms/step - loss: 6.3737 - val_loss: 3.1197\n",
      "Epoch 117/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6894\n",
      "Epoch 00117: val_loss did not improve from 3.11969\n",
      "\n",
      "[20210302-0948-30] Learning rate for epoch 117 is 0.0028289591427892447\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.6894 - val_loss: 5.2627\n",
      "Epoch 118/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4225\n",
      "Epoch 00118: val_loss did not improve from 3.11969\n",
      "\n",
      "[20210302-0948-35] Learning rate for epoch 118 is 0.0030023413710296154\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.4225 - val_loss: 3.7205\n",
      "Epoch 119/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2133\n",
      "Epoch 00119: val_loss did not improve from 3.11969\n",
      "\n",
      "[20210302-0948-40] Learning rate for epoch 119 is 0.003175323596224189\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.2133 - val_loss: 3.5291\n",
      "Epoch 120/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1587\n",
      "Epoch 00120: val_loss did not improve from 3.11969\n",
      "\n",
      "[20210302-0948-44] Learning rate for epoch 120 is 0.003347905818372965\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.1587 - val_loss: 5.0298\n",
      "Epoch 121/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3167\n",
      "Epoch 00121: val_loss improved from 3.11969 to 2.97270, saving model to ./20210301-225844/heel_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-0948-50] Learning rate for epoch 121 is 0.0035200880374759436\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 6.3167 - val_loss: 2.9727\n",
      "Epoch 122/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0901\n",
      "Epoch 00122: val_loss did not improve from 2.97270\n",
      "\n",
      "[20210302-0948-55] Learning rate for epoch 122 is 0.003691870253533125\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.0901 - val_loss: 4.3431\n",
      "Epoch 123/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1000\n",
      "Epoch 00123: val_loss did not improve from 2.97270\n",
      "\n",
      "[20210302-0949-00] Learning rate for epoch 123 is 0.0038632526993751526\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.1000 - val_loss: 5.6081\n",
      "Epoch 124/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0224\n",
      "Epoch 00124: val_loss did not improve from 2.97270\n",
      "\n",
      "[20210302-0949-04] Learning rate for epoch 124 is 0.004034235142171383\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.0224 - val_loss: 3.6752\n",
      "Epoch 125/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1782\n",
      "Epoch 00125: val_loss did not improve from 2.97270\n",
      "\n",
      "[20210302-0949-09] Learning rate for epoch 125 is 0.004204817581921816\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.1782 - val_loss: 3.1882\n",
      "Epoch 126/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3349\n",
      "Epoch 00126: val_loss did not improve from 2.97270\n",
      "\n",
      "[20210302-0949-14] Learning rate for epoch 126 is 0.0043750000186264515\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.3349 - val_loss: 3.0815\n",
      "Epoch 127/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5616\n",
      "Epoch 00127: val_loss did not improve from 2.97270\n",
      "\n",
      "[20210302-0949-19] Learning rate for epoch 127 is 0.0041952175088226795\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.5616 - val_loss: 3.4232\n",
      "Epoch 128/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5972\n",
      "Epoch 00128: val_loss did not improve from 2.97270\n",
      "\n",
      "[20210302-0949-24] Learning rate for epoch 128 is 0.004015835002064705\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.5972 - val_loss: 4.5923\n",
      "Epoch 129/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4632\n",
      "Epoch 00129: val_loss did not improve from 2.97270\n",
      "\n",
      "[20210302-0949-29] Learning rate for epoch 129 is 0.003836852265521884\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.4632 - val_loss: 4.2144\n",
      "Epoch 130/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6688\n",
      "Epoch 00130: val_loss did not improve from 2.97270\n",
      "\n",
      "[20210302-0949-34] Learning rate for epoch 130 is 0.003658269764855504\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.6688 - val_loss: 3.9171\n",
      "Epoch 131/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0194\n",
      "Epoch 00131: val_loss did not improve from 2.97270\n",
      "\n",
      "[20210302-0949-39] Learning rate for epoch 131 is 0.003480087034404278\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.0194 - val_loss: 3.4203\n",
      "Epoch 132/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3595\n",
      "Epoch 00132: val_loss did not improve from 2.97270\n",
      "\n",
      "[20210302-0949-44] Learning rate for epoch 132 is 0.003302304306998849\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.3595 - val_loss: 3.4739\n",
      "Epoch 133/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6838\n",
      "Epoch 00133: val_loss did not improve from 2.97270\n",
      "\n",
      "[20210302-0949-48] Learning rate for epoch 133 is 0.0031249215826392174\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.6838 - val_loss: 3.1863\n",
      "Epoch 134/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1027\n",
      "Epoch 00134: val_loss did not improve from 2.97270\n",
      "\n",
      "[20210302-0949-53] Learning rate for epoch 134 is 0.0029479386284947395\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.1027 - val_loss: 3.7845\n",
      "Epoch 135/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8240\n",
      "Epoch 00135: val_loss did not improve from 2.97270\n",
      "\n",
      "[20210302-0949-58] Learning rate for epoch 135 is 0.0027713559102267027\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.8240 - val_loss: 3.7245\n",
      "Epoch 136/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2032\n",
      "Epoch 00136: val_loss did not improve from 2.97270\n",
      "\n",
      "[20210302-0950-03] Learning rate for epoch 136 is 0.0025951729621738195\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.2032 - val_loss: 3.5517\n",
      "Epoch 137/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9790\n",
      "Epoch 00137: val_loss did not improve from 2.97270\n",
      "\n",
      "[20210302-0950-08] Learning rate for epoch 137 is 0.0024193900171667337\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.9790 - val_loss: 3.5390\n",
      "Epoch 138/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9126\n",
      "Epoch 00138: val_loss did not improve from 2.97270\n",
      "\n",
      "[20210302-0950-13] Learning rate for epoch 138 is 0.0022440070752054453\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.9126 - val_loss: 3.4095\n",
      "Epoch 139/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9231\n",
      "Epoch 00139: val_loss did not improve from 2.97270\n",
      "\n",
      "[20210302-0950-18] Learning rate for epoch 139 is 0.002069024136289954\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.9231 - val_loss: 3.4059\n",
      "Epoch 140/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0418\n",
      "Epoch 00140: val_loss did not improve from 2.97270\n",
      "\n",
      "[20210302-0950-23] Learning rate for epoch 140 is 0.0018944410840049386\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.0418 - val_loss: 3.8380\n",
      "Epoch 141/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8880\n",
      "Epoch 00141: val_loss did not improve from 2.97270\n",
      "\n",
      "[20210302-0950-27] Learning rate for epoch 141 is 0.0017202580347657204\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.8880 - val_loss: 3.3514\n",
      "Epoch 142/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3591\n",
      "Epoch 00142: val_loss did not improve from 2.97270\n",
      "\n",
      "[20210302-0950-32] Learning rate for epoch 142 is 0.0015464748721569777\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.3591 - val_loss: 3.6207\n",
      "Epoch 143/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7668\n",
      "Epoch 00143: val_loss did not improve from 2.97270\n",
      "\n",
      "[20210302-0950-37] Learning rate for epoch 143 is 0.0013730917125940323\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.7668 - val_loss: 3.4027\n",
      "Epoch 144/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6271\n",
      "Epoch 00144: val_loss did not improve from 2.97270\n",
      "\n",
      "[20210302-0950-42] Learning rate for epoch 144 is 0.0012001085560768843\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.6271 - val_loss: 3.0641\n",
      "Epoch 145/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2996\n",
      "Epoch 00145: val_loss did not improve from 2.97270\n",
      "\n",
      "[20210302-0950-47] Learning rate for epoch 145 is 0.0010275252861902118\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.2996 - val_loss: 3.3944\n",
      "Epoch 146/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7771\n",
      "Epoch 00146: val_loss did not improve from 2.97270\n",
      "\n",
      "[20210302-0950-52] Learning rate for epoch 146 is 0.0008553420193493366\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.7771 - val_loss: 3.2364\n",
      "Epoch 147/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5114\n",
      "Epoch 00147: val_loss did not improve from 2.97270\n",
      "\n",
      "[20210302-0950-57] Learning rate for epoch 147 is 0.0006835586973465979\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.5114 - val_loss: 3.1955\n",
      "Epoch 148/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5143\n",
      "Epoch 00148: val_loss did not improve from 2.97270\n",
      "\n",
      "[20210302-0951-02] Learning rate for epoch 148 is 0.0005121753201819956\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.5143 - val_loss: 3.1756\n",
      "Epoch 149/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4839\n",
      "Epoch 00149: val_loss did not improve from 2.97270\n",
      "\n",
      "[20210302-0951-07] Learning rate for epoch 149 is 0.00034119191695936024\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.4839 - val_loss: 3.1772\n",
      "Epoch 150/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6456\n",
      "Epoch 00150: val_loss did not improve from 2.97270\n",
      "\n",
      "[20210302-0951-12] Learning rate for epoch 150 is 0.00017060847312677652\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6456 - val_loss: 3.1435\n",
      "Epoch 151/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2415\n",
      "Epoch 00151: val_loss did not improve from 2.97270\n",
      "\n",
      "[20210302-0951-17] Learning rate for epoch 151 is 4.249999960848072e-07\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.2415 - val_loss: 3.1402\n",
      "Epoch 152/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6446\n",
      "Epoch 00152: val_loss did not improve from 2.97270\n",
      "\n",
      "[20210302-0951-22] Learning rate for epoch 152 is 0.00017020752420648932\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.6446 - val_loss: 3.1264\n",
      "Epoch 153/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6673\n",
      "Epoch 00153: val_loss did not improve from 2.97270\n",
      "\n",
      "[20210302-0951-26] Learning rate for epoch 153 is 0.0003395900712348521\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6673 - val_loss: 3.1845\n",
      "Epoch 154/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7657\n",
      "Epoch 00154: val_loss did not improve from 2.97270\n",
      "\n",
      "[20210302-0951-31] Learning rate for epoch 154 is 0.0005085726734250784\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.7657 - val_loss: 3.1673\n",
      "Epoch 155/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7851\n",
      "Epoch 00155: val_loss did not improve from 2.97270\n",
      "\n",
      "[20210302-0951-36] Learning rate for epoch 155 is 0.0006771553307771683\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.7851 - val_loss: 3.0520\n",
      "Epoch 156/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6325\n",
      "Epoch 00156: val_loss did not improve from 2.97270\n",
      "\n",
      "[20210302-0951-41] Learning rate for epoch 156 is 0.0008453379850834608\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.6325 - val_loss: 3.0709\n",
      "Epoch 157/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9016\n",
      "Epoch 00157: val_loss did not improve from 2.97270\n",
      "\n",
      "[20210302-0951-46] Learning rate for epoch 157 is 0.0010131207527592778\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.9016 - val_loss: 3.3267\n",
      "Epoch 158/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1066\n",
      "Epoch 00158: val_loss did not improve from 2.97270\n",
      "\n",
      "[20210302-0951-51] Learning rate for epoch 158 is 0.0011805035173892975\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.1066 - val_loss: 3.1172\n",
      "Epoch 159/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9726\n",
      "Epoch 00159: val_loss did not improve from 2.97270\n",
      "\n",
      "[20210302-0951-55] Learning rate for epoch 159 is 0.0013474862789735198\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.9726 - val_loss: 3.2603\n",
      "Epoch 160/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9733\n",
      "Epoch 00160: val_loss improved from 2.97270 to 2.97182, saving model to ./20210301-225844/heel_K5_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-0952-01] Learning rate for epoch 160 is 0.0015140691539272666\n",
      "20/20 [==============================] - 2s 91ms/step - loss: 5.9733 - val_loss: 2.9718\n",
      "Epoch 161/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0506\n",
      "Epoch 00161: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0952-06] Learning rate for epoch 161 is 0.001680252025835216\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.0506 - val_loss: 3.8710\n",
      "Epoch 162/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9661\n",
      "Epoch 00162: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0952-10] Learning rate for epoch 162 is 0.0018460348946973681\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.9661 - val_loss: 3.5441\n",
      "Epoch 163/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0775\n",
      "Epoch 00163: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0952-15] Learning rate for epoch 163 is 0.0020114178769290447\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.0775 - val_loss: 3.2250\n",
      "Epoch 164/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8751\n",
      "Epoch 00164: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0952-20] Learning rate for epoch 164 is 0.0021764009725302458\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.8751 - val_loss: 3.2202\n",
      "Epoch 165/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8049\n",
      "Epoch 00165: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0952-25] Learning rate for epoch 165 is 0.002340983832255006\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.8049 - val_loss: 3.3203\n",
      "Epoch 166/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1820\n",
      "Epoch 00166: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0952-30] Learning rate for epoch 166 is 0.002505166921764612\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.1820 - val_loss: 3.4091\n",
      "Epoch 167/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8018\n",
      "Epoch 00167: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0952-35] Learning rate for epoch 167 is 0.002668950008228421\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.8018 - val_loss: 3.0374\n",
      "Epoch 168/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9478\n",
      "Epoch 00168: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0952-40] Learning rate for epoch 168 is 0.0028323333244770765\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.9478 - val_loss: 3.2414\n",
      "Epoch 169/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7820\n",
      "Epoch 00169: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0952-44] Learning rate for epoch 169 is 0.002995316404849291\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.7820 - val_loss: 3.5954\n",
      "Epoch 170/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4115\n",
      "Epoch 00170: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0952-49] Learning rate for epoch 170 is 0.0031578997150063515\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.4115 - val_loss: 3.5899\n",
      "Epoch 171/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9250\n",
      "Epoch 00171: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0952-54] Learning rate for epoch 171 is 0.0033200830221176147\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.9250 - val_loss: 3.9545\n",
      "Epoch 172/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8332\n",
      "Epoch 00172: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0952-59] Learning rate for epoch 172 is 0.0034818663261830807\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.8332 - val_loss: 4.3347\n",
      "Epoch 173/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4014\n",
      "Epoch 00173: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0953-04] Learning rate for epoch 173 is 0.0036432496272027493\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.4014 - val_loss: 3.9955\n",
      "Epoch 174/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0935\n",
      "Epoch 00174: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0953-09] Learning rate for epoch 174 is 0.003804233158007264\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.0935 - val_loss: 4.5997\n",
      "Epoch 175/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9078\n",
      "Epoch 00175: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0953-14] Learning rate for epoch 175 is 0.003964816685765982\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.9078 - val_loss: 4.0190\n",
      "Epoch 176/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0395\n",
      "Epoch 00176: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0953-19] Learning rate for epoch 176 is 0.004124999977648258\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.0395 - val_loss: 4.9800\n",
      "Epoch 177/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1408\n",
      "Epoch 00177: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0953-24] Learning rate for epoch 177 is 0.003955216612666845\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.1408 - val_loss: 7.4432\n",
      "Epoch 178/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9535\n",
      "Epoch 00178: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0953-29] Learning rate for epoch 178 is 0.003785833017900586\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.9535 - val_loss: 6.8168\n",
      "Epoch 179/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3042\n",
      "Epoch 00179: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0953-34] Learning rate for epoch 179 is 0.0036168494261801243\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.3042 - val_loss: 5.3968\n",
      "Epoch 180/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8918\n",
      "Epoch 00180: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0953-38] Learning rate for epoch 180 is 0.003448265604674816\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8918 - val_loss: 3.6106\n",
      "Epoch 181/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9370\n",
      "Epoch 00181: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0953-43] Learning rate for epoch 181 is 0.003280082019045949\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.9370 - val_loss: 5.0994\n",
      "Epoch 182/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6501\n",
      "Epoch 00182: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0953-48] Learning rate for epoch 182 is 0.0031122982036322355\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.6501 - val_loss: 1686.4385\n",
      "Epoch 183/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0552\n",
      "Epoch 00183: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0953-53] Learning rate for epoch 183 is 0.002944914624094963\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.0552 - val_loss: 1234.8684\n",
      "Epoch 184/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5412\n",
      "Epoch 00184: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0953-58] Learning rate for epoch 184 is 0.0027779308147728443\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.5412 - val_loss: 441.3693\n",
      "Epoch 185/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0561\n",
      "Epoch 00185: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0954-03] Learning rate for epoch 185 is 0.0026113467756658792\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.0561 - val_loss: 648.3464\n",
      "Epoch 186/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0958\n",
      "Epoch 00186: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0954-08] Learning rate for epoch 186 is 0.002445162972435355\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.0958 - val_loss: 371.7985\n",
      "Epoch 187/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5402\n",
      "Epoch 00187: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0954-13] Learning rate for epoch 187 is 0.0022793791722506285\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.5402 - val_loss: 21.0471\n",
      "Epoch 188/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0350\n",
      "Epoch 00188: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0954-17] Learning rate for epoch 188 is 0.0021139951422810555\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.0350 - val_loss: 41.6347\n",
      "Epoch 189/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8905\n",
      "Epoch 00189: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0954-22] Learning rate for epoch 189 is 0.0019490111153572798\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.8905 - val_loss: 10.1370\n",
      "Epoch 190/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5986\n",
      "Epoch 00190: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0954-27] Learning rate for epoch 190 is 0.0017844270914793015\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.5986 - val_loss: 16.5549\n",
      "Epoch 191/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8245\n",
      "Epoch 00191: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0954-32] Learning rate for epoch 191 is 0.0016202429542317986\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.8245 - val_loss: 7.0385\n",
      "Epoch 192/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1962\n",
      "Epoch 00192: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0954-37] Learning rate for epoch 192 is 0.001456458936445415\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.1962 - val_loss: 4.9205\n",
      "Epoch 193/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5155\n",
      "Epoch 00193: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0954-42] Learning rate for epoch 193 is 0.001293074688874185\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.5155 - val_loss: 3.4714\n",
      "Epoch 194/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7950\n",
      "Epoch 00194: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0954-47] Learning rate for epoch 194 is 0.0011300905607640743\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.7950 - val_loss: 3.2449\n",
      "Epoch 195/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6257\n",
      "Epoch 00195: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0954-51] Learning rate for epoch 195 is 0.0009675062610767782\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.6257 - val_loss: 3.3533\n",
      "Epoch 196/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9361\n",
      "Epoch 00196: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0954-56] Learning rate for epoch 196 is 0.0008053220226429403\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.9361 - val_loss: 3.8179\n",
      "Epoch 197/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7245\n",
      "Epoch 00197: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0955-01] Learning rate for epoch 197 is 0.0006435376708395779\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.7245 - val_loss: 3.2223\n",
      "Epoch 198/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4935\n",
      "Epoch 00198: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0955-06] Learning rate for epoch 198 is 0.0004821533220820129\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.4935 - val_loss: 3.1079\n",
      "Epoch 199/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6289\n",
      "Epoch 00199: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0955-11] Learning rate for epoch 199 is 0.0003211689181625843\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.6289 - val_loss: 3.2426\n",
      "Epoch 200/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6627\n",
      "Epoch 00200: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0955-16] Learning rate for epoch 200 is 0.00016058447363320738\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.6627 - val_loss: 3.1838\n",
      "Epoch 201/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5584\n",
      "Epoch 00201: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0955-21] Learning rate for epoch 201 is 4.0000000467443897e-07\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5584 - val_loss: 3.1861\n",
      "Epoch 202/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4363\n",
      "Epoch 00202: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0955-25] Learning rate for epoch 202 is 0.0001601835247129202\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.4363 - val_loss: 3.0955\n",
      "Epoch 203/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4177\n",
      "Epoch 00203: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0955-30] Learning rate for epoch 203 is 0.00031956707243807614\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.4177 - val_loss: 3.0529\n",
      "Epoch 204/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5617\n",
      "Epoch 00204: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0955-35] Learning rate for epoch 204 is 0.00047855067532509565\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.5617 - val_loss: 3.3702\n",
      "Epoch 205/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4226\n",
      "Epoch 00205: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0955-40] Learning rate for epoch 205 is 0.0006371343042701483\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.4226 - val_loss: 3.3168\n",
      "Epoch 206/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9832\n",
      "Epoch 00206: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0955-45] Learning rate for epoch 206 is 0.0007953179883770645\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.9832 - val_loss: 3.6541\n",
      "Epoch 207/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4759\n",
      "Epoch 00207: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0955-50] Learning rate for epoch 207 is 0.0009531017276458442\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.4759 - val_loss: 3.2667\n",
      "Epoch 208/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1773\n",
      "Epoch 00208: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0955-55] Learning rate for epoch 208 is 0.0011104855220764875\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.1773 - val_loss: 3.3678\n",
      "Epoch 209/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8462\n",
      "Epoch 00209: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0956-00] Learning rate for epoch 209 is 0.0012674692552536726\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.8462 - val_loss: 3.2559\n",
      "Epoch 210/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2132\n",
      "Epoch 00210: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0956-05] Learning rate for epoch 210 is 0.0014240531018003821\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.2132 - val_loss: 3.3724\n",
      "Epoch 211/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3289\n",
      "Epoch 00211: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0956-10] Learning rate for epoch 211 is 0.0015802369453012943\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.3289 - val_loss: 3.6003\n",
      "Epoch 212/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8393\n",
      "Epoch 00212: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0956-14] Learning rate for epoch 212 is 0.001736020902171731\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.8393 - val_loss: 4.3969\n",
      "Epoch 213/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2235\n",
      "Epoch 00213: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0956-19] Learning rate for epoch 213 is 0.0018914048559963703\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.2235 - val_loss: 4.3175\n",
      "Epoch 214/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3278\n",
      "Epoch 00214: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0956-24] Learning rate for epoch 214 is 0.0020463888067752123\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.3278 - val_loss: 4.5374\n",
      "Epoch 215/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5444\n",
      "Epoch 00215: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0956-29] Learning rate for epoch 215 is 0.0022009729873389006\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.5444 - val_loss: 4.0308\n",
      "Epoch 216/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7386\n",
      "Epoch 00216: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0956-34] Learning rate for epoch 216 is 0.002355156932026148\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.7386 - val_loss: 4.3609\n",
      "Epoch 217/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7945\n",
      "Epoch 00217: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0956-39] Learning rate for epoch 217 is 0.0025089411064982414\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.7945 - val_loss: 5.4699\n",
      "Epoch 218/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8965\n",
      "Epoch 00218: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0956-44] Learning rate for epoch 218 is 0.0026623252779245377\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.8965 - val_loss: 3.2085\n",
      "Epoch 219/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7737\n",
      "Epoch 00219: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0956-49] Learning rate for epoch 219 is 0.0028153094463050365\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.7737 - val_loss: 3.3421\n",
      "Epoch 220/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8828\n",
      "Epoch 00220: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0956-54] Learning rate for epoch 220 is 0.002967893611639738\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8828 - val_loss: 5.3769\n",
      "Epoch 221/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5477\n",
      "Epoch 00221: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0956-58] Learning rate for epoch 221 is 0.003120078006759286\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5477 - val_loss: 4.0319\n",
      "Epoch 222/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1860\n",
      "Epoch 00222: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0957-03] Learning rate for epoch 222 is 0.0032718623988330364\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.1860 - val_loss: 4.4244\n",
      "Epoch 223/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1873\n",
      "Epoch 00223: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0957-08] Learning rate for epoch 223 is 0.0034232467878609896\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.1873 - val_loss: 5.7166\n",
      "Epoch 224/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2196\n",
      "Epoch 00224: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0957-13] Learning rate for epoch 224 is 0.0035742311738431454\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.2196 - val_loss: 4.7274\n",
      "Epoch 225/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3902\n",
      "Epoch 00225: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0957-18] Learning rate for epoch 225 is 0.003724815556779504\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.3902 - val_loss: 26.5172\n",
      "Epoch 226/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2759\n",
      "Epoch 00226: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0957-23] Learning rate for epoch 226 is 0.003874999936670065\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.2759 - val_loss: 9.1056\n",
      "Epoch 227/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8288\n",
      "Epoch 00227: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0957-28] Learning rate for epoch 227 is 0.0037152154836803675\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8288 - val_loss: 5.5413\n",
      "Epoch 228/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4969\n",
      "Epoch 00228: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0957-33] Learning rate for epoch 228 is 0.0035558310337364674\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.4969 - val_loss: 5.0890\n",
      "Epoch 229/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4085\n",
      "Epoch 00229: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0957-37] Learning rate for epoch 229 is 0.003396846354007721\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.4085 - val_loss: 3.2278\n",
      "Epoch 230/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9104\n",
      "Epoch 00230: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0957-42] Learning rate for epoch 230 is 0.003238261677324772\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.9104 - val_loss: 3.2384\n",
      "Epoch 231/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0903\n",
      "Epoch 00231: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0957-47] Learning rate for epoch 231 is 0.00308007700368762\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.0903 - val_loss: 4.7616\n",
      "Epoch 232/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3290\n",
      "Epoch 00232: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0957-52] Learning rate for epoch 232 is 0.002922292333096266\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.3290 - val_loss: 8.2253\n",
      "Epoch 233/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4837\n",
      "Epoch 00233: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0957-57] Learning rate for epoch 233 is 0.002764907432720065\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.4837 - val_loss: 7.1959\n",
      "Epoch 234/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0246\n",
      "Epoch 00234: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0958-02] Learning rate for epoch 234 is 0.0026079227682203054\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.0246 - val_loss: 8.0961\n",
      "Epoch 235/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2194\n",
      "Epoch 00235: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0958-07] Learning rate for epoch 235 is 0.0024513378739356995\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.2194 - val_loss: 3.7204\n",
      "Epoch 236/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4569\n",
      "Epoch 00236: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0958-12] Learning rate for epoch 236 is 0.002295152982696891\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.4569 - val_loss: 3.4043\n",
      "Epoch 237/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6819\n",
      "Epoch 00237: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0958-16] Learning rate for epoch 237 is 0.0021393680945038795\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6819 - val_loss: 3.7881\n",
      "Epoch 238/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3128\n",
      "Epoch 00238: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0958-21] Learning rate for epoch 238 is 0.0019839832093566656\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.3128 - val_loss: 4.2677\n",
      "Epoch 239/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6473\n",
      "Epoch 00239: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0958-26] Learning rate for epoch 239 is 0.0018289980944246054\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.6473 - val_loss: 3.7072\n",
      "Epoch 240/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6709\n",
      "Epoch 00240: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0958-31] Learning rate for epoch 240 is 0.0016744130989536643\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.6709 - val_loss: 4.7974\n",
      "Epoch 241/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5918\n",
      "Epoch 00241: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0958-36] Learning rate for epoch 241 is 0.0015202279901131988\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.5918 - val_loss: 4.5169\n",
      "Epoch 242/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6148\n",
      "Epoch 00242: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0958-41] Learning rate for epoch 242 is 0.0013664428843185306\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6148 - val_loss: 3.3879\n",
      "Epoch 243/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6073\n",
      "Epoch 00243: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0958-45] Learning rate for epoch 243 is 0.0012130576651543379\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.6073 - val_loss: 3.4568\n",
      "Epoch 244/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0032\n",
      "Epoch 00244: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0958-50] Learning rate for epoch 244 is 0.0010600725654512644\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.0032 - val_loss: 3.6358\n",
      "Epoch 245/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6173\n",
      "Epoch 00245: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0958-55] Learning rate for epoch 245 is 0.0009074872941710055\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.6173 - val_loss: 3.5348\n",
      "Epoch 246/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9000\n",
      "Epoch 00246: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0959-00] Learning rate for epoch 246 is 0.0007553020259365439\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.9000 - val_loss: 3.2743\n",
      "Epoch 247/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6971\n",
      "Epoch 00247: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0959-05] Learning rate for epoch 247 is 0.0006035167025402188\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.6971 - val_loss: 3.1448\n",
      "Epoch 248/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6143\n",
      "Epoch 00248: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0959-10] Learning rate for epoch 248 is 0.00045213132398203015\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.6143 - val_loss: 3.2499\n",
      "Epoch 249/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.0528\n",
      "Epoch 00249: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0959-15] Learning rate for epoch 249 is 0.00030114591936580837\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.0528 - val_loss: 3.2205\n",
      "Epoch 250/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1379\n",
      "Epoch 00250: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0959-19] Learning rate for epoch 250 is 0.00015056047413963825\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.1379 - val_loss: 3.2125\n",
      "Epoch 251/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5951\n",
      "Epoch 00251: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0959-24] Learning rate for epoch 251 is 3.7500001326407073e-07\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.5951 - val_loss: 3.2053\n",
      "Epoch 252/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2730\n",
      "Epoch 00252: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0959-29] Learning rate for epoch 252 is 0.00015015952521935105\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.2730 - val_loss: 3.2147\n",
      "Epoch 253/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5728\n",
      "Epoch 00253: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0959-34] Learning rate for epoch 253 is 0.0002995440736413002\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.5728 - val_loss: 3.1963\n",
      "Epoch 254/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4102\n",
      "Epoch 00254: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0959-39] Learning rate for epoch 254 is 0.0004485286772251129\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.4102 - val_loss: 3.2908\n",
      "Epoch 255/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3005\n",
      "Epoch 00255: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0959-44] Learning rate for epoch 255 is 0.0005971133359707892\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.3005 - val_loss: 3.1705\n",
      "Epoch 256/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2119\n",
      "Epoch 00256: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0959-48] Learning rate for epoch 256 is 0.0007452979916706681\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.2119 - val_loss: 3.5031\n",
      "Epoch 257/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5080\n",
      "Epoch 00257: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0959-53] Learning rate for epoch 257 is 0.0008930827025324106\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5080 - val_loss: 3.2502\n",
      "Epoch 258/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3614\n",
      "Epoch 00258: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-0959-58] Learning rate for epoch 258 is 0.0010404675267636776\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.3614 - val_loss: 3.1940\n",
      "Epoch 259/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1209\n",
      "Epoch 00259: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-1000-03] Learning rate for epoch 259 is 0.0011874522315338254\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.1209 - val_loss: 3.1856\n",
      "Epoch 260/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6370\n",
      "Epoch 00260: val_loss did not improve from 2.97182\n",
      "\n",
      "[20210302-1000-08] Learning rate for epoch 260 is 0.0013340371660888195\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.6370 - val_loss: 3.7332\n",
      "K= 6\n",
      "Check cache in memory:Y True\n",
      "Check keypoints process01: Tensor(\"args_1:0\", shape=(2,), dtype=int64) (2,) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Check keypoints process02: Tensor(\"Reshape:0\", shape=(1, 2), dtype=int64) (1, 2) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Check keypoints process03: Tensor(\"PyFunc:1\", dtype=float32, device=/job:localhost/replica:0/task:0)\n",
      "Check keypoints process04: Tensor(\"Reshape_1:0\", shape=(2,), dtype=float32)\n",
      "Check augment :Y True\n",
      "Check ds cache[True] and augment[True]\n",
      "Check cache in memory:Y True\n",
      "Check augment :N False\n",
      "Check ds cache[True] and augment[False]\n",
      "best_model_name: ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "base_model.trainable :  False\n",
      "\n",
      "[20210302-1000-11] Learning rate for epoch 1 is 0.00047855067532509565\n",
      "Epoch 1/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 106.6293\n",
      "Epoch 00001: val_loss improved from inf to 78.19557, saving model to ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 5s 258ms/step - loss: 104.3883 - val_loss: 78.1956\n",
      "\n",
      "[20210302-1000-26] Learning rate for epoch 2 is 0.00047855067532509565\n",
      "Epoch 2/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 42.0029\n",
      "Epoch 00002: val_loss improved from 78.19557 to 26.40537, saving model to ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 41.3289 - val_loss: 26.4054\n",
      "\n",
      "[20210302-1000-30] Learning rate for epoch 3 is 0.00047855067532509565\n",
      "Epoch 3/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 23.5496\n",
      "Epoch 00003: val_loss improved from 26.40537 to 23.01927, saving model to ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 8s 421ms/step - loss: 23.5496 - val_loss: 23.0193\n",
      "\n",
      "[20210302-1000-42] Learning rate for epoch 4 is 0.00047855067532509565\n",
      "Epoch 4/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 20.8727\n",
      "Epoch 00004: val_loss improved from 23.01927 to 21.92275, saving model to ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 20.8682 - val_loss: 21.9228\n",
      "\n",
      "[20210302-1000-46] Learning rate for epoch 5 is 0.00047855067532509565\n",
      "Epoch 5/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 19.5675\n",
      "Epoch 00005: val_loss improved from 21.92275 to 20.59617, saving model to ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 19.5120 - val_loss: 20.5962\n",
      "\n",
      "[20210302-1000-51] Learning rate for epoch 6 is 0.00047855067532509565\n",
      "Epoch 6/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 18.5994\n",
      "Epoch 00006: val_loss improved from 20.59617 to 20.15241, saving model to ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 18.5994 - val_loss: 20.1524\n",
      "\n",
      "[20210302-1000-55] Learning rate for epoch 7 is 0.00047855067532509565\n",
      "Epoch 7/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 18.4481\n",
      "Epoch 00007: val_loss improved from 20.15241 to 20.14363, saving model to ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 18.4481 - val_loss: 20.1436\n",
      "\n",
      "[20210302-1001-00] Learning rate for epoch 8 is 0.00047855067532509565\n",
      "Epoch 8/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 17.6253\n",
      "Epoch 00008: val_loss improved from 20.14363 to 18.90030, saving model to ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 17.6834 - val_loss: 18.9003\n",
      "\n",
      "[20210302-1001-04] Learning rate for epoch 9 is 0.00047855067532509565\n",
      "Epoch 9/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 18.1777\n",
      "Epoch 00009: val_loss did not improve from 18.90030\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 18.2471 - val_loss: 18.9350\n",
      "\n",
      "[20210302-1001-08] Learning rate for epoch 10 is 0.00047855067532509565\n",
      "Epoch 10/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 17.2264\n",
      "Epoch 00010: val_loss improved from 18.90030 to 18.33826, saving model to ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 17.2135 - val_loss: 18.3383\n",
      "\n",
      "[20210302-1001-13] Learning rate for epoch 11 is 0.00047855067532509565\n",
      "Epoch 11/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 17.6167\n",
      "Epoch 00011: val_loss did not improve from 18.33826\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 17.7071 - val_loss: 18.5595\n",
      "\n",
      "[20210302-1001-17] Learning rate for epoch 12 is 0.00047855067532509565\n",
      "Epoch 12/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 17.4130\n",
      "Epoch 00012: val_loss improved from 18.33826 to 17.85586, saving model to ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 17.2236 - val_loss: 17.8559\n",
      "\n",
      "[20210302-1001-21] Learning rate for epoch 13 is 0.00047855067532509565\n",
      "Epoch 13/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 16.8230\n",
      "Epoch 00013: val_loss improved from 17.85586 to 17.08379, saving model to ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 16.9847 - val_loss: 17.0838\n",
      "\n",
      "[20210302-1001-25] Learning rate for epoch 14 is 0.00047855067532509565\n",
      "Epoch 14/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 16.7654\n",
      "Epoch 00014: val_loss improved from 17.08379 to 16.39659, saving model to ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.6578 - val_loss: 16.3966\n",
      "\n",
      "[20210302-1001-29] Learning rate for epoch 15 is 0.00047855067532509565\n",
      "Epoch 15/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.9336\n",
      "Epoch 00015: val_loss improved from 16.39659 to 16.21196, saving model to ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 16.9829 - val_loss: 16.2120\n",
      "\n",
      "[20210302-1001-34] Learning rate for epoch 16 is 0.00047855067532509565\n",
      "Epoch 16/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.9542\n",
      "Epoch 00016: val_loss did not improve from 16.21196\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.9008 - val_loss: 16.4506\n",
      "\n",
      "[20210302-1001-38] Learning rate for epoch 17 is 0.00047855067532509565\n",
      "Epoch 17/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 17.0149\n",
      "Epoch 00017: val_loss improved from 16.21196 to 15.55078, saving model to ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.9469 - val_loss: 15.5508\n",
      "\n",
      "[20210302-1001-42] Learning rate for epoch 18 is 0.00047855067532509565\n",
      "Epoch 18/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.2379\n",
      "Epoch 00018: val_loss improved from 15.55078 to 15.19149, saving model to ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 16.2559 - val_loss: 15.1915\n",
      "\n",
      "[20210302-1001-46] Learning rate for epoch 19 is 0.00047855067532509565\n",
      "Epoch 19/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.7364\n",
      "Epoch 00019: val_loss improved from 15.19149 to 14.57143, saving model to ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.6601 - val_loss: 14.5714\n",
      "\n",
      "[20210302-1001-51] Learning rate for epoch 20 is 0.00047855067532509565\n",
      "Epoch 20/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 16.3919\n",
      "Epoch 00020: val_loss improved from 14.57143 to 14.26212, saving model to ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 16.4195 - val_loss: 14.2621\n",
      "\n",
      "[20210302-1001-55] Learning rate for epoch 21 is 0.00047855067532509565\n",
      "Epoch 21/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.7595\n",
      "Epoch 00021: val_loss did not improve from 14.26212\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.7595 - val_loss: 14.4070\n",
      "\n",
      "[20210302-1001-59] Learning rate for epoch 22 is 0.00047855067532509565\n",
      "Epoch 22/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.0739\n",
      "Epoch 00022: val_loss improved from 14.26212 to 13.24866, saving model to ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.0739 - val_loss: 13.2487\n",
      "\n",
      "[20210302-1002-03] Learning rate for epoch 23 is 0.00047855067532509565\n",
      "Epoch 23/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.6589\n",
      "Epoch 00023: val_loss improved from 13.24866 to 12.85274, saving model to ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 16.5954 - val_loss: 12.8527\n",
      "\n",
      "[20210302-1002-08] Learning rate for epoch 24 is 0.00047855067532509565\n",
      "Epoch 24/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 16.0547\n",
      "Epoch 00024: val_loss improved from 12.85274 to 12.35454, saving model to ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 16.0460 - val_loss: 12.3545\n",
      "\n",
      "[20210302-1002-12] Learning rate for epoch 25 is 0.00047855067532509565\n",
      "Epoch 25/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.5775\n",
      "Epoch 00025: val_loss improved from 12.35454 to 12.19303, saving model to ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.4215 - val_loss: 12.1930\n",
      "\n",
      "[20210302-1002-16] Learning rate for epoch 26 is 0.00047855067532509565\n",
      "Epoch 26/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 16.1005\n",
      "Epoch 00026: val_loss improved from 12.19303 to 11.64593, saving model to ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 16.0010 - val_loss: 11.6459\n",
      "\n",
      "[20210302-1002-21] Learning rate for epoch 27 is 0.00047855067532509565\n",
      "Epoch 27/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.1344\n",
      "Epoch 00027: val_loss did not improve from 11.64593\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.1944 - val_loss: 11.8486\n",
      "\n",
      "[20210302-1002-25] Learning rate for epoch 28 is 0.00047855067532509565\n",
      "Epoch 28/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 16.5445\n",
      "Epoch 00028: val_loss improved from 11.64593 to 11.25930, saving model to ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 16.5208 - val_loss: 11.2593\n",
      "\n",
      "[20210302-1002-29] Learning rate for epoch 29 is 0.00047855067532509565\n",
      "Epoch 29/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.5677\n",
      "Epoch 00029: val_loss did not improve from 11.25930\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.4729 - val_loss: 11.3478\n",
      "\n",
      "[20210302-1002-33] Learning rate for epoch 30 is 0.00047855067532509565\n",
      "Epoch 30/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.1600\n",
      "Epoch 00030: val_loss improved from 11.25930 to 10.98730, saving model to ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.1600 - val_loss: 10.9873\n",
      "\n",
      "[20210302-1002-37] Learning rate for epoch 31 is 0.00047855067532509565\n",
      "Epoch 31/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.8238\n",
      "Epoch 00031: val_loss improved from 10.98730 to 10.54917, saving model to ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 15.9003 - val_loss: 10.5492\n",
      "\n",
      "[20210302-1002-42] Learning rate for epoch 32 is 0.00047855067532509565\n",
      "Epoch 32/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.1003\n",
      "Epoch 00032: val_loss improved from 10.54917 to 10.44286, saving model to ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 16.1003 - val_loss: 10.4429\n",
      "\n",
      "[20210302-1002-46] Learning rate for epoch 33 is 0.00047855067532509565\n",
      "Epoch 33/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.1540\n",
      "Epoch 00033: val_loss did not improve from 10.44286\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.0877 - val_loss: 10.5900\n",
      "\n",
      "[20210302-1002-50] Learning rate for epoch 34 is 0.00047855067532509565\n",
      "Epoch 34/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.2899\n",
      "Epoch 00034: val_loss did not improve from 10.44286\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.2899 - val_loss: 10.9731\n",
      "\n",
      "[20210302-1002-54] Learning rate for epoch 35 is 0.00047855067532509565\n",
      "Epoch 35/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.9825\n",
      "Epoch 00035: val_loss improved from 10.44286 to 10.32256, saving model to ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 15.9613 - val_loss: 10.3226\n",
      "\n",
      "[20210302-1002-58] Learning rate for epoch 36 is 0.00047855067532509565\n",
      "Epoch 36/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.7980\n",
      "Epoch 00036: val_loss did not improve from 10.32256\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 15.7766 - val_loss: 10.3645\n",
      "\n",
      "[20210302-1003-02] Learning rate for epoch 37 is 0.00047855067532509565\n",
      "Epoch 37/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.8543\n",
      "Epoch 00037: val_loss improved from 10.32256 to 10.14485, saving model to ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 15.7903 - val_loss: 10.1449\n",
      "\n",
      "[20210302-1003-06] Learning rate for epoch 38 is 0.00047855067532509565\n",
      "Epoch 38/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 16.1520\n",
      "Epoch 00038: val_loss improved from 10.14485 to 9.88804, saving model to ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 16.2815 - val_loss: 9.8880\n",
      "\n",
      "[20210302-1003-10] Learning rate for epoch 39 is 0.00047855067532509565\n",
      "Epoch 39/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.4783\n",
      "Epoch 00039: val_loss did not improve from 9.88804\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.4055 - val_loss: 10.2294\n",
      "\n",
      "[20210302-1003-14] Learning rate for epoch 40 is 0.00047855067532509565\n",
      "Epoch 40/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 15.8760\n",
      "Epoch 00040: val_loss did not improve from 9.88804\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.0277 - val_loss: 12.4065\n",
      "\n",
      "[20210302-1003-18] Learning rate for epoch 41 is 0.00047855067532509565\n",
      "Epoch 41/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.4426\n",
      "Epoch 00041: val_loss did not improve from 9.88804\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.4426 - val_loss: 10.9233\n",
      "\n",
      "[20210302-1003-22] Learning rate for epoch 42 is 0.00047855067532509565\n",
      "Epoch 42/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.8074\n",
      "Epoch 00042: val_loss did not improve from 9.88804\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.8074 - val_loss: 11.3258\n",
      "\n",
      "[20210302-1003-26] Learning rate for epoch 43 is 0.00047855067532509565\n",
      "Epoch 43/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.4442\n",
      "Epoch 00043: val_loss did not improve from 9.88804\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.4442 - val_loss: 10.5648\n",
      "\n",
      "[20210302-1003-30] Learning rate for epoch 44 is 0.00047855067532509565\n",
      "Epoch 44/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.7956\n",
      "Epoch 00044: val_loss did not improve from 9.88804\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.7956 - val_loss: 10.1595\n",
      "\n",
      "[20210302-1003-34] Learning rate for epoch 45 is 0.00047855067532509565\n",
      "Epoch 45/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.0541\n",
      "Epoch 00045: val_loss did not improve from 9.88804\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.0117 - val_loss: 10.7284\n",
      "\n",
      "[20210302-1003-38] Learning rate for epoch 46 is 0.00047855067532509565\n",
      "Epoch 46/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.4055\n",
      "Epoch 00046: val_loss did not improve from 9.88804\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.4055 - val_loss: 10.4981\n",
      "\n",
      "[20210302-1003-42] Learning rate for epoch 47 is 0.00047855067532509565\n",
      "Epoch 47/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.7448\n",
      "Epoch 00047: val_loss did not improve from 9.88804\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.7729 - val_loss: 10.3022\n",
      "\n",
      "[20210302-1003-46] Learning rate for epoch 48 is 0.00047855067532509565\n",
      "Epoch 48/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 16.0057\n",
      "Epoch 00048: val_loss did not improve from 9.88804\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.0820 - val_loss: 10.0476\n",
      "\n",
      "[20210302-1003-50] Learning rate for epoch 49 is 0.00047855067532509565\n",
      "Epoch 49/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.8443\n",
      "Epoch 00049: val_loss did not improve from 9.88804\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 15.8749 - val_loss: 10.8116\n",
      "\n",
      "[20210302-1003-54] Learning rate for epoch 50 is 0.00047855067532509565\n",
      "Epoch 50/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.6451\n",
      "Epoch 00050: val_loss improved from 9.88804 to 9.74122, saving model to ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 15.7068 - val_loss: 9.7412\n",
      "\n",
      "[20210302-1003-58] Learning rate for epoch 51 is 0.00047855067532509565\n",
      "Epoch 51/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.9616\n",
      "Epoch 00051: val_loss did not improve from 9.74122\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.8099 - val_loss: 9.8579\n",
      "\n",
      "[20210302-1004-02] Learning rate for epoch 52 is 0.00047855067532509565\n",
      "Epoch 52/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.8670\n",
      "Epoch 00052: val_loss did not improve from 9.74122\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.8844 - val_loss: 10.0429\n",
      "\n",
      "[20210302-1004-06] Learning rate for epoch 53 is 0.00047855067532509565\n",
      "Epoch 53/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.6240\n",
      "Epoch 00053: val_loss did not improve from 9.74122\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.5416 - val_loss: 10.0377\n",
      "\n",
      "[20210302-1004-10] Learning rate for epoch 54 is 0.00047855067532509565\n",
      "Epoch 54/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.6709\n",
      "Epoch 00054: val_loss did not improve from 9.74122\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.7110 - val_loss: 10.2825\n",
      "\n",
      "[20210302-1004-14] Learning rate for epoch 55 is 0.00047855067532509565\n",
      "Epoch 55/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.4977\n",
      "Epoch 00055: val_loss did not improve from 9.74122\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.4297 - val_loss: 10.4269\n",
      "\n",
      "[20210302-1004-18] Learning rate for epoch 56 is 0.00047855067532509565\n",
      "Epoch 56/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.5692\n",
      "Epoch 00056: val_loss did not improve from 9.74122\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.5632 - val_loss: 10.0469\n",
      "\n",
      "[20210302-1004-22] Learning rate for epoch 57 is 0.00047855067532509565\n",
      "Epoch 57/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.5851\n",
      "Epoch 00057: val_loss improved from 9.74122 to 9.23938, saving model to ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 15.5851 - val_loss: 9.2394\n",
      "\n",
      "[20210302-1004-27] Learning rate for epoch 58 is 0.00047855067532509565\n",
      "Epoch 58/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.4633\n",
      "Epoch 00058: val_loss did not improve from 9.23938\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.4301 - val_loss: 9.6793\n",
      "\n",
      "[20210302-1004-31] Learning rate for epoch 59 is 0.00047855067532509565\n",
      "Epoch 59/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.6962\n",
      "Epoch 00059: val_loss did not improve from 9.23938\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.8189 - val_loss: 9.8051\n",
      "\n",
      "[20210302-1004-35] Learning rate for epoch 60 is 0.00047855067532509565\n",
      "Epoch 60/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 15.4795\n",
      "Epoch 00060: val_loss did not improve from 9.23938\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 15.7445 - val_loss: 9.7327\n",
      "\n",
      "[20210302-1004-39] Learning rate for epoch 61 is 0.00047855067532509565\n",
      "Epoch 61/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.1344\n",
      "Epoch 00061: val_loss did not improve from 9.23938\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.1304 - val_loss: 10.1323\n",
      "\n",
      "[20210302-1004-43] Learning rate for epoch 62 is 0.00047855067532509565\n",
      "Epoch 62/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.7703\n",
      "Epoch 00062: val_loss did not improve from 9.23938\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 15.7630 - val_loss: 9.9304\n",
      "\n",
      "[20210302-1004-46] Learning rate for epoch 63 is 0.00047855067532509565\n",
      "Epoch 63/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.4454\n",
      "Epoch 00063: val_loss did not improve from 9.23938\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 15.3800 - val_loss: 9.3797\n",
      "\n",
      "[20210302-1004-50] Learning rate for epoch 64 is 0.00047855067532509565\n",
      "Epoch 64/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.8396\n",
      "Epoch 00064: val_loss did not improve from 9.23938\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.9162 - val_loss: 10.5414\n",
      "\n",
      "[20210302-1004-54] Learning rate for epoch 65 is 0.00047855067532509565\n",
      "Epoch 65/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.4753\n",
      "Epoch 00065: val_loss did not improve from 9.23938\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.5158 - val_loss: 9.4882\n",
      "\n",
      "[20210302-1004-58] Learning rate for epoch 66 is 0.00047855067532509565\n",
      "Epoch 66/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.5013\n",
      "Epoch 00066: val_loss did not improve from 9.23938\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.4082 - val_loss: 9.3587\n",
      "\n",
      "[20210302-1005-02] Learning rate for epoch 67 is 0.00047855067532509565\n",
      "Epoch 67/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.8755\n",
      "Epoch 00067: val_loss did not improve from 9.23938\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.9211 - val_loss: 9.5930\n",
      "\n",
      "[20210302-1005-06] Learning rate for epoch 68 is 0.00047855067532509565\n",
      "Epoch 68/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.1966\n",
      "Epoch 00068: val_loss did not improve from 9.23938\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.1282 - val_loss: 9.6862\n",
      "\n",
      "[20210302-1005-10] Learning rate for epoch 69 is 0.00047855067532509565\n",
      "Epoch 69/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.7868\n",
      "Epoch 00069: val_loss improved from 9.23938 to 9.16354, saving model to ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 15.7868 - val_loss: 9.1635\n",
      "\n",
      "[20210302-1005-14] Learning rate for epoch 70 is 0.00047855067532509565\n",
      "Epoch 70/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.9749\n",
      "Epoch 00070: val_loss did not improve from 9.16354\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.1254 - val_loss: 9.6889\n",
      "\n",
      "[20210302-1005-19] Learning rate for epoch 71 is 0.00047855067532509565\n",
      "Epoch 71/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.7431\n",
      "Epoch 00071: val_loss did not improve from 9.16354\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.8184 - val_loss: 9.3098\n",
      "\n",
      "[20210302-1005-22] Learning rate for epoch 72 is 0.00047855067532509565\n",
      "Epoch 72/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.8523\n",
      "Epoch 00072: val_loss did not improve from 9.16354\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.8523 - val_loss: 9.4593\n",
      "\n",
      "[20210302-1005-26] Learning rate for epoch 73 is 0.00047855067532509565\n",
      "Epoch 73/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.4264\n",
      "Epoch 00073: val_loss did not improve from 9.16354\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.5327 - val_loss: 9.5172\n",
      "\n",
      "[20210302-1005-30] Learning rate for epoch 74 is 0.00047855067532509565\n",
      "Epoch 74/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.8147\n",
      "Epoch 00074: val_loss did not improve from 9.16354\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.8147 - val_loss: 9.3994\n",
      "\n",
      "[20210302-1005-34] Learning rate for epoch 75 is 0.00047855067532509565\n",
      "Epoch 75/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.6370\n",
      "Epoch 00075: val_loss did not improve from 9.16354\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.6370 - val_loss: 9.8890\n",
      "\n",
      "[20210302-1005-38] Learning rate for epoch 76 is 0.00047855067532509565\n",
      "Epoch 76/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.7899\n",
      "Epoch 00076: val_loss did not improve from 9.16354\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.9851 - val_loss: 9.5169\n",
      "\n",
      "[20210302-1005-42] Learning rate for epoch 77 is 0.00047855067532509565\n",
      "Epoch 77/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.4012\n",
      "Epoch 00077: val_loss did not improve from 9.16354\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.4290 - val_loss: 9.3617\n",
      "\n",
      "[20210302-1005-46] Learning rate for epoch 78 is 0.00047855067532509565\n",
      "Epoch 78/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.5031\n",
      "Epoch 00078: val_loss did not improve from 9.16354\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.4586 - val_loss: 9.6199\n",
      "\n",
      "[20210302-1005-50] Learning rate for epoch 79 is 0.00047855067532509565\n",
      "Epoch 79/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.9138\n",
      "Epoch 00079: val_loss did not improve from 9.16354\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.1486 - val_loss: 10.0961\n",
      "\n",
      "[20210302-1005-54] Learning rate for epoch 80 is 0.00047855067532509565\n",
      "Epoch 80/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.1347\n",
      "Epoch 00080: val_loss did not improve from 9.16354\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.1287 - val_loss: 9.8108\n",
      "\n",
      "[20210302-1005-58] Learning rate for epoch 81 is 0.00047855067532509565\n",
      "Epoch 81/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.6874\n",
      "Epoch 00081: val_loss did not improve from 9.16354\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.6874 - val_loss: 9.2714\n",
      "\n",
      "[20210302-1006-02] Learning rate for epoch 82 is 0.00047855067532509565\n",
      "Epoch 82/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.5259\n",
      "Epoch 00082: val_loss did not improve from 9.16354\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.5072 - val_loss: 9.9933\n",
      "\n",
      "[20210302-1006-06] Learning rate for epoch 83 is 0.00047855067532509565\n",
      "Epoch 83/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.4911\n",
      "Epoch 00083: val_loss did not improve from 9.16354\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.4848 - val_loss: 9.8548\n",
      "\n",
      "[20210302-1006-10] Learning rate for epoch 84 is 0.00047855067532509565\n",
      "Epoch 84/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.0765\n",
      "Epoch 00084: val_loss did not improve from 9.16354\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.0839 - val_loss: 11.4995\n",
      "\n",
      "[20210302-1006-14] Learning rate for epoch 85 is 0.00047855067532509565\n",
      "Epoch 85/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.3695\n",
      "Epoch 00085: val_loss did not improve from 9.16354\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 15.4320 - val_loss: 10.5954\n",
      "\n",
      "[20210302-1006-18] Learning rate for epoch 86 is 0.00047855067532509565\n",
      "Epoch 86/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.9719\n",
      "Epoch 00086: val_loss did not improve from 9.16354\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.9637 - val_loss: 10.5270\n",
      "\n",
      "[20210302-1006-22] Learning rate for epoch 87 is 0.00047855067532509565\n",
      "Epoch 87/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.6095\n",
      "Epoch 00087: val_loss did not improve from 9.16354\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 15.7451 - val_loss: 10.1311\n",
      "\n",
      "[20210302-1006-26] Learning rate for epoch 88 is 0.00047855067532509565\n",
      "Epoch 88/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.0764\n",
      "Epoch 00088: val_loss did not improve from 9.16354\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.0261 - val_loss: 10.0289\n",
      "\n",
      "[20210302-1006-30] Learning rate for epoch 89 is 0.00047855067532509565\n",
      "Epoch 89/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.4921\n",
      "Epoch 00089: val_loss did not improve from 9.16354\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 15.3736 - val_loss: 9.8020\n",
      "\n",
      "*********************************** Start fine tune ***********************************\n",
      "tt: 195, nt:46, total layers:241\n",
      "*********************************** Start fine tune ***********************************\n",
      "Epoch 1/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 24.7260\n",
      "Epoch 00001: val_loss did not improve from 9.16354\n",
      "\n",
      "[20210302-1007-04] Learning rate for epoch 1 is 4.999999987376214e-07\n",
      "20/20 [==============================] - 14s 691ms/step - loss: 24.7260 - val_loss: 16.0931\n",
      "Epoch 2/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 22.8074\n",
      "Epoch 00002: val_loss did not improve from 9.16354\n",
      "\n",
      "[20210302-1007-09] Learning rate for epoch 2 is 5.044450517743826e-05\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 22.8074 - val_loss: 16.7922\n",
      "Epoch 3/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 17.4896\n",
      "Epoch 00003: val_loss did not improve from 9.16354\n",
      "\n",
      "[20210302-1007-30] Learning rate for epoch 3 is 0.00010028902033809572\n",
      "20/20 [==============================] - 17s 857ms/step - loss: 17.4896 - val_loss: 14.0605\n",
      "Epoch 4/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.2497\n",
      "Epoch 00004: val_loss did not improve from 9.16354\n",
      "\n",
      "[20210302-1007-35] Learning rate for epoch 4 is 0.00015003354928921908\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 15.2497 - val_loss: 14.0896\n",
      "Epoch 5/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.5999\n",
      "Epoch 00005: val_loss did not improve from 9.16354\n",
      "\n",
      "[20210302-1007-39] Learning rate for epoch 5 is 0.0001996780774788931\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 14.5999 - val_loss: 10.1841\n",
      "Epoch 6/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.9081\n",
      "Epoch 00006: val_loss did not improve from 9.16354\n",
      "\n",
      "[20210302-1007-44] Learning rate for epoch 6 is 0.000249222619459033\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 13.9081 - val_loss: 10.0842\n",
      "Epoch 7/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.6732\n",
      "Epoch 00007: val_loss improved from 9.16354 to 8.04605, saving model to ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1007-50] Learning rate for epoch 7 is 0.0002986671752296388\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 13.6732 - val_loss: 8.0460\n",
      "Epoch 8/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.0964\n",
      "Epoch 00008: val_loss improved from 8.04605 to 7.04277, saving model to ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1007-55] Learning rate for epoch 8 is 0.00034801175934262574\n",
      "20/20 [==============================] - 2s 91ms/step - loss: 13.0964 - val_loss: 7.0428\n",
      "Epoch 9/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 12.6783\n",
      "Epoch 00009: val_loss improved from 7.04277 to 6.32663, saving model to ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1008-00] Learning rate for epoch 9 is 0.00039725631359033287\n",
      "20/20 [==============================] - 2s 91ms/step - loss: 12.6783 - val_loss: 6.3266\n",
      "Epoch 10/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.9052\n",
      "Epoch 00010: val_loss did not improve from 6.32663\n",
      "\n",
      "[20210302-1008-05] Learning rate for epoch 10 is 0.0004464008961804211\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 11.9052 - val_loss: 9.3120\n",
      "Epoch 11/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 12.3305\n",
      "Epoch 00011: val_loss did not improve from 6.32663\n",
      "\n",
      "[20210302-1008-10] Learning rate for epoch 11 is 0.0004954455071128905\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 12.3305 - val_loss: 8.0015\n",
      "Epoch 12/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.8269\n",
      "Epoch 00012: val_loss did not improve from 6.32663\n",
      "\n",
      "[20210302-1008-15] Learning rate for epoch 12 is 0.0005443901172839105\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 11.8269 - val_loss: 10.1341\n",
      "Epoch 13/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 12.0343\n",
      "Epoch 00013: val_loss improved from 6.32663 to 5.93997, saving model to ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1008-20] Learning rate for epoch 13 is 0.0005932347266934812\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 12.0343 - val_loss: 5.9400\n",
      "Epoch 14/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.2280\n",
      "Epoch 00014: val_loss improved from 5.93997 to 5.64666, saving model to ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1008-26] Learning rate for epoch 14 is 0.0006419793353416026\n",
      "20/20 [==============================] - 2s 91ms/step - loss: 11.2280 - val_loss: 5.6467\n",
      "Epoch 15/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.8354\n",
      "Epoch 00015: val_loss did not improve from 5.64666\n",
      "\n",
      "[20210302-1008-30] Learning rate for epoch 15 is 0.0006906240014359355\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 10.8354 - val_loss: 7.1033\n",
      "Epoch 16/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.5912\n",
      "Epoch 00016: val_loss did not improve from 5.64666\n",
      "\n",
      "[20210302-1008-35] Learning rate for epoch 16 is 0.0007391686085611582\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 10.5912 - val_loss: 7.3669\n",
      "Epoch 17/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.8593\n",
      "Epoch 00017: val_loss improved from 5.64666 to 5.05760, saving model to ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1008-40] Learning rate for epoch 17 is 0.0007876132731325924\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 9.8593 - val_loss: 5.0576\n",
      "Epoch 18/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.9022\n",
      "Epoch 00018: val_loss improved from 5.05760 to 5.01425, saving model to ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1008-46] Learning rate for epoch 18 is 0.0008359579369425774\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 9.9022 - val_loss: 5.0142\n",
      "Epoch 19/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.2848\n",
      "Epoch 00019: val_loss did not improve from 5.01425\n",
      "\n",
      "[20210302-1008-51] Learning rate for epoch 19 is 0.000884202599991113\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 9.2848 - val_loss: 8.2059\n",
      "Epoch 20/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.2313\n",
      "Epoch 00020: val_loss did not improve from 5.01425\n",
      "\n",
      "[20210302-1008-55] Learning rate for epoch 20 is 0.0009323473204858601\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 9.2313 - val_loss: 5.0315\n",
      "Epoch 21/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.1561\n",
      "Epoch 00021: val_loss did not improve from 5.01425\n",
      "\n",
      "[20210302-1009-00] Learning rate for epoch 21 is 0.000980391982011497\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 9.1561 - val_loss: 5.4234\n",
      "Epoch 22/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.1407\n",
      "Epoch 00022: val_loss improved from 5.01425 to 4.64271, saving model to ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1009-06] Learning rate for epoch 22 is 0.0010283367009833455\n",
      "20/20 [==============================] - 2s 89ms/step - loss: 9.1407 - val_loss: 4.6427\n",
      "Epoch 23/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.0196\n",
      "Epoch 00023: val_loss did not improve from 4.64271\n",
      "\n",
      "[20210302-1009-10] Learning rate for epoch 23 is 0.0010761814191937447\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 8.0196 - val_loss: 9.9837\n",
      "Epoch 24/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.9670\n",
      "Epoch 00024: val_loss did not improve from 4.64271\n",
      "\n",
      "[20210302-1009-15] Learning rate for epoch 24 is 0.0011239261366426945\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.9670 - val_loss: 6.6312\n",
      "Epoch 25/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7949\n",
      "Epoch 00025: val_loss did not improve from 4.64271\n",
      "\n",
      "[20210302-1009-20] Learning rate for epoch 25 is 0.001171570853330195\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.7949 - val_loss: 17.2782\n",
      "Epoch 26/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.0880\n",
      "Epoch 00026: val_loss improved from 4.64271 to 4.49113, saving model to ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1009-25] Learning rate for epoch 26 is 0.001219115569256246\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 8.0880 - val_loss: 4.4911\n",
      "Epoch 27/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7509\n",
      "Epoch 00027: val_loss improved from 4.49113 to 4.42487, saving model to ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1009-31] Learning rate for epoch 27 is 0.0012665604008361697\n",
      "20/20 [==============================] - 2s 89ms/step - loss: 7.7509 - val_loss: 4.4249\n",
      "Epoch 28/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4166\n",
      "Epoch 00028: val_loss did not improve from 4.42487\n",
      "\n",
      "[20210302-1009-36] Learning rate for epoch 28 is 0.0013139051152393222\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.4166 - val_loss: 4.6229\n",
      "Epoch 29/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6757\n",
      "Epoch 00029: val_loss improved from 4.42487 to 3.66528, saving model to ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1009-41] Learning rate for epoch 29 is 0.0013611499452963471\n",
      "20/20 [==============================] - 2s 92ms/step - loss: 7.6757 - val_loss: 3.6653\n",
      "Epoch 30/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6946\n",
      "Epoch 00030: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1009-46] Learning rate for epoch 30 is 0.001408294658176601\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.6946 - val_loss: 7.3510\n",
      "Epoch 31/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4376\n",
      "Epoch 00031: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1009-51] Learning rate for epoch 31 is 0.0014553394867107272\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.4376 - val_loss: 6.4174\n",
      "Epoch 32/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1304\n",
      "Epoch 00032: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1009-55] Learning rate for epoch 32 is 0.0015022843144834042\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.1304 - val_loss: 7.4876\n",
      "Epoch 33/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7001\n",
      "Epoch 00033: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1010-00] Learning rate for epoch 33 is 0.0015491291414946318\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.7001 - val_loss: 8.9508\n",
      "Epoch 34/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2095\n",
      "Epoch 00034: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1010-05] Learning rate for epoch 34 is 0.00159587396774441\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.2095 - val_loss: 6.5829\n",
      "Epoch 35/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.9698\n",
      "Epoch 00035: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1010-10] Learning rate for epoch 35 is 0.001642518793232739\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.9698 - val_loss: 8.0033\n",
      "Epoch 36/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7054\n",
      "Epoch 00036: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1010-15] Learning rate for epoch 36 is 0.0016890636179596186\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.7054 - val_loss: 8.0803\n",
      "Epoch 37/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5190\n",
      "Epoch 00037: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1010-19] Learning rate for epoch 37 is 0.0017355084419250488\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.5190 - val_loss: 6.7392\n",
      "Epoch 38/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0480\n",
      "Epoch 00038: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1010-24] Learning rate for epoch 38 is 0.0017818533815443516\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.0480 - val_loss: 7.5638\n",
      "Epoch 39/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8885\n",
      "Epoch 00039: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1010-29] Learning rate for epoch 39 is 0.0018280982039868832\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.8885 - val_loss: 6.8720\n",
      "Epoch 40/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1503\n",
      "Epoch 00040: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1010-34] Learning rate for epoch 40 is 0.0018742431420832872\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 7.1503 - val_loss: 4.8871\n",
      "Epoch 41/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1024\n",
      "Epoch 00041: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1010-39] Learning rate for epoch 41 is 0.0019202879630029202\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.1024 - val_loss: 4.7211\n",
      "Epoch 42/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0220\n",
      "Epoch 00042: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1010-44] Learning rate for epoch 42 is 0.0019662328995764256\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.0220 - val_loss: 4.2737\n",
      "Epoch 43/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5300\n",
      "Epoch 00043: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1010-49] Learning rate for epoch 43 is 0.00201207771897316\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.5300 - val_loss: 5.7515\n",
      "Epoch 44/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0721\n",
      "Epoch 00044: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1010-53] Learning rate for epoch 44 is 0.0020578226540237665\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.0721 - val_loss: 5.9641\n",
      "Epoch 45/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2681\n",
      "Epoch 00045: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1010-58] Learning rate for epoch 45 is 0.0021034677047282457\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.2681 - val_loss: 6.8625\n",
      "Epoch 46/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7043\n",
      "Epoch 00046: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1011-03] Learning rate for epoch 46 is 0.002149012638255954\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.7043 - val_loss: 5.6120\n",
      "Epoch 47/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9690\n",
      "Epoch 00047: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1011-08] Learning rate for epoch 47 is 0.0021944576874375343\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.9690 - val_loss: 5.9219\n",
      "Epoch 48/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8582\n",
      "Epoch 00048: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1011-13] Learning rate for epoch 48 is 0.0022398026194423437\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.8582 - val_loss: 5.3540\n",
      "Epoch 49/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0373\n",
      "Epoch 00049: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1011-18] Learning rate for epoch 49 is 0.002285047434270382\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.0373 - val_loss: 6.3588\n",
      "Epoch 50/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1114\n",
      "Epoch 00050: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1011-22] Learning rate for epoch 50 is 0.0023301925975829363\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.1114 - val_loss: 9.6779\n",
      "Epoch 51/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1506\n",
      "Epoch 00051: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1011-27] Learning rate for epoch 51 is 0.002375237410888076\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.1506 - val_loss: 7.2934\n",
      "Epoch 52/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8936\n",
      "Epoch 00052: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1011-32] Learning rate for epoch 52 is 0.0024201825726777315\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.8936 - val_loss: 7.3380\n",
      "Epoch 53/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0581\n",
      "Epoch 00053: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1011-37] Learning rate for epoch 53 is 0.002465027617290616\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.0581 - val_loss: 6.0739\n",
      "Epoch 54/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2735\n",
      "Epoch 00054: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1011-42] Learning rate for epoch 54 is 0.0025097725447267294\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.2735 - val_loss: 6.7643\n",
      "Epoch 55/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1713\n",
      "Epoch 00055: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1011-47] Learning rate for epoch 55 is 0.0025544175878167152\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.1713 - val_loss: 5.1359\n",
      "Epoch 56/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0720\n",
      "Epoch 00056: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1011-57] Learning rate for epoch 56 is 0.00259896251372993\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.0720 - val_loss: 4.9714\n",
      "Epoch 57/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9776\n",
      "Epoch 00057: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1012-01] Learning rate for epoch 57 is 0.0026434077881276608\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.9776 - val_loss: 5.4401\n",
      "Epoch 58/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7255\n",
      "Epoch 00058: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1012-06] Learning rate for epoch 58 is 0.0026877527125179768\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.7255 - val_loss: 4.6429\n",
      "Epoch 59/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9246\n",
      "Epoch 00059: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1012-11] Learning rate for epoch 59 is 0.0027319977525621653\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.9246 - val_loss: 10.5476\n",
      "Epoch 60/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5622\n",
      "Epoch 00060: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1012-16] Learning rate for epoch 60 is 0.0027761429082602262\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 6.5622 - val_loss: 8.3996\n",
      "Epoch 61/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3477\n",
      "Epoch 00061: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1012-21] Learning rate for epoch 61 is 0.002820187946781516\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.3477 - val_loss: 6.0901\n",
      "Epoch 62/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8634\n",
      "Epoch 00062: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1012-26] Learning rate for epoch 62 is 0.0028641331009566784\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.8634 - val_loss: 4.9526\n",
      "Epoch 63/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8652\n",
      "Epoch 00063: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1012-31] Learning rate for epoch 63 is 0.0029079781379550695\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.8652 - val_loss: 4.9887\n",
      "Epoch 64/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6607\n",
      "Epoch 00064: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1012-36] Learning rate for epoch 64 is 0.002951723290607333\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.6607 - val_loss: 4.9099\n",
      "Epoch 65/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0156\n",
      "Epoch 00065: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1012-40] Learning rate for epoch 65 is 0.0029953685589134693\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.0156 - val_loss: 8.9658\n",
      "Epoch 66/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7732\n",
      "Epoch 00066: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1012-45] Learning rate for epoch 66 is 0.0030389137100428343\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.7732 - val_loss: 6.3798\n",
      "Epoch 67/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6150\n",
      "Epoch 00067: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1012-50] Learning rate for epoch 67 is 0.003082358743995428\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.6150 - val_loss: 6.5221\n",
      "Epoch 68/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6867\n",
      "Epoch 00068: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1012-55] Learning rate for epoch 68 is 0.0031257038936018944\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.6867 - val_loss: 8.1207\n",
      "Epoch 69/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5694\n",
      "Epoch 00069: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1013-00] Learning rate for epoch 69 is 0.003168949158862233\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.5694 - val_loss: 8.2649\n",
      "Epoch 70/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8498\n",
      "Epoch 00070: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1013-05] Learning rate for epoch 70 is 0.0032120943069458008\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.8498 - val_loss: 6.0773\n",
      "Epoch 71/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7476\n",
      "Epoch 00071: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1013-09] Learning rate for epoch 71 is 0.003255139570683241\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.7476 - val_loss: 3.7511\n",
      "Epoch 72/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1470\n",
      "Epoch 00072: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1013-14] Learning rate for epoch 72 is 0.00329808471724391\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 7.1470 - val_loss: 3.9888\n",
      "Epoch 73/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7112\n",
      "Epoch 00073: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1013-19] Learning rate for epoch 73 is 0.0033409299794584513\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.7112 - val_loss: 3.7885\n",
      "Epoch 74/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6754\n",
      "Epoch 00074: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1013-24] Learning rate for epoch 74 is 0.0033836751244962215\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.6754 - val_loss: 4.4837\n",
      "Epoch 75/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1159\n",
      "Epoch 00075: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1013-29] Learning rate for epoch 75 is 0.0034263203851878643\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 7.1159 - val_loss: 6.1960\n",
      "Epoch 76/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6124\n",
      "Epoch 00076: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1013-34] Learning rate for epoch 76 is 0.003468865528702736\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.6124 - val_loss: 4.8508\n",
      "Epoch 77/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3661\n",
      "Epoch 00077: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1013-38] Learning rate for epoch 77 is 0.00351131078787148\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.3661 - val_loss: 6.7121\n",
      "Epoch 78/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0063\n",
      "Epoch 00078: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1013-43] Learning rate for epoch 78 is 0.0035536561626940966\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.0063 - val_loss: 6.3090\n",
      "Epoch 79/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8401\n",
      "Epoch 00079: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1013-48] Learning rate for epoch 79 is 0.003595901420339942\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.8401 - val_loss: 4.2470\n",
      "Epoch 80/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4553\n",
      "Epoch 00080: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1013-53] Learning rate for epoch 80 is 0.00363804679363966\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.4553 - val_loss: 5.2026\n",
      "Epoch 81/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0216\n",
      "Epoch 00081: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1013-58] Learning rate for epoch 81 is 0.0036800920497626066\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.0216 - val_loss: 4.5206\n",
      "Epoch 82/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6389\n",
      "Epoch 00082: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1014-03] Learning rate for epoch 82 is 0.003722037188708782\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.6389 - val_loss: 6.3873\n",
      "Epoch 83/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3994\n",
      "Epoch 00083: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1014-08] Learning rate for epoch 83 is 0.003763882676139474\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.3994 - val_loss: 6.1756\n",
      "Epoch 84/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4818\n",
      "Epoch 00084: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1014-12] Learning rate for epoch 84 is 0.0038056280463933945\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.4818 - val_loss: 6.7455\n",
      "Epoch 85/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2987\n",
      "Epoch 00085: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1014-17] Learning rate for epoch 85 is 0.003847273299470544\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.2987 - val_loss: 21.0526\n",
      "Epoch 86/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2198\n",
      "Epoch 00086: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1014-22] Learning rate for epoch 86 is 0.0038888186682015657\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.2198 - val_loss: 21.1489\n",
      "Epoch 87/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7208\n",
      "Epoch 00087: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1014-27] Learning rate for epoch 87 is 0.00393026415258646\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.7208 - val_loss: 83.6123\n",
      "Epoch 88/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5374\n",
      "Epoch 00088: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1014-32] Learning rate for epoch 88 is 0.00397160928696394\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.5374 - val_loss: 1073.5690\n",
      "Epoch 89/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6992\n",
      "Epoch 00089: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1014-37] Learning rate for epoch 89 is 0.004012854769825935\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.6992 - val_loss: 149.1190\n",
      "Epoch 90/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9724\n",
      "Epoch 00090: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1014-42] Learning rate for epoch 90 is 0.00405400013551116\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.9724 - val_loss: 1717.3262\n",
      "Epoch 91/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4754\n",
      "Epoch 00091: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1014-46] Learning rate for epoch 91 is 0.004095045384019613\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.4754 - val_loss: 276.1952\n",
      "Epoch 92/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6523\n",
      "Epoch 00092: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1014-51] Learning rate for epoch 92 is 0.004135990981012583\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.6523 - val_loss: 8.8647\n",
      "Epoch 93/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5299\n",
      "Epoch 00093: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1014-56] Learning rate for epoch 93 is 0.004176836460828781\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.5299 - val_loss: 26.9216\n",
      "Epoch 94/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4853\n",
      "Epoch 00094: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1015-01] Learning rate for epoch 94 is 0.004217581823468208\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.4853 - val_loss: 38.3548\n",
      "Epoch 95/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9984\n",
      "Epoch 00095: val_loss did not improve from 3.66528\n",
      "\n",
      "[20210302-1015-06] Learning rate for epoch 95 is 0.004258227068930864\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.9984 - val_loss: 5.5911\n",
      "Epoch 96/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3282\n",
      "Epoch 00096: val_loss improved from 3.66528 to 3.62545, saving model to ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1015-11] Learning rate for epoch 96 is 0.0042987726628780365\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 6.3282 - val_loss: 3.6254\n",
      "Epoch 97/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8747\n",
      "Epoch 00097: val_loss did not improve from 3.62545\n",
      "\n",
      "[20210302-1015-16] Learning rate for epoch 97 is 0.0043392181396484375\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.8747 - val_loss: 4.7956\n",
      "Epoch 98/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3552\n",
      "Epoch 00098: val_loss did not improve from 3.62545\n",
      "\n",
      "[20210302-1015-21] Learning rate for epoch 98 is 0.004379563499242067\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.3552 - val_loss: 4.4091\n",
      "Epoch 99/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4139\n",
      "Epoch 00099: val_loss improved from 3.62545 to 3.45195, saving model to ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1015-26] Learning rate for epoch 99 is 0.004419809207320213\n",
      "20/20 [==============================] - 2s 93ms/step - loss: 6.4139 - val_loss: 3.4520\n",
      "Epoch 100/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6497\n",
      "Epoch 00100: val_loss did not improve from 3.45195\n",
      "\n",
      "[20210302-1015-31] Learning rate for epoch 100 is 0.004459954332560301\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.6497 - val_loss: 4.2925\n",
      "Epoch 101/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9684\n",
      "Epoch 00101: val_loss did not improve from 3.45195\n",
      "\n",
      "[20210302-1015-36] Learning rate for epoch 101 is 4.4999998749517545e-07\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.9684 - val_loss: 3.8756\n",
      "Epoch 102/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9906\n",
      "Epoch 00102: val_loss did not improve from 3.45195\n",
      "\n",
      "[20210302-1015-41] Learning rate for epoch 102 is 0.00018023152370005846\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.9906 - val_loss: 3.5189\n",
      "Epoch 103/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9370\n",
      "Epoch 00103: val_loss improved from 3.45195 to 3.34446, saving model to ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1015-46] Learning rate for epoch 103 is 0.000359613070031628\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 5.9370 - val_loss: 3.3445\n",
      "Epoch 104/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1792\n",
      "Epoch 00104: val_loss did not improve from 3.34446\n",
      "\n",
      "[20210302-1015-51] Learning rate for epoch 104 is 0.0005385946715250611\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.1792 - val_loss: 3.5404\n",
      "Epoch 105/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9879\n",
      "Epoch 00105: val_loss did not improve from 3.34446\n",
      "\n",
      "[20210302-1015-56] Learning rate for epoch 105 is 0.0007171762990765274\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.9879 - val_loss: 3.3736\n",
      "Epoch 106/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6273\n",
      "Epoch 00106: val_loss improved from 3.34446 to 3.22840, saving model to ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1016-01] Learning rate for epoch 106 is 0.0008953579817898571\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 5.6273 - val_loss: 3.2284\n",
      "Epoch 107/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0296\n",
      "Epoch 00107: val_loss did not improve from 3.22840\n",
      "\n",
      "[20210302-1016-06] Learning rate for epoch 107 is 0.0010731397196650505\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.0296 - val_loss: 3.6250\n",
      "Epoch 108/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4122\n",
      "Epoch 00108: val_loss did not improve from 3.22840\n",
      "\n",
      "[20210302-1016-11] Learning rate for epoch 108 is 0.0012505215127021074\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.4122 - val_loss: 3.4059\n",
      "Epoch 109/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3935\n",
      "Epoch 00109: val_loss did not improve from 3.22840\n",
      "\n",
      "[20210302-1016-16] Learning rate for epoch 109 is 0.001427503302693367\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.3935 - val_loss: 3.5076\n",
      "Epoch 110/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0691\n",
      "Epoch 00110: val_loss did not improve from 3.22840\n",
      "\n",
      "[20210302-1016-21] Learning rate for epoch 110 is 0.0016040850896388292\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 6.0691 - val_loss: 4.2547\n",
      "Epoch 111/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1823\n",
      "Epoch 00111: val_loss did not improve from 3.22840\n",
      "\n",
      "[20210302-1016-25] Learning rate for epoch 111 is 0.001780266989953816\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.1823 - val_loss: 3.6351\n",
      "Epoch 112/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1510\n",
      "Epoch 00112: val_loss did not improve from 3.22840\n",
      "\n",
      "[20210302-1016-30] Learning rate for epoch 112 is 0.0019560488872230053\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.1510 - val_loss: 3.9425\n",
      "Epoch 113/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1849\n",
      "Epoch 00113: val_loss did not improve from 3.22840\n",
      "\n",
      "[20210302-1016-35] Learning rate for epoch 113 is 0.002131430897861719\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.1849 - val_loss: 3.9309\n",
      "Epoch 114/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9552\n",
      "Epoch 00114: val_loss did not improve from 3.22840\n",
      "\n",
      "[20210302-1016-40] Learning rate for epoch 114 is 0.0023064129054546356\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.9552 - val_loss: 4.0332\n",
      "Epoch 115/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8155\n",
      "Epoch 00115: val_loss did not improve from 3.22840\n",
      "\n",
      "[20210302-1016-45] Learning rate for epoch 115 is 0.0024809949100017548\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.8155 - val_loss: 3.3981\n",
      "Epoch 116/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7824\n",
      "Epoch 00116: val_loss did not improve from 3.22840\n",
      "\n",
      "[20210302-1016-50] Learning rate for epoch 116 is 0.0026551769115030766\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.7824 - val_loss: 3.8213\n",
      "Epoch 117/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7091\n",
      "Epoch 00117: val_loss did not improve from 3.22840\n",
      "\n",
      "[20210302-1016-55] Learning rate for epoch 117 is 0.0028289591427892447\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.7091 - val_loss: 3.7648\n",
      "Epoch 118/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2762\n",
      "Epoch 00118: val_loss improved from 3.22840 to 3.18777, saving model to ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1017-00] Learning rate for epoch 118 is 0.0030023413710296154\n",
      "20/20 [==============================] - 2s 89ms/step - loss: 6.2762 - val_loss: 3.1878\n",
      "Epoch 119/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5473\n",
      "Epoch 00119: val_loss did not improve from 3.18777\n",
      "\n",
      "[20210302-1017-05] Learning rate for epoch 119 is 0.003175323596224189\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.5473 - val_loss: 3.6208\n",
      "Epoch 120/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1708\n",
      "Epoch 00120: val_loss did not improve from 3.18777\n",
      "\n",
      "[20210302-1017-09] Learning rate for epoch 120 is 0.003347905818372965\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.1708 - val_loss: 4.0167\n",
      "Epoch 121/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7240\n",
      "Epoch 00121: val_loss did not improve from 3.18777\n",
      "\n",
      "[20210302-1017-14] Learning rate for epoch 121 is 0.0035200880374759436\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.7240 - val_loss: 3.2833\n",
      "Epoch 122/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3733\n",
      "Epoch 00122: val_loss did not improve from 3.18777\n",
      "\n",
      "[20210302-1017-19] Learning rate for epoch 122 is 0.003691870253533125\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.3733 - val_loss: 3.4117\n",
      "Epoch 123/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2122\n",
      "Epoch 00123: val_loss did not improve from 3.18777\n",
      "\n",
      "[20210302-1017-24] Learning rate for epoch 123 is 0.0038632526993751526\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.2122 - val_loss: 3.2842\n",
      "Epoch 124/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7962\n",
      "Epoch 00124: val_loss did not improve from 3.18777\n",
      "\n",
      "[20210302-1017-29] Learning rate for epoch 124 is 0.004034235142171383\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.7962 - val_loss: 3.5479\n",
      "Epoch 125/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2666\n",
      "Epoch 00125: val_loss did not improve from 3.18777\n",
      "\n",
      "[20210302-1017-34] Learning rate for epoch 125 is 0.004204817581921816\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.2666 - val_loss: 3.9508\n",
      "Epoch 126/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9776\n",
      "Epoch 00126: val_loss did not improve from 3.18777\n",
      "\n",
      "[20210302-1017-38] Learning rate for epoch 126 is 0.0043750000186264515\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.9776 - val_loss: 4.2780\n",
      "Epoch 127/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9343\n",
      "Epoch 00127: val_loss improved from 3.18777 to 3.10013, saving model to ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1017-44] Learning rate for epoch 127 is 0.0041952175088226795\n",
      "20/20 [==============================] - 2s 92ms/step - loss: 5.9343 - val_loss: 3.1001\n",
      "Epoch 128/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9222\n",
      "Epoch 00128: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1017-49] Learning rate for epoch 128 is 0.004015835002064705\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.9222 - val_loss: 3.5074\n",
      "Epoch 129/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7577\n",
      "Epoch 00129: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1017-54] Learning rate for epoch 129 is 0.003836852265521884\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.7577 - val_loss: 4.1965\n",
      "Epoch 130/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1323\n",
      "Epoch 00130: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1017-58] Learning rate for epoch 130 is 0.003658269764855504\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.1323 - val_loss: 5.7470\n",
      "Epoch 131/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3784\n",
      "Epoch 00131: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1018-03] Learning rate for epoch 131 is 0.003480087034404278\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.3784 - val_loss: 3.8171\n",
      "Epoch 132/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8250\n",
      "Epoch 00132: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1018-08] Learning rate for epoch 132 is 0.003302304306998849\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.8250 - val_loss: 5.1237\n",
      "Epoch 133/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3261\n",
      "Epoch 00133: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1018-13] Learning rate for epoch 133 is 0.0031249215826392174\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.3261 - val_loss: 3.8653\n",
      "Epoch 134/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0996\n",
      "Epoch 00134: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1018-18] Learning rate for epoch 134 is 0.0029479386284947395\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.0996 - val_loss: 3.8578\n",
      "Epoch 135/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9240\n",
      "Epoch 00135: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1018-22] Learning rate for epoch 135 is 0.0027713559102267027\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.9240 - val_loss: 3.3103\n",
      "Epoch 136/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9591\n",
      "Epoch 00136: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1018-27] Learning rate for epoch 136 is 0.0025951729621738195\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.9591 - val_loss: 3.3658\n",
      "Epoch 137/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5954\n",
      "Epoch 00137: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1018-32] Learning rate for epoch 137 is 0.0024193900171667337\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.5954 - val_loss: 3.2753\n",
      "Epoch 138/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7956\n",
      "Epoch 00138: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1018-37] Learning rate for epoch 138 is 0.0022440070752054453\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.7956 - val_loss: 3.2983\n",
      "Epoch 139/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3094\n",
      "Epoch 00139: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1018-42] Learning rate for epoch 139 is 0.002069024136289954\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.3094 - val_loss: 4.2195\n",
      "Epoch 140/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7805\n",
      "Epoch 00140: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1018-47] Learning rate for epoch 140 is 0.0018944410840049386\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.7805 - val_loss: 3.3729\n",
      "Epoch 141/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4301\n",
      "Epoch 00141: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1018-51] Learning rate for epoch 141 is 0.0017202580347657204\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.4301 - val_loss: 3.3346\n",
      "Epoch 142/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4981\n",
      "Epoch 00142: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1018-56] Learning rate for epoch 142 is 0.0015464748721569777\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.4981 - val_loss: 3.4559\n",
      "Epoch 143/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4580\n",
      "Epoch 00143: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1019-01] Learning rate for epoch 143 is 0.0013730917125940323\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.4580 - val_loss: 3.7301\n",
      "Epoch 144/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9046\n",
      "Epoch 00144: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1019-06] Learning rate for epoch 144 is 0.0012001085560768843\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.9046 - val_loss: 3.2588\n",
      "Epoch 145/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6847\n",
      "Epoch 00145: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1019-11] Learning rate for epoch 145 is 0.0010275252861902118\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.6847 - val_loss: 3.4611\n",
      "Epoch 146/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6049\n",
      "Epoch 00146: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1019-16] Learning rate for epoch 146 is 0.0008553420193493366\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6049 - val_loss: 3.5057\n",
      "Epoch 147/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9565\n",
      "Epoch 00147: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1019-20] Learning rate for epoch 147 is 0.0006835586973465979\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.9565 - val_loss: 3.1376\n",
      "Epoch 148/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4203\n",
      "Epoch 00148: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1019-25] Learning rate for epoch 148 is 0.0005121753201819956\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.4203 - val_loss: 3.1434\n",
      "Epoch 149/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6668\n",
      "Epoch 00149: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1019-30] Learning rate for epoch 149 is 0.00034119191695936024\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.6668 - val_loss: 3.1775\n",
      "Epoch 150/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6481\n",
      "Epoch 00150: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1019-35] Learning rate for epoch 150 is 0.00017060847312677652\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.6481 - val_loss: 3.1548\n",
      "Epoch 151/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1071\n",
      "Epoch 00151: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1019-40] Learning rate for epoch 151 is 4.249999960848072e-07\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.1071 - val_loss: 3.1676\n",
      "Epoch 152/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2796\n",
      "Epoch 00152: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1019-45] Learning rate for epoch 152 is 0.00017020752420648932\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.2796 - val_loss: 3.1774\n",
      "Epoch 153/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1358\n",
      "Epoch 00153: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1019-50] Learning rate for epoch 153 is 0.0003395900712348521\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.1358 - val_loss: 3.1659\n",
      "Epoch 154/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3913\n",
      "Epoch 00154: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1019-54] Learning rate for epoch 154 is 0.0005085726734250784\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.3913 - val_loss: 3.2196\n",
      "Epoch 155/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6664\n",
      "Epoch 00155: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1019-59] Learning rate for epoch 155 is 0.0006771553307771683\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.6664 - val_loss: 3.3485\n",
      "Epoch 156/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3229\n",
      "Epoch 00156: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1020-04] Learning rate for epoch 156 is 0.0008453379850834608\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.3229 - val_loss: 3.3051\n",
      "Epoch 157/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3081\n",
      "Epoch 00157: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1020-09] Learning rate for epoch 157 is 0.0010131207527592778\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.3081 - val_loss: 3.5013\n",
      "Epoch 158/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8826\n",
      "Epoch 00158: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1020-14] Learning rate for epoch 158 is 0.0011805035173892975\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.8826 - val_loss: 3.1761\n",
      "Epoch 159/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6999\n",
      "Epoch 00159: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1020-18] Learning rate for epoch 159 is 0.0013474862789735198\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6999 - val_loss: 3.4732\n",
      "Epoch 160/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2248\n",
      "Epoch 00160: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1020-23] Learning rate for epoch 160 is 0.0015140691539272666\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.2248 - val_loss: 3.4285\n",
      "Epoch 161/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6429\n",
      "Epoch 00161: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1020-28] Learning rate for epoch 161 is 0.001680252025835216\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.6429 - val_loss: 3.4598\n",
      "Epoch 162/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7800\n",
      "Epoch 00162: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1020-33] Learning rate for epoch 162 is 0.0018460348946973681\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.7800 - val_loss: 3.5357\n",
      "Epoch 163/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7840\n",
      "Epoch 00163: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1020-38] Learning rate for epoch 163 is 0.0020114178769290447\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.7840 - val_loss: 3.7458\n",
      "Epoch 164/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5290\n",
      "Epoch 00164: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1020-43] Learning rate for epoch 164 is 0.0021764009725302458\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.5290 - val_loss: 3.5969\n",
      "Epoch 165/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7359\n",
      "Epoch 00165: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1020-48] Learning rate for epoch 165 is 0.002340983832255006\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.7359 - val_loss: 3.8450\n",
      "Epoch 166/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9350\n",
      "Epoch 00166: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1020-52] Learning rate for epoch 166 is 0.002505166921764612\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.9350 - val_loss: 3.4646\n",
      "Epoch 167/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3936\n",
      "Epoch 00167: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1020-57] Learning rate for epoch 167 is 0.002668950008228421\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.3936 - val_loss: 3.6330\n",
      "Epoch 168/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8645\n",
      "Epoch 00168: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1021-02] Learning rate for epoch 168 is 0.0028323333244770765\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.8645 - val_loss: 3.3710\n",
      "Epoch 169/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6198\n",
      "Epoch 00169: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1021-07] Learning rate for epoch 169 is 0.002995316404849291\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6198 - val_loss: 3.9343\n",
      "Epoch 170/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9351\n",
      "Epoch 00170: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1021-12] Learning rate for epoch 170 is 0.0031578997150063515\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.9351 - val_loss: 3.7357\n",
      "Epoch 171/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5127\n",
      "Epoch 00171: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1021-16] Learning rate for epoch 171 is 0.0033200830221176147\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.5127 - val_loss: 4.0185\n",
      "Epoch 172/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1157\n",
      "Epoch 00172: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1021-21] Learning rate for epoch 172 is 0.0034818663261830807\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.1157 - val_loss: 4.4304\n",
      "Epoch 173/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3528\n",
      "Epoch 00173: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1021-26] Learning rate for epoch 173 is 0.0036432496272027493\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.3528 - val_loss: 3.9607\n",
      "Epoch 174/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5305\n",
      "Epoch 00174: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1021-31] Learning rate for epoch 174 is 0.003804233158007264\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.5305 - val_loss: 4.0401\n",
      "Epoch 175/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7568\n",
      "Epoch 00175: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1021-36] Learning rate for epoch 175 is 0.003964816685765982\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.7568 - val_loss: 3.3782\n",
      "Epoch 176/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8927\n",
      "Epoch 00176: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1021-41] Learning rate for epoch 176 is 0.004124999977648258\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.8927 - val_loss: 5.6116\n",
      "Epoch 177/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7312\n",
      "Epoch 00177: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1021-45] Learning rate for epoch 177 is 0.003955216612666845\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.7312 - val_loss: 4.0177\n",
      "Epoch 178/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1648\n",
      "Epoch 00178: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1021-50] Learning rate for epoch 178 is 0.003785833017900586\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.1648 - val_loss: 4.0206\n",
      "Epoch 179/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8488\n",
      "Epoch 00179: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1021-55] Learning rate for epoch 179 is 0.0036168494261801243\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8488 - val_loss: 5.3489\n",
      "Epoch 180/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8528\n",
      "Epoch 00180: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1022-00] Learning rate for epoch 180 is 0.003448265604674816\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.8528 - val_loss: 6.6808\n",
      "Epoch 181/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7878\n",
      "Epoch 00181: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1022-05] Learning rate for epoch 181 is 0.003280082019045949\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.7878 - val_loss: 4.9320\n",
      "Epoch 182/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9369\n",
      "Epoch 00182: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1022-10] Learning rate for epoch 182 is 0.0031122982036322355\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.9369 - val_loss: 3.8114\n",
      "Epoch 183/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2131\n",
      "Epoch 00183: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1022-14] Learning rate for epoch 183 is 0.002944914624094963\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.2131 - val_loss: 3.9492\n",
      "Epoch 184/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1711\n",
      "Epoch 00184: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1022-19] Learning rate for epoch 184 is 0.0027779308147728443\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.1711 - val_loss: 4.4485\n",
      "Epoch 185/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5150\n",
      "Epoch 00185: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1022-24] Learning rate for epoch 185 is 0.0026113467756658792\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5150 - val_loss: 4.1272\n",
      "Epoch 186/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1108\n",
      "Epoch 00186: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1022-29] Learning rate for epoch 186 is 0.002445162972435355\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.1108 - val_loss: 4.1371\n",
      "Epoch 187/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0905\n",
      "Epoch 00187: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1022-34] Learning rate for epoch 187 is 0.0022793791722506285\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.0905 - val_loss: 3.1977\n",
      "Epoch 188/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9834\n",
      "Epoch 00188: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1022-39] Learning rate for epoch 188 is 0.0021139951422810555\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.9834 - val_loss: 3.1751\n",
      "Epoch 189/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0632\n",
      "Epoch 00189: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1022-43] Learning rate for epoch 189 is 0.0019490111153572798\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.0632 - val_loss: 3.5942\n",
      "Epoch 190/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9035\n",
      "Epoch 00190: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1022-48] Learning rate for epoch 190 is 0.0017844270914793015\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.9035 - val_loss: 3.1769\n",
      "Epoch 191/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8082\n",
      "Epoch 00191: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1022-53] Learning rate for epoch 191 is 0.0016202429542317986\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.8082 - val_loss: 3.5492\n",
      "Epoch 192/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3754\n",
      "Epoch 00192: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1022-58] Learning rate for epoch 192 is 0.001456458936445415\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.3754 - val_loss: 3.7609\n",
      "Epoch 193/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6525\n",
      "Epoch 00193: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1023-03] Learning rate for epoch 193 is 0.001293074688874185\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.6525 - val_loss: 3.6638\n",
      "Epoch 194/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3958\n",
      "Epoch 00194: val_loss did not improve from 3.10013\n",
      "\n",
      "[20210302-1023-07] Learning rate for epoch 194 is 0.0011300905607640743\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.3958 - val_loss: 3.8566\n",
      "Epoch 195/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3840\n",
      "Epoch 00195: val_loss improved from 3.10013 to 3.07744, saving model to ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1023-13] Learning rate for epoch 195 is 0.0009675062610767782\n",
      "20/20 [==============================] - 2s 89ms/step - loss: 5.3840 - val_loss: 3.0774\n",
      "Epoch 196/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4592\n",
      "Epoch 00196: val_loss did not improve from 3.07744\n",
      "\n",
      "[20210302-1023-18] Learning rate for epoch 196 is 0.0008053220226429403\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.4592 - val_loss: 3.0994\n",
      "Epoch 197/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6010\n",
      "Epoch 00197: val_loss did not improve from 3.07744\n",
      "\n",
      "[20210302-1023-22] Learning rate for epoch 197 is 0.0006435376708395779\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.6010 - val_loss: 3.4224\n",
      "Epoch 198/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2786\n",
      "Epoch 00198: val_loss did not improve from 3.07744\n",
      "\n",
      "[20210302-1023-27] Learning rate for epoch 198 is 0.0004821533220820129\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.2786 - val_loss: 3.2648\n",
      "Epoch 199/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1840\n",
      "Epoch 00199: val_loss did not improve from 3.07744\n",
      "\n",
      "[20210302-1023-32] Learning rate for epoch 199 is 0.0003211689181625843\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.1840 - val_loss: 3.1201\n",
      "Epoch 200/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4228\n",
      "Epoch 00200: val_loss did not improve from 3.07744\n",
      "\n",
      "[20210302-1023-37] Learning rate for epoch 200 is 0.00016058447363320738\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.4228 - val_loss: 3.1443\n",
      "Epoch 201/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3965\n",
      "Epoch 00201: val_loss did not improve from 3.07744\n",
      "\n",
      "[20210302-1023-42] Learning rate for epoch 201 is 4.0000000467443897e-07\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.3965 - val_loss: 3.1134\n",
      "Epoch 202/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1778\n",
      "Epoch 00202: val_loss did not improve from 3.07744\n",
      "\n",
      "[20210302-1023-47] Learning rate for epoch 202 is 0.0001601835247129202\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.1778 - val_loss: 3.0911\n",
      "Epoch 203/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4667\n",
      "Epoch 00203: val_loss improved from 3.07744 to 3.06490, saving model to ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1023-52] Learning rate for epoch 203 is 0.00031956707243807614\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 5.4667 - val_loss: 3.0649\n",
      "Epoch 204/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5367\n",
      "Epoch 00204: val_loss did not improve from 3.06490\n",
      "\n",
      "[20210302-1023-57] Learning rate for epoch 204 is 0.00047855067532509565\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.5367 - val_loss: 3.0961\n",
      "Epoch 205/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4793\n",
      "Epoch 00205: val_loss did not improve from 3.06490\n",
      "\n",
      "[20210302-1024-02] Learning rate for epoch 205 is 0.0006371343042701483\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.4793 - val_loss: 3.3224\n",
      "Epoch 206/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3903\n",
      "Epoch 00206: val_loss improved from 3.06490 to 3.00232, saving model to ./20210301-225844/heel_K6_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1024-07] Learning rate for epoch 206 is 0.0007953179883770645\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 5.3903 - val_loss: 3.0023\n",
      "Epoch 207/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6797\n",
      "Epoch 00207: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1024-12] Learning rate for epoch 207 is 0.0009531017276458442\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.6797 - val_loss: 3.1474\n",
      "Epoch 208/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0364\n",
      "Epoch 00208: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1024-17] Learning rate for epoch 208 is 0.0011104855220764875\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.0364 - val_loss: 3.4354\n",
      "Epoch 209/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1335\n",
      "Epoch 00209: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1024-22] Learning rate for epoch 209 is 0.0012674692552536726\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 6.1335 - val_loss: 3.6742\n",
      "Epoch 210/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4615\n",
      "Epoch 00210: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1024-26] Learning rate for epoch 210 is 0.0014240531018003821\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.4615 - val_loss: 6.4456\n",
      "Epoch 211/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3585\n",
      "Epoch 00211: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1024-31] Learning rate for epoch 211 is 0.0015802369453012943\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.3585 - val_loss: 5.6725\n",
      "Epoch 212/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2125\n",
      "Epoch 00212: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1024-36] Learning rate for epoch 212 is 0.001736020902171731\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.2125 - val_loss: 14.6818\n",
      "Epoch 213/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3650\n",
      "Epoch 00213: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1024-41] Learning rate for epoch 213 is 0.0018914048559963703\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.3650 - val_loss: 12.2597\n",
      "Epoch 214/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9049\n",
      "Epoch 00214: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1024-46] Learning rate for epoch 214 is 0.0020463888067752123\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.9049 - val_loss: 3.1548\n",
      "Epoch 215/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6205\n",
      "Epoch 00215: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1024-51] Learning rate for epoch 215 is 0.0022009729873389006\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.6205 - val_loss: 3.5012\n",
      "Epoch 216/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6190\n",
      "Epoch 00216: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1024-56] Learning rate for epoch 216 is 0.002355156932026148\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.6190 - val_loss: 6.2372\n",
      "Epoch 217/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4158\n",
      "Epoch 00217: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1025-00] Learning rate for epoch 217 is 0.0025089411064982414\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.4158 - val_loss: 3.9981\n",
      "Epoch 218/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2844\n",
      "Epoch 00218: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1025-05] Learning rate for epoch 218 is 0.0026623252779245377\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.2844 - val_loss: 5.6776\n",
      "Epoch 219/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9981\n",
      "Epoch 00219: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1025-10] Learning rate for epoch 219 is 0.0028153094463050365\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.9981 - val_loss: 6.3883\n",
      "Epoch 220/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8071\n",
      "Epoch 00220: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1025-15] Learning rate for epoch 220 is 0.002967893611639738\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.8071 - val_loss: 3.9503\n",
      "Epoch 221/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0229\n",
      "Epoch 00221: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1025-20] Learning rate for epoch 221 is 0.003120078006759286\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.0229 - val_loss: 3.6048\n",
      "Epoch 222/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0276\n",
      "Epoch 00222: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1025-24] Learning rate for epoch 222 is 0.0032718623988330364\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.0276 - val_loss: 4.6080\n",
      "Epoch 223/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2966\n",
      "Epoch 00223: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1025-29] Learning rate for epoch 223 is 0.0034232467878609896\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.2966 - val_loss: 4.0681\n",
      "Epoch 224/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9105\n",
      "Epoch 00224: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1025-34] Learning rate for epoch 224 is 0.0035742311738431454\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 5.9105 - val_loss: 5.1604\n",
      "Epoch 225/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6234\n",
      "Epoch 00225: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1025-39] Learning rate for epoch 225 is 0.003724815556779504\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.6234 - val_loss: 6.4894\n",
      "Epoch 226/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8617\n",
      "Epoch 00226: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1025-44] Learning rate for epoch 226 is 0.003874999936670065\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.8617 - val_loss: 5.8297\n",
      "Epoch 227/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8044\n",
      "Epoch 00227: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1025-49] Learning rate for epoch 227 is 0.0037152154836803675\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.8044 - val_loss: 8.3122\n",
      "Epoch 228/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0610\n",
      "Epoch 00228: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1025-54] Learning rate for epoch 228 is 0.0035558310337364674\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.0610 - val_loss: 4.9805\n",
      "Epoch 229/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9100\n",
      "Epoch 00229: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1025-58] Learning rate for epoch 229 is 0.003396846354007721\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.9100 - val_loss: 79.0852\n",
      "Epoch 230/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4467\n",
      "Epoch 00230: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1026-03] Learning rate for epoch 230 is 0.003238261677324772\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.4467 - val_loss: 75.4047\n",
      "Epoch 231/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8864\n",
      "Epoch 00231: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1026-08] Learning rate for epoch 231 is 0.00308007700368762\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.8864 - val_loss: 19.2782\n",
      "Epoch 232/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8008\n",
      "Epoch 00232: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1026-13] Learning rate for epoch 232 is 0.002922292333096266\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8008 - val_loss: 10.8574\n",
      "Epoch 233/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4216\n",
      "Epoch 00233: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1026-18] Learning rate for epoch 233 is 0.002764907432720065\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.4216 - val_loss: 7.2414\n",
      "Epoch 234/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9385\n",
      "Epoch 00234: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1026-23] Learning rate for epoch 234 is 0.0026079227682203054\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.9385 - val_loss: 5.0021\n",
      "Epoch 235/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1718\n",
      "Epoch 00235: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1026-27] Learning rate for epoch 235 is 0.0024513378739356995\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.1718 - val_loss: 3.3773\n",
      "Epoch 236/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0037\n",
      "Epoch 00236: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1026-32] Learning rate for epoch 236 is 0.002295152982696891\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.0037 - val_loss: 3.7715\n",
      "Epoch 237/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7244\n",
      "Epoch 00237: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1026-37] Learning rate for epoch 237 is 0.0021393680945038795\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.7244 - val_loss: 4.9953\n",
      "Epoch 238/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7860\n",
      "Epoch 00238: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1026-42] Learning rate for epoch 238 is 0.0019839832093566656\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 5.7860 - val_loss: 4.1462\n",
      "Epoch 239/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8223\n",
      "Epoch 00239: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1026-47] Learning rate for epoch 239 is 0.0018289980944246054\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.8223 - val_loss: 4.3059\n",
      "Epoch 240/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0414\n",
      "Epoch 00240: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1026-52] Learning rate for epoch 240 is 0.0016744130989536643\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.0414 - val_loss: 4.1177\n",
      "Epoch 241/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5247\n",
      "Epoch 00241: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1026-56] Learning rate for epoch 241 is 0.0015202279901131988\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.5247 - val_loss: 4.0179\n",
      "Epoch 242/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5934\n",
      "Epoch 00242: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1027-01] Learning rate for epoch 242 is 0.0013664428843185306\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5934 - val_loss: 3.5231\n",
      "Epoch 243/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5528\n",
      "Epoch 00243: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1027-06] Learning rate for epoch 243 is 0.0012130576651543379\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.5528 - val_loss: 3.4866\n",
      "Epoch 244/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3332\n",
      "Epoch 00244: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1027-11] Learning rate for epoch 244 is 0.0010600725654512644\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.3332 - val_loss: 3.5812\n",
      "Epoch 245/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7560\n",
      "Epoch 00245: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1027-16] Learning rate for epoch 245 is 0.0009074872941710055\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.7560 - val_loss: 3.2996\n",
      "Epoch 246/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5031\n",
      "Epoch 00246: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1027-21] Learning rate for epoch 246 is 0.0007553020259365439\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.5031 - val_loss: 3.5979\n",
      "Epoch 247/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5053\n",
      "Epoch 00247: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1027-25] Learning rate for epoch 247 is 0.0006035167025402188\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.5053 - val_loss: 3.3895\n",
      "Epoch 248/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3297\n",
      "Epoch 00248: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1027-30] Learning rate for epoch 248 is 0.00045213132398203015\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.3297 - val_loss: 3.3329\n",
      "Epoch 249/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4735\n",
      "Epoch 00249: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1027-35] Learning rate for epoch 249 is 0.00030114591936580837\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.4735 - val_loss: 3.4891\n",
      "Epoch 250/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5258\n",
      "Epoch 00250: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1027-40] Learning rate for epoch 250 is 0.00015056047413963825\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.5258 - val_loss: 3.3230\n",
      "Epoch 251/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5269\n",
      "Epoch 00251: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1027-45] Learning rate for epoch 251 is 3.7500001326407073e-07\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.5269 - val_loss: 3.2831\n",
      "Epoch 252/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5924\n",
      "Epoch 00252: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1027-50] Learning rate for epoch 252 is 0.00015015952521935105\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5924 - val_loss: 3.2738\n",
      "Epoch 253/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3869\n",
      "Epoch 00253: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1027-54] Learning rate for epoch 253 is 0.0002995440736413002\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.3869 - val_loss: 3.1729\n",
      "Epoch 254/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3363\n",
      "Epoch 00254: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1027-59] Learning rate for epoch 254 is 0.0004485286772251129\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.3363 - val_loss: 3.2128\n",
      "Epoch 255/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2876\n",
      "Epoch 00255: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1028-04] Learning rate for epoch 255 is 0.0005971133359707892\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.2876 - val_loss: 3.1323\n",
      "Epoch 256/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3202\n",
      "Epoch 00256: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1028-09] Learning rate for epoch 256 is 0.0007452979916706681\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.3202 - val_loss: 3.4080\n",
      "Epoch 257/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5402\n",
      "Epoch 00257: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1028-14] Learning rate for epoch 257 is 0.0008930827025324106\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5402 - val_loss: 3.5427\n",
      "Epoch 258/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4367\n",
      "Epoch 00258: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1028-19] Learning rate for epoch 258 is 0.0010404675267636776\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.4367 - val_loss: 3.4448\n",
      "Epoch 259/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5916\n",
      "Epoch 00259: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1028-23] Learning rate for epoch 259 is 0.0011874522315338254\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5916 - val_loss: 3.5340\n",
      "Epoch 260/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4325\n",
      "Epoch 00260: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1028-28] Learning rate for epoch 260 is 0.0013340371660888195\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.4325 - val_loss: 3.3570\n",
      "Epoch 261/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1633\n",
      "Epoch 00261: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1028-33] Learning rate for epoch 261 is 0.0014802219811826944\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.1633 - val_loss: 3.6484\n",
      "Epoch 262/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5635\n",
      "Epoch 00262: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1028-38] Learning rate for epoch 262 is 0.0016260069096460938\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.5635 - val_loss: 3.2067\n",
      "Epoch 263/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6089\n",
      "Epoch 00263: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1028-43] Learning rate for epoch 263 is 0.001771391835063696\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6089 - val_loss: 4.1995\n",
      "Epoch 264/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5825\n",
      "Epoch 00264: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1028-48] Learning rate for epoch 264 is 0.0019163768738508224\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5825 - val_loss: 3.5185\n",
      "Epoch 265/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7835\n",
      "Epoch 00265: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1028-52] Learning rate for epoch 265 is 0.0020609619095921516\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.7835 - val_loss: 3.5080\n",
      "Epoch 266/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7167\n",
      "Epoch 00266: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1028-57] Learning rate for epoch 266 is 0.0022051469422876835\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.7167 - val_loss: 5.4645\n",
      "Epoch 267/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4461\n",
      "Epoch 00267: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1029-02] Learning rate for epoch 267 is 0.0023489322047680616\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.4461 - val_loss: 3.6689\n",
      "Epoch 268/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3769\n",
      "Epoch 00268: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1029-07] Learning rate for epoch 268 is 0.002492317231371999\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.3769 - val_loss: 3.4086\n",
      "Epoch 269/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3106\n",
      "Epoch 00269: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1029-12] Learning rate for epoch 269 is 0.0026353024877607822\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.3106 - val_loss: 4.1399\n",
      "Epoch 270/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4508\n",
      "Epoch 00270: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1029-16] Learning rate for epoch 270 is 0.0027778877411037683\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.4508 - val_loss: 3.7291\n",
      "Epoch 271/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7312\n",
      "Epoch 00271: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1029-21] Learning rate for epoch 271 is 0.002920072991400957\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.7312 - val_loss: 3.3972\n",
      "Epoch 272/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6534\n",
      "Epoch 00272: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1029-26] Learning rate for epoch 272 is 0.0030618582386523485\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.6534 - val_loss: 4.3547\n",
      "Epoch 273/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7049\n",
      "Epoch 00273: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1029-31] Learning rate for epoch 273 is 0.0032032437156885862\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.7049 - val_loss: 5.2490\n",
      "Epoch 274/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0833\n",
      "Epoch 00274: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1029-36] Learning rate for epoch 274 is 0.0033442291896790266\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.0833 - val_loss: 3.3068\n",
      "Epoch 275/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1553\n",
      "Epoch 00275: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1029-41] Learning rate for epoch 275 is 0.003484814427793026\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.1553 - val_loss: 3.6142\n",
      "Epoch 276/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9558\n",
      "Epoch 00276: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1029-45] Learning rate for epoch 276 is 0.0036249998956918716\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.9558 - val_loss: 3.9588\n",
      "Epoch 277/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5976\n",
      "Epoch 00277: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1029-50] Learning rate for epoch 277 is 0.0034752145875245333\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.5976 - val_loss: 5.3290\n",
      "Epoch 278/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0503\n",
      "Epoch 00278: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1029-55] Learning rate for epoch 278 is 0.003325828816741705\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.0503 - val_loss: 4.7423\n",
      "Epoch 279/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3498\n",
      "Epoch 00279: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1030-00] Learning rate for epoch 279 is 0.0031768432818353176\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.3498 - val_loss: 4.0854\n",
      "Epoch 280/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8343\n",
      "Epoch 00280: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1030-05] Learning rate for epoch 280 is 0.0030282577499747276\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.8343 - val_loss: 4.2396\n",
      "Epoch 281/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1291\n",
      "Epoch 00281: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1030-09] Learning rate for epoch 281 is 0.0028800719883292913\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.1291 - val_loss: 3.3049\n",
      "Epoch 282/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0578\n",
      "Epoch 00282: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1030-14] Learning rate for epoch 282 is 0.0027322862297296524\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.0578 - val_loss: 3.4713\n",
      "Epoch 283/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3806\n",
      "Epoch 00283: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1030-19] Learning rate for epoch 283 is 0.002584900474175811\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.3806 - val_loss: 3.3697\n",
      "Epoch 284/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5697\n",
      "Epoch 00284: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1030-24] Learning rate for epoch 284 is 0.0024379147216677666\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.5697 - val_loss: 3.9196\n",
      "Epoch 285/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3175\n",
      "Epoch 00285: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1030-29] Learning rate for epoch 285 is 0.0022913289722055197\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.3175 - val_loss: 3.2030\n",
      "Epoch 286/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3196\n",
      "Epoch 00286: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1030-34] Learning rate for epoch 286 is 0.0021451429929584265\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.3196 - val_loss: 3.3189\n",
      "Epoch 287/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1170\n",
      "Epoch 00287: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1030-38] Learning rate for epoch 287 is 0.0019993570167571306\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.1170 - val_loss: 4.1087\n",
      "Epoch 288/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5530\n",
      "Epoch 00288: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1030-43] Learning rate for epoch 288 is 0.001853971160016954\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.5530 - val_loss: 6.4219\n",
      "Epoch 289/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7197\n",
      "Epoch 00289: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1030-48] Learning rate for epoch 289 is 0.001708985073491931\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.7197 - val_loss: 4.0148\n",
      "Epoch 290/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9707\n",
      "Epoch 00290: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1030-53] Learning rate for epoch 290 is 0.0015643991064280272\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.9707 - val_loss: 5.6510\n",
      "Epoch 291/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7478\n",
      "Epoch 00291: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1030-58] Learning rate for epoch 291 is 0.0014202130259945989\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.7478 - val_loss: 3.1888\n",
      "Epoch 292/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4131\n",
      "Epoch 00292: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1031-02] Learning rate for epoch 292 is 0.001276426832191646\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.4131 - val_loss: 3.6043\n",
      "Epoch 293/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6572\n",
      "Epoch 00293: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1031-07] Learning rate for epoch 293 is 0.0011330407578498125\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.6572 - val_loss: 3.5892\n",
      "Epoch 294/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3283\n",
      "Epoch 00294: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1031-12] Learning rate for epoch 294 is 0.0009900545701384544\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.3283 - val_loss: 3.3693\n",
      "Epoch 295/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3902\n",
      "Epoch 00295: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1031-17] Learning rate for epoch 295 is 0.0008474682690575719\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.3902 - val_loss: 3.2516\n",
      "Epoch 296/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.0069\n",
      "Epoch 00296: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1031-22] Learning rate for epoch 296 is 0.0007052819710224867\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.0069 - val_loss: 3.3415\n",
      "Epoch 297/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7766\n",
      "Epoch 00297: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1031-27] Learning rate for epoch 297 is 0.0005634956760331988\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.7766 - val_loss: 3.4404\n",
      "Epoch 298/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7357\n",
      "Epoch 00298: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1031-31] Learning rate for epoch 298 is 0.0004221093258820474\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.7357 - val_loss: 3.3791\n",
      "Epoch 299/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1294\n",
      "Epoch 00299: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1031-36] Learning rate for epoch 299 is 0.00028112292056903243\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.1294 - val_loss: 3.1528\n",
      "Epoch 300/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6403\n",
      "Epoch 00300: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1031-41] Learning rate for epoch 300 is 0.0001405364746460691\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.6403 - val_loss: 3.0981\n",
      "Epoch 301/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6225\n",
      "Epoch 00301: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1031-46] Learning rate for epoch 301 is 3.4999999343199306e-07\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.6225 - val_loss: 3.0756\n",
      "Epoch 302/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5401\n",
      "Epoch 00302: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1031-51] Learning rate for epoch 302 is 0.00014013552572578192\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.5401 - val_loss: 3.1125\n",
      "Epoch 303/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4573\n",
      "Epoch 00303: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1031-55] Learning rate for epoch 303 is 0.00027952107484452426\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.4573 - val_loss: 3.1682\n",
      "Epoch 304/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2541\n",
      "Epoch 00304: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1032-00] Learning rate for epoch 304 is 0.0004185066791251302\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.2541 - val_loss: 3.1662\n",
      "Epoch 305/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4241\n",
      "Epoch 00305: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1032-05] Learning rate for epoch 305 is 0.0005570923094637692\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.4241 - val_loss: 3.1207\n",
      "Epoch 306/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6127\n",
      "Epoch 00306: val_loss did not improve from 3.00232\n",
      "\n",
      "[20210302-1032-10] Learning rate for epoch 306 is 0.0006952779949642718\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.6127 - val_loss: 3.0993\n",
      "K= 7\n",
      "Check cache in memory:Y True\n",
      "Check keypoints process01: Tensor(\"args_1:0\", shape=(2,), dtype=int64) (2,) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Check keypoints process02: Tensor(\"Reshape:0\", shape=(1, 2), dtype=int64) (1, 2) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Check keypoints process03: Tensor(\"PyFunc:1\", dtype=float32, device=/job:localhost/replica:0/task:0)\n",
      "Check keypoints process04: Tensor(\"Reshape_1:0\", shape=(2,), dtype=float32)\n",
      "Check augment :Y True\n",
      "Check ds cache[True] and augment[True]\n",
      "Check cache in memory:Y True\n",
      "Check augment :N False\n",
      "Check ds cache[True] and augment[False]\n",
      "best_model_name: ./20210301-225844/heel_K7_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "base_model.trainable :  False\n",
      "\n",
      "[20210302-1032-13] Learning rate for epoch 1 is 0.00047855067532509565\n",
      "Epoch 1/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 105.7138\n",
      "Epoch 00001: val_loss improved from inf to 76.47449, saving model to ./20210301-225844/heel_K7_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 6s 290ms/step - loss: 104.4707 - val_loss: 76.4745\n",
      "\n",
      "[20210302-1032-28] Learning rate for epoch 2 is 0.00047855067532509565\n",
      "Epoch 2/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 41.0637\n",
      "Epoch 00002: val_loss improved from 76.47449 to 26.04586, saving model to ./20210301-225844/heel_K7_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 41.0637 - val_loss: 26.0459\n",
      "\n",
      "[20210302-1032-33] Learning rate for epoch 3 is 0.00047855067532509565\n",
      "Epoch 3/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 23.1582\n",
      "Epoch 00003: val_loss improved from 26.04586 to 22.33297, saving model to ./20210301-225844/heel_K7_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 9s 449ms/step - loss: 23.1582 - val_loss: 22.3330\n",
      "\n",
      "[20210302-1032-45] Learning rate for epoch 4 is 0.00047855067532509565\n",
      "Epoch 4/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 21.1793\n",
      "Epoch 00004: val_loss improved from 22.33297 to 21.49328, saving model to ./20210301-225844/heel_K7_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 21.0696 - val_loss: 21.4933\n",
      "\n",
      "[20210302-1032-49] Learning rate for epoch 5 is 0.00047855067532509565\n",
      "Epoch 5/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 19.2996\n",
      "Epoch 00005: val_loss improved from 21.49328 to 21.35248, saving model to ./20210301-225844/heel_K7_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 19.3433 - val_loss: 21.3525\n",
      "\n",
      "[20210302-1032-54] Learning rate for epoch 6 is 0.00047855067532509565\n",
      "Epoch 6/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 18.2830\n",
      "Epoch 00006: val_loss improved from 21.35248 to 20.33788, saving model to ./20210301-225844/heel_K7_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 18.2946 - val_loss: 20.3379\n",
      "\n",
      "[20210302-1032-58] Learning rate for epoch 7 is 0.00047855067532509565\n",
      "Epoch 7/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 18.3821\n",
      "Epoch 00007: val_loss improved from 20.33788 to 19.61037, saving model to ./20210301-225844/heel_K7_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 18.4187 - val_loss: 19.6104\n",
      "\n",
      "[20210302-1033-02] Learning rate for epoch 8 is 0.00047855067532509565\n",
      "Epoch 8/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 17.8284\n",
      "Epoch 00008: val_loss improved from 19.61037 to 19.02831, saving model to ./20210301-225844/heel_K7_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 17.8284 - val_loss: 19.0283\n",
      "\n",
      "[20210302-1033-07] Learning rate for epoch 9 is 0.00047855067532509565\n",
      "Epoch 9/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 17.9028\n",
      "Epoch 00009: val_loss improved from 19.02831 to 18.89366, saving model to ./20210301-225844/heel_K7_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 17.8313 - val_loss: 18.8937\n",
      "\n",
      "[20210302-1033-11] Learning rate for epoch 10 is 0.00047855067532509565\n",
      "Epoch 10/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 17.7576\n",
      "Epoch 00010: val_loss improved from 18.89366 to 18.00395, saving model to ./20210301-225844/heel_K7_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 17.7576 - val_loss: 18.0039\n",
      "\n",
      "[20210302-1033-15] Learning rate for epoch 11 is 0.00047855067532509565\n",
      "Epoch 11/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 17.3241\n",
      "Epoch 00011: val_loss improved from 18.00395 to 17.14140, saving model to ./20210301-225844/heel_K7_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 17.2782 - val_loss: 17.1414\n",
      "\n",
      "[20210302-1033-20] Learning rate for epoch 12 is 0.00047855067532509565\n",
      "Epoch 12/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 17.7992\n",
      "Epoch 00012: val_loss improved from 17.14140 to 17.03420, saving model to ./20210301-225844/heel_K7_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 17.7992 - val_loss: 17.0342\n",
      "\n",
      "[20210302-1033-24] Learning rate for epoch 13 is 0.00047855067532509565\n",
      "Epoch 13/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 17.8632\n",
      "Epoch 00013: val_loss improved from 17.03420 to 16.82293, saving model to ./20210301-225844/heel_K7_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 17.8176 - val_loss: 16.8229\n",
      "\n",
      "[20210302-1033-28] Learning rate for epoch 14 is 0.00047855067532509565\n",
      "Epoch 14/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 17.3936\n",
      "Epoch 00014: val_loss improved from 16.82293 to 16.70499, saving model to ./20210301-225844/heel_K7_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 17.3936 - val_loss: 16.7050\n",
      "\n",
      "[20210302-1033-33] Learning rate for epoch 15 is 0.00047855067532509565\n",
      "Epoch 15/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.7909\n",
      "Epoch 00015: val_loss improved from 16.70499 to 15.75984, saving model to ./20210301-225844/heel_K7_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 16.7909 - val_loss: 15.7598\n",
      "\n",
      "[20210302-1033-37] Learning rate for epoch 16 is 0.00047855067532509565\n",
      "Epoch 16/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.9588\n",
      "Epoch 00016: val_loss improved from 15.75984 to 15.63627, saving model to ./20210301-225844/heel_K7_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 16.8927 - val_loss: 15.6363\n",
      "\n",
      "[20210302-1033-41] Learning rate for epoch 17 is 0.00047855067532509565\n",
      "Epoch 17/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 17.4689\n",
      "Epoch 00017: val_loss improved from 15.63627 to 14.82198, saving model to ./20210301-225844/heel_K7_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 17.4043 - val_loss: 14.8220\n",
      "\n",
      "[20210302-1033-45] Learning rate for epoch 18 is 0.00047855067532509565\n",
      "Epoch 18/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.5714\n",
      "Epoch 00018: val_loss improved from 14.82198 to 14.49599, saving model to ./20210301-225844/heel_K7_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 16.5941 - val_loss: 14.4960\n",
      "\n",
      "[20210302-1033-50] Learning rate for epoch 19 is 0.00047855067532509565\n",
      "Epoch 19/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.4713\n",
      "Epoch 00019: val_loss improved from 14.49599 to 14.04425, saving model to ./20210301-225844/heel_K7_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.4713 - val_loss: 14.0442\n",
      "\n",
      "[20210302-1033-54] Learning rate for epoch 20 is 0.00047855067532509565\n",
      "Epoch 20/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.6711\n",
      "Epoch 00020: val_loss did not improve from 14.04425\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.6711 - val_loss: 14.5278\n",
      "\n",
      "[20210302-1033-58] Learning rate for epoch 21 is 0.00047855067532509565\n",
      "Epoch 21/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.8320\n",
      "Epoch 00021: val_loss improved from 14.04425 to 14.01092, saving model to ./20210301-225844/heel_K7_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 16.8320 - val_loss: 14.0109\n",
      "\n",
      "[20210302-1034-02] Learning rate for epoch 22 is 0.00047855067532509565\n",
      "Epoch 22/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.7132\n",
      "Epoch 00022: val_loss improved from 14.01092 to 12.58239, saving model to ./20210301-225844/heel_K7_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.8938 - val_loss: 12.5824\n",
      "\n",
      "[20210302-1034-07] Learning rate for epoch 23 is 0.00047855067532509565\n",
      "Epoch 23/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 16.3289\n",
      "Epoch 00023: val_loss improved from 12.58239 to 12.30610, saving model to ./20210301-225844/heel_K7_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 16.3182 - val_loss: 12.3061\n",
      "\n",
      "[20210302-1034-11] Learning rate for epoch 24 is 0.00047855067532509565\n",
      "Epoch 24/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.7850\n",
      "Epoch 00024: val_loss improved from 12.30610 to 11.64739, saving model to ./20210301-225844/heel_K7_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 16.6365 - val_loss: 11.6474\n",
      "\n",
      "[20210302-1034-15] Learning rate for epoch 25 is 0.00047855067532509565\n",
      "Epoch 25/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.5056\n",
      "Epoch 00025: val_loss improved from 11.64739 to 11.48635, saving model to ./20210301-225844/heel_K7_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 16.5056 - val_loss: 11.4864\n",
      "\n",
      "[20210302-1034-19] Learning rate for epoch 26 is 0.00047855067532509565\n",
      "Epoch 26/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.5682\n",
      "Epoch 00026: val_loss did not improve from 11.48635\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.5176 - val_loss: 12.0056\n",
      "\n",
      "[20210302-1034-23] Learning rate for epoch 27 is 0.00047855067532509565\n",
      "Epoch 27/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 16.4512\n",
      "Epoch 00027: val_loss did not improve from 11.48635\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.4328 - val_loss: 11.6061\n",
      "\n",
      "[20210302-1034-27] Learning rate for epoch 28 is 0.00047855067532509565\n",
      "Epoch 28/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 16.3915\n",
      "Epoch 00028: val_loss did not improve from 11.48635\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.4282 - val_loss: 12.3632\n",
      "\n",
      "[20210302-1034-31] Learning rate for epoch 29 is 0.00047855067532509565\n",
      "Epoch 29/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.7131\n",
      "Epoch 00029: val_loss improved from 11.48635 to 11.05325, saving model to ./20210301-225844/heel_K7_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 15.7131 - val_loss: 11.0532\n",
      "\n",
      "[20210302-1034-36] Learning rate for epoch 30 is 0.00047855067532509565\n",
      "Epoch 30/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.0366\n",
      "Epoch 00030: val_loss improved from 11.05325 to 10.33150, saving model to ./20210301-225844/heel_K7_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 16.0366 - val_loss: 10.3315\n",
      "\n",
      "[20210302-1034-40] Learning rate for epoch 31 is 0.00047855067532509565\n",
      "Epoch 31/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.8357\n",
      "Epoch 00031: val_loss did not improve from 10.33150\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.8357 - val_loss: 11.0357\n",
      "\n",
      "[20210302-1034-44] Learning rate for epoch 32 is 0.00047855067532509565\n",
      "Epoch 32/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 16.1427\n",
      "Epoch 00032: val_loss did not improve from 10.33150\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 16.0823 - val_loss: 10.7854\n",
      "\n",
      "[20210302-1034-48] Learning rate for epoch 33 is 0.00047855067532509565\n",
      "Epoch 33/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.3099\n",
      "Epoch 00033: val_loss improved from 10.33150 to 9.62696, saving model to ./20210301-225844/heel_K7_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 16.3099 - val_loss: 9.6270\n",
      "\n",
      "[20210302-1034-52] Learning rate for epoch 34 is 0.00047855067532509565\n",
      "Epoch 34/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.6067\n",
      "Epoch 00034: val_loss did not improve from 9.62696\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.6067 - val_loss: 10.3116\n",
      "\n",
      "[20210302-1034-56] Learning rate for epoch 35 is 0.00047855067532509565\n",
      "Epoch 35/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.8988\n",
      "Epoch 00035: val_loss did not improve from 9.62696\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 15.8988 - val_loss: 11.2851\n",
      "\n",
      "[20210302-1035-00] Learning rate for epoch 36 is 0.00047855067532509565\n",
      "Epoch 36/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 15.7926\n",
      "Epoch 00036: val_loss did not improve from 9.62696\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 15.8028 - val_loss: 10.5099\n",
      "\n",
      "[20210302-1035-04] Learning rate for epoch 37 is 0.00047855067532509565\n",
      "Epoch 37/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.9982\n",
      "Epoch 00037: val_loss did not improve from 9.62696\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.0222 - val_loss: 9.7407\n",
      "\n",
      "[20210302-1035-08] Learning rate for epoch 38 is 0.00047855067532509565\n",
      "Epoch 38/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.1892\n",
      "Epoch 00038: val_loss improved from 9.62696 to 8.98871, saving model to ./20210301-225844/heel_K7_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 16.1892 - val_loss: 8.9887\n",
      "\n",
      "[20210302-1035-12] Learning rate for epoch 39 is 0.00047855067532509565\n",
      "Epoch 39/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.2264\n",
      "Epoch 00039: val_loss improved from 8.98871 to 8.44367, saving model to ./20210301-225844/heel_K7_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 16.2573 - val_loss: 8.4437\n",
      "\n",
      "[20210302-1035-17] Learning rate for epoch 40 is 0.00047855067532509565\n",
      "Epoch 40/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.6342\n",
      "Epoch 00040: val_loss improved from 8.44367 to 8.39783, saving model to ./20210301-225844/heel_K7_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.6342 - val_loss: 8.3978\n",
      "\n",
      "[20210302-1035-21] Learning rate for epoch 41 is 0.00047855067532509565\n",
      "Epoch 41/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.2722\n",
      "Epoch 00041: val_loss did not improve from 8.39783\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.2697 - val_loss: 9.0054\n",
      "\n",
      "[20210302-1035-25] Learning rate for epoch 42 is 0.00047855067532509565\n",
      "Epoch 42/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 16.1132\n",
      "Epoch 00042: val_loss improved from 8.39783 to 8.18690, saving model to ./20210301-225844/heel_K7_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 16.0074 - val_loss: 8.1869\n",
      "\n",
      "[20210302-1035-29] Learning rate for epoch 43 is 0.00047855067532509565\n",
      "Epoch 43/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.0970\n",
      "Epoch 00043: val_loss did not improve from 8.18690\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.0542 - val_loss: 8.9066\n",
      "\n",
      "[20210302-1035-33] Learning rate for epoch 44 is 0.00047855067532509565\n",
      "Epoch 44/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.2223\n",
      "Epoch 00044: val_loss did not improve from 8.18690\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.2223 - val_loss: 8.9258\n",
      "\n",
      "[20210302-1035-37] Learning rate for epoch 45 is 0.00047855067532509565\n",
      "Epoch 45/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.3957\n",
      "Epoch 00045: val_loss did not improve from 8.18690\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.3920 - val_loss: 9.1004\n",
      "\n",
      "[20210302-1035-41] Learning rate for epoch 46 is 0.00047855067532509565\n",
      "Epoch 46/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 16.4981\n",
      "Epoch 00046: val_loss did not improve from 8.18690\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.7215 - val_loss: 8.5456\n",
      "\n",
      "[20210302-1035-45] Learning rate for epoch 47 is 0.00047855067532509565\n",
      "Epoch 47/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.5305\n",
      "Epoch 00047: val_loss did not improve from 8.18690\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.4974 - val_loss: 8.8465\n",
      "\n",
      "[20210302-1035-49] Learning rate for epoch 48 is 0.00047855067532509565\n",
      "Epoch 48/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.0954\n",
      "Epoch 00048: val_loss did not improve from 8.18690\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.0954 - val_loss: 9.1572\n",
      "\n",
      "[20210302-1035-53] Learning rate for epoch 49 is 0.00047855067532509565\n",
      "Epoch 49/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.8935\n",
      "Epoch 00049: val_loss did not improve from 8.18690\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.9113 - val_loss: 9.8452\n",
      "\n",
      "[20210302-1035-57] Learning rate for epoch 50 is 0.00047855067532509565\n",
      "Epoch 50/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.5817\n",
      "Epoch 00050: val_loss did not improve from 8.18690\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.6328 - val_loss: 8.6466\n",
      "\n",
      "[20210302-1036-01] Learning rate for epoch 51 is 0.00047855067532509565\n",
      "Epoch 51/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.7169\n",
      "Epoch 00051: val_loss did not improve from 8.18690\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.7688 - val_loss: 9.0069\n",
      "\n",
      "[20210302-1036-05] Learning rate for epoch 52 is 0.00047855067532509565\n",
      "Epoch 52/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.8474\n",
      "Epoch 00052: val_loss did not improve from 8.18690\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.8474 - val_loss: 8.3034\n",
      "\n",
      "[20210302-1036-09] Learning rate for epoch 53 is 0.00047855067532509565\n",
      "Epoch 53/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.9632\n",
      "Epoch 00053: val_loss did not improve from 8.18690\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.9632 - val_loss: 9.3427\n",
      "\n",
      "[20210302-1036-13] Learning rate for epoch 54 is 0.00047855067532509565\n",
      "Epoch 54/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 16.2772\n",
      "Epoch 00054: val_loss did not improve from 8.18690\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.0171 - val_loss: 9.0292\n",
      "\n",
      "[20210302-1036-17] Learning rate for epoch 55 is 0.00047855067532509565\n",
      "Epoch 55/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.6310\n",
      "Epoch 00055: val_loss did not improve from 8.18690\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.6996 - val_loss: 8.7666\n",
      "\n",
      "[20210302-1036-20] Learning rate for epoch 56 is 0.00047855067532509565\n",
      "Epoch 56/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.9336\n",
      "Epoch 00056: val_loss did not improve from 8.18690\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.9336 - val_loss: 8.3332\n",
      "\n",
      "[20210302-1036-24] Learning rate for epoch 57 is 0.00047855067532509565\n",
      "Epoch 57/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.2610\n",
      "Epoch 00057: val_loss did not improve from 8.18690\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.1225 - val_loss: 9.7874\n",
      "\n",
      "[20210302-1036-28] Learning rate for epoch 58 is 0.00047855067532509565\n",
      "Epoch 58/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.7581\n",
      "Epoch 00058: val_loss did not improve from 8.18690\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.8275 - val_loss: 11.2738\n",
      "\n",
      "[20210302-1036-32] Learning rate for epoch 59 is 0.00047855067532509565\n",
      "Epoch 59/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.4242\n",
      "Epoch 00059: val_loss did not improve from 8.18690\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.4242 - val_loss: 9.7203\n",
      "\n",
      "[20210302-1036-36] Learning rate for epoch 60 is 0.00047855067532509565\n",
      "Epoch 60/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.8617\n",
      "Epoch 00060: val_loss did not improve from 8.18690\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.8869 - val_loss: 9.9408\n",
      "\n",
      "[20210302-1036-40] Learning rate for epoch 61 is 0.00047855067532509565\n",
      "Epoch 61/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 15.6599\n",
      "Epoch 00061: val_loss did not improve from 8.18690\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.5889 - val_loss: 8.5652\n",
      "\n",
      "[20210302-1036-44] Learning rate for epoch 62 is 0.00047855067532509565\n",
      "Epoch 62/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.5195\n",
      "Epoch 00062: val_loss did not improve from 8.18690\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.4345 - val_loss: 9.2333\n",
      "\n",
      "*********************************** Start fine tune ***********************************\n",
      "tt: 195, nt:46, total layers:241\n",
      "*********************************** Start fine tune ***********************************\n",
      "Epoch 1/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 25.0724\n",
      "Epoch 00001: val_loss did not improve from 8.18690\n",
      "\n",
      "[20210302-1037-18] Learning rate for epoch 1 is 4.999999987376214e-07\n",
      "20/20 [==============================] - 14s 688ms/step - loss: 25.0724 - val_loss: 14.8830\n",
      "Epoch 2/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 23.1987\n",
      "Epoch 00002: val_loss did not improve from 8.18690\n",
      "\n",
      "[20210302-1037-23] Learning rate for epoch 2 is 5.044450517743826e-05\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 23.1987 - val_loss: 16.0092\n",
      "Epoch 3/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 18.4546\n",
      "Epoch 00003: val_loss did not improve from 8.18690\n",
      "\n",
      "[20210302-1037-44] Learning rate for epoch 3 is 0.00010028902033809572\n",
      "20/20 [==============================] - 17s 845ms/step - loss: 18.4546 - val_loss: 13.6180\n",
      "Epoch 4/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.5062\n",
      "Epoch 00004: val_loss did not improve from 8.18690\n",
      "\n",
      "[20210302-1037-49] Learning rate for epoch 4 is 0.00015003354928921908\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 15.5062 - val_loss: 13.3014\n",
      "Epoch 5/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.2759\n",
      "Epoch 00005: val_loss did not improve from 8.18690\n",
      "\n",
      "[20210302-1037-53] Learning rate for epoch 5 is 0.0001996780774788931\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 14.2759 - val_loss: 12.2982\n",
      "Epoch 6/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.8054\n",
      "Epoch 00006: val_loss did not improve from 8.18690\n",
      "\n",
      "[20210302-1037-58] Learning rate for epoch 6 is 0.000249222619459033\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 13.8054 - val_loss: 9.3095\n",
      "Epoch 7/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.2988\n",
      "Epoch 00007: val_loss improved from 8.18690 to 7.92468, saving model to ./20210301-225844/heel_K7_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1038-04] Learning rate for epoch 7 is 0.0002986671752296388\n",
      "20/20 [==============================] - 2s 91ms/step - loss: 13.2988 - val_loss: 7.9247\n",
      "Epoch 8/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 12.8016\n",
      "Epoch 00008: val_loss improved from 7.92468 to 7.66372, saving model to ./20210301-225844/heel_K7_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1038-09] Learning rate for epoch 8 is 0.00034801175934262574\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 12.8016 - val_loss: 7.6637\n",
      "Epoch 9/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 12.3284\n",
      "Epoch 00009: val_loss improved from 7.66372 to 6.35952, saving model to ./20210301-225844/heel_K7_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1038-15] Learning rate for epoch 9 is 0.00039725631359033287\n",
      "20/20 [==============================] - 2s 91ms/step - loss: 12.3284 - val_loss: 6.3595\n",
      "Epoch 10/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 12.5016\n",
      "Epoch 00010: val_loss did not improve from 6.35952\n",
      "\n",
      "[20210302-1038-19] Learning rate for epoch 10 is 0.0004464008961804211\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 12.5016 - val_loss: 6.8414\n",
      "Epoch 11/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 12.2815\n",
      "Epoch 00011: val_loss did not improve from 6.35952\n",
      "\n",
      "[20210302-1038-24] Learning rate for epoch 11 is 0.0004954455071128905\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 12.2815 - val_loss: 8.2412\n",
      "Epoch 12/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.7466\n",
      "Epoch 00012: val_loss did not improve from 6.35952\n",
      "\n",
      "[20210302-1038-29] Learning rate for epoch 12 is 0.0005443901172839105\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 11.7466 - val_loss: 9.7535\n",
      "Epoch 13/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.7823\n",
      "Epoch 00013: val_loss did not improve from 6.35952\n",
      "\n",
      "[20210302-1038-34] Learning rate for epoch 13 is 0.0005932347266934812\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 11.7823 - val_loss: 8.2225\n",
      "Epoch 14/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.8725\n",
      "Epoch 00014: val_loss did not improve from 6.35952\n",
      "\n",
      "[20210302-1038-39] Learning rate for epoch 14 is 0.0006419793353416026\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 11.8725 - val_loss: 8.3939\n",
      "Epoch 15/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.2166\n",
      "Epoch 00015: val_loss did not improve from 6.35952\n",
      "\n",
      "[20210302-1038-43] Learning rate for epoch 15 is 0.0006906240014359355\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 11.2166 - val_loss: 11.3023\n",
      "Epoch 16/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.4484\n",
      "Epoch 00016: val_loss did not improve from 6.35952\n",
      "\n",
      "[20210302-1038-48] Learning rate for epoch 16 is 0.0007391686085611582\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 10.4484 - val_loss: 11.1892\n",
      "Epoch 17/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.2602\n",
      "Epoch 00017: val_loss did not improve from 6.35952\n",
      "\n",
      "[20210302-1038-53] Learning rate for epoch 17 is 0.0007876132731325924\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 10.2602 - val_loss: 7.0123\n",
      "Epoch 18/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.4348\n",
      "Epoch 00018: val_loss improved from 6.35952 to 4.70202, saving model to ./20210301-225844/heel_K7_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1038-58] Learning rate for epoch 18 is 0.0008359579369425774\n",
      "20/20 [==============================] - 2s 87ms/step - loss: 9.4348 - val_loss: 4.7020\n",
      "Epoch 19/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.7057\n",
      "Epoch 00019: val_loss improved from 4.70202 to 4.46120, saving model to ./20210301-225844/heel_K7_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1039-04] Learning rate for epoch 19 is 0.000884202599991113\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 8.7057 - val_loss: 4.4612\n",
      "Epoch 20/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.9750\n",
      "Epoch 00020: val_loss did not improve from 4.46120\n",
      "\n",
      "[20210302-1039-09] Learning rate for epoch 20 is 0.0009323473204858601\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 8.9750 - val_loss: 5.2211\n",
      "Epoch 21/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.3908\n",
      "Epoch 00021: val_loss improved from 4.46120 to 4.31465, saving model to ./20210301-225844/heel_K7_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1039-14] Learning rate for epoch 21 is 0.000980391982011497\n",
      "20/20 [==============================] - 2s 91ms/step - loss: 8.3908 - val_loss: 4.3146\n",
      "Epoch 22/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.2727\n",
      "Epoch 00022: val_loss did not improve from 4.31465\n",
      "\n",
      "[20210302-1039-19] Learning rate for epoch 22 is 0.0010283367009833455\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 8.2727 - val_loss: 4.3837\n",
      "Epoch 23/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.0498\n",
      "Epoch 00023: val_loss improved from 4.31465 to 4.02537, saving model to ./20210301-225844/heel_K7_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1039-24] Learning rate for epoch 23 is 0.0010761814191937447\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 8.0498 - val_loss: 4.0254\n",
      "Epoch 24/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.4141\n",
      "Epoch 00024: val_loss did not improve from 4.02537\n",
      "\n",
      "[20210302-1039-29] Learning rate for epoch 24 is 0.0011239261366426945\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 8.4141 - val_loss: 5.3801\n",
      "Epoch 25/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5196\n",
      "Epoch 00025: val_loss improved from 4.02537 to 3.64156, saving model to ./20210301-225844/heel_K7_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1039-35] Learning rate for epoch 25 is 0.001171570853330195\n",
      "20/20 [==============================] - 2s 92ms/step - loss: 7.5196 - val_loss: 3.6416\n",
      "Epoch 26/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.9818\n",
      "Epoch 00026: val_loss did not improve from 3.64156\n",
      "\n",
      "[20210302-1039-40] Learning rate for epoch 26 is 0.001219115569256246\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 7.9818 - val_loss: 5.0904\n",
      "Epoch 27/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.9336\n",
      "Epoch 00027: val_loss did not improve from 3.64156\n",
      "\n",
      "[20210302-1039-44] Learning rate for epoch 27 is 0.0012665604008361697\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.9336 - val_loss: 5.6416\n",
      "Epoch 28/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3675\n",
      "Epoch 00028: val_loss did not improve from 3.64156\n",
      "\n",
      "[20210302-1039-49] Learning rate for epoch 28 is 0.0013139051152393222\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.3675 - val_loss: 5.1131\n",
      "Epoch 29/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8473\n",
      "Epoch 00029: val_loss did not improve from 3.64156\n",
      "\n",
      "[20210302-1039-54] Learning rate for epoch 29 is 0.0013611499452963471\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.8473 - val_loss: 3.9821\n",
      "Epoch 30/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3078\n",
      "Epoch 00030: val_loss did not improve from 3.64156\n",
      "\n",
      "[20210302-1039-59] Learning rate for epoch 30 is 0.001408294658176601\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.3078 - val_loss: 5.7650\n",
      "Epoch 31/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2603\n",
      "Epoch 00031: val_loss did not improve from 3.64156\n",
      "\n",
      "[20210302-1040-04] Learning rate for epoch 31 is 0.0014553394867107272\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 7.2603 - val_loss: 10.6071\n",
      "Epoch 32/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3651\n",
      "Epoch 00032: val_loss did not improve from 3.64156\n",
      "\n",
      "[20210302-1040-09] Learning rate for epoch 32 is 0.0015022843144834042\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.3651 - val_loss: 6.7046\n",
      "Epoch 33/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8490\n",
      "Epoch 00033: val_loss did not improve from 3.64156\n",
      "\n",
      "[20210302-1040-13] Learning rate for epoch 33 is 0.0015491291414946318\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.8490 - val_loss: 6.0412\n",
      "Epoch 34/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6384\n",
      "Epoch 00034: val_loss did not improve from 3.64156\n",
      "\n",
      "[20210302-1040-18] Learning rate for epoch 34 is 0.00159587396774441\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.6384 - val_loss: 6.2895\n",
      "Epoch 35/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2526\n",
      "Epoch 00035: val_loss did not improve from 3.64156\n",
      "\n",
      "[20210302-1040-23] Learning rate for epoch 35 is 0.001642518793232739\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.2526 - val_loss: 8.5072\n",
      "Epoch 36/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1532\n",
      "Epoch 00036: val_loss did not improve from 3.64156\n",
      "\n",
      "[20210302-1040-28] Learning rate for epoch 36 is 0.0016890636179596186\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 7.1532 - val_loss: 4.3492\n",
      "Epoch 37/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5769\n",
      "Epoch 00037: val_loss did not improve from 3.64156\n",
      "\n",
      "[20210302-1040-33] Learning rate for epoch 37 is 0.0017355084419250488\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 7.5769 - val_loss: 4.4560\n",
      "Epoch 38/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4219\n",
      "Epoch 00038: val_loss did not improve from 3.64156\n",
      "\n",
      "[20210302-1040-37] Learning rate for epoch 38 is 0.0017818533815443516\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.4219 - val_loss: 3.6997\n",
      "Epoch 39/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1822\n",
      "Epoch 00039: val_loss did not improve from 3.64156\n",
      "\n",
      "[20210302-1040-42] Learning rate for epoch 39 is 0.0018280982039868832\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.1822 - val_loss: 4.3670\n",
      "Epoch 40/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1283\n",
      "Epoch 00040: val_loss improved from 3.64156 to 3.47452, saving model to ./20210301-225844/heel_K7_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1040-48] Learning rate for epoch 40 is 0.0018742431420832872\n",
      "20/20 [==============================] - 2s 87ms/step - loss: 7.1283 - val_loss: 3.4745\n",
      "Epoch 41/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1385\n",
      "Epoch 00041: val_loss did not improve from 3.47452\n",
      "\n",
      "[20210302-1040-52] Learning rate for epoch 41 is 0.0019202879630029202\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.1385 - val_loss: 4.1772\n",
      "Epoch 42/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4087\n",
      "Epoch 00042: val_loss did not improve from 3.47452\n",
      "\n",
      "[20210302-1040-57] Learning rate for epoch 42 is 0.0019662328995764256\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.4087 - val_loss: 3.9158\n",
      "Epoch 43/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5510\n",
      "Epoch 00043: val_loss did not improve from 3.47452\n",
      "\n",
      "[20210302-1041-02] Learning rate for epoch 43 is 0.00201207771897316\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.5510 - val_loss: 5.4475\n",
      "Epoch 44/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2445\n",
      "Epoch 00044: val_loss did not improve from 3.47452\n",
      "\n",
      "[20210302-1041-07] Learning rate for epoch 44 is 0.0020578226540237665\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.2445 - val_loss: 4.1398\n",
      "Epoch 45/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8961\n",
      "Epoch 00045: val_loss did not improve from 3.47452\n",
      "\n",
      "[20210302-1041-12] Learning rate for epoch 45 is 0.0021034677047282457\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.8961 - val_loss: 4.3544\n",
      "Epoch 46/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6342\n",
      "Epoch 00046: val_loss did not improve from 3.47452\n",
      "\n",
      "[20210302-1041-17] Learning rate for epoch 46 is 0.002149012638255954\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 7.6342 - val_loss: 3.5099\n",
      "Epoch 47/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3742\n",
      "Epoch 00047: val_loss did not improve from 3.47452\n",
      "\n",
      "[20210302-1041-21] Learning rate for epoch 47 is 0.0021944576874375343\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.3742 - val_loss: 8.3353\n",
      "Epoch 48/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6077\n",
      "Epoch 00048: val_loss did not improve from 3.47452\n",
      "\n",
      "[20210302-1041-26] Learning rate for epoch 48 is 0.0022398026194423437\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.6077 - val_loss: 3.8493\n",
      "Epoch 49/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8158\n",
      "Epoch 00049: val_loss did not improve from 3.47452\n",
      "\n",
      "[20210302-1041-31] Learning rate for epoch 49 is 0.002285047434270382\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.8158 - val_loss: 5.3594\n",
      "Epoch 50/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5095\n",
      "Epoch 00050: val_loss did not improve from 3.47452\n",
      "\n",
      "[20210302-1041-36] Learning rate for epoch 50 is 0.0023301925975829363\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.5095 - val_loss: 4.1995\n",
      "Epoch 51/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8478\n",
      "Epoch 00051: val_loss did not improve from 3.47452\n",
      "\n",
      "[20210302-1041-41] Learning rate for epoch 51 is 0.002375237410888076\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.8478 - val_loss: 4.9699\n",
      "Epoch 52/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1665\n",
      "Epoch 00052: val_loss did not improve from 3.47452\n",
      "\n",
      "[20210302-1041-45] Learning rate for epoch 52 is 0.0024201825726777315\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.1665 - val_loss: 4.9768\n",
      "Epoch 53/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8331\n",
      "Epoch 00053: val_loss did not improve from 3.47452\n",
      "\n",
      "[20210302-1041-50] Learning rate for epoch 53 is 0.002465027617290616\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.8331 - val_loss: 4.2680\n",
      "Epoch 54/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3226\n",
      "Epoch 00054: val_loss did not improve from 3.47452\n",
      "\n",
      "[20210302-1041-55] Learning rate for epoch 54 is 0.0025097725447267294\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.3226 - val_loss: 9.7254\n",
      "Epoch 55/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9872\n",
      "Epoch 00055: val_loss did not improve from 3.47452\n",
      "\n",
      "[20210302-1042-00] Learning rate for epoch 55 is 0.0025544175878167152\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.9872 - val_loss: 3.8192\n",
      "Epoch 56/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9311\n",
      "Epoch 00056: val_loss did not improve from 3.47452\n",
      "\n",
      "[20210302-1042-05] Learning rate for epoch 56 is 0.00259896251372993\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.9311 - val_loss: 9.7345\n",
      "Epoch 57/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9409\n",
      "Epoch 00057: val_loss did not improve from 3.47452\n",
      "\n",
      "[20210302-1042-10] Learning rate for epoch 57 is 0.0026434077881276608\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.9409 - val_loss: 8.7583\n",
      "Epoch 58/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9499\n",
      "Epoch 00058: val_loss did not improve from 3.47452\n",
      "\n",
      "[20210302-1042-14] Learning rate for epoch 58 is 0.0026877527125179768\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.9499 - val_loss: 4.7920\n",
      "Epoch 59/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1940\n",
      "Epoch 00059: val_loss did not improve from 3.47452\n",
      "\n",
      "[20210302-1042-19] Learning rate for epoch 59 is 0.0027319977525621653\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 7.1940 - val_loss: 5.6575\n",
      "Epoch 60/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0276\n",
      "Epoch 00060: val_loss did not improve from 3.47452\n",
      "\n",
      "[20210302-1042-24] Learning rate for epoch 60 is 0.0027761429082602262\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.0276 - val_loss: 4.3509\n",
      "Epoch 61/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3247\n",
      "Epoch 00061: val_loss did not improve from 3.47452\n",
      "\n",
      "[20210302-1042-29] Learning rate for epoch 61 is 0.002820187946781516\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 7.3247 - val_loss: 4.6960\n",
      "Epoch 62/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8264\n",
      "Epoch 00062: val_loss did not improve from 3.47452\n",
      "\n",
      "[20210302-1042-34] Learning rate for epoch 62 is 0.0028641331009566784\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.8264 - val_loss: 3.6121\n",
      "Epoch 63/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9093\n",
      "Epoch 00063: val_loss did not improve from 3.47452\n",
      "\n",
      "[20210302-1042-38] Learning rate for epoch 63 is 0.0029079781379550695\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.9093 - val_loss: 3.9123\n",
      "Epoch 64/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8381\n",
      "Epoch 00064: val_loss did not improve from 3.47452\n",
      "\n",
      "[20210302-1042-43] Learning rate for epoch 64 is 0.002951723290607333\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.8381 - val_loss: 4.4320\n",
      "Epoch 65/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2939\n",
      "Epoch 00065: val_loss did not improve from 3.47452\n",
      "\n",
      "[20210302-1042-48] Learning rate for epoch 65 is 0.0029953685589134693\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.2939 - val_loss: 4.6253\n",
      "Epoch 66/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6520\n",
      "Epoch 00066: val_loss did not improve from 3.47452\n",
      "\n",
      "[20210302-1042-53] Learning rate for epoch 66 is 0.0030389137100428343\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.6520 - val_loss: 5.0771\n",
      "Epoch 67/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4840\n",
      "Epoch 00067: val_loss did not improve from 3.47452\n",
      "\n",
      "[20210302-1042-58] Learning rate for epoch 67 is 0.003082358743995428\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.4840 - val_loss: 4.4999\n",
      "Epoch 68/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4372\n",
      "Epoch 00068: val_loss did not improve from 3.47452\n",
      "\n",
      "[20210302-1043-03] Learning rate for epoch 68 is 0.0031257038936018944\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.4372 - val_loss: 4.6218\n",
      "Epoch 69/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4942\n",
      "Epoch 00069: val_loss did not improve from 3.47452\n",
      "\n",
      "[20210302-1043-08] Learning rate for epoch 69 is 0.003168949158862233\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.4942 - val_loss: 4.4137\n",
      "Epoch 70/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7461\n",
      "Epoch 00070: val_loss did not improve from 3.47452\n",
      "\n",
      "[20210302-1043-13] Learning rate for epoch 70 is 0.0032120943069458008\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.7461 - val_loss: 4.5465\n",
      "Epoch 71/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2416\n",
      "Epoch 00071: val_loss did not improve from 3.47452\n",
      "\n",
      "[20210302-1043-17] Learning rate for epoch 71 is 0.003255139570683241\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.2416 - val_loss: 3.9112\n",
      "Epoch 72/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9876\n",
      "Epoch 00072: val_loss did not improve from 3.47452\n",
      "\n",
      "[20210302-1043-22] Learning rate for epoch 72 is 0.00329808471724391\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.9876 - val_loss: 3.6866\n",
      "Epoch 73/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1910\n",
      "Epoch 00073: val_loss did not improve from 3.47452\n",
      "\n",
      "[20210302-1043-27] Learning rate for epoch 73 is 0.0033409299794584513\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.1910 - val_loss: 4.4170\n",
      "Epoch 74/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7275\n",
      "Epoch 00074: val_loss did not improve from 3.47452\n",
      "\n",
      "[20210302-1043-32] Learning rate for epoch 74 is 0.0033836751244962215\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.7275 - val_loss: 5.0389\n",
      "Epoch 75/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3181\n",
      "Epoch 00075: val_loss did not improve from 3.47452\n",
      "\n",
      "[20210302-1043-37] Learning rate for epoch 75 is 0.0034263203851878643\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.3181 - val_loss: 5.3129\n",
      "Epoch 76/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2493\n",
      "Epoch 00076: val_loss did not improve from 3.47452\n",
      "\n",
      "[20210302-1043-42] Learning rate for epoch 76 is 0.003468865528702736\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.2493 - val_loss: 3.6340\n",
      "Epoch 77/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9633\n",
      "Epoch 00077: val_loss did not improve from 3.47452\n",
      "\n",
      "[20210302-1043-46] Learning rate for epoch 77 is 0.00351131078787148\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.9633 - val_loss: 4.2074\n",
      "Epoch 78/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5564\n",
      "Epoch 00078: val_loss did not improve from 3.47452\n",
      "\n",
      "[20210302-1043-51] Learning rate for epoch 78 is 0.0035536561626940966\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.5564 - val_loss: 4.5607\n",
      "Epoch 79/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9824\n",
      "Epoch 00079: val_loss did not improve from 3.47452\n",
      "\n",
      "[20210302-1043-56] Learning rate for epoch 79 is 0.003595901420339942\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.9824 - val_loss: 5.6831\n",
      "Epoch 80/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9227\n",
      "Epoch 00080: val_loss did not improve from 3.47452\n",
      "\n",
      "[20210302-1044-01] Learning rate for epoch 80 is 0.00363804679363966\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.9227 - val_loss: 3.7548\n",
      "Epoch 81/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4996\n",
      "Epoch 00081: val_loss did not improve from 3.47452\n",
      "\n",
      "[20210302-1044-06] Learning rate for epoch 81 is 0.0036800920497626066\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 6.4996 - val_loss: 4.3109\n",
      "Epoch 82/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2891\n",
      "Epoch 00082: val_loss did not improve from 3.47452\n",
      "\n",
      "[20210302-1044-10] Learning rate for epoch 82 is 0.003722037188708782\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.2891 - val_loss: 3.5124\n",
      "Epoch 83/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5832\n",
      "Epoch 00083: val_loss did not improve from 3.47452\n",
      "\n",
      "[20210302-1044-15] Learning rate for epoch 83 is 0.003763882676139474\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.5832 - val_loss: 4.1184\n",
      "Epoch 84/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1607\n",
      "Epoch 00084: val_loss did not improve from 3.47452\n",
      "\n",
      "[20210302-1044-20] Learning rate for epoch 84 is 0.0038056280463933945\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.1607 - val_loss: 4.9337\n",
      "Epoch 85/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4393\n",
      "Epoch 00085: val_loss improved from 3.47452 to 3.26478, saving model to ./20210301-225844/heel_K7_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1044-26] Learning rate for epoch 85 is 0.003847273299470544\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 6.4393 - val_loss: 3.2648\n",
      "Epoch 86/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4948\n",
      "Epoch 00086: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1044-30] Learning rate for epoch 86 is 0.0038888186682015657\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.4948 - val_loss: 4.0586\n",
      "Epoch 87/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4028\n",
      "Epoch 00087: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1044-35] Learning rate for epoch 87 is 0.00393026415258646\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.4028 - val_loss: 3.4185\n",
      "Epoch 88/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6051\n",
      "Epoch 00088: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1044-40] Learning rate for epoch 88 is 0.00397160928696394\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.6051 - val_loss: 4.3905\n",
      "Epoch 89/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2921\n",
      "Epoch 00089: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1044-45] Learning rate for epoch 89 is 0.004012854769825935\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.2921 - val_loss: 5.0926\n",
      "Epoch 90/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4521\n",
      "Epoch 00090: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1044-50] Learning rate for epoch 90 is 0.00405400013551116\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.4521 - val_loss: 7.6007\n",
      "Epoch 91/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2227\n",
      "Epoch 00091: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1044-54] Learning rate for epoch 91 is 0.004095045384019613\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.2227 - val_loss: 5.7716\n",
      "Epoch 92/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3122\n",
      "Epoch 00092: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1044-59] Learning rate for epoch 92 is 0.004135990981012583\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.3122 - val_loss: 5.3265\n",
      "Epoch 93/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3437\n",
      "Epoch 00093: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1045-04] Learning rate for epoch 93 is 0.004176836460828781\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.3437 - val_loss: 5.8116\n",
      "Epoch 94/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0471\n",
      "Epoch 00094: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1045-09] Learning rate for epoch 94 is 0.004217581823468208\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.0471 - val_loss: 5.4999\n",
      "Epoch 95/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9483\n",
      "Epoch 00095: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1045-14] Learning rate for epoch 95 is 0.004258227068930864\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.9483 - val_loss: 6.0119\n",
      "Epoch 96/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4950\n",
      "Epoch 00096: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1045-19] Learning rate for epoch 96 is 0.0042987726628780365\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.4950 - val_loss: 8.1384\n",
      "Epoch 97/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6894\n",
      "Epoch 00097: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1045-23] Learning rate for epoch 97 is 0.0043392181396484375\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.6894 - val_loss: 4.9747\n",
      "Epoch 98/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8209\n",
      "Epoch 00098: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1045-28] Learning rate for epoch 98 is 0.004379563499242067\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.8209 - val_loss: 3.4691\n",
      "Epoch 99/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5873\n",
      "Epoch 00099: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1045-33] Learning rate for epoch 99 is 0.004419809207320213\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.5873 - val_loss: 4.6001\n",
      "Epoch 100/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9438\n",
      "Epoch 00100: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1045-38] Learning rate for epoch 100 is 0.004459954332560301\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.9438 - val_loss: 3.3889\n",
      "Epoch 101/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0470\n",
      "Epoch 00101: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1045-43] Learning rate for epoch 101 is 4.4999998749517545e-07\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.0470 - val_loss: 3.2830\n",
      "Epoch 102/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3684\n",
      "Epoch 00102: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1045-48] Learning rate for epoch 102 is 0.00018023152370005846\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.3684 - val_loss: 3.3499\n",
      "Epoch 103/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0122\n",
      "Epoch 00103: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1045-53] Learning rate for epoch 103 is 0.000359613070031628\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.0122 - val_loss: 3.3929\n",
      "Epoch 104/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9366\n",
      "Epoch 00104: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1045-57] Learning rate for epoch 104 is 0.0005385946715250611\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.9366 - val_loss: 3.3518\n",
      "Epoch 105/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5635\n",
      "Epoch 00105: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1046-02] Learning rate for epoch 105 is 0.0007171762990765274\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.5635 - val_loss: 3.3634\n",
      "Epoch 106/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8388\n",
      "Epoch 00106: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1046-07] Learning rate for epoch 106 is 0.0008953579817898571\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.8388 - val_loss: 3.5092\n",
      "Epoch 107/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7243\n",
      "Epoch 00107: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1046-12] Learning rate for epoch 107 is 0.0010731397196650505\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.7243 - val_loss: 4.1098\n",
      "Epoch 108/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2218\n",
      "Epoch 00108: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1046-16] Learning rate for epoch 108 is 0.0012505215127021074\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.2218 - val_loss: 3.9158\n",
      "Epoch 109/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2462\n",
      "Epoch 00109: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1046-21] Learning rate for epoch 109 is 0.001427503302693367\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.2462 - val_loss: 3.4182\n",
      "Epoch 110/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9224\n",
      "Epoch 00110: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1046-26] Learning rate for epoch 110 is 0.0016040850896388292\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.9224 - val_loss: 3.8545\n",
      "Epoch 111/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8142\n",
      "Epoch 00111: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1046-31] Learning rate for epoch 111 is 0.001780266989953816\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.8142 - val_loss: 3.6948\n",
      "Epoch 112/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6359\n",
      "Epoch 00112: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1046-36] Learning rate for epoch 112 is 0.0019560488872230053\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.6359 - val_loss: 3.4970\n",
      "Epoch 113/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7350\n",
      "Epoch 00113: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1046-45] Learning rate for epoch 113 is 0.002131430897861719\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.7350 - val_loss: 3.5757\n",
      "Epoch 114/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9901\n",
      "Epoch 00114: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1046-50] Learning rate for epoch 114 is 0.0023064129054546356\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.9901 - val_loss: 4.3011\n",
      "Epoch 115/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7614\n",
      "Epoch 00115: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1046-55] Learning rate for epoch 115 is 0.0024809949100017548\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.7614 - val_loss: 4.3925\n",
      "Epoch 116/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1155\n",
      "Epoch 00116: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1047-00] Learning rate for epoch 116 is 0.0026551769115030766\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.1155 - val_loss: 3.6634\n",
      "Epoch 117/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2336\n",
      "Epoch 00117: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1047-05] Learning rate for epoch 117 is 0.0028289591427892447\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.2336 - val_loss: 14.1347\n",
      "Epoch 118/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1629\n",
      "Epoch 00118: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1047-10] Learning rate for epoch 118 is 0.0030023413710296154\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.1629 - val_loss: 4.2239\n",
      "Epoch 119/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1209\n",
      "Epoch 00119: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1047-15] Learning rate for epoch 119 is 0.003175323596224189\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.1209 - val_loss: 3.7146\n",
      "Epoch 120/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7993\n",
      "Epoch 00120: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1047-19] Learning rate for epoch 120 is 0.003347905818372965\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.7993 - val_loss: 7.6015\n",
      "Epoch 121/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0471\n",
      "Epoch 00121: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1047-24] Learning rate for epoch 121 is 0.0035200880374759436\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.0471 - val_loss: 3.9076\n",
      "Epoch 122/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9827\n",
      "Epoch 00122: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1047-29] Learning rate for epoch 122 is 0.003691870253533125\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.9827 - val_loss: 3.7458\n",
      "Epoch 123/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5990\n",
      "Epoch 00123: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1047-34] Learning rate for epoch 123 is 0.0038632526993751526\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.5990 - val_loss: 5.4390\n",
      "Epoch 124/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1279\n",
      "Epoch 00124: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1047-39] Learning rate for epoch 124 is 0.004034235142171383\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.1279 - val_loss: 4.5625\n",
      "Epoch 125/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2839\n",
      "Epoch 00125: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1047-44] Learning rate for epoch 125 is 0.004204817581921816\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.2839 - val_loss: 3.7588\n",
      "Epoch 126/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6810\n",
      "Epoch 00126: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1047-48] Learning rate for epoch 126 is 0.0043750000186264515\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.6810 - val_loss: 4.6525\n",
      "Epoch 127/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8041\n",
      "Epoch 00127: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1047-53] Learning rate for epoch 127 is 0.0041952175088226795\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.8041 - val_loss: 4.9842\n",
      "Epoch 128/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2857\n",
      "Epoch 00128: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1047-58] Learning rate for epoch 128 is 0.004015835002064705\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.2857 - val_loss: 4.1809\n",
      "Epoch 129/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5064\n",
      "Epoch 00129: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1048-03] Learning rate for epoch 129 is 0.003836852265521884\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.5064 - val_loss: 5.1168\n",
      "Epoch 130/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8952\n",
      "Epoch 00130: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1048-08] Learning rate for epoch 130 is 0.003658269764855504\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.8952 - val_loss: 5.7825\n",
      "Epoch 131/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0866\n",
      "Epoch 00131: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1048-13] Learning rate for epoch 131 is 0.003480087034404278\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.0866 - val_loss: 5.5800\n",
      "Epoch 132/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0365\n",
      "Epoch 00132: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1048-18] Learning rate for epoch 132 is 0.003302304306998849\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.0365 - val_loss: 6.6856\n",
      "Epoch 133/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8003\n",
      "Epoch 00133: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1048-22] Learning rate for epoch 133 is 0.0031249215826392174\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.8003 - val_loss: 7.3551\n",
      "Epoch 134/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2440\n",
      "Epoch 00134: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1048-27] Learning rate for epoch 134 is 0.0029479386284947395\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.2440 - val_loss: 4.2825\n",
      "Epoch 135/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1247\n",
      "Epoch 00135: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1048-32] Learning rate for epoch 135 is 0.0027713559102267027\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.1247 - val_loss: 6.4898\n",
      "Epoch 136/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3403\n",
      "Epoch 00136: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1048-37] Learning rate for epoch 136 is 0.0025951729621738195\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.3403 - val_loss: 3.6312\n",
      "Epoch 137/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0291\n",
      "Epoch 00137: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1048-42] Learning rate for epoch 137 is 0.0024193900171667337\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.0291 - val_loss: 3.5124\n",
      "Epoch 138/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7975\n",
      "Epoch 00138: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1048-47] Learning rate for epoch 138 is 0.0022440070752054453\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.7975 - val_loss: 3.9618\n",
      "Epoch 139/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6230\n",
      "Epoch 00139: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1048-52] Learning rate for epoch 139 is 0.002069024136289954\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6230 - val_loss: 4.4081\n",
      "Epoch 140/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8126\n",
      "Epoch 00140: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1048-56] Learning rate for epoch 140 is 0.0018944410840049386\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.8126 - val_loss: 5.9332\n",
      "Epoch 141/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2508\n",
      "Epoch 00141: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1049-01] Learning rate for epoch 141 is 0.0017202580347657204\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.2508 - val_loss: 4.0385\n",
      "Epoch 142/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5113\n",
      "Epoch 00142: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1049-06] Learning rate for epoch 142 is 0.0015464748721569777\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.5113 - val_loss: 4.2682\n",
      "Epoch 143/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0774\n",
      "Epoch 00143: val_loss did not improve from 3.26478\n",
      "\n",
      "[20210302-1049-11] Learning rate for epoch 143 is 0.0013730917125940323\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.0774 - val_loss: 3.6039\n",
      "Epoch 144/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8905\n",
      "Epoch 00144: val_loss improved from 3.26478 to 3.23661, saving model to ./20210301-225844/heel_K7_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1049-16] Learning rate for epoch 144 is 0.0012001085560768843\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 5.8905 - val_loss: 3.2366\n",
      "Epoch 145/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4532\n",
      "Epoch 00145: val_loss did not improve from 3.23661\n",
      "\n",
      "[20210302-1049-21] Learning rate for epoch 145 is 0.0010275252861902118\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.4532 - val_loss: 3.6203\n",
      "Epoch 146/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5144\n",
      "Epoch 00146: val_loss did not improve from 3.23661\n",
      "\n",
      "[20210302-1049-26] Learning rate for epoch 146 is 0.0008553420193493366\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.5144 - val_loss: 3.4044\n",
      "Epoch 147/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6700\n",
      "Epoch 00147: val_loss improved from 3.23661 to 3.20303, saving model to ./20210301-225844/heel_K7_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1049-31] Learning rate for epoch 147 is 0.0006835586973465979\n",
      "20/20 [==============================] - 2s 91ms/step - loss: 5.6700 - val_loss: 3.2030\n",
      "Epoch 148/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4907\n",
      "Epoch 00148: val_loss did not improve from 3.20303\n",
      "\n",
      "[20210302-1049-36] Learning rate for epoch 148 is 0.0005121753201819956\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.4907 - val_loss: 3.2899\n",
      "Epoch 149/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8409\n",
      "Epoch 00149: val_loss did not improve from 3.20303\n",
      "\n",
      "[20210302-1049-41] Learning rate for epoch 149 is 0.00034119191695936024\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.8409 - val_loss: 3.2150\n",
      "Epoch 150/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5605\n",
      "Epoch 00150: val_loss did not improve from 3.20303\n",
      "\n",
      "[20210302-1049-46] Learning rate for epoch 150 is 0.00017060847312677652\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.5605 - val_loss: 3.2546\n",
      "Epoch 151/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8699\n",
      "Epoch 00151: val_loss did not improve from 3.20303\n",
      "\n",
      "[20210302-1049-51] Learning rate for epoch 151 is 4.249999960848072e-07\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.8699 - val_loss: 3.2213\n",
      "Epoch 152/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4239\n",
      "Epoch 00152: val_loss did not improve from 3.20303\n",
      "\n",
      "[20210302-1049-55] Learning rate for epoch 152 is 0.00017020752420648932\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.4239 - val_loss: 3.2156\n",
      "Epoch 153/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4215\n",
      "Epoch 00153: val_loss improved from 3.20303 to 3.11481, saving model to ./20210301-225844/heel_K7_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1050-01] Learning rate for epoch 153 is 0.0003395900712348521\n",
      "20/20 [==============================] - 2s 89ms/step - loss: 5.4215 - val_loss: 3.1148\n",
      "Epoch 154/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9279\n",
      "Epoch 00154: val_loss improved from 3.11481 to 3.10144, saving model to ./20210301-225844/heel_K7_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1050-06] Learning rate for epoch 154 is 0.0005085726734250784\n",
      "20/20 [==============================] - 2s 93ms/step - loss: 5.9279 - val_loss: 3.1014\n",
      "Epoch 155/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4447\n",
      "Epoch 00155: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1050-11] Learning rate for epoch 155 is 0.0006771553307771683\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.4447 - val_loss: 3.2787\n",
      "Epoch 156/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1170\n",
      "Epoch 00156: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1050-16] Learning rate for epoch 156 is 0.0008453379850834608\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.1170 - val_loss: 3.2365\n",
      "Epoch 157/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7315\n",
      "Epoch 00157: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1050-21] Learning rate for epoch 157 is 0.0010131207527592778\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.7315 - val_loss: 3.4647\n",
      "Epoch 158/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9690\n",
      "Epoch 00158: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1050-26] Learning rate for epoch 158 is 0.0011805035173892975\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.9690 - val_loss: 3.4587\n",
      "Epoch 159/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4710\n",
      "Epoch 00159: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1050-30] Learning rate for epoch 159 is 0.0013474862789735198\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.4710 - val_loss: 3.4593\n",
      "Epoch 160/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7233\n",
      "Epoch 00160: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1050-35] Learning rate for epoch 160 is 0.0015140691539272666\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.7233 - val_loss: 3.2300\n",
      "Epoch 161/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1985\n",
      "Epoch 00161: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1050-40] Learning rate for epoch 161 is 0.001680252025835216\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.1985 - val_loss: 3.9385\n",
      "Epoch 162/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4909\n",
      "Epoch 00162: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1050-45] Learning rate for epoch 162 is 0.0018460348946973681\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.4909 - val_loss: 3.5757\n",
      "Epoch 163/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7364\n",
      "Epoch 00163: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1050-50] Learning rate for epoch 163 is 0.0020114178769290447\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.7364 - val_loss: 3.9898\n",
      "Epoch 164/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4911\n",
      "Epoch 00164: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1050-55] Learning rate for epoch 164 is 0.0021764009725302458\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.4911 - val_loss: 3.2156\n",
      "Epoch 165/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9257\n",
      "Epoch 00165: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1050-59] Learning rate for epoch 165 is 0.002340983832255006\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.9257 - val_loss: 4.2950\n",
      "Epoch 166/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1275\n",
      "Epoch 00166: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1051-04] Learning rate for epoch 166 is 0.002505166921764612\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.1275 - val_loss: 4.1793\n",
      "Epoch 167/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8394\n",
      "Epoch 00167: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1051-09] Learning rate for epoch 167 is 0.002668950008228421\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.8394 - val_loss: 3.3304\n",
      "Epoch 168/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7816\n",
      "Epoch 00168: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1051-14] Learning rate for epoch 168 is 0.0028323333244770765\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.7816 - val_loss: 4.1207\n",
      "Epoch 169/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9105\n",
      "Epoch 00169: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1051-19] Learning rate for epoch 169 is 0.002995316404849291\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.9105 - val_loss: 3.9586\n",
      "Epoch 170/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6589\n",
      "Epoch 00170: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1051-24] Learning rate for epoch 170 is 0.0031578997150063515\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.6589 - val_loss: 3.6122\n",
      "Epoch 171/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3228\n",
      "Epoch 00171: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1051-28] Learning rate for epoch 171 is 0.0033200830221176147\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.3228 - val_loss: 4.4226\n",
      "Epoch 172/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8715\n",
      "Epoch 00172: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1051-33] Learning rate for epoch 172 is 0.0034818663261830807\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.8715 - val_loss: 3.6462\n",
      "Epoch 173/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5362\n",
      "Epoch 00173: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1051-38] Learning rate for epoch 173 is 0.0036432496272027493\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5362 - val_loss: 3.4098\n",
      "Epoch 174/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1243\n",
      "Epoch 00174: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1051-43] Learning rate for epoch 174 is 0.003804233158007264\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.1243 - val_loss: 5.5271\n",
      "Epoch 175/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6134\n",
      "Epoch 00175: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1051-48] Learning rate for epoch 175 is 0.003964816685765982\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.6134 - val_loss: 3.9616\n",
      "Epoch 176/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8050\n",
      "Epoch 00176: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1051-53] Learning rate for epoch 176 is 0.004124999977648258\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.8050 - val_loss: 3.6767\n",
      "Epoch 177/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8980\n",
      "Epoch 00177: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1051-58] Learning rate for epoch 177 is 0.003955216612666845\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8980 - val_loss: 4.4964\n",
      "Epoch 178/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8508\n",
      "Epoch 00178: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1052-03] Learning rate for epoch 178 is 0.003785833017900586\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.8508 - val_loss: 22.8700\n",
      "Epoch 179/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8622\n",
      "Epoch 00179: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1052-07] Learning rate for epoch 179 is 0.0036168494261801243\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.8622 - val_loss: 37.8842\n",
      "Epoch 180/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6957\n",
      "Epoch 00180: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1052-12] Learning rate for epoch 180 is 0.003448265604674816\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.6957 - val_loss: 6.2449\n",
      "Epoch 181/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9549\n",
      "Epoch 00181: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1052-17] Learning rate for epoch 181 is 0.003280082019045949\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.9549 - val_loss: 4.2652\n",
      "Epoch 182/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8725\n",
      "Epoch 00182: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1052-22] Learning rate for epoch 182 is 0.0031122982036322355\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8725 - val_loss: 4.5353\n",
      "Epoch 183/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7477\n",
      "Epoch 00183: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1052-27] Learning rate for epoch 183 is 0.002944914624094963\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.7477 - val_loss: 4.4824\n",
      "Epoch 184/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6221\n",
      "Epoch 00184: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1052-32] Learning rate for epoch 184 is 0.0027779308147728443\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.6221 - val_loss: 3.8482\n",
      "Epoch 185/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9420\n",
      "Epoch 00185: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1052-36] Learning rate for epoch 185 is 0.0026113467756658792\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.9420 - val_loss: 3.5155\n",
      "Epoch 186/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1617\n",
      "Epoch 00186: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1052-41] Learning rate for epoch 186 is 0.002445162972435355\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.1617 - val_loss: 3.2209\n",
      "Epoch 187/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9649\n",
      "Epoch 00187: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1052-46] Learning rate for epoch 187 is 0.0022793791722506285\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.9649 - val_loss: 3.9652\n",
      "Epoch 188/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3991\n",
      "Epoch 00188: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1052-51] Learning rate for epoch 188 is 0.0021139951422810555\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.3991 - val_loss: 3.1058\n",
      "Epoch 189/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6305\n",
      "Epoch 00189: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1052-56] Learning rate for epoch 189 is 0.0019490111153572798\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.6305 - val_loss: 3.4545\n",
      "Epoch 190/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0097\n",
      "Epoch 00190: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1053-01] Learning rate for epoch 190 is 0.0017844270914793015\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.0097 - val_loss: 3.3323\n",
      "Epoch 191/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6212\n",
      "Epoch 00191: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1053-05] Learning rate for epoch 191 is 0.0016202429542317986\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.6212 - val_loss: 3.4580\n",
      "Epoch 192/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5937\n",
      "Epoch 00192: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1053-10] Learning rate for epoch 192 is 0.001456458936445415\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.5937 - val_loss: 3.3610\n",
      "Epoch 193/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7071\n",
      "Epoch 00193: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1053-15] Learning rate for epoch 193 is 0.001293074688874185\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.7071 - val_loss: 3.7251\n",
      "Epoch 194/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3333\n",
      "Epoch 00194: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1053-20] Learning rate for epoch 194 is 0.0011300905607640743\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.3333 - val_loss: 3.2785\n",
      "Epoch 195/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4905\n",
      "Epoch 00195: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1053-25] Learning rate for epoch 195 is 0.0009675062610767782\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.4905 - val_loss: 3.6459\n",
      "Epoch 196/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4447\n",
      "Epoch 00196: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1053-29] Learning rate for epoch 196 is 0.0008053220226429403\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.4447 - val_loss: 3.3613\n",
      "Epoch 197/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2449\n",
      "Epoch 00197: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1053-34] Learning rate for epoch 197 is 0.0006435376708395779\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.2449 - val_loss: 3.4646\n",
      "Epoch 198/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6198\n",
      "Epoch 00198: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1053-39] Learning rate for epoch 198 is 0.0004821533220820129\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 5.6198 - val_loss: 3.7556\n",
      "Epoch 199/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2125\n",
      "Epoch 00199: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1053-44] Learning rate for epoch 199 is 0.0003211689181625843\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.2125 - val_loss: 3.3589\n",
      "Epoch 200/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5943\n",
      "Epoch 00200: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1053-49] Learning rate for epoch 200 is 0.00016058447363320738\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.5943 - val_loss: 3.1930\n",
      "Epoch 201/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4826\n",
      "Epoch 00201: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1053-54] Learning rate for epoch 201 is 4.0000000467443897e-07\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.4826 - val_loss: 3.1849\n",
      "Epoch 202/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7816\n",
      "Epoch 00202: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1053-58] Learning rate for epoch 202 is 0.0001601835247129202\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.7816 - val_loss: 3.1609\n",
      "Epoch 203/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3869\n",
      "Epoch 00203: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1054-03] Learning rate for epoch 203 is 0.00031956707243807614\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.3869 - val_loss: 3.2678\n",
      "Epoch 204/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2884\n",
      "Epoch 00204: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1054-08] Learning rate for epoch 204 is 0.00047855067532509565\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.2884 - val_loss: 3.1706\n",
      "Epoch 205/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3304\n",
      "Epoch 00205: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1054-13] Learning rate for epoch 205 is 0.0006371343042701483\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.3304 - val_loss: 3.3279\n",
      "Epoch 206/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1953\n",
      "Epoch 00206: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1054-18] Learning rate for epoch 206 is 0.0007953179883770645\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.1953 - val_loss: 3.1425\n",
      "Epoch 207/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7012\n",
      "Epoch 00207: val_loss did not improve from 3.10144\n",
      "\n",
      "[20210302-1054-23] Learning rate for epoch 207 is 0.0009531017276458442\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.7012 - val_loss: 3.3196\n",
      "Epoch 208/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6981\n",
      "Epoch 00208: val_loss improved from 3.10144 to 3.08746, saving model to ./20210301-225844/heel_K7_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1054-28] Learning rate for epoch 208 is 0.0011104855220764875\n",
      "20/20 [==============================] - 2s 93ms/step - loss: 5.6981 - val_loss: 3.0875\n",
      "Epoch 209/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2814\n",
      "Epoch 00209: val_loss did not improve from 3.08746\n",
      "\n",
      "[20210302-1054-33] Learning rate for epoch 209 is 0.0012674692552536726\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.2814 - val_loss: 3.1982\n",
      "Epoch 210/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2895\n",
      "Epoch 00210: val_loss did not improve from 3.08746\n",
      "\n",
      "[20210302-1054-38] Learning rate for epoch 210 is 0.0014240531018003821\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.2895 - val_loss: 4.1113\n",
      "Epoch 211/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6494\n",
      "Epoch 00211: val_loss did not improve from 3.08746\n",
      "\n",
      "[20210302-1054-43] Learning rate for epoch 211 is 0.0015802369453012943\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.6494 - val_loss: 3.5254\n",
      "Epoch 212/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8573\n",
      "Epoch 00212: val_loss did not improve from 3.08746\n",
      "\n",
      "[20210302-1054-48] Learning rate for epoch 212 is 0.001736020902171731\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 5.8573 - val_loss: 3.2366\n",
      "Epoch 213/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4554\n",
      "Epoch 00213: val_loss did not improve from 3.08746\n",
      "\n",
      "[20210302-1054-52] Learning rate for epoch 213 is 0.0018914048559963703\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.4554 - val_loss: 3.3744\n",
      "Epoch 214/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8009\n",
      "Epoch 00214: val_loss did not improve from 3.08746\n",
      "\n",
      "[20210302-1054-57] Learning rate for epoch 214 is 0.0020463888067752123\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.8009 - val_loss: 3.3988\n",
      "Epoch 215/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8240\n",
      "Epoch 00215: val_loss did not improve from 3.08746\n",
      "\n",
      "[20210302-1055-02] Learning rate for epoch 215 is 0.0022009729873389006\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 5.8240 - val_loss: 3.2283\n",
      "Epoch 216/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4888\n",
      "Epoch 00216: val_loss did not improve from 3.08746\n",
      "\n",
      "[20210302-1055-07] Learning rate for epoch 216 is 0.002355156932026148\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.4888 - val_loss: 3.4370\n",
      "Epoch 217/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9669\n",
      "Epoch 00217: val_loss did not improve from 3.08746\n",
      "\n",
      "[20210302-1055-12] Learning rate for epoch 217 is 0.0025089411064982414\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.9669 - val_loss: 3.1787\n",
      "Epoch 218/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5349\n",
      "Epoch 00218: val_loss did not improve from 3.08746\n",
      "\n",
      "[20210302-1055-17] Learning rate for epoch 218 is 0.0026623252779245377\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.5349 - val_loss: 3.6999\n",
      "Epoch 219/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3063\n",
      "Epoch 00219: val_loss did not improve from 3.08746\n",
      "\n",
      "[20210302-1055-22] Learning rate for epoch 219 is 0.0028153094463050365\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.3063 - val_loss: 4.5851\n",
      "Epoch 220/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7403\n",
      "Epoch 00220: val_loss did not improve from 3.08746\n",
      "\n",
      "[20210302-1055-26] Learning rate for epoch 220 is 0.002967893611639738\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.7403 - val_loss: 3.3653\n",
      "Epoch 221/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7585\n",
      "Epoch 00221: val_loss did not improve from 3.08746\n",
      "\n",
      "[20210302-1055-31] Learning rate for epoch 221 is 0.003120078006759286\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.7585 - val_loss: 3.3689\n",
      "Epoch 222/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1290\n",
      "Epoch 00222: val_loss did not improve from 3.08746\n",
      "\n",
      "[20210302-1055-36] Learning rate for epoch 222 is 0.0032718623988330364\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.1290 - val_loss: 3.9870\n",
      "Epoch 223/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2148\n",
      "Epoch 00223: val_loss did not improve from 3.08746\n",
      "\n",
      "[20210302-1055-41] Learning rate for epoch 223 is 0.0034232467878609896\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.2148 - val_loss: 3.5197\n",
      "Epoch 224/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9497\n",
      "Epoch 00224: val_loss did not improve from 3.08746\n",
      "\n",
      "[20210302-1055-46] Learning rate for epoch 224 is 0.0035742311738431454\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.9497 - val_loss: 3.5143\n",
      "Epoch 225/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2663\n",
      "Epoch 00225: val_loss did not improve from 3.08746\n",
      "\n",
      "[20210302-1055-51] Learning rate for epoch 225 is 0.003724815556779504\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.2663 - val_loss: 5.5513\n",
      "Epoch 226/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9501\n",
      "Epoch 00226: val_loss did not improve from 3.08746\n",
      "\n",
      "[20210302-1055-55] Learning rate for epoch 226 is 0.003874999936670065\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.9501 - val_loss: 4.3470\n",
      "Epoch 227/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6921\n",
      "Epoch 00227: val_loss did not improve from 3.08746\n",
      "\n",
      "[20210302-1056-00] Learning rate for epoch 227 is 0.0037152154836803675\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.6921 - val_loss: 3.3975\n",
      "Epoch 228/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8707\n",
      "Epoch 00228: val_loss did not improve from 3.08746\n",
      "\n",
      "[20210302-1056-05] Learning rate for epoch 228 is 0.0035558310337364674\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.8707 - val_loss: 5.7551\n",
      "Epoch 229/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1944\n",
      "Epoch 00229: val_loss did not improve from 3.08746\n",
      "\n",
      "[20210302-1056-10] Learning rate for epoch 229 is 0.003396846354007721\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.1944 - val_loss: 3.8192\n",
      "Epoch 230/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0747\n",
      "Epoch 00230: val_loss did not improve from 3.08746\n",
      "\n",
      "[20210302-1056-15] Learning rate for epoch 230 is 0.003238261677324772\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.0747 - val_loss: 5.7081\n",
      "Epoch 231/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7778\n",
      "Epoch 00231: val_loss did not improve from 3.08746\n",
      "\n",
      "[20210302-1056-20] Learning rate for epoch 231 is 0.00308007700368762\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.7778 - val_loss: 3.8069\n",
      "Epoch 232/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8555\n",
      "Epoch 00232: val_loss did not improve from 3.08746\n",
      "\n",
      "[20210302-1056-25] Learning rate for epoch 232 is 0.002922292333096266\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8555 - val_loss: 4.3862\n",
      "Epoch 233/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8574\n",
      "Epoch 00233: val_loss did not improve from 3.08746\n",
      "\n",
      "[20210302-1056-29] Learning rate for epoch 233 is 0.002764907432720065\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.8574 - val_loss: 4.6244\n",
      "Epoch 234/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6912\n",
      "Epoch 00234: val_loss did not improve from 3.08746\n",
      "\n",
      "[20210302-1056-34] Learning rate for epoch 234 is 0.0026079227682203054\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.6912 - val_loss: 6.2744\n",
      "Epoch 235/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5511\n",
      "Epoch 00235: val_loss did not improve from 3.08746\n",
      "\n",
      "[20210302-1056-39] Learning rate for epoch 235 is 0.0024513378739356995\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.5511 - val_loss: 3.3522\n",
      "Epoch 236/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5346\n",
      "Epoch 00236: val_loss did not improve from 3.08746\n",
      "\n",
      "[20210302-1056-44] Learning rate for epoch 236 is 0.002295152982696891\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5346 - val_loss: 3.7904\n",
      "Epoch 237/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6476\n",
      "Epoch 00237: val_loss did not improve from 3.08746\n",
      "\n",
      "[20210302-1056-49] Learning rate for epoch 237 is 0.0021393680945038795\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.6476 - val_loss: 5.1502\n",
      "Epoch 238/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3944\n",
      "Epoch 00238: val_loss did not improve from 3.08746\n",
      "\n",
      "[20210302-1056-54] Learning rate for epoch 238 is 0.0019839832093566656\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.3944 - val_loss: 3.7498\n",
      "Epoch 239/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8450\n",
      "Epoch 00239: val_loss did not improve from 3.08746\n",
      "\n",
      "[20210302-1056-59] Learning rate for epoch 239 is 0.0018289980944246054\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.8450 - val_loss: 3.4286\n",
      "Epoch 240/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1703\n",
      "Epoch 00240: val_loss did not improve from 3.08746\n",
      "\n",
      "[20210302-1057-03] Learning rate for epoch 240 is 0.0016744130989536643\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.1703 - val_loss: 3.1602\n",
      "Epoch 241/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5375\n",
      "Epoch 00241: val_loss did not improve from 3.08746\n",
      "\n",
      "[20210302-1057-08] Learning rate for epoch 241 is 0.0015202279901131988\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5375 - val_loss: 4.2922\n",
      "Epoch 242/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2638\n",
      "Epoch 00242: val_loss improved from 3.08746 to 2.95261, saving model to ./20210301-225844/heel_K7_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1057-14] Learning rate for epoch 242 is 0.0013664428843185306\n",
      "20/20 [==============================] - 2s 92ms/step - loss: 5.2638 - val_loss: 2.9526\n",
      "Epoch 243/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5426\n",
      "Epoch 00243: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1057-18] Learning rate for epoch 243 is 0.0012130576651543379\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5426 - val_loss: 3.0119\n",
      "Epoch 244/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1617\n",
      "Epoch 00244: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1057-23] Learning rate for epoch 244 is 0.0010600725654512644\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.1617 - val_loss: 3.1900\n",
      "Epoch 245/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2883\n",
      "Epoch 00245: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1057-28] Learning rate for epoch 245 is 0.0009074872941710055\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.2883 - val_loss: 3.4016\n",
      "Epoch 246/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1405\n",
      "Epoch 00246: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1057-33] Learning rate for epoch 246 is 0.0007553020259365439\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.1405 - val_loss: 3.1335\n",
      "Epoch 247/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2424\n",
      "Epoch 00247: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1057-38] Learning rate for epoch 247 is 0.0006035167025402188\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.2424 - val_loss: 3.2243\n",
      "Epoch 248/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3122\n",
      "Epoch 00248: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1057-42] Learning rate for epoch 248 is 0.00045213132398203015\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.3122 - val_loss: 3.0916\n",
      "Epoch 249/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2647\n",
      "Epoch 00249: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1057-47] Learning rate for epoch 249 is 0.00030114591936580837\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.2647 - val_loss: 3.1809\n",
      "Epoch 250/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4185\n",
      "Epoch 00250: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1057-52] Learning rate for epoch 250 is 0.00015056047413963825\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.4185 - val_loss: 3.1201\n",
      "Epoch 251/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8264\n",
      "Epoch 00251: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1057-57] Learning rate for epoch 251 is 3.7500001326407073e-07\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8264 - val_loss: 3.0999\n",
      "Epoch 252/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4768\n",
      "Epoch 00252: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1058-02] Learning rate for epoch 252 is 0.00015015952521935105\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.4768 - val_loss: 3.1139\n",
      "Epoch 253/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.0478\n",
      "Epoch 00253: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1058-07] Learning rate for epoch 253 is 0.0002995440736413002\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.0478 - val_loss: 3.0952\n",
      "Epoch 254/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2555\n",
      "Epoch 00254: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1058-11] Learning rate for epoch 254 is 0.0004485286772251129\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.2555 - val_loss: 3.1064\n",
      "Epoch 255/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4683\n",
      "Epoch 00255: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1058-16] Learning rate for epoch 255 is 0.0005971133359707892\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.4683 - val_loss: 3.6559\n",
      "Epoch 256/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2233\n",
      "Epoch 00256: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1058-21] Learning rate for epoch 256 is 0.0007452979916706681\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.2233 - val_loss: 3.2641\n",
      "Epoch 257/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5214\n",
      "Epoch 00257: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1058-26] Learning rate for epoch 257 is 0.0008930827025324106\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.5214 - val_loss: 3.2580\n",
      "Epoch 258/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4105\n",
      "Epoch 00258: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1058-31] Learning rate for epoch 258 is 0.0010404675267636776\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.4105 - val_loss: 3.4036\n",
      "Epoch 259/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1683\n",
      "Epoch 00259: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1058-36] Learning rate for epoch 259 is 0.0011874522315338254\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.1683 - val_loss: 3.7812\n",
      "Epoch 260/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.0894\n",
      "Epoch 00260: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1058-41] Learning rate for epoch 260 is 0.0013340371660888195\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.0894 - val_loss: 3.4189\n",
      "Epoch 261/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3106\n",
      "Epoch 00261: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1058-45] Learning rate for epoch 261 is 0.0014802219811826944\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.3106 - val_loss: 3.7721\n",
      "Epoch 262/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5768\n",
      "Epoch 00262: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1058-50] Learning rate for epoch 262 is 0.0016260069096460938\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.5768 - val_loss: 3.6919\n",
      "Epoch 263/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4128\n",
      "Epoch 00263: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1058-55] Learning rate for epoch 263 is 0.001771391835063696\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.4128 - val_loss: 3.8347\n",
      "Epoch 264/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5997\n",
      "Epoch 00264: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1059-00] Learning rate for epoch 264 is 0.0019163768738508224\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.5997 - val_loss: 4.2960\n",
      "Epoch 265/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6303\n",
      "Epoch 00265: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1059-05] Learning rate for epoch 265 is 0.0020609619095921516\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.6303 - val_loss: 3.7529\n",
      "Epoch 266/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0388\n",
      "Epoch 00266: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1059-09] Learning rate for epoch 266 is 0.0022051469422876835\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.0388 - val_loss: 4.8463\n",
      "Epoch 267/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6678\n",
      "Epoch 00267: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1059-14] Learning rate for epoch 267 is 0.0023489322047680616\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.6678 - val_loss: 5.2649\n",
      "Epoch 268/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2764\n",
      "Epoch 00268: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1059-19] Learning rate for epoch 268 is 0.002492317231371999\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.2764 - val_loss: 5.2217\n",
      "Epoch 269/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8681\n",
      "Epoch 00269: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1059-24] Learning rate for epoch 269 is 0.0026353024877607822\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.8681 - val_loss: 3.6732\n",
      "Epoch 270/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7731\n",
      "Epoch 00270: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1059-29] Learning rate for epoch 270 is 0.0027778877411037683\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.7731 - val_loss: 3.4645\n",
      "Epoch 271/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9708\n",
      "Epoch 00271: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1059-34] Learning rate for epoch 271 is 0.002920072991400957\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.9708 - val_loss: 4.8234\n",
      "Epoch 272/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8335\n",
      "Epoch 00272: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1059-39] Learning rate for epoch 272 is 0.0030618582386523485\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.8335 - val_loss: 5.0660\n",
      "Epoch 273/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8914\n",
      "Epoch 00273: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1059-43] Learning rate for epoch 273 is 0.0032032437156885862\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.8914 - val_loss: 3.7600\n",
      "Epoch 274/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8445\n",
      "Epoch 00274: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1059-48] Learning rate for epoch 274 is 0.0033442291896790266\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.8445 - val_loss: 4.0869\n",
      "Epoch 275/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7264\n",
      "Epoch 00275: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1059-53] Learning rate for epoch 275 is 0.003484814427793026\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.7264 - val_loss: 8.7685\n",
      "Epoch 276/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3531\n",
      "Epoch 00276: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1059-58] Learning rate for epoch 276 is 0.0036249998956918716\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.3531 - val_loss: 4.1348\n",
      "Epoch 277/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5724\n",
      "Epoch 00277: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1100-03] Learning rate for epoch 277 is 0.0034752145875245333\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5724 - val_loss: 3.4741\n",
      "Epoch 278/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7626\n",
      "Epoch 00278: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1100-08] Learning rate for epoch 278 is 0.003325828816741705\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.7626 - val_loss: 4.1497\n",
      "Epoch 279/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0738\n",
      "Epoch 00279: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1100-13] Learning rate for epoch 279 is 0.0031768432818353176\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.0738 - val_loss: 3.1165\n",
      "Epoch 280/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6602\n",
      "Epoch 00280: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1100-17] Learning rate for epoch 280 is 0.0030282577499747276\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.6602 - val_loss: 4.4016\n",
      "Epoch 281/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8285\n",
      "Epoch 00281: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1100-22] Learning rate for epoch 281 is 0.0028800719883292913\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.8285 - val_loss: 3.3517\n",
      "Epoch 282/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9053\n",
      "Epoch 00282: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1100-27] Learning rate for epoch 282 is 0.0027322862297296524\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.9053 - val_loss: 3.8308\n",
      "Epoch 283/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6347\n",
      "Epoch 00283: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1100-32] Learning rate for epoch 283 is 0.002584900474175811\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.6347 - val_loss: 3.5875\n",
      "Epoch 284/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1321\n",
      "Epoch 00284: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1100-37] Learning rate for epoch 284 is 0.0024379147216677666\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.1321 - val_loss: 3.5416\n",
      "Epoch 285/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1532\n",
      "Epoch 00285: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1100-41] Learning rate for epoch 285 is 0.0022913289722055197\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.1532 - val_loss: 4.1279\n",
      "Epoch 286/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8782\n",
      "Epoch 00286: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1100-46] Learning rate for epoch 286 is 0.0021451429929584265\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.8782 - val_loss: 3.4770\n",
      "Epoch 287/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5923\n",
      "Epoch 00287: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1100-51] Learning rate for epoch 287 is 0.0019993570167571306\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5923 - val_loss: 5.6775\n",
      "Epoch 288/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1001\n",
      "Epoch 00288: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1100-56] Learning rate for epoch 288 is 0.001853971160016954\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.1001 - val_loss: 3.4433\n",
      "Epoch 289/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0118\n",
      "Epoch 00289: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1101-01] Learning rate for epoch 289 is 0.001708985073491931\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.0118 - val_loss: 4.6393\n",
      "Epoch 290/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3980\n",
      "Epoch 00290: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1101-06] Learning rate for epoch 290 is 0.0015643991064280272\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.3980 - val_loss: 9.8351\n",
      "Epoch 291/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5428\n",
      "Epoch 00291: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1101-11] Learning rate for epoch 291 is 0.0014202130259945989\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.5428 - val_loss: 8.5709\n",
      "Epoch 292/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.0042\n",
      "Epoch 00292: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1101-15] Learning rate for epoch 292 is 0.001276426832191646\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.0042 - val_loss: 3.8552\n",
      "Epoch 293/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5321\n",
      "Epoch 00293: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1101-20] Learning rate for epoch 293 is 0.0011330407578498125\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.5321 - val_loss: 3.1253\n",
      "Epoch 294/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7271\n",
      "Epoch 00294: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1101-25] Learning rate for epoch 294 is 0.0009900545701384544\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.7271 - val_loss: 3.4303\n",
      "Epoch 295/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.0688\n",
      "Epoch 00295: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1101-30] Learning rate for epoch 295 is 0.0008474682690575719\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.0688 - val_loss: 3.1393\n",
      "Epoch 296/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3235\n",
      "Epoch 00296: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1101-35] Learning rate for epoch 296 is 0.0007052819710224867\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.3235 - val_loss: 3.2206\n",
      "Epoch 297/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2011\n",
      "Epoch 00297: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1101-39] Learning rate for epoch 297 is 0.0005634956760331988\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.2011 - val_loss: 3.0138\n",
      "Epoch 298/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4527\n",
      "Epoch 00298: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1101-44] Learning rate for epoch 298 is 0.0004221093258820474\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.4527 - val_loss: 3.0207\n",
      "Epoch 299/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2425\n",
      "Epoch 00299: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1101-49] Learning rate for epoch 299 is 0.00028112292056903243\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.2425 - val_loss: 3.0003\n",
      "Epoch 300/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1974\n",
      "Epoch 00300: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1101-54] Learning rate for epoch 300 is 0.0001405364746460691\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.1974 - val_loss: 3.0019\n",
      "Epoch 301/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2000\n",
      "Epoch 00301: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1101-59] Learning rate for epoch 301 is 3.4999999343199306e-07\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.2000 - val_loss: 3.0054\n",
      "Epoch 302/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2738\n",
      "Epoch 00302: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1102-04] Learning rate for epoch 302 is 0.00014013552572578192\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.2738 - val_loss: 3.0042\n",
      "Epoch 303/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1487\n",
      "Epoch 00303: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1102-09] Learning rate for epoch 303 is 0.00027952107484452426\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.1487 - val_loss: 3.0035\n",
      "Epoch 304/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 4.8819\n",
      "Epoch 00304: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1102-13] Learning rate for epoch 304 is 0.0004185066791251302\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 4.8819 - val_loss: 3.0387\n",
      "Epoch 305/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 4.8875\n",
      "Epoch 00305: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1102-18] Learning rate for epoch 305 is 0.0005570923094637692\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 4.8875 - val_loss: 3.0914\n",
      "Epoch 306/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4326\n",
      "Epoch 00306: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1102-23] Learning rate for epoch 306 is 0.0006952779949642718\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.4326 - val_loss: 3.4073\n",
      "Epoch 307/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4812\n",
      "Epoch 00307: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1102-28] Learning rate for epoch 307 is 0.0008330637356266379\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.4812 - val_loss: 3.3823\n",
      "Epoch 308/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3357\n",
      "Epoch 00308: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1102-33] Learning rate for epoch 308 is 0.0009704494732432067\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.3357 - val_loss: 3.5345\n",
      "Epoch 309/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 4.9499\n",
      "Epoch 00309: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1102-38] Learning rate for epoch 309 is 0.0011074353242293\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 4.9499 - val_loss: 3.2111\n",
      "Epoch 310/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2031\n",
      "Epoch 00310: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1102-42] Learning rate for epoch 310 is 0.001244021113961935\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.2031 - val_loss: 3.1876\n",
      "Epoch 311/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1444\n",
      "Epoch 00311: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1102-47] Learning rate for epoch 311 is 0.0013802070170640945\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.1444 - val_loss: 4.0411\n",
      "Epoch 312/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4352\n",
      "Epoch 00312: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1102-52] Learning rate for epoch 312 is 0.0015159929171204567\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.4352 - val_loss: 3.5870\n",
      "Epoch 313/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6788\n",
      "Epoch 00313: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1102-57] Learning rate for epoch 313 is 0.0016513789305463433\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6788 - val_loss: 3.5166\n",
      "Epoch 314/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5853\n",
      "Epoch 00314: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1103-02] Learning rate for epoch 314 is 0.0017863648245111108\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.5853 - val_loss: 3.3063\n",
      "Epoch 315/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3388\n",
      "Epoch 00315: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1103-07] Learning rate for epoch 315 is 0.0019209509482607245\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.3388 - val_loss: 3.3623\n",
      "Epoch 316/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7122\n",
      "Epoch 00316: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1103-11] Learning rate for epoch 316 is 0.002055136952549219\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.7122 - val_loss: 3.5472\n",
      "Epoch 317/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7017\n",
      "Epoch 00317: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1103-16] Learning rate for epoch 317 is 0.002188923070207238\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.7017 - val_loss: 6.2078\n",
      "Epoch 318/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4117\n",
      "Epoch 00318: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1103-21] Learning rate for epoch 318 is 0.00232230918481946\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.4117 - val_loss: 3.8031\n",
      "Epoch 319/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1650\n",
      "Epoch 00319: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1103-26] Learning rate for epoch 319 is 0.002455295529216528\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.1650 - val_loss: 3.9414\n",
      "Epoch 320/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5256\n",
      "Epoch 00320: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1103-31] Learning rate for epoch 320 is 0.002587881637737155\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5256 - val_loss: 5.8619\n",
      "Epoch 321/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7240\n",
      "Epoch 00321: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1103-35] Learning rate for epoch 321 is 0.0027200679760426283\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.7240 - val_loss: 3.4573\n",
      "Epoch 322/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2240\n",
      "Epoch 00322: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1103-40] Learning rate for epoch 322 is 0.0028518543113023043\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.2240 - val_loss: 3.9917\n",
      "Epoch 323/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6397\n",
      "Epoch 00323: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1103-45] Learning rate for epoch 323 is 0.002983240643516183\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6397 - val_loss: 4.0186\n",
      "Epoch 324/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7942\n",
      "Epoch 00324: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1103-50] Learning rate for epoch 324 is 0.003114226972684264\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.7942 - val_loss: 439.1534\n",
      "Epoch 325/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7007\n",
      "Epoch 00325: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1103-55] Learning rate for epoch 325 is 0.0032448135316371918\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.7007 - val_loss: 65.8957\n",
      "Epoch 326/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9500\n",
      "Epoch 00326: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1104-00] Learning rate for epoch 326 is 0.003375000087544322\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.9500 - val_loss: 31.3694\n",
      "Epoch 327/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8801\n",
      "Epoch 00327: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1104-04] Learning rate for epoch 327 is 0.0032352134585380554\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8801 - val_loss: 7.3203\n",
      "Epoch 328/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5964\n",
      "Epoch 00328: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1104-09] Learning rate for epoch 328 is 0.003095826832577586\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 5.5964 - val_loss: 29.9458\n",
      "Epoch 329/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2548\n",
      "Epoch 00329: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1104-14] Learning rate for epoch 329 is 0.0029568402096629143\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.2548 - val_loss: 9.2432\n",
      "Epoch 330/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2601\n",
      "Epoch 00330: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1104-19] Learning rate for epoch 330 is 0.0028182535897940397\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.2601 - val_loss: 5.0811\n",
      "Epoch 331/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1034\n",
      "Epoch 00331: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1104-24] Learning rate for epoch 331 is 0.0026800669729709625\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.1034 - val_loss: 4.2900\n",
      "Epoch 332/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5914\n",
      "Epoch 00332: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1104-29] Learning rate for epoch 332 is 0.0025422803591936827\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.5914 - val_loss: 6.0309\n",
      "Epoch 333/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5049\n",
      "Epoch 00333: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1104-33] Learning rate for epoch 333 is 0.0024048935156315565\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.5049 - val_loss: 3.6739\n",
      "Epoch 334/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3519\n",
      "Epoch 00334: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1104-38] Learning rate for epoch 334 is 0.0022679066751152277\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.3519 - val_loss: 3.2777\n",
      "Epoch 335/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2581\n",
      "Epoch 00335: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1104-43] Learning rate for epoch 335 is 0.0021313198376446962\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.2581 - val_loss: 3.6562\n",
      "Epoch 336/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1809\n",
      "Epoch 00336: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1104-48] Learning rate for epoch 336 is 0.001995133003219962\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.1809 - val_loss: 3.4963\n",
      "Epoch 337/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4804\n",
      "Epoch 00337: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1104-53] Learning rate for epoch 337 is 0.0018593460554257035\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.4804 - val_loss: 3.5827\n",
      "Epoch 338/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1092\n",
      "Epoch 00338: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1104-58] Learning rate for epoch 338 is 0.0017239591106772423\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.1092 - val_loss: 3.5638\n",
      "Epoch 339/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2157\n",
      "Epoch 00339: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1105-03] Learning rate for epoch 339 is 0.0015889721689745784\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 5.2157 - val_loss: 3.2283\n",
      "Epoch 340/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5551\n",
      "Epoch 00340: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1105-07] Learning rate for epoch 340 is 0.00145438511390239\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.5551 - val_loss: 3.5406\n",
      "Epoch 341/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3125\n",
      "Epoch 00341: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1105-12] Learning rate for epoch 341 is 0.0013201979454606771\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.3125 - val_loss: 3.8929\n",
      "Epoch 342/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.0951\n",
      "Epoch 00342: val_loss did not improve from 2.95261\n",
      "\n",
      "[20210302-1105-17] Learning rate for epoch 342 is 0.0011864108964800835\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.0951 - val_loss: 3.5676\n",
      "K= 8\n",
      "Check cache in memory:Y True\n",
      "Check keypoints process01: Tensor(\"args_1:0\", shape=(2,), dtype=int64) (2,) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Check keypoints process02: Tensor(\"Reshape:0\", shape=(1, 2), dtype=int64) (1, 2) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Check keypoints process03: Tensor(\"PyFunc:1\", dtype=float32, device=/job:localhost/replica:0/task:0)\n",
      "Check keypoints process04: Tensor(\"Reshape_1:0\", shape=(2,), dtype=float32)\n",
      "Check augment :Y True\n",
      "Check ds cache[True] and augment[True]\n",
      "Check cache in memory:Y True\n",
      "Check augment :N False\n",
      "Check ds cache[True] and augment[False]\n",
      "best_model_name: ./20210301-225844/heel_K8_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "base_model.trainable :  False\n",
      "\n",
      "[20210302-1105-20] Learning rate for epoch 1 is 0.00047855067532509565\n",
      "Epoch 1/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 105.9687\n",
      "Epoch 00001: val_loss improved from inf to 76.21897, saving model to ./20210301-225844/heel_K8_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 15s 730ms/step - loss: 104.5982 - val_loss: 76.2190\n",
      "\n",
      "[20210302-1105-44] Learning rate for epoch 2 is 0.00047855067532509565\n",
      "Epoch 2/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 41.5557\n",
      "Epoch 00002: val_loss improved from 76.21897 to 27.67149, saving model to ./20210301-225844/heel_K8_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 40.9072 - val_loss: 27.6715\n",
      "\n",
      "[20210302-1105-48] Learning rate for epoch 3 is 0.00047855067532509565\n",
      "Epoch 3/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 23.1510\n",
      "Epoch 00003: val_loss improved from 27.67149 to 23.28555, saving model to ./20210301-225844/heel_K8_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 8s 397ms/step - loss: 23.1510 - val_loss: 23.2855\n",
      "\n",
      "[20210302-1106-00] Learning rate for epoch 4 is 0.00047855067532509565\n",
      "Epoch 4/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 21.0181\n",
      "Epoch 00004: val_loss improved from 23.28555 to 21.80652, saving model to ./20210301-225844/heel_K8_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 20.9734 - val_loss: 21.8065\n",
      "\n",
      "[20210302-1106-04] Learning rate for epoch 5 is 0.00047855067532509565\n",
      "Epoch 5/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 19.5027\n",
      "Epoch 00005: val_loss improved from 21.80652 to 20.98569, saving model to ./20210301-225844/heel_K8_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 19.5027 - val_loss: 20.9857\n",
      "\n",
      "[20210302-1106-08] Learning rate for epoch 6 is 0.00047855067532509565\n",
      "Epoch 6/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 18.6164\n",
      "Epoch 00006: val_loss improved from 20.98569 to 20.23637, saving model to ./20210301-225844/heel_K8_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 18.6026 - val_loss: 20.2364\n",
      "\n",
      "[20210302-1106-13] Learning rate for epoch 7 is 0.00047855067532509565\n",
      "Epoch 7/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 17.9070\n",
      "Epoch 00007: val_loss improved from 20.23637 to 19.16057, saving model to ./20210301-225844/heel_K8_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 17.8882 - val_loss: 19.1606\n",
      "\n",
      "[20210302-1106-17] Learning rate for epoch 8 is 0.00047855067532509565\n",
      "Epoch 8/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 18.2950\n",
      "Epoch 00008: val_loss improved from 19.16057 to 18.73966, saving model to ./20210301-225844/heel_K8_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 18.2950 - val_loss: 18.7397\n",
      "\n",
      "[20210302-1106-21] Learning rate for epoch 9 is 0.00047855067532509565\n",
      "Epoch 9/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 17.5296\n",
      "Epoch 00009: val_loss improved from 18.73966 to 18.54859, saving model to ./20210301-225844/heel_K8_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 17.4631 - val_loss: 18.5486\n",
      "\n",
      "[20210302-1106-26] Learning rate for epoch 10 is 0.00047855067532509565\n",
      "Epoch 10/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 17.7889\n",
      "Epoch 00010: val_loss improved from 18.54859 to 17.68539, saving model to ./20210301-225844/heel_K8_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 17.7889 - val_loss: 17.6854\n",
      "\n",
      "[20210302-1106-30] Learning rate for epoch 11 is 0.00047855067532509565\n",
      "Epoch 11/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 17.4590\n",
      "Epoch 00011: val_loss did not improve from 17.68539\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 17.3727 - val_loss: 17.8743\n",
      "\n",
      "[20210302-1106-34] Learning rate for epoch 12 is 0.00047855067532509565\n",
      "Epoch 12/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 17.1020\n",
      "Epoch 00012: val_loss improved from 17.68539 to 17.12476, saving model to ./20210301-225844/heel_K8_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 17.1173 - val_loss: 17.1248\n",
      "\n",
      "[20210302-1106-38] Learning rate for epoch 13 is 0.00047855067532509565\n",
      "Epoch 13/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 17.0827\n",
      "Epoch 00013: val_loss improved from 17.12476 to 16.67761, saving model to ./20210301-225844/heel_K8_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 17.0827 - val_loss: 16.6776\n",
      "\n",
      "[20210302-1106-43] Learning rate for epoch 14 is 0.00047855067532509565\n",
      "Epoch 14/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.6807\n",
      "Epoch 00014: val_loss did not improve from 16.67761\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.6807 - val_loss: 16.7797\n",
      "\n",
      "[20210302-1106-47] Learning rate for epoch 15 is 0.00047855067532509565\n",
      "Epoch 15/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 16.9602\n",
      "Epoch 00015: val_loss improved from 16.67761 to 15.87798, saving model to ./20210301-225844/heel_K8_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 16.8009 - val_loss: 15.8780\n",
      "\n",
      "[20210302-1106-51] Learning rate for epoch 16 is 0.00047855067532509565\n",
      "Epoch 16/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 16.9706\n",
      "Epoch 00016: val_loss did not improve from 15.87798\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 17.0423 - val_loss: 15.8852\n",
      "\n",
      "[20210302-1106-55] Learning rate for epoch 17 is 0.00047855067532509565\n",
      "Epoch 17/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 16.8729\n",
      "Epoch 00017: val_loss improved from 15.87798 to 15.37675, saving model to ./20210301-225844/heel_K8_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 16.8882 - val_loss: 15.3768\n",
      "\n",
      "[20210302-1106-59] Learning rate for epoch 18 is 0.00047855067532509565\n",
      "Epoch 18/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.9431\n",
      "Epoch 00018: val_loss improved from 15.37675 to 14.96568, saving model to ./20210301-225844/heel_K8_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 16.9431 - val_loss: 14.9657\n",
      "\n",
      "[20210302-1107-03] Learning rate for epoch 19 is 0.00047855067532509565\n",
      "Epoch 19/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.6053\n",
      "Epoch 00019: val_loss improved from 14.96568 to 14.27569, saving model to ./20210301-225844/heel_K8_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 16.6053 - val_loss: 14.2757\n",
      "\n",
      "[20210302-1107-08] Learning rate for epoch 20 is 0.00047855067532509565\n",
      "Epoch 20/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.6790\n",
      "Epoch 00020: val_loss improved from 14.27569 to 14.13772, saving model to ./20210301-225844/heel_K8_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 16.7762 - val_loss: 14.1377\n",
      "\n",
      "[20210302-1107-12] Learning rate for epoch 21 is 0.00047855067532509565\n",
      "Epoch 21/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.9952\n",
      "Epoch 00021: val_loss improved from 14.13772 to 13.06359, saving model to ./20210301-225844/heel_K8_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.8621 - val_loss: 13.0636\n",
      "\n",
      "[20210302-1107-16] Learning rate for epoch 22 is 0.00047855067532509565\n",
      "Epoch 22/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 16.4695\n",
      "Epoch 00022: val_loss improved from 13.06359 to 12.45306, saving model to ./20210301-225844/heel_K8_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 16.4799 - val_loss: 12.4531\n",
      "\n",
      "[20210302-1107-21] Learning rate for epoch 23 is 0.00047855067532509565\n",
      "Epoch 23/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 16.7466\n",
      "Epoch 00023: val_loss improved from 12.45306 to 12.40780, saving model to ./20210301-225844/heel_K8_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 16.7211 - val_loss: 12.4078\n",
      "\n",
      "[20210302-1107-25] Learning rate for epoch 24 is 0.00047855067532509565\n",
      "Epoch 24/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 17.1084\n",
      "Epoch 00024: val_loss did not improve from 12.40780\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 17.1213 - val_loss: 12.6786\n",
      "\n",
      "[20210302-1107-29] Learning rate for epoch 25 is 0.00047855067532509565\n",
      "Epoch 25/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 16.9644\n",
      "Epoch 00025: val_loss improved from 12.40780 to 11.81680, saving model to ./20210301-225844/heel_K8_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 16.8897 - val_loss: 11.8168\n",
      "\n",
      "[20210302-1107-33] Learning rate for epoch 26 is 0.00047855067532509565\n",
      "Epoch 26/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.7991\n",
      "Epoch 00026: val_loss improved from 11.81680 to 11.63424, saving model to ./20210301-225844/heel_K8_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 16.8087 - val_loss: 11.6342\n",
      "\n",
      "[20210302-1107-37] Learning rate for epoch 27 is 0.00047855067532509565\n",
      "Epoch 27/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.3529\n",
      "Epoch 00027: val_loss improved from 11.63424 to 10.63929, saving model to ./20210301-225844/heel_K8_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.3850 - val_loss: 10.6393\n",
      "\n",
      "[20210302-1107-42] Learning rate for epoch 28 is 0.00047855067532509565\n",
      "Epoch 28/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.8817\n",
      "Epoch 00028: val_loss improved from 10.63929 to 10.61715, saving model to ./20210301-225844/heel_K8_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.7982 - val_loss: 10.6172\n",
      "\n",
      "[20210302-1107-46] Learning rate for epoch 29 is 0.00047855067532509565\n",
      "Epoch 29/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.9839\n",
      "Epoch 00029: val_loss did not improve from 10.61715\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.9839 - val_loss: 10.8158\n",
      "\n",
      "[20210302-1107-50] Learning rate for epoch 30 is 0.00047855067532509565\n",
      "Epoch 30/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 17.0369\n",
      "Epoch 00030: val_loss did not improve from 10.61715\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 16.9111 - val_loss: 11.2390\n",
      "\n",
      "[20210302-1107-54] Learning rate for epoch 31 is 0.00047855067532509565\n",
      "Epoch 31/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 16.5638\n",
      "Epoch 00031: val_loss did not improve from 10.61715\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 16.6799 - val_loss: 10.8300\n",
      "\n",
      "[20210302-1107-58] Learning rate for epoch 32 is 0.00047855067532509565\n",
      "Epoch 32/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.1178\n",
      "Epoch 00032: val_loss did not improve from 10.61715\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.2037 - val_loss: 12.2476\n",
      "\n",
      "[20210302-1108-02] Learning rate for epoch 33 is 0.00047855067532509565\n",
      "Epoch 33/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 16.2456\n",
      "Epoch 00033: val_loss did not improve from 10.61715\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.3276 - val_loss: 13.2608\n",
      "\n",
      "[20210302-1108-06] Learning rate for epoch 34 is 0.00047855067532509565\n",
      "Epoch 34/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.2847\n",
      "Epoch 00034: val_loss did not improve from 10.61715\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.2597 - val_loss: 13.4056\n",
      "\n",
      "[20210302-1108-10] Learning rate for epoch 35 is 0.00047855067532509565\n",
      "Epoch 35/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.4189\n",
      "Epoch 00035: val_loss did not improve from 10.61715\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 16.4189 - val_loss: 11.1897\n",
      "\n",
      "[20210302-1108-14] Learning rate for epoch 36 is 0.00047855067532509565\n",
      "Epoch 36/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.9062\n",
      "Epoch 00036: val_loss improved from 10.61715 to 9.41812, saving model to ./20210301-225844/heel_K8_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 15.8558 - val_loss: 9.4181\n",
      "\n",
      "[20210302-1108-18] Learning rate for epoch 37 is 0.00047855067532509565\n",
      "Epoch 37/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.3923\n",
      "Epoch 00037: val_loss improved from 9.41812 to 9.10341, saving model to ./20210301-225844/heel_K8_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.3923 - val_loss: 9.1034\n",
      "\n",
      "[20210302-1108-23] Learning rate for epoch 38 is 0.00047855067532509565\n",
      "Epoch 38/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.2350\n",
      "Epoch 00038: val_loss did not improve from 9.10341\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.1524 - val_loss: 10.5741\n",
      "\n",
      "[20210302-1108-27] Learning rate for epoch 39 is 0.00047855067532509565\n",
      "Epoch 39/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 15.7788\n",
      "Epoch 00039: val_loss did not improve from 9.10341\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 15.8047 - val_loss: 11.7410\n",
      "\n",
      "[20210302-1108-31] Learning rate for epoch 40 is 0.00047855067532509565\n",
      "Epoch 40/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.7851\n",
      "Epoch 00040: val_loss did not improve from 9.10341\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.7851 - val_loss: 11.2590\n",
      "\n",
      "[20210302-1108-35] Learning rate for epoch 41 is 0.00047855067532509565\n",
      "Epoch 41/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.4525\n",
      "Epoch 00041: val_loss did not improve from 9.10341\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.4525 - val_loss: 11.3351\n",
      "\n",
      "[20210302-1108-38] Learning rate for epoch 42 is 0.00047855067532509565\n",
      "Epoch 42/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.2889\n",
      "Epoch 00042: val_loss did not improve from 9.10341\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.1902 - val_loss: 10.5653\n",
      "\n",
      "[20210302-1108-42] Learning rate for epoch 43 is 0.00047855067532509565\n",
      "Epoch 43/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.9388\n",
      "Epoch 00043: val_loss did not improve from 9.10341\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.9258 - val_loss: 10.9854\n",
      "\n",
      "[20210302-1108-46] Learning rate for epoch 44 is 0.00047855067532509565\n",
      "Epoch 44/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 16.3140\n",
      "Epoch 00044: val_loss did not improve from 9.10341\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 16.1857 - val_loss: 11.5625\n",
      "\n",
      "[20210302-1108-50] Learning rate for epoch 45 is 0.00047855067532509565\n",
      "Epoch 45/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 16.6536\n",
      "Epoch 00045: val_loss did not improve from 9.10341\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 16.5722 - val_loss: 10.6704\n",
      "\n",
      "[20210302-1108-54] Learning rate for epoch 46 is 0.00047855067532509565\n",
      "Epoch 46/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.0073\n",
      "Epoch 00046: val_loss did not improve from 9.10341\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.0073 - val_loss: 9.6050\n",
      "\n",
      "[20210302-1108-58] Learning rate for epoch 47 is 0.00047855067532509565\n",
      "Epoch 47/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.9686\n",
      "Epoch 00047: val_loss did not improve from 9.10341\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.9686 - val_loss: 10.9598\n",
      "\n",
      "[20210302-1109-02] Learning rate for epoch 48 is 0.00047855067532509565\n",
      "Epoch 48/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.1027\n",
      "Epoch 00048: val_loss did not improve from 9.10341\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.1027 - val_loss: 11.1177\n",
      "\n",
      "[20210302-1109-06] Learning rate for epoch 49 is 0.00047855067532509565\n",
      "Epoch 49/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.7479\n",
      "Epoch 00049: val_loss did not improve from 9.10341\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 15.7479 - val_loss: 10.0429\n",
      "\n",
      "[20210302-1109-10] Learning rate for epoch 50 is 0.00047855067532509565\n",
      "Epoch 50/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.0588\n",
      "Epoch 00050: val_loss did not improve from 9.10341\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.1498 - val_loss: 9.3336\n",
      "\n",
      "[20210302-1109-14] Learning rate for epoch 51 is 0.00047855067532509565\n",
      "Epoch 51/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.4818\n",
      "Epoch 00051: val_loss improved from 9.10341 to 8.61863, saving model to ./20210301-225844/heel_K8_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.4885 - val_loss: 8.6186\n",
      "\n",
      "[20210302-1109-19] Learning rate for epoch 52 is 0.00047855067532509565\n",
      "Epoch 52/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.3298\n",
      "Epoch 00052: val_loss did not improve from 8.61863\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.2783 - val_loss: 8.9845\n",
      "\n",
      "[20210302-1109-23] Learning rate for epoch 53 is 0.00047855067532509565\n",
      "Epoch 53/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.8113\n",
      "Epoch 00053: val_loss did not improve from 8.61863\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.7700 - val_loss: 9.7107\n",
      "\n",
      "[20210302-1109-27] Learning rate for epoch 54 is 0.00047855067532509565\n",
      "Epoch 54/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.8476\n",
      "Epoch 00054: val_loss did not improve from 8.61863\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.8700 - val_loss: 9.9438\n",
      "\n",
      "[20210302-1109-31] Learning rate for epoch 55 is 0.00047855067532509565\n",
      "Epoch 55/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.5188\n",
      "Epoch 00055: val_loss did not improve from 8.61863\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.5686 - val_loss: 9.2057\n",
      "\n",
      "[20210302-1109-35] Learning rate for epoch 56 is 0.00047855067532509565\n",
      "Epoch 56/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.0280\n",
      "Epoch 00056: val_loss did not improve from 8.61863\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.0351 - val_loss: 8.7044\n",
      "\n",
      "[20210302-1109-39] Learning rate for epoch 57 is 0.00047855067532509565\n",
      "Epoch 57/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 15.8330\n",
      "Epoch 00057: val_loss did not improve from 8.61863\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 16.0547 - val_loss: 8.8652\n",
      "\n",
      "[20210302-1109-43] Learning rate for epoch 58 is 0.00047855067532509565\n",
      "Epoch 58/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.1168\n",
      "Epoch 00058: val_loss did not improve from 8.61863\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 15.2659 - val_loss: 9.1537\n",
      "\n",
      "[20210302-1109-46] Learning rate for epoch 59 is 0.00047855067532509565\n",
      "Epoch 59/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.4590\n",
      "Epoch 00059: val_loss did not improve from 8.61863\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 15.7281 - val_loss: 9.3981\n",
      "\n",
      "[20210302-1109-50] Learning rate for epoch 60 is 0.00047855067532509565\n",
      "Epoch 60/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.7283\n",
      "Epoch 00060: val_loss did not improve from 8.61863\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.6800 - val_loss: 9.3468\n",
      "\n",
      "[20210302-1109-54] Learning rate for epoch 61 is 0.00047855067532509565\n",
      "Epoch 61/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 15.9499\n",
      "Epoch 00061: val_loss did not improve from 8.61863\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 15.9944 - val_loss: 9.1623\n",
      "\n",
      "[20210302-1109-58] Learning rate for epoch 62 is 0.00047855067532509565\n",
      "Epoch 62/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.3483\n",
      "Epoch 00062: val_loss did not improve from 8.61863\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.3483 - val_loss: 8.9648\n",
      "\n",
      "[20210302-1110-02] Learning rate for epoch 63 is 0.00047855067532509565\n",
      "Epoch 63/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.4618\n",
      "Epoch 00063: val_loss did not improve from 8.61863\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.3962 - val_loss: 9.1352\n",
      "\n",
      "[20210302-1110-06] Learning rate for epoch 64 is 0.00047855067532509565\n",
      "Epoch 64/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 16.1141\n",
      "Epoch 00064: val_loss did not improve from 8.61863\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 16.2062 - val_loss: 10.3785\n",
      "\n",
      "[20210302-1110-10] Learning rate for epoch 65 is 0.00047855067532509565\n",
      "Epoch 65/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.3563\n",
      "Epoch 00065: val_loss did not improve from 8.61863\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.3484 - val_loss: 9.7611\n",
      "\n",
      "[20210302-1110-14] Learning rate for epoch 66 is 0.00047855067532509565\n",
      "Epoch 66/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.4411\n",
      "Epoch 00066: val_loss did not improve from 8.61863\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.5222 - val_loss: 10.1343\n",
      "\n",
      "[20210302-1110-18] Learning rate for epoch 67 is 0.00047855067532509565\n",
      "Epoch 67/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.1952\n",
      "Epoch 00067: val_loss did not improve from 8.61863\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.1876 - val_loss: 10.7239\n",
      "\n",
      "[20210302-1110-22] Learning rate for epoch 68 is 0.00047855067532509565\n",
      "Epoch 68/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.9425\n",
      "Epoch 00068: val_loss did not improve from 8.61863\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.9425 - val_loss: 9.2163\n",
      "\n",
      "[20210302-1110-26] Learning rate for epoch 69 is 0.00047855067532509565\n",
      "Epoch 69/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.8198\n",
      "Epoch 00069: val_loss improved from 8.61863 to 8.32009, saving model to ./20210301-225844/heel_K8_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 15.7400 - val_loss: 8.3201\n",
      "\n",
      "[20210302-1110-30] Learning rate for epoch 70 is 0.00047855067532509565\n",
      "Epoch 70/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.8660\n",
      "Epoch 00070: val_loss did not improve from 8.32009\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.0277 - val_loss: 8.8789\n",
      "\n",
      "[20210302-1110-34] Learning rate for epoch 71 is 0.00047855067532509565\n",
      "Epoch 71/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.2415\n",
      "Epoch 00071: val_loss did not improve from 8.32009\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.2579 - val_loss: 8.7800\n",
      "\n",
      "[20210302-1110-38] Learning rate for epoch 72 is 0.00047855067532509565\n",
      "Epoch 72/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.1138\n",
      "Epoch 00072: val_loss did not improve from 8.32009\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.0411 - val_loss: 9.0068\n",
      "\n",
      "[20210302-1110-42] Learning rate for epoch 73 is 0.00047855067532509565\n",
      "Epoch 73/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.4463\n",
      "Epoch 00073: val_loss did not improve from 8.32009\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.4958 - val_loss: 9.0470\n",
      "\n",
      "[20210302-1110-46] Learning rate for epoch 74 is 0.00047855067532509565\n",
      "Epoch 74/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.6172\n",
      "Epoch 00074: val_loss did not improve from 8.32009\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.6172 - val_loss: 8.3606\n",
      "\n",
      "[20210302-1110-50] Learning rate for epoch 75 is 0.00047855067532509565\n",
      "Epoch 75/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.8127\n",
      "Epoch 00075: val_loss did not improve from 8.32009\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.8127 - val_loss: 11.2283\n",
      "\n",
      "[20210302-1110-54] Learning rate for epoch 76 is 0.00047855067532509565\n",
      "Epoch 76/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.9504\n",
      "Epoch 00076: val_loss did not improve from 8.32009\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.8908 - val_loss: 9.8398\n",
      "\n",
      "[20210302-1110-58] Learning rate for epoch 77 is 0.00047855067532509565\n",
      "Epoch 77/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.8153\n",
      "Epoch 00077: val_loss did not improve from 8.32009\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.8153 - val_loss: 10.8824\n",
      "\n",
      "[20210302-1111-02] Learning rate for epoch 78 is 0.00047855067532509565\n",
      "Epoch 78/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 16.3796\n",
      "Epoch 00078: val_loss did not improve from 8.32009\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.2833 - val_loss: 9.6125\n",
      "\n",
      "[20210302-1111-06] Learning rate for epoch 79 is 0.00047855067532509565\n",
      "Epoch 79/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.5141\n",
      "Epoch 00079: val_loss did not improve from 8.32009\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.4162 - val_loss: 8.6320\n",
      "\n",
      "[20210302-1111-10] Learning rate for epoch 80 is 0.00047855067532509565\n",
      "Epoch 80/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.8254\n",
      "Epoch 00080: val_loss did not improve from 8.32009\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 15.8648 - val_loss: 8.5559\n",
      "\n",
      "[20210302-1111-14] Learning rate for epoch 81 is 0.00047855067532509565\n",
      "Epoch 81/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.5669\n",
      "Epoch 00081: val_loss did not improve from 8.32009\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.5988 - val_loss: 8.3453\n",
      "\n",
      "[20210302-1111-18] Learning rate for epoch 82 is 0.00047855067532509565\n",
      "Epoch 82/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.5195\n",
      "Epoch 00082: val_loss did not improve from 8.32009\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.5195 - val_loss: 9.7212\n",
      "\n",
      "[20210302-1111-22] Learning rate for epoch 83 is 0.00047855067532509565\n",
      "Epoch 83/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.6476\n",
      "Epoch 00083: val_loss did not improve from 8.32009\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.6476 - val_loss: 8.9421\n",
      "\n",
      "[20210302-1111-26] Learning rate for epoch 84 is 0.00047855067532509565\n",
      "Epoch 84/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.8531\n",
      "Epoch 00084: val_loss did not improve from 8.32009\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.8531 - val_loss: 8.8779\n",
      "\n",
      "[20210302-1111-30] Learning rate for epoch 85 is 0.00047855067532509565\n",
      "Epoch 85/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.7024\n",
      "Epoch 00085: val_loss did not improve from 8.32009\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.7024 - val_loss: 8.9489\n",
      "\n",
      "[20210302-1111-34] Learning rate for epoch 86 is 0.00047855067532509565\n",
      "Epoch 86/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 15.8180\n",
      "Epoch 00086: val_loss did not improve from 8.32009\n",
      "20/20 [==============================] - 1s 25ms/step - loss: 15.9250 - val_loss: 8.7017\n",
      "\n",
      "[20210302-1111-38] Learning rate for epoch 87 is 0.00047855067532509565\n",
      "Epoch 87/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.8583\n",
      "Epoch 00087: val_loss did not improve from 8.32009\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.8734 - val_loss: 9.7408\n",
      "\n",
      "[20210302-1111-42] Learning rate for epoch 88 is 0.00047855067532509565\n",
      "Epoch 88/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.5848\n",
      "Epoch 00088: val_loss did not improve from 8.32009\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 15.5879 - val_loss: 10.2018\n",
      "\n",
      "[20210302-1111-46] Learning rate for epoch 89 is 0.00047855067532509565\n",
      "Epoch 89/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.7506\n",
      "Epoch 00089: val_loss did not improve from 8.32009\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.7348 - val_loss: 9.2778\n",
      "\n",
      "*********************************** Start fine tune ***********************************\n",
      "tt: 195, nt:46, total layers:241\n",
      "*********************************** Start fine tune ***********************************\n",
      "Epoch 1/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 24.4658\n",
      "Epoch 00001: val_loss did not improve from 8.32009\n",
      "\n",
      "[20210302-1112-20] Learning rate for epoch 1 is 4.999999987376214e-07\n",
      "20/20 [==============================] - 14s 699ms/step - loss: 24.4658 - val_loss: 15.3568\n",
      "Epoch 2/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 22.6640\n",
      "Epoch 00002: val_loss did not improve from 8.32009\n",
      "\n",
      "[20210302-1112-25] Learning rate for epoch 2 is 5.044450517743826e-05\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 22.6640 - val_loss: 16.3748\n",
      "Epoch 3/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 18.2995\n",
      "Epoch 00003: val_loss did not improve from 8.32009\n",
      "\n",
      "[20210302-1112-46] Learning rate for epoch 3 is 0.00010028902033809572\n",
      "20/20 [==============================] - 17s 867ms/step - loss: 18.2995 - val_loss: 12.1347\n",
      "Epoch 4/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.3104\n",
      "Epoch 00004: val_loss did not improve from 8.32009\n",
      "\n",
      "[20210302-1112-51] Learning rate for epoch 4 is 0.00015003354928921908\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 15.3104 - val_loss: 11.6478\n",
      "Epoch 5/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.4832\n",
      "Epoch 00005: val_loss did not improve from 8.32009\n",
      "\n",
      "[20210302-1112-56] Learning rate for epoch 5 is 0.0001996780774788931\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 14.4832 - val_loss: 10.9102\n",
      "Epoch 6/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.0856\n",
      "Epoch 00006: val_loss did not improve from 8.32009\n",
      "\n",
      "[20210302-1113-01] Learning rate for epoch 6 is 0.000249222619459033\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 14.0856 - val_loss: 9.9079\n",
      "Epoch 7/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.3773\n",
      "Epoch 00007: val_loss improved from 8.32009 to 7.84337, saving model to ./20210301-225844/heel_K8_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1113-06] Learning rate for epoch 7 is 0.0002986671752296388\n",
      "20/20 [==============================] - 2s 93ms/step - loss: 13.3773 - val_loss: 7.8434\n",
      "Epoch 8/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 12.7436\n",
      "Epoch 00008: val_loss improved from 7.84337 to 7.73610, saving model to ./20210301-225844/heel_K8_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1113-12] Learning rate for epoch 8 is 0.00034801175934262574\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 12.7436 - val_loss: 7.7361\n",
      "Epoch 9/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 12.6299\n",
      "Epoch 00009: val_loss did not improve from 7.73610\n",
      "\n",
      "[20210302-1113-16] Learning rate for epoch 9 is 0.00039725631359033287\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 12.6299 - val_loss: 9.0112\n",
      "Epoch 10/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 12.1518\n",
      "Epoch 00010: val_loss did not improve from 7.73610\n",
      "\n",
      "[20210302-1113-21] Learning rate for epoch 10 is 0.0004464008961804211\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 12.1518 - val_loss: 9.4960\n",
      "Epoch 11/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 12.2987\n",
      "Epoch 00011: val_loss improved from 7.73610 to 6.44066, saving model to ./20210301-225844/heel_K8_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1113-27] Learning rate for epoch 11 is 0.0004954455071128905\n",
      "20/20 [==============================] - 2s 91ms/step - loss: 12.2987 - val_loss: 6.4407\n",
      "Epoch 12/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.6470\n",
      "Epoch 00012: val_loss improved from 6.44066 to 5.82572, saving model to ./20210301-225844/heel_K8_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1113-32] Learning rate for epoch 12 is 0.0005443901172839105\n",
      "20/20 [==============================] - 2s 92ms/step - loss: 11.6470 - val_loss: 5.8257\n",
      "Epoch 13/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.6967\n",
      "Epoch 00013: val_loss did not improve from 5.82572\n",
      "\n",
      "[20210302-1113-37] Learning rate for epoch 13 is 0.0005932347266934812\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 11.6967 - val_loss: 6.8426\n",
      "Epoch 14/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.9294\n",
      "Epoch 00014: val_loss did not improve from 5.82572\n",
      "\n",
      "[20210302-1113-42] Learning rate for epoch 14 is 0.0006419793353416026\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 10.9294 - val_loss: 7.2154\n",
      "Epoch 15/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.7052\n",
      "Epoch 00015: val_loss improved from 5.82572 to 4.72421, saving model to ./20210301-225844/heel_K8_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1113-47] Learning rate for epoch 15 is 0.0006906240014359355\n",
      "20/20 [==============================] - 2s 91ms/step - loss: 10.7052 - val_loss: 4.7242\n",
      "Epoch 16/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.7589\n",
      "Epoch 00016: val_loss did not improve from 4.72421\n",
      "\n",
      "[20210302-1113-52] Learning rate for epoch 16 is 0.0007391686085611582\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 9.7589 - val_loss: 5.7632\n",
      "Epoch 17/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.3546\n",
      "Epoch 00017: val_loss did not improve from 4.72421\n",
      "\n",
      "[20210302-1113-57] Learning rate for epoch 17 is 0.0007876132731325924\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 9.3546 - val_loss: 5.1391\n",
      "Epoch 18/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.4905\n",
      "Epoch 00018: val_loss did not improve from 4.72421\n",
      "\n",
      "[20210302-1114-02] Learning rate for epoch 18 is 0.0008359579369425774\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 9.4905 - val_loss: 6.2976\n",
      "Epoch 19/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.7398\n",
      "Epoch 00019: val_loss did not improve from 4.72421\n",
      "\n",
      "[20210302-1114-07] Learning rate for epoch 19 is 0.000884202599991113\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 8.7398 - val_loss: 7.7293\n",
      "Epoch 20/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.7285\n",
      "Epoch 00020: val_loss did not improve from 4.72421\n",
      "\n",
      "[20210302-1114-12] Learning rate for epoch 20 is 0.0009323473204858601\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 8.7285 - val_loss: 6.5185\n",
      "Epoch 21/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.1128\n",
      "Epoch 00021: val_loss did not improve from 4.72421\n",
      "\n",
      "[20210302-1114-17] Learning rate for epoch 21 is 0.000980391982011497\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 8.1128 - val_loss: 6.7573\n",
      "Epoch 22/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.1769\n",
      "Epoch 00022: val_loss did not improve from 4.72421\n",
      "\n",
      "[20210302-1114-22] Learning rate for epoch 22 is 0.0010283367009833455\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 8.1769 - val_loss: 6.5414\n",
      "Epoch 23/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.1571\n",
      "Epoch 00023: val_loss improved from 4.72421 to 4.72383, saving model to ./20210301-225844/heel_K8_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1114-27] Learning rate for epoch 23 is 0.0010761814191937447\n",
      "20/20 [==============================] - 2s 89ms/step - loss: 8.1571 - val_loss: 4.7238\n",
      "Epoch 24/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.3185\n",
      "Epoch 00024: val_loss did not improve from 4.72383\n",
      "\n",
      "[20210302-1114-32] Learning rate for epoch 24 is 0.0011239261366426945\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 8.3185 - val_loss: 4.7453\n",
      "Epoch 25/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7708\n",
      "Epoch 00025: val_loss did not improve from 4.72383\n",
      "\n",
      "[20210302-1114-37] Learning rate for epoch 25 is 0.001171570853330195\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.7708 - val_loss: 5.3001\n",
      "Epoch 26/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.9084\n",
      "Epoch 00026: val_loss improved from 4.72383 to 4.60961, saving model to ./20210301-225844/heel_K8_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1114-42] Learning rate for epoch 26 is 0.001219115569256246\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 7.9084 - val_loss: 4.6096\n",
      "Epoch 27/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.2121\n",
      "Epoch 00027: val_loss did not improve from 4.60961\n",
      "\n",
      "[20210302-1114-47] Learning rate for epoch 27 is 0.0012665604008361697\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 8.2121 - val_loss: 7.0743\n",
      "Epoch 28/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7857\n",
      "Epoch 00028: val_loss improved from 4.60961 to 4.19183, saving model to ./20210301-225844/heel_K8_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1114-53] Learning rate for epoch 28 is 0.0013139051152393222\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 7.7857 - val_loss: 4.1918\n",
      "Epoch 29/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.3827\n",
      "Epoch 00029: val_loss did not improve from 4.19183\n",
      "\n",
      "[20210302-1114-57] Learning rate for epoch 29 is 0.0013611499452963471\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 8.3827 - val_loss: 6.8635\n",
      "Epoch 30/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4812\n",
      "Epoch 00030: val_loss did not improve from 4.19183\n",
      "\n",
      "[20210302-1115-02] Learning rate for epoch 30 is 0.001408294658176601\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.4812 - val_loss: 6.0544\n",
      "Epoch 31/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5400\n",
      "Epoch 00031: val_loss did not improve from 4.19183\n",
      "\n",
      "[20210302-1115-07] Learning rate for epoch 31 is 0.0014553394867107272\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.5400 - val_loss: 6.2940\n",
      "Epoch 32/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5389\n",
      "Epoch 00032: val_loss did not improve from 4.19183\n",
      "\n",
      "[20210302-1115-12] Learning rate for epoch 32 is 0.0015022843144834042\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.5389 - val_loss: 4.3486\n",
      "Epoch 33/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5773\n",
      "Epoch 00033: val_loss improved from 4.19183 to 3.80809, saving model to ./20210301-225844/heel_K8_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1115-17] Learning rate for epoch 33 is 0.0015491291414946318\n",
      "20/20 [==============================] - 2s 89ms/step - loss: 7.5773 - val_loss: 3.8081\n",
      "Epoch 34/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8788\n",
      "Epoch 00034: val_loss did not improve from 3.80809\n",
      "\n",
      "[20210302-1115-22] Learning rate for epoch 34 is 0.00159587396774441\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.8788 - val_loss: 5.8228\n",
      "Epoch 35/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2644\n",
      "Epoch 00035: val_loss did not improve from 3.80809\n",
      "\n",
      "[20210302-1115-27] Learning rate for epoch 35 is 0.001642518793232739\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.2644 - val_loss: 4.4455\n",
      "Epoch 36/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2699\n",
      "Epoch 00036: val_loss did not improve from 3.80809\n",
      "\n",
      "[20210302-1115-32] Learning rate for epoch 36 is 0.0016890636179596186\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.2699 - val_loss: 6.4209\n",
      "Epoch 37/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7968\n",
      "Epoch 00037: val_loss did not improve from 3.80809\n",
      "\n",
      "[20210302-1115-37] Learning rate for epoch 37 is 0.0017355084419250488\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.7968 - val_loss: 5.9844\n",
      "Epoch 38/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7593\n",
      "Epoch 00038: val_loss did not improve from 3.80809\n",
      "\n",
      "[20210302-1115-42] Learning rate for epoch 38 is 0.0017818533815443516\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.7593 - val_loss: 5.9727\n",
      "Epoch 39/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1072\n",
      "Epoch 00039: val_loss did not improve from 3.80809\n",
      "\n",
      "[20210302-1115-46] Learning rate for epoch 39 is 0.0018280982039868832\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.1072 - val_loss: 6.2020\n",
      "Epoch 40/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2299\n",
      "Epoch 00040: val_loss did not improve from 3.80809\n",
      "\n",
      "[20210302-1115-51] Learning rate for epoch 40 is 0.0018742431420832872\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.2299 - val_loss: 6.2883\n",
      "Epoch 41/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4715\n",
      "Epoch 00041: val_loss did not improve from 3.80809\n",
      "\n",
      "[20210302-1115-56] Learning rate for epoch 41 is 0.0019202879630029202\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.4715 - val_loss: 4.9875\n",
      "Epoch 42/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6807\n",
      "Epoch 00042: val_loss did not improve from 3.80809\n",
      "\n",
      "[20210302-1116-01] Learning rate for epoch 42 is 0.0019662328995764256\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.6807 - val_loss: 6.3210\n",
      "Epoch 43/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4298\n",
      "Epoch 00043: val_loss did not improve from 3.80809\n",
      "\n",
      "[20210302-1116-06] Learning rate for epoch 43 is 0.00201207771897316\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.4298 - val_loss: 11.0866\n",
      "Epoch 44/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4356\n",
      "Epoch 00044: val_loss did not improve from 3.80809\n",
      "\n",
      "[20210302-1116-11] Learning rate for epoch 44 is 0.0020578226540237665\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.4356 - val_loss: 46.3260\n",
      "Epoch 45/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2767\n",
      "Epoch 00045: val_loss did not improve from 3.80809\n",
      "\n",
      "[20210302-1116-16] Learning rate for epoch 45 is 0.0021034677047282457\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.2767 - val_loss: 8.9124\n",
      "Epoch 46/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5332\n",
      "Epoch 00046: val_loss did not improve from 3.80809\n",
      "\n",
      "[20210302-1116-20] Learning rate for epoch 46 is 0.002149012638255954\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.5332 - val_loss: 626.4567\n",
      "Epoch 47/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7509\n",
      "Epoch 00047: val_loss did not improve from 3.80809\n",
      "\n",
      "[20210302-1116-25] Learning rate for epoch 47 is 0.0021944576874375343\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.7509 - val_loss: 21.9749\n",
      "Epoch 48/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2586\n",
      "Epoch 00048: val_loss did not improve from 3.80809\n",
      "\n",
      "[20210302-1116-30] Learning rate for epoch 48 is 0.0022398026194423437\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 7.2586 - val_loss: 11.1138\n",
      "Epoch 49/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1680\n",
      "Epoch 00049: val_loss did not improve from 3.80809\n",
      "\n",
      "[20210302-1116-35] Learning rate for epoch 49 is 0.002285047434270382\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 7.1680 - val_loss: 16.0506\n",
      "Epoch 50/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9663\n",
      "Epoch 00050: val_loss did not improve from 3.80809\n",
      "\n",
      "[20210302-1116-40] Learning rate for epoch 50 is 0.0023301925975829363\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.9663 - val_loss: 10.8944\n",
      "Epoch 51/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3867\n",
      "Epoch 00051: val_loss did not improve from 3.80809\n",
      "\n",
      "[20210302-1116-45] Learning rate for epoch 51 is 0.002375237410888076\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.3867 - val_loss: 6.2715\n",
      "Epoch 52/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8563\n",
      "Epoch 00052: val_loss did not improve from 3.80809\n",
      "\n",
      "[20210302-1116-50] Learning rate for epoch 52 is 0.0024201825726777315\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.8563 - val_loss: 5.6603\n",
      "Epoch 53/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9745\n",
      "Epoch 00053: val_loss did not improve from 3.80809\n",
      "\n",
      "[20210302-1116-55] Learning rate for epoch 53 is 0.002465027617290616\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.9745 - val_loss: 4.1783\n",
      "Epoch 54/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8665\n",
      "Epoch 00054: val_loss did not improve from 3.80809\n",
      "\n",
      "[20210302-1116-59] Learning rate for epoch 54 is 0.0025097725447267294\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.8665 - val_loss: 5.1365\n",
      "Epoch 55/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8719\n",
      "Epoch 00055: val_loss did not improve from 3.80809\n",
      "\n",
      "[20210302-1117-04] Learning rate for epoch 55 is 0.0025544175878167152\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.8719 - val_loss: 10.8362\n",
      "Epoch 56/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6158\n",
      "Epoch 00056: val_loss did not improve from 3.80809\n",
      "\n",
      "[20210302-1117-09] Learning rate for epoch 56 is 0.00259896251372993\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.6158 - val_loss: 7.7232\n",
      "Epoch 57/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0295\n",
      "Epoch 00057: val_loss did not improve from 3.80809\n",
      "\n",
      "[20210302-1117-14] Learning rate for epoch 57 is 0.0026434077881276608\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.0295 - val_loss: 5.0239\n",
      "Epoch 58/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0279\n",
      "Epoch 00058: val_loss improved from 3.80809 to 3.62636, saving model to ./20210301-225844/heel_K8_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1117-19] Learning rate for epoch 58 is 0.0026877527125179768\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 7.0279 - val_loss: 3.6264\n",
      "Epoch 59/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4112\n",
      "Epoch 00059: val_loss did not improve from 3.62636\n",
      "\n",
      "[20210302-1117-24] Learning rate for epoch 59 is 0.0027319977525621653\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.4112 - val_loss: 4.9188\n",
      "Epoch 60/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6281\n",
      "Epoch 00060: val_loss did not improve from 3.62636\n",
      "\n",
      "[20210302-1117-29] Learning rate for epoch 60 is 0.0027761429082602262\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.6281 - val_loss: 8.3528\n",
      "Epoch 61/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4948\n",
      "Epoch 00061: val_loss did not improve from 3.62636\n",
      "\n",
      "[20210302-1117-34] Learning rate for epoch 61 is 0.002820187946781516\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.4948 - val_loss: 3.9441\n",
      "Epoch 62/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3057\n",
      "Epoch 00062: val_loss improved from 3.62636 to 3.58358, saving model to ./20210301-225844/heel_K8_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1117-39] Learning rate for epoch 62 is 0.0028641331009566784\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 7.3057 - val_loss: 3.5836\n",
      "Epoch 63/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8737\n",
      "Epoch 00063: val_loss did not improve from 3.58358\n",
      "\n",
      "[20210302-1117-44] Learning rate for epoch 63 is 0.0029079781379550695\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.8737 - val_loss: 6.1658\n",
      "Epoch 64/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5056\n",
      "Epoch 00064: val_loss did not improve from 3.58358\n",
      "\n",
      "[20210302-1117-49] Learning rate for epoch 64 is 0.002951723290607333\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.5056 - val_loss: 4.9431\n",
      "Epoch 65/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3626\n",
      "Epoch 00065: val_loss did not improve from 3.58358\n",
      "\n",
      "[20210302-1117-54] Learning rate for epoch 65 is 0.0029953685589134693\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.3626 - val_loss: 4.1431\n",
      "Epoch 66/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8479\n",
      "Epoch 00066: val_loss improved from 3.58358 to 3.55367, saving model to ./20210301-225844/heel_K8_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1117-59] Learning rate for epoch 66 is 0.0030389137100428343\n",
      "20/20 [==============================] - 2s 91ms/step - loss: 6.8479 - val_loss: 3.5537\n",
      "Epoch 67/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5741\n",
      "Epoch 00067: val_loss did not improve from 3.55367\n",
      "\n",
      "[20210302-1118-04] Learning rate for epoch 67 is 0.003082358743995428\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.5741 - val_loss: 3.8616\n",
      "Epoch 68/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5246\n",
      "Epoch 00068: val_loss did not improve from 3.55367\n",
      "\n",
      "[20210302-1118-09] Learning rate for epoch 68 is 0.0031257038936018944\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.5246 - val_loss: 3.9007\n",
      "Epoch 69/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7542\n",
      "Epoch 00069: val_loss did not improve from 3.55367\n",
      "\n",
      "[20210302-1118-14] Learning rate for epoch 69 is 0.003168949158862233\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.7542 - val_loss: 4.5897\n",
      "Epoch 70/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0240\n",
      "Epoch 00070: val_loss did not improve from 3.55367\n",
      "\n",
      "[20210302-1118-19] Learning rate for epoch 70 is 0.0032120943069458008\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.0240 - val_loss: 4.6894\n",
      "Epoch 71/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9715\n",
      "Epoch 00071: val_loss did not improve from 3.55367\n",
      "\n",
      "[20210302-1118-23] Learning rate for epoch 71 is 0.003255139570683241\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.9715 - val_loss: 4.3556\n",
      "Epoch 72/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9991\n",
      "Epoch 00072: val_loss did not improve from 3.55367\n",
      "\n",
      "[20210302-1118-28] Learning rate for epoch 72 is 0.00329808471724391\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.9991 - val_loss: 3.7844\n",
      "Epoch 73/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3983\n",
      "Epoch 00073: val_loss did not improve from 3.55367\n",
      "\n",
      "[20210302-1118-33] Learning rate for epoch 73 is 0.0033409299794584513\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.3983 - val_loss: 3.9406\n",
      "Epoch 74/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0645\n",
      "Epoch 00074: val_loss did not improve from 3.55367\n",
      "\n",
      "[20210302-1118-38] Learning rate for epoch 74 is 0.0033836751244962215\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.0645 - val_loss: 21.5848\n",
      "Epoch 75/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9021\n",
      "Epoch 00075: val_loss did not improve from 3.55367\n",
      "\n",
      "[20210302-1118-43] Learning rate for epoch 75 is 0.0034263203851878643\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.9021 - val_loss: 188.7355\n",
      "Epoch 76/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5680\n",
      "Epoch 00076: val_loss did not improve from 3.55367\n",
      "\n",
      "[20210302-1118-48] Learning rate for epoch 76 is 0.003468865528702736\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.5680 - val_loss: 29.7343\n",
      "Epoch 77/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0454\n",
      "Epoch 00077: val_loss did not improve from 3.55367\n",
      "\n",
      "[20210302-1118-52] Learning rate for epoch 77 is 0.00351131078787148\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.0454 - val_loss: 19.2203\n",
      "Epoch 78/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4476\n",
      "Epoch 00078: val_loss did not improve from 3.55367\n",
      "\n",
      "[20210302-1118-57] Learning rate for epoch 78 is 0.0035536561626940966\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.4476 - val_loss: 14.8747\n",
      "Epoch 79/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5298\n",
      "Epoch 00079: val_loss did not improve from 3.55367\n",
      "\n",
      "[20210302-1119-02] Learning rate for epoch 79 is 0.003595901420339942\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.5298 - val_loss: 4.0147\n",
      "Epoch 80/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4746\n",
      "Epoch 00080: val_loss did not improve from 3.55367\n",
      "\n",
      "[20210302-1119-07] Learning rate for epoch 80 is 0.00363804679363966\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.4746 - val_loss: 4.1241\n",
      "Epoch 81/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7875\n",
      "Epoch 00081: val_loss did not improve from 3.55367\n",
      "\n",
      "[20210302-1119-12] Learning rate for epoch 81 is 0.0036800920497626066\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.7875 - val_loss: 7.3012\n",
      "Epoch 82/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3367\n",
      "Epoch 00082: val_loss did not improve from 3.55367\n",
      "\n",
      "[20210302-1119-17] Learning rate for epoch 82 is 0.003722037188708782\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.3367 - val_loss: 3.9652\n",
      "Epoch 83/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0407\n",
      "Epoch 00083: val_loss did not improve from 3.55367\n",
      "\n",
      "[20210302-1119-21] Learning rate for epoch 83 is 0.003763882676139474\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.0407 - val_loss: 488.0380\n",
      "Epoch 84/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3105\n",
      "Epoch 00084: val_loss did not improve from 3.55367\n",
      "\n",
      "[20210302-1119-26] Learning rate for epoch 84 is 0.0038056280463933945\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.3105 - val_loss: 1395.5526\n",
      "Epoch 85/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0144\n",
      "Epoch 00085: val_loss did not improve from 3.55367\n",
      "\n",
      "[20210302-1119-31] Learning rate for epoch 85 is 0.003847273299470544\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.0144 - val_loss: 1907.3461\n",
      "Epoch 86/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2928\n",
      "Epoch 00086: val_loss did not improve from 3.55367\n",
      "\n",
      "[20210302-1119-36] Learning rate for epoch 86 is 0.0038888186682015657\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.2928 - val_loss: 596.5278\n",
      "Epoch 87/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2443\n",
      "Epoch 00087: val_loss did not improve from 3.55367\n",
      "\n",
      "[20210302-1119-41] Learning rate for epoch 87 is 0.00393026415258646\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.2443 - val_loss: 134.6341\n",
      "Epoch 88/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6335\n",
      "Epoch 00088: val_loss did not improve from 3.55367\n",
      "\n",
      "[20210302-1119-46] Learning rate for epoch 88 is 0.00397160928696394\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.6335 - val_loss: 66.4039\n",
      "Epoch 89/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1568\n",
      "Epoch 00089: val_loss did not improve from 3.55367\n",
      "\n",
      "[20210302-1119-50] Learning rate for epoch 89 is 0.004012854769825935\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.1568 - val_loss: 320.8863\n",
      "Epoch 90/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5708\n",
      "Epoch 00090: val_loss did not improve from 3.55367\n",
      "\n",
      "[20210302-1119-55] Learning rate for epoch 90 is 0.00405400013551116\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.5708 - val_loss: 88.7081\n",
      "Epoch 91/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3739\n",
      "Epoch 00091: val_loss did not improve from 3.55367\n",
      "\n",
      "[20210302-1120-00] Learning rate for epoch 91 is 0.004095045384019613\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.3739 - val_loss: 44.8769\n",
      "Epoch 92/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5825\n",
      "Epoch 00092: val_loss did not improve from 3.55367\n",
      "\n",
      "[20210302-1120-05] Learning rate for epoch 92 is 0.004135990981012583\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.5825 - val_loss: 81.1657\n",
      "Epoch 93/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4473\n",
      "Epoch 00093: val_loss did not improve from 3.55367\n",
      "\n",
      "[20210302-1120-10] Learning rate for epoch 93 is 0.004176836460828781\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.4473 - val_loss: 9.9918\n",
      "Epoch 94/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3252\n",
      "Epoch 00094: val_loss did not improve from 3.55367\n",
      "\n",
      "[20210302-1120-15] Learning rate for epoch 94 is 0.004217581823468208\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.3252 - val_loss: 3.6266\n",
      "Epoch 95/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4430\n",
      "Epoch 00095: val_loss improved from 3.55367 to 3.33739, saving model to ./20210301-225844/heel_K8_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1120-20] Learning rate for epoch 95 is 0.004258227068930864\n",
      "20/20 [==============================] - 2s 89ms/step - loss: 6.4430 - val_loss: 3.3374\n",
      "Epoch 96/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6711\n",
      "Epoch 00096: val_loss did not improve from 3.33739\n",
      "\n",
      "[20210302-1120-25] Learning rate for epoch 96 is 0.0042987726628780365\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.6711 - val_loss: 9.7999\n",
      "Epoch 97/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5451\n",
      "Epoch 00097: val_loss did not improve from 3.33739\n",
      "\n",
      "[20210302-1120-30] Learning rate for epoch 97 is 0.0043392181396484375\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.5451 - val_loss: 6.8603\n",
      "Epoch 98/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2901\n",
      "Epoch 00098: val_loss did not improve from 3.33739\n",
      "\n",
      "[20210302-1120-35] Learning rate for epoch 98 is 0.004379563499242067\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.2901 - val_loss: 3.3444\n",
      "Epoch 99/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0709\n",
      "Epoch 00099: val_loss did not improve from 3.33739\n",
      "\n",
      "[20210302-1120-39] Learning rate for epoch 99 is 0.004419809207320213\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.0709 - val_loss: 4.4207\n",
      "Epoch 100/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4690\n",
      "Epoch 00100: val_loss did not improve from 3.33739\n",
      "\n",
      "[20210302-1120-44] Learning rate for epoch 100 is 0.004459954332560301\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.4690 - val_loss: 4.8662\n",
      "Epoch 101/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4001\n",
      "Epoch 00101: val_loss did not improve from 3.33739\n",
      "\n",
      "[20210302-1120-49] Learning rate for epoch 101 is 4.4999998749517545e-07\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.4001 - val_loss: 4.6435\n",
      "Epoch 102/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3307\n",
      "Epoch 00102: val_loss did not improve from 3.33739\n",
      "\n",
      "[20210302-1120-54] Learning rate for epoch 102 is 0.00018023152370005846\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.3307 - val_loss: 3.8204\n",
      "Epoch 103/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1804\n",
      "Epoch 00103: val_loss did not improve from 3.33739\n",
      "\n",
      "[20210302-1120-59] Learning rate for epoch 103 is 0.000359613070031628\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.1804 - val_loss: 3.4857\n",
      "Epoch 104/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1773\n",
      "Epoch 00104: val_loss improved from 3.33739 to 3.32702, saving model to ./20210301-225844/heel_K8_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1121-04] Learning rate for epoch 104 is 0.0005385946715250611\n",
      "20/20 [==============================] - 2s 89ms/step - loss: 6.1773 - val_loss: 3.3270\n",
      "Epoch 105/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2521\n",
      "Epoch 00105: val_loss improved from 3.32702 to 3.16528, saving model to ./20210301-225844/heel_K8_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1121-10] Learning rate for epoch 105 is 0.0007171762990765274\n",
      "20/20 [==============================] - 2s 93ms/step - loss: 6.2521 - val_loss: 3.1653\n",
      "Epoch 106/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0141\n",
      "Epoch 00106: val_loss did not improve from 3.16528\n",
      "\n",
      "[20210302-1121-20] Learning rate for epoch 106 is 0.0008953579817898571\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.0141 - val_loss: 4.0850\n",
      "Epoch 107/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0641\n",
      "Epoch 00107: val_loss improved from 3.16528 to 3.11070, saving model to ./20210301-225844/heel_K8_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1121-25] Learning rate for epoch 107 is 0.0010731397196650505\n",
      "20/20 [==============================] - 2s 92ms/step - loss: 6.0641 - val_loss: 3.1107\n",
      "Epoch 108/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9028\n",
      "Epoch 00108: val_loss did not improve from 3.11070\n",
      "\n",
      "[20210302-1121-30] Learning rate for epoch 108 is 0.0012505215127021074\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.9028 - val_loss: 4.2614\n",
      "Epoch 109/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4704\n",
      "Epoch 00109: val_loss did not improve from 3.11070\n",
      "\n",
      "[20210302-1121-35] Learning rate for epoch 109 is 0.001427503302693367\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.4704 - val_loss: 3.5614\n",
      "Epoch 110/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1245\n",
      "Epoch 00110: val_loss did not improve from 3.11070\n",
      "\n",
      "[20210302-1121-40] Learning rate for epoch 110 is 0.0016040850896388292\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.1245 - val_loss: 3.9636\n",
      "Epoch 111/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8021\n",
      "Epoch 00111: val_loss did not improve from 3.11070\n",
      "\n",
      "[20210302-1121-45] Learning rate for epoch 111 is 0.001780266989953816\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.8021 - val_loss: 5.1123\n",
      "Epoch 112/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1609\n",
      "Epoch 00112: val_loss did not improve from 3.11070\n",
      "\n",
      "[20210302-1121-50] Learning rate for epoch 112 is 0.0019560488872230053\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 6.1609 - val_loss: 6.4037\n",
      "Epoch 113/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4790\n",
      "Epoch 00113: val_loss did not improve from 3.11070\n",
      "\n",
      "[20210302-1121-55] Learning rate for epoch 113 is 0.002131430897861719\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.4790 - val_loss: 3.1897\n",
      "Epoch 114/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7824\n",
      "Epoch 00114: val_loss did not improve from 3.11070\n",
      "\n",
      "[20210302-1122-00] Learning rate for epoch 114 is 0.0023064129054546356\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.7824 - val_loss: 3.2520\n",
      "Epoch 115/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0946\n",
      "Epoch 00115: val_loss did not improve from 3.11070\n",
      "\n",
      "[20210302-1122-05] Learning rate for epoch 115 is 0.0024809949100017548\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.0946 - val_loss: 3.9712\n",
      "Epoch 116/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6390\n",
      "Epoch 00116: val_loss did not improve from 3.11070\n",
      "\n",
      "[20210302-1122-10] Learning rate for epoch 116 is 0.0026551769115030766\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.6390 - val_loss: 3.5104\n",
      "Epoch 117/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3932\n",
      "Epoch 00117: val_loss did not improve from 3.11070\n",
      "\n",
      "[20210302-1122-15] Learning rate for epoch 117 is 0.0028289591427892447\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.3932 - val_loss: 5.1115\n",
      "Epoch 118/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2254\n",
      "Epoch 00118: val_loss did not improve from 3.11070\n",
      "\n",
      "[20210302-1122-19] Learning rate for epoch 118 is 0.0030023413710296154\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.2254 - val_loss: 3.1310\n",
      "Epoch 119/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8972\n",
      "Epoch 00119: val_loss did not improve from 3.11070\n",
      "\n",
      "[20210302-1122-24] Learning rate for epoch 119 is 0.003175323596224189\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.8972 - val_loss: 3.5583\n",
      "Epoch 120/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3330\n",
      "Epoch 00120: val_loss did not improve from 3.11070\n",
      "\n",
      "[20210302-1122-29] Learning rate for epoch 120 is 0.003347905818372965\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.3330 - val_loss: 3.9804\n",
      "Epoch 121/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5400\n",
      "Epoch 00121: val_loss did not improve from 3.11070\n",
      "\n",
      "[20210302-1122-34] Learning rate for epoch 121 is 0.0035200880374759436\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.5400 - val_loss: 3.5762\n",
      "Epoch 122/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9374\n",
      "Epoch 00122: val_loss did not improve from 3.11070\n",
      "\n",
      "[20210302-1122-39] Learning rate for epoch 122 is 0.003691870253533125\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.9374 - val_loss: 3.5387\n",
      "Epoch 123/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9101\n",
      "Epoch 00123: val_loss did not improve from 3.11070\n",
      "\n",
      "[20210302-1122-44] Learning rate for epoch 123 is 0.0038632526993751526\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.9101 - val_loss: 4.9036\n",
      "Epoch 124/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0190\n",
      "Epoch 00124: val_loss did not improve from 3.11070\n",
      "\n",
      "[20210302-1122-49] Learning rate for epoch 124 is 0.004034235142171383\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.0190 - val_loss: 3.6178\n",
      "Epoch 125/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2244\n",
      "Epoch 00125: val_loss did not improve from 3.11070\n",
      "\n",
      "[20210302-1122-54] Learning rate for epoch 125 is 0.004204817581921816\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.2244 - val_loss: 4.1063\n",
      "Epoch 126/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4226\n",
      "Epoch 00126: val_loss did not improve from 3.11070\n",
      "\n",
      "[20210302-1122-59] Learning rate for epoch 126 is 0.0043750000186264515\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.4226 - val_loss: 3.4482\n",
      "Epoch 127/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8784\n",
      "Epoch 00127: val_loss did not improve from 3.11070\n",
      "\n",
      "[20210302-1123-04] Learning rate for epoch 127 is 0.0041952175088226795\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.8784 - val_loss: 4.1392\n",
      "Epoch 128/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6902\n",
      "Epoch 00128: val_loss did not improve from 3.11070\n",
      "\n",
      "[20210302-1123-08] Learning rate for epoch 128 is 0.004015835002064705\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.6902 - val_loss: 4.1535\n",
      "Epoch 129/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4853\n",
      "Epoch 00129: val_loss did not improve from 3.11070\n",
      "\n",
      "[20210302-1123-13] Learning rate for epoch 129 is 0.003836852265521884\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.4853 - val_loss: 3.6186\n",
      "Epoch 130/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9783\n",
      "Epoch 00130: val_loss did not improve from 3.11070\n",
      "\n",
      "[20210302-1123-18] Learning rate for epoch 130 is 0.003658269764855504\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.9783 - val_loss: 5.4289\n",
      "Epoch 131/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9817\n",
      "Epoch 00131: val_loss did not improve from 3.11070\n",
      "\n",
      "[20210302-1123-23] Learning rate for epoch 131 is 0.003480087034404278\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.9817 - val_loss: 4.9068\n",
      "Epoch 132/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4298\n",
      "Epoch 00132: val_loss did not improve from 3.11070\n",
      "\n",
      "[20210302-1123-28] Learning rate for epoch 132 is 0.003302304306998849\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.4298 - val_loss: 3.5595\n",
      "Epoch 133/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1346\n",
      "Epoch 00133: val_loss did not improve from 3.11070\n",
      "\n",
      "[20210302-1123-33] Learning rate for epoch 133 is 0.0031249215826392174\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.1346 - val_loss: 3.5035\n",
      "Epoch 134/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9559\n",
      "Epoch 00134: val_loss did not improve from 3.11070\n",
      "\n",
      "[20210302-1123-38] Learning rate for epoch 134 is 0.0029479386284947395\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.9559 - val_loss: 3.2846\n",
      "Epoch 135/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1496\n",
      "Epoch 00135: val_loss did not improve from 3.11070\n",
      "\n",
      "[20210302-1123-43] Learning rate for epoch 135 is 0.0027713559102267027\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.1496 - val_loss: 4.4410\n",
      "Epoch 136/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2638\n",
      "Epoch 00136: val_loss did not improve from 3.11070\n",
      "\n",
      "[20210302-1123-48] Learning rate for epoch 136 is 0.0025951729621738195\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 6.2638 - val_loss: 4.6138\n",
      "Epoch 137/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8505\n",
      "Epoch 00137: val_loss did not improve from 3.11070\n",
      "\n",
      "[20210302-1123-53] Learning rate for epoch 137 is 0.0024193900171667337\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8505 - val_loss: 3.2121\n",
      "Epoch 138/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9467\n",
      "Epoch 00138: val_loss improved from 3.11070 to 2.99766, saving model to ./20210301-225844/heel_K8_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1123-58] Learning rate for epoch 138 is 0.0022440070752054453\n",
      "20/20 [==============================] - 2s 91ms/step - loss: 5.9467 - val_loss: 2.9977\n",
      "Epoch 139/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5503\n",
      "Epoch 00139: val_loss did not improve from 2.99766\n",
      "\n",
      "[20210302-1124-03] Learning rate for epoch 139 is 0.002069024136289954\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.5503 - val_loss: 3.0919\n",
      "Epoch 140/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9908\n",
      "Epoch 00140: val_loss did not improve from 2.99766\n",
      "\n",
      "[20210302-1124-08] Learning rate for epoch 140 is 0.0018944410840049386\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.9908 - val_loss: 3.8439\n",
      "Epoch 141/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1361\n",
      "Epoch 00141: val_loss did not improve from 2.99766\n",
      "\n",
      "[20210302-1124-13] Learning rate for epoch 141 is 0.0017202580347657204\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.1361 - val_loss: 4.5711\n",
      "Epoch 142/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7803\n",
      "Epoch 00142: val_loss did not improve from 2.99766\n",
      "\n",
      "[20210302-1124-18] Learning rate for epoch 142 is 0.0015464748721569777\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.7803 - val_loss: 3.3693\n",
      "Epoch 143/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1034\n",
      "Epoch 00143: val_loss did not improve from 2.99766\n",
      "\n",
      "[20210302-1124-22] Learning rate for epoch 143 is 0.0013730917125940323\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.1034 - val_loss: 3.2230\n",
      "Epoch 144/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5700\n",
      "Epoch 00144: val_loss did not improve from 2.99766\n",
      "\n",
      "[20210302-1124-27] Learning rate for epoch 144 is 0.0012001085560768843\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.5700 - val_loss: 3.1427\n",
      "Epoch 145/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2793\n",
      "Epoch 00145: val_loss did not improve from 2.99766\n",
      "\n",
      "[20210302-1124-32] Learning rate for epoch 145 is 0.0010275252861902118\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.2793 - val_loss: 3.0557\n",
      "Epoch 146/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3698\n",
      "Epoch 00146: val_loss did not improve from 2.99766\n",
      "\n",
      "[20210302-1124-37] Learning rate for epoch 146 is 0.0008553420193493366\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.3698 - val_loss: 3.0290\n",
      "Epoch 147/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9678\n",
      "Epoch 00147: val_loss improved from 2.99766 to 2.95894, saving model to ./20210301-225844/heel_K8_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1124-43] Learning rate for epoch 147 is 0.0006835586973465979\n",
      "20/20 [==============================] - 2s 92ms/step - loss: 5.9678 - val_loss: 2.9589\n",
      "Epoch 148/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7044\n",
      "Epoch 00148: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1124-47] Learning rate for epoch 148 is 0.0005121753201819956\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.7044 - val_loss: 3.0148\n",
      "Epoch 149/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9657\n",
      "Epoch 00149: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1124-52] Learning rate for epoch 149 is 0.00034119191695936024\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.9657 - val_loss: 3.0146\n",
      "Epoch 150/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7477\n",
      "Epoch 00150: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1124-57] Learning rate for epoch 150 is 0.00017060847312677652\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.7477 - val_loss: 3.0650\n",
      "Epoch 151/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2906\n",
      "Epoch 00151: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1125-02] Learning rate for epoch 151 is 4.249999960848072e-07\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.2906 - val_loss: 3.0887\n",
      "Epoch 152/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5472\n",
      "Epoch 00152: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1125-07] Learning rate for epoch 152 is 0.00017020752420648932\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.5472 - val_loss: 3.0106\n",
      "Epoch 153/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3728\n",
      "Epoch 00153: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1125-12] Learning rate for epoch 153 is 0.0003395900712348521\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.3728 - val_loss: 3.0270\n",
      "Epoch 154/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3452\n",
      "Epoch 00154: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1125-17] Learning rate for epoch 154 is 0.0005085726734250784\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.3452 - val_loss: 3.0846\n",
      "Epoch 155/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6944\n",
      "Epoch 00155: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1125-21] Learning rate for epoch 155 is 0.0006771553307771683\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.6944 - val_loss: 3.2399\n",
      "Epoch 156/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4367\n",
      "Epoch 00156: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1125-26] Learning rate for epoch 156 is 0.0008453379850834608\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.4367 - val_loss: 3.1271\n",
      "Epoch 157/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5211\n",
      "Epoch 00157: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1125-31] Learning rate for epoch 157 is 0.0010131207527592778\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.5211 - val_loss: 3.2590\n",
      "Epoch 158/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4841\n",
      "Epoch 00158: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1125-36] Learning rate for epoch 158 is 0.0011805035173892975\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.4841 - val_loss: 3.3387\n",
      "Epoch 159/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4376\n",
      "Epoch 00159: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1125-41] Learning rate for epoch 159 is 0.0013474862789735198\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.4376 - val_loss: 3.0911\n",
      "Epoch 160/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3514\n",
      "Epoch 00160: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1125-46] Learning rate for epoch 160 is 0.0015140691539272666\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.3514 - val_loss: 3.9810\n",
      "Epoch 161/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5525\n",
      "Epoch 00161: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1125-51] Learning rate for epoch 161 is 0.001680252025835216\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5525 - val_loss: 3.3802\n",
      "Epoch 162/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5957\n",
      "Epoch 00162: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1125-56] Learning rate for epoch 162 is 0.0018460348946973681\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.5957 - val_loss: 3.1833\n",
      "Epoch 163/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0897\n",
      "Epoch 00163: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1126-00] Learning rate for epoch 163 is 0.0020114178769290447\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.0897 - val_loss: 3.7078\n",
      "Epoch 164/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6589\n",
      "Epoch 00164: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1126-05] Learning rate for epoch 164 is 0.0021764009725302458\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.6589 - val_loss: 3.2713\n",
      "Epoch 165/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3991\n",
      "Epoch 00165: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1126-10] Learning rate for epoch 165 is 0.002340983832255006\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.3991 - val_loss: 3.2452\n",
      "Epoch 166/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4844\n",
      "Epoch 00166: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1126-15] Learning rate for epoch 166 is 0.002505166921764612\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.4844 - val_loss: 4.0954\n",
      "Epoch 167/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8380\n",
      "Epoch 00167: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1126-20] Learning rate for epoch 167 is 0.002668950008228421\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 5.8380 - val_loss: 5.3150\n",
      "Epoch 168/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2085\n",
      "Epoch 00168: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1126-25] Learning rate for epoch 168 is 0.0028323333244770765\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.2085 - val_loss: 3.6195\n",
      "Epoch 169/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0958\n",
      "Epoch 00169: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1126-30] Learning rate for epoch 169 is 0.002995316404849291\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.0958 - val_loss: 7.1328\n",
      "Epoch 170/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4134\n",
      "Epoch 00170: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1126-35] Learning rate for epoch 170 is 0.0031578997150063515\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.4134 - val_loss: 12.1743\n",
      "Epoch 171/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0650\n",
      "Epoch 00171: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1126-39] Learning rate for epoch 171 is 0.0033200830221176147\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.0650 - val_loss: 4.2246\n",
      "Epoch 172/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5925\n",
      "Epoch 00172: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1126-44] Learning rate for epoch 172 is 0.0034818663261830807\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.5925 - val_loss: 5.1202\n",
      "Epoch 173/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0979\n",
      "Epoch 00173: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1126-49] Learning rate for epoch 173 is 0.0036432496272027493\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.0979 - val_loss: 4.9604\n",
      "Epoch 174/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7151\n",
      "Epoch 00174: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1126-54] Learning rate for epoch 174 is 0.003804233158007264\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.7151 - val_loss: 6.4420\n",
      "Epoch 175/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0149\n",
      "Epoch 00175: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1126-59] Learning rate for epoch 175 is 0.003964816685765982\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.0149 - val_loss: 5.2969\n",
      "Epoch 176/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1061\n",
      "Epoch 00176: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1127-04] Learning rate for epoch 176 is 0.004124999977648258\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.1061 - val_loss: 5.5366\n",
      "Epoch 177/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7201\n",
      "Epoch 00177: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1127-08] Learning rate for epoch 177 is 0.003955216612666845\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.7201 - val_loss: 4.3999\n",
      "Epoch 178/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5858\n",
      "Epoch 00178: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1127-13] Learning rate for epoch 178 is 0.003785833017900586\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5858 - val_loss: 8.1629\n",
      "Epoch 179/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0288\n",
      "Epoch 00179: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1127-18] Learning rate for epoch 179 is 0.0036168494261801243\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.0288 - val_loss: 3.5453\n",
      "Epoch 180/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3304\n",
      "Epoch 00180: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1127-23] Learning rate for epoch 180 is 0.003448265604674816\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.3304 - val_loss: 4.4217\n",
      "Epoch 181/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5064\n",
      "Epoch 00181: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1127-28] Learning rate for epoch 181 is 0.003280082019045949\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5064 - val_loss: 5.2267\n",
      "Epoch 182/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9320\n",
      "Epoch 00182: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1127-33] Learning rate for epoch 182 is 0.0031122982036322355\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.9320 - val_loss: 3.6155\n",
      "Epoch 183/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0969\n",
      "Epoch 00183: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1127-38] Learning rate for epoch 183 is 0.002944914624094963\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.0969 - val_loss: 5.1190\n",
      "Epoch 184/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2207\n",
      "Epoch 00184: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1127-42] Learning rate for epoch 184 is 0.0027779308147728443\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.2207 - val_loss: 30.8331\n",
      "Epoch 185/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6629\n",
      "Epoch 00185: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1127-47] Learning rate for epoch 185 is 0.0026113467756658792\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.6629 - val_loss: 6.0378\n",
      "Epoch 186/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6229\n",
      "Epoch 00186: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1127-52] Learning rate for epoch 186 is 0.002445162972435355\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6229 - val_loss: 3.7663\n",
      "Epoch 187/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6182\n",
      "Epoch 00187: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1127-57] Learning rate for epoch 187 is 0.0022793791722506285\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.6182 - val_loss: 4.7395\n",
      "Epoch 188/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7518\n",
      "Epoch 00188: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1128-02] Learning rate for epoch 188 is 0.0021139951422810555\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.7518 - val_loss: 3.7119\n",
      "Epoch 189/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3996\n",
      "Epoch 00189: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1128-06] Learning rate for epoch 189 is 0.0019490111153572798\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.3996 - val_loss: 6.7805\n",
      "Epoch 190/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4213\n",
      "Epoch 00190: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1128-11] Learning rate for epoch 190 is 0.0017844270914793015\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.4213 - val_loss: 3.5073\n",
      "Epoch 191/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5806\n",
      "Epoch 00191: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1128-16] Learning rate for epoch 191 is 0.0016202429542317986\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.5806 - val_loss: 4.5411\n",
      "Epoch 192/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6427\n",
      "Epoch 00192: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1128-21] Learning rate for epoch 192 is 0.001456458936445415\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.6427 - val_loss: 4.3123\n",
      "Epoch 193/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8316\n",
      "Epoch 00193: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1128-26] Learning rate for epoch 193 is 0.001293074688874185\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.8316 - val_loss: 3.5199\n",
      "Epoch 194/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6095\n",
      "Epoch 00194: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1128-31] Learning rate for epoch 194 is 0.0011300905607640743\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6095 - val_loss: 3.1767\n",
      "Epoch 195/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3129\n",
      "Epoch 00195: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1128-36] Learning rate for epoch 195 is 0.0009675062610767782\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.3129 - val_loss: 3.2987\n",
      "Epoch 196/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5362\n",
      "Epoch 00196: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1128-40] Learning rate for epoch 196 is 0.0008053220226429403\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 6.5362 - val_loss: 4.8555\n",
      "Epoch 197/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1050\n",
      "Epoch 00197: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1128-45] Learning rate for epoch 197 is 0.0006435376708395779\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.1050 - val_loss: 4.3023\n",
      "Epoch 198/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9109\n",
      "Epoch 00198: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1128-50] Learning rate for epoch 198 is 0.0004821533220820129\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.9109 - val_loss: 4.2753\n",
      "Epoch 199/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0754\n",
      "Epoch 00199: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1128-55] Learning rate for epoch 199 is 0.0003211689181625843\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.0754 - val_loss: 3.5289\n",
      "Epoch 200/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8080\n",
      "Epoch 00200: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1129-00] Learning rate for epoch 200 is 0.00016058447363320738\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.8080 - val_loss: 3.6800\n",
      "Epoch 201/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7141\n",
      "Epoch 00201: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1129-05] Learning rate for epoch 201 is 4.0000000467443897e-07\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.7141 - val_loss: 3.4820\n",
      "Epoch 202/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4745\n",
      "Epoch 00202: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1129-10] Learning rate for epoch 202 is 0.0001601835247129202\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.4745 - val_loss: 3.2202\n",
      "Epoch 203/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5470\n",
      "Epoch 00203: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1129-15] Learning rate for epoch 203 is 0.00031956707243807614\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.5470 - val_loss: 3.0722\n",
      "Epoch 204/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7729\n",
      "Epoch 00204: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1129-19] Learning rate for epoch 204 is 0.00047855067532509565\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.7729 - val_loss: 3.2413\n",
      "Epoch 205/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6740\n",
      "Epoch 00205: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1129-24] Learning rate for epoch 205 is 0.0006371343042701483\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.6740 - val_loss: 3.3316\n",
      "Epoch 206/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8986\n",
      "Epoch 00206: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1129-29] Learning rate for epoch 206 is 0.0007953179883770645\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8986 - val_loss: 3.2464\n",
      "Epoch 207/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3691\n",
      "Epoch 00207: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1129-34] Learning rate for epoch 207 is 0.0009531017276458442\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.3691 - val_loss: 3.1613\n",
      "Epoch 208/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2544\n",
      "Epoch 00208: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1129-39] Learning rate for epoch 208 is 0.0011104855220764875\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.2544 - val_loss: 3.4171\n",
      "Epoch 209/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5755\n",
      "Epoch 00209: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1129-44] Learning rate for epoch 209 is 0.0012674692552536726\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.5755 - val_loss: 3.5337\n",
      "Epoch 210/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8476\n",
      "Epoch 00210: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1129-49] Learning rate for epoch 210 is 0.0014240531018003821\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8476 - val_loss: 3.3932\n",
      "Epoch 211/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7292\n",
      "Epoch 00211: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1129-54] Learning rate for epoch 211 is 0.0015802369453012943\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.7292 - val_loss: 3.4187\n",
      "Epoch 212/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7444\n",
      "Epoch 00212: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1129-58] Learning rate for epoch 212 is 0.001736020902171731\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.7444 - val_loss: 3.9987\n",
      "Epoch 213/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5282\n",
      "Epoch 00213: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1130-03] Learning rate for epoch 213 is 0.0018914048559963703\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.5282 - val_loss: 5.7398\n",
      "Epoch 214/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3439\n",
      "Epoch 00214: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1130-08] Learning rate for epoch 214 is 0.0020463888067752123\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.3439 - val_loss: 4.6362\n",
      "Epoch 215/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4691\n",
      "Epoch 00215: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1130-13] Learning rate for epoch 215 is 0.0022009729873389006\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.4691 - val_loss: 3.9946\n",
      "Epoch 216/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7877\n",
      "Epoch 00216: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1130-18] Learning rate for epoch 216 is 0.002355156932026148\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.7877 - val_loss: 3.5040\n",
      "Epoch 217/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6182\n",
      "Epoch 00217: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1130-23] Learning rate for epoch 217 is 0.0025089411064982414\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.6182 - val_loss: 3.3014\n",
      "Epoch 218/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6990\n",
      "Epoch 00218: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1130-27] Learning rate for epoch 218 is 0.0026623252779245377\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.6990 - val_loss: 3.7665\n",
      "Epoch 219/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0946\n",
      "Epoch 00219: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1130-32] Learning rate for epoch 219 is 0.0028153094463050365\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.0946 - val_loss: 3.7019\n",
      "Epoch 220/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1485\n",
      "Epoch 00220: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1130-37] Learning rate for epoch 220 is 0.002967893611639738\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.1485 - val_loss: 4.5792\n",
      "Epoch 221/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5977\n",
      "Epoch 00221: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1130-42] Learning rate for epoch 221 is 0.003120078006759286\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.5977 - val_loss: 3.6455\n",
      "Epoch 222/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1182\n",
      "Epoch 00222: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1130-47] Learning rate for epoch 222 is 0.0032718623988330364\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.1182 - val_loss: 3.4256\n",
      "Epoch 223/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1443\n",
      "Epoch 00223: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1130-52] Learning rate for epoch 223 is 0.0034232467878609896\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.1443 - val_loss: 3.8492\n",
      "Epoch 224/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7116\n",
      "Epoch 00224: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1130-57] Learning rate for epoch 224 is 0.0035742311738431454\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.7116 - val_loss: 3.7237\n",
      "Epoch 225/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1263\n",
      "Epoch 00225: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1131-02] Learning rate for epoch 225 is 0.003724815556779504\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.1263 - val_loss: 3.5387\n",
      "Epoch 226/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0032\n",
      "Epoch 00226: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1131-06] Learning rate for epoch 226 is 0.003874999936670065\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.0032 - val_loss: 5.0347\n",
      "Epoch 227/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0911\n",
      "Epoch 00227: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1131-11] Learning rate for epoch 227 is 0.0037152154836803675\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 6.0911 - val_loss: 3.6487\n",
      "Epoch 228/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8146\n",
      "Epoch 00228: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1131-16] Learning rate for epoch 228 is 0.0035558310337364674\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.8146 - val_loss: 3.7761\n",
      "Epoch 229/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3097\n",
      "Epoch 00229: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1131-21] Learning rate for epoch 229 is 0.003396846354007721\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.3097 - val_loss: 4.2972\n",
      "Epoch 230/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4085\n",
      "Epoch 00230: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1131-26] Learning rate for epoch 230 is 0.003238261677324772\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.4085 - val_loss: 3.9703\n",
      "Epoch 231/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8981\n",
      "Epoch 00231: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1131-31] Learning rate for epoch 231 is 0.00308007700368762\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.8981 - val_loss: 3.2162\n",
      "Epoch 232/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8961\n",
      "Epoch 00232: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1131-36] Learning rate for epoch 232 is 0.002922292333096266\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.8961 - val_loss: 3.7027\n",
      "Epoch 233/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8145\n",
      "Epoch 00233: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1131-41] Learning rate for epoch 233 is 0.002764907432720065\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.8145 - val_loss: 3.9899\n",
      "Epoch 234/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3819\n",
      "Epoch 00234: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1131-46] Learning rate for epoch 234 is 0.0026079227682203054\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.3819 - val_loss: 4.6758\n",
      "Epoch 235/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9368\n",
      "Epoch 00235: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1131-51] Learning rate for epoch 235 is 0.0024513378739356995\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.9368 - val_loss: 4.9071\n",
      "Epoch 236/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9850\n",
      "Epoch 00236: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1131-56] Learning rate for epoch 236 is 0.002295152982696891\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.9850 - val_loss: 5.3823\n",
      "Epoch 237/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4902\n",
      "Epoch 00237: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1132-00] Learning rate for epoch 237 is 0.0021393680945038795\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.4902 - val_loss: 4.4651\n",
      "Epoch 238/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9236\n",
      "Epoch 00238: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1132-05] Learning rate for epoch 238 is 0.0019839832093566656\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.9236 - val_loss: 4.8442\n",
      "Epoch 239/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2285\n",
      "Epoch 00239: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1132-10] Learning rate for epoch 239 is 0.0018289980944246054\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.2285 - val_loss: 8.0156\n",
      "Epoch 240/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9652\n",
      "Epoch 00240: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1132-15] Learning rate for epoch 240 is 0.0016744130989536643\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.9652 - val_loss: 3.8476\n",
      "Epoch 241/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7671\n",
      "Epoch 00241: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1132-20] Learning rate for epoch 241 is 0.0015202279901131988\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.7671 - val_loss: 3.8070\n",
      "Epoch 242/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3323\n",
      "Epoch 00242: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1132-25] Learning rate for epoch 242 is 0.0013664428843185306\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.3323 - val_loss: 3.5264\n",
      "Epoch 243/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3991\n",
      "Epoch 00243: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1132-30] Learning rate for epoch 243 is 0.0012130576651543379\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.3991 - val_loss: 3.8298\n",
      "Epoch 244/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2935\n",
      "Epoch 00244: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1132-34] Learning rate for epoch 244 is 0.0010600725654512644\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.2935 - val_loss: 4.8476\n",
      "Epoch 245/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1658\n",
      "Epoch 00245: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1132-39] Learning rate for epoch 245 is 0.0009074872941710055\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.1658 - val_loss: 3.5796\n",
      "Epoch 246/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2697\n",
      "Epoch 00246: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1132-44] Learning rate for epoch 246 is 0.0007553020259365439\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.2697 - val_loss: 3.3151\n",
      "Epoch 247/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4153\n",
      "Epoch 00247: val_loss did not improve from 2.95894\n",
      "\n",
      "[20210302-1132-49] Learning rate for epoch 247 is 0.0006035167025402188\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.4153 - val_loss: 3.0711\n",
      "K= 9\n",
      "Check cache in memory:Y True\n",
      "Check keypoints process01: Tensor(\"args_1:0\", shape=(2,), dtype=int64) (2,) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Check keypoints process02: Tensor(\"Reshape:0\", shape=(1, 2), dtype=int64) (1, 2) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Check keypoints process03: Tensor(\"PyFunc:1\", dtype=float32, device=/job:localhost/replica:0/task:0)\n",
      "Check keypoints process04: Tensor(\"Reshape_1:0\", shape=(2,), dtype=float32)\n",
      "Check augment :Y True\n",
      "Check ds cache[True] and augment[True]\n",
      "Check cache in memory:Y True\n",
      "Check augment :N False\n",
      "Check ds cache[True] and augment[False]\n",
      "best_model_name: ./20210301-225844/heel_K9_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "base_model.trainable :  False\n",
      "\n",
      "[20210302-1132-52] Learning rate for epoch 1 is 0.00047855067532509565\n",
      "Epoch 1/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 107.1743\n",
      "Epoch 00001: val_loss improved from inf to 79.74212, saving model to ./20210301-225844/heel_K9_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 5s 266ms/step - loss: 104.9613 - val_loss: 79.7421\n",
      "\n",
      "[20210302-1133-07] Learning rate for epoch 2 is 0.00047855067532509565\n",
      "Epoch 2/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 42.5639\n",
      "Epoch 00002: val_loss improved from 79.74212 to 24.15857, saving model to ./20210301-225844/heel_K9_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 41.8233 - val_loss: 24.1586\n",
      "\n",
      "[20210302-1133-11] Learning rate for epoch 3 is 0.00047855067532509565\n",
      "Epoch 3/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 23.8898\n",
      "Epoch 00003: val_loss improved from 24.15857 to 21.97779, saving model to ./20210301-225844/heel_K9_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 8s 399ms/step - loss: 23.8898 - val_loss: 21.9778\n",
      "\n",
      "[20210302-1133-23] Learning rate for epoch 4 is 0.00047855067532509565\n",
      "Epoch 4/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 21.0176\n",
      "Epoch 00004: val_loss improved from 21.97779 to 21.14162, saving model to ./20210301-225844/heel_K9_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 20.9904 - val_loss: 21.1416\n",
      "\n",
      "[20210302-1133-27] Learning rate for epoch 5 is 0.00047855067532509565\n",
      "Epoch 5/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 19.0049\n",
      "Epoch 00005: val_loss did not improve from 21.14162\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 19.0480 - val_loss: 21.5840\n",
      "\n",
      "[20210302-1133-31] Learning rate for epoch 6 is 0.00047855067532509565\n",
      "Epoch 6/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 18.7679\n",
      "Epoch 00006: val_loss improved from 21.14162 to 20.35142, saving model to ./20210301-225844/heel_K9_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 18.7679 - val_loss: 20.3514\n",
      "\n",
      "[20210302-1133-36] Learning rate for epoch 7 is 0.00047855067532509565\n",
      "Epoch 7/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 17.9036\n",
      "Epoch 00007: val_loss improved from 20.35142 to 19.91534, saving model to ./20210301-225844/heel_K9_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 17.9050 - val_loss: 19.9153\n",
      "\n",
      "[20210302-1133-40] Learning rate for epoch 8 is 0.00047855067532509565\n",
      "Epoch 8/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 18.1929\n",
      "Epoch 00008: val_loss improved from 19.91534 to 19.32197, saving model to ./20210301-225844/heel_K9_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 18.1929 - val_loss: 19.3220\n",
      "\n",
      "[20210302-1133-44] Learning rate for epoch 9 is 0.00047855067532509565\n",
      "Epoch 9/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 17.7592\n",
      "Epoch 00009: val_loss improved from 19.32197 to 18.84490, saving model to ./20210301-225844/heel_K9_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 17.7959 - val_loss: 18.8449\n",
      "\n",
      "[20210302-1133-49] Learning rate for epoch 10 is 0.00047855067532509565\n",
      "Epoch 10/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 17.0696\n",
      "Epoch 00010: val_loss improved from 18.84490 to 17.76617, saving model to ./20210301-225844/heel_K9_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 17.0696 - val_loss: 17.7662\n",
      "\n",
      "[20210302-1133-53] Learning rate for epoch 11 is 0.00047855067532509565\n",
      "Epoch 11/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 17.3077\n",
      "Epoch 00011: val_loss did not improve from 17.76617\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 17.3077 - val_loss: 18.9904\n",
      "\n",
      "[20210302-1133-57] Learning rate for epoch 12 is 0.00047855067532509565\n",
      "Epoch 12/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 17.3585\n",
      "Epoch 00012: val_loss did not improve from 17.76617\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 17.3585 - val_loss: 18.8400\n",
      "\n",
      "[20210302-1134-01] Learning rate for epoch 13 is 0.00047855067532509565\n",
      "Epoch 13/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 17.6335\n",
      "Epoch 00013: val_loss did not improve from 17.76617\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 17.5787 - val_loss: 18.4741\n",
      "\n",
      "[20210302-1134-05] Learning rate for epoch 14 is 0.00047855067532509565\n",
      "Epoch 14/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 17.4820\n",
      "Epoch 00014: val_loss improved from 17.76617 to 15.44917, saving model to ./20210301-225844/heel_K9_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 17.4820 - val_loss: 15.4492\n",
      "\n",
      "[20210302-1134-10] Learning rate for epoch 15 is 0.00047855067532509565\n",
      "Epoch 15/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 17.5896\n",
      "Epoch 00015: val_loss did not improve from 15.44917\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 17.5226 - val_loss: 15.8480\n",
      "\n",
      "[20210302-1134-14] Learning rate for epoch 16 is 0.00047855067532509565\n",
      "Epoch 16/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 16.9604\n",
      "Epoch 00016: val_loss did not improve from 15.44917\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 16.8355 - val_loss: 16.6418\n",
      "\n",
      "[20210302-1134-18] Learning rate for epoch 17 is 0.00047855067532509565\n",
      "Epoch 17/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 17.2233\n",
      "Epoch 00017: val_loss did not improve from 15.44917\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 17.0863 - val_loss: 15.8021\n",
      "\n",
      "[20210302-1134-22] Learning rate for epoch 18 is 0.00047855067532509565\n",
      "Epoch 18/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.8071\n",
      "Epoch 00018: val_loss did not improve from 15.44917\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.8071 - val_loss: 16.3758\n",
      "\n",
      "[20210302-1134-26] Learning rate for epoch 19 is 0.00047855067532509565\n",
      "Epoch 19/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 16.6001\n",
      "Epoch 00019: val_loss improved from 15.44917 to 14.27120, saving model to ./20210301-225844/heel_K9_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 16.4658 - val_loss: 14.2712\n",
      "\n",
      "[20210302-1134-30] Learning rate for epoch 20 is 0.00047855067532509565\n",
      "Epoch 20/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.6037\n",
      "Epoch 00020: val_loss improved from 14.27120 to 13.28040, saving model to ./20210301-225844/heel_K9_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.6037 - val_loss: 13.2804\n",
      "\n",
      "[20210302-1134-34] Learning rate for epoch 21 is 0.00047855067532509565\n",
      "Epoch 21/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.8623\n",
      "Epoch 00021: val_loss did not improve from 13.28040\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.9332 - val_loss: 14.0995\n",
      "\n",
      "[20210302-1134-39] Learning rate for epoch 22 is 0.00047855067532509565\n",
      "Epoch 22/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.2442\n",
      "Epoch 00022: val_loss improved from 13.28040 to 12.41461, saving model to ./20210301-225844/heel_K9_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 16.2442 - val_loss: 12.4146\n",
      "\n",
      "[20210302-1134-43] Learning rate for epoch 23 is 0.00047855067532509565\n",
      "Epoch 23/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.2322\n",
      "Epoch 00023: val_loss did not improve from 12.41461\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.2452 - val_loss: 14.0162\n",
      "\n",
      "[20210302-1134-47] Learning rate for epoch 24 is 0.00047855067532509565\n",
      "Epoch 24/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.6292\n",
      "Epoch 00024: val_loss improved from 12.41461 to 11.91023, saving model to ./20210301-225844/heel_K9_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 16.6292 - val_loss: 11.9102\n",
      "\n",
      "[20210302-1134-51] Learning rate for epoch 25 is 0.00047855067532509565\n",
      "Epoch 25/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.6733\n",
      "Epoch 00025: val_loss did not improve from 11.91023\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.6971 - val_loss: 12.7792\n",
      "\n",
      "[20210302-1134-55] Learning rate for epoch 26 is 0.00047855067532509565\n",
      "Epoch 26/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.2397\n",
      "Epoch 00026: val_loss improved from 11.91023 to 11.76136, saving model to ./20210301-225844/heel_K9_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 16.2397 - val_loss: 11.7614\n",
      "\n",
      "[20210302-1135-00] Learning rate for epoch 27 is 0.00047855067532509565\n",
      "Epoch 27/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.3735\n",
      "Epoch 00027: val_loss did not improve from 11.76136\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.4178 - val_loss: 14.1831\n",
      "\n",
      "[20210302-1135-04] Learning rate for epoch 28 is 0.00047855067532509565\n",
      "Epoch 28/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 17.1044\n",
      "Epoch 00028: val_loss did not improve from 11.76136\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 17.1044 - val_loss: 13.4878\n",
      "\n",
      "[20210302-1135-08] Learning rate for epoch 29 is 0.00047855067532509565\n",
      "Epoch 29/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.2901\n",
      "Epoch 00029: val_loss did not improve from 11.76136\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.3925 - val_loss: 13.0704\n",
      "\n",
      "[20210302-1135-11] Learning rate for epoch 30 is 0.00047855067532509565\n",
      "Epoch 30/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.1904\n",
      "Epoch 00030: val_loss improved from 11.76136 to 10.76879, saving model to ./20210301-225844/heel_K9_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.2280 - val_loss: 10.7688\n",
      "\n",
      "[20210302-1135-16] Learning rate for epoch 31 is 0.00047855067532509565\n",
      "Epoch 31/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.6896\n",
      "Epoch 00031: val_loss did not improve from 10.76879\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.7653 - val_loss: 10.8362\n",
      "\n",
      "[20210302-1135-20] Learning rate for epoch 32 is 0.00047855067532509565\n",
      "Epoch 32/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.1099\n",
      "Epoch 00032: val_loss did not improve from 10.76879\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.1099 - val_loss: 11.0696\n",
      "\n",
      "[20210302-1135-24] Learning rate for epoch 33 is 0.00047855067532509565\n",
      "Epoch 33/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 17.0738\n",
      "Epoch 00033: val_loss improved from 10.76879 to 10.27419, saving model to ./20210301-225844/heel_K9_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 17.0738 - val_loss: 10.2742\n",
      "\n",
      "[20210302-1135-28] Learning rate for epoch 34 is 0.00047855067532509565\n",
      "Epoch 34/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 16.6795\n",
      "Epoch 00034: val_loss did not improve from 10.27419\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 16.5109 - val_loss: 11.7160\n",
      "\n",
      "[20210302-1135-32] Learning rate for epoch 35 is 0.00047855067532509565\n",
      "Epoch 35/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.3340\n",
      "Epoch 00035: val_loss did not improve from 10.27419\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.3340 - val_loss: 10.3017\n",
      "\n",
      "[20210302-1135-36] Learning rate for epoch 36 is 0.00047855067532509565\n",
      "Epoch 36/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.5411\n",
      "Epoch 00036: val_loss did not improve from 10.27419\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.5411 - val_loss: 10.4603\n",
      "\n",
      "[20210302-1135-40] Learning rate for epoch 37 is 0.00047855067532509565\n",
      "Epoch 37/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.1521\n",
      "Epoch 00037: val_loss improved from 10.27419 to 9.89479, saving model to ./20210301-225844/heel_K9_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 16.1521 - val_loss: 9.8948\n",
      "\n",
      "[20210302-1135-44] Learning rate for epoch 38 is 0.00047855067532509565\n",
      "Epoch 38/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.2289\n",
      "Epoch 00038: val_loss did not improve from 9.89479\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.2103 - val_loss: 10.0778\n",
      "\n",
      "[20210302-1135-48] Learning rate for epoch 39 is 0.00047855067532509565\n",
      "Epoch 39/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.7625\n",
      "Epoch 00039: val_loss did not improve from 9.89479\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 15.7016 - val_loss: 10.0660\n",
      "\n",
      "[20210302-1135-52] Learning rate for epoch 40 is 0.00047855067532509565\n",
      "Epoch 40/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.0459\n",
      "Epoch 00040: val_loss improved from 9.89479 to 9.36140, saving model to ./20210301-225844/heel_K9_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 16.0459 - val_loss: 9.3614\n",
      "\n",
      "[20210302-1135-57] Learning rate for epoch 41 is 0.00047855067532509565\n",
      "Epoch 41/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 15.7197\n",
      "Epoch 00041: val_loss did not improve from 9.36140\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 15.8466 - val_loss: 10.2836\n",
      "\n",
      "[20210302-1136-01] Learning rate for epoch 42 is 0.00047855067532509565\n",
      "Epoch 42/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.8680\n",
      "Epoch 00042: val_loss did not improve from 9.36140\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.8525 - val_loss: 10.0995\n",
      "\n",
      "[20210302-1136-05] Learning rate for epoch 43 is 0.00047855067532509565\n",
      "Epoch 43/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.3592\n",
      "Epoch 00043: val_loss did not improve from 9.36140\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.2767 - val_loss: 9.5104\n",
      "\n",
      "[20210302-1136-09] Learning rate for epoch 44 is 0.00047855067532509565\n",
      "Epoch 44/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.7169\n",
      "Epoch 00044: val_loss did not improve from 9.36140\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.7169 - val_loss: 10.8386\n",
      "\n",
      "[20210302-1136-13] Learning rate for epoch 45 is 0.00047855067532509565\n",
      "Epoch 45/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.7957\n",
      "Epoch 00045: val_loss did not improve from 9.36140\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.7519 - val_loss: 9.5611\n",
      "\n",
      "[20210302-1136-17] Learning rate for epoch 46 is 0.00047855067532509565\n",
      "Epoch 46/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.6227\n",
      "Epoch 00046: val_loss did not improve from 9.36140\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.6227 - val_loss: 9.9841\n",
      "\n",
      "[20210302-1136-21] Learning rate for epoch 47 is 0.00047855067532509565\n",
      "Epoch 47/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.6135\n",
      "Epoch 00047: val_loss did not improve from 9.36140\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.6135 - val_loss: 11.2698\n",
      "\n",
      "[20210302-1136-25] Learning rate for epoch 48 is 0.00047855067532509565\n",
      "Epoch 48/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.8416\n",
      "Epoch 00048: val_loss did not improve from 9.36140\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.9955 - val_loss: 9.4982\n",
      "\n",
      "[20210302-1136-29] Learning rate for epoch 49 is 0.00047855067532509565\n",
      "Epoch 49/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.4730\n",
      "Epoch 00049: val_loss did not improve from 9.36140\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.4730 - val_loss: 10.2447\n",
      "\n",
      "[20210302-1136-33] Learning rate for epoch 50 is 0.00047855067532509565\n",
      "Epoch 50/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.5522\n",
      "Epoch 00050: val_loss did not improve from 9.36140\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.5432 - val_loss: 9.7068\n",
      "\n",
      "[20210302-1136-37] Learning rate for epoch 51 is 0.00047855067532509565\n",
      "Epoch 51/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.0686\n",
      "Epoch 00051: val_loss did not improve from 9.36140\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.0686 - val_loss: 9.7739\n",
      "\n",
      "[20210302-1136-41] Learning rate for epoch 52 is 0.00047855067532509565\n",
      "Epoch 52/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.1272\n",
      "Epoch 00052: val_loss did not improve from 9.36140\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.0193 - val_loss: 10.6441\n",
      "\n",
      "[20210302-1136-45] Learning rate for epoch 53 is 0.00047855067532509565\n",
      "Epoch 53/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.0679\n",
      "Epoch 00053: val_loss did not improve from 9.36140\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.0571 - val_loss: 10.4604\n",
      "\n",
      "[20210302-1136-49] Learning rate for epoch 54 is 0.00047855067532509565\n",
      "Epoch 54/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.8051\n",
      "Epoch 00054: val_loss did not improve from 9.36140\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.8149 - val_loss: 11.6746\n",
      "\n",
      "[20210302-1136-53] Learning rate for epoch 55 is 0.00047855067532509565\n",
      "Epoch 55/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.2016\n",
      "Epoch 00055: val_loss did not improve from 9.36140\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.1830 - val_loss: 9.4279\n",
      "\n",
      "[20210302-1136-57] Learning rate for epoch 56 is 0.00047855067532509565\n",
      "Epoch 56/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.8702\n",
      "Epoch 00056: val_loss did not improve from 9.36140\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.8223 - val_loss: 9.8517\n",
      "\n",
      "[20210302-1137-01] Learning rate for epoch 57 is 0.00047855067532509565\n",
      "Epoch 57/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.8673\n",
      "Epoch 00057: val_loss did not improve from 9.36140\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.8673 - val_loss: 11.1773\n",
      "\n",
      "[20210302-1137-05] Learning rate for epoch 58 is 0.00047855067532509565\n",
      "Epoch 58/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.4929\n",
      "Epoch 00058: val_loss did not improve from 9.36140\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.4929 - val_loss: 9.8089\n",
      "\n",
      "[20210302-1137-09] Learning rate for epoch 59 is 0.00047855067532509565\n",
      "Epoch 59/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.5676\n",
      "Epoch 00059: val_loss improved from 9.36140 to 9.28259, saving model to ./20210301-225844/heel_K9_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 16.5676 - val_loss: 9.2826\n",
      "\n",
      "[20210302-1137-13] Learning rate for epoch 60 is 0.00047855067532509565\n",
      "Epoch 60/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.6145\n",
      "Epoch 00060: val_loss did not improve from 9.28259\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.6145 - val_loss: 10.5260\n",
      "\n",
      "[20210302-1137-17] Learning rate for epoch 61 is 0.00047855067532509565\n",
      "Epoch 61/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.9817\n",
      "Epoch 00061: val_loss did not improve from 9.28259\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.9817 - val_loss: 10.1393\n",
      "\n",
      "[20210302-1137-21] Learning rate for epoch 62 is 0.00047855067532509565\n",
      "Epoch 62/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.0832\n",
      "Epoch 00062: val_loss did not improve from 9.28259\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.0477 - val_loss: 13.0960\n",
      "\n",
      "[20210302-1137-25] Learning rate for epoch 63 is 0.00047855067532509565\n",
      "Epoch 63/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.1915\n",
      "Epoch 00063: val_loss did not improve from 9.28259\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.1915 - val_loss: 10.6819\n",
      "\n",
      "[20210302-1137-29] Learning rate for epoch 64 is 0.00047855067532509565\n",
      "Epoch 64/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.6403\n",
      "Epoch 00064: val_loss did not improve from 9.28259\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.6403 - val_loss: 11.9431\n",
      "\n",
      "[20210302-1137-33] Learning rate for epoch 65 is 0.00047855067532509565\n",
      "Epoch 65/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.7741\n",
      "Epoch 00065: val_loss did not improve from 9.28259\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 15.7741 - val_loss: 9.8580\n",
      "\n",
      "[20210302-1137-37] Learning rate for epoch 66 is 0.00047855067532509565\n",
      "Epoch 66/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.4749\n",
      "Epoch 00066: val_loss did not improve from 9.28259\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.5316 - val_loss: 10.0627\n",
      "\n",
      "[20210302-1137-41] Learning rate for epoch 67 is 0.00047855067532509565\n",
      "Epoch 67/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.5871\n",
      "Epoch 00067: val_loss did not improve from 9.28259\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.5871 - val_loss: 9.9920\n",
      "\n",
      "[20210302-1137-45] Learning rate for epoch 68 is 0.00047855067532509565\n",
      "Epoch 68/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.6793\n",
      "Epoch 00068: val_loss did not improve from 9.28259\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.6233 - val_loss: 11.0980\n",
      "\n",
      "[20210302-1137-49] Learning rate for epoch 69 is 0.00047855067532509565\n",
      "Epoch 69/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 16.0074\n",
      "Epoch 00069: val_loss did not improve from 9.28259\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.8847 - val_loss: 10.4674\n",
      "\n",
      "[20210302-1137-53] Learning rate for epoch 70 is 0.00047855067532509565\n",
      "Epoch 70/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.1736\n",
      "Epoch 00070: val_loss did not improve from 9.28259\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.2366 - val_loss: 10.7993\n",
      "\n",
      "[20210302-1137-57] Learning rate for epoch 71 is 0.00047855067532509565\n",
      "Epoch 71/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.2644\n",
      "Epoch 00071: val_loss did not improve from 9.28259\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.2644 - val_loss: 11.4965\n",
      "\n",
      "[20210302-1138-01] Learning rate for epoch 72 is 0.00047855067532509565\n",
      "Epoch 72/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.4689\n",
      "Epoch 00072: val_loss did not improve from 9.28259\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.4540 - val_loss: 10.8125\n",
      "\n",
      "[20210302-1138-05] Learning rate for epoch 73 is 0.00047855067532509565\n",
      "Epoch 73/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.0804\n",
      "Epoch 00073: val_loss did not improve from 9.28259\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.0804 - val_loss: 9.6820\n",
      "\n",
      "[20210302-1138-09] Learning rate for epoch 74 is 0.00047855067532509565\n",
      "Epoch 74/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.5755\n",
      "Epoch 00074: val_loss did not improve from 9.28259\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.5962 - val_loss: 9.3367\n",
      "\n",
      "[20210302-1138-13] Learning rate for epoch 75 is 0.00047855067532509565\n",
      "Epoch 75/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.6271\n",
      "Epoch 00075: val_loss did not improve from 9.28259\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.6271 - val_loss: 9.6726\n",
      "\n",
      "[20210302-1138-17] Learning rate for epoch 76 is 0.00047855067532509565\n",
      "Epoch 76/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.5215\n",
      "Epoch 00076: val_loss did not improve from 9.28259\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.5995 - val_loss: 11.8484\n",
      "\n",
      "[20210302-1138-21] Learning rate for epoch 77 is 0.00047855067532509565\n",
      "Epoch 77/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.9794\n",
      "Epoch 00077: val_loss did not improve from 9.28259\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.9794 - val_loss: 10.7280\n",
      "\n",
      "[20210302-1138-25] Learning rate for epoch 78 is 0.00047855067532509565\n",
      "Epoch 78/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.3799\n",
      "Epoch 00078: val_loss did not improve from 9.28259\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.4495 - val_loss: 10.3591\n",
      "\n",
      "[20210302-1138-29] Learning rate for epoch 79 is 0.00047855067532509565\n",
      "Epoch 79/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.1369\n",
      "Epoch 00079: val_loss did not improve from 9.28259\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.2032 - val_loss: 9.7200\n",
      "\n",
      "*********************************** Start fine tune ***********************************\n",
      "tt: 195, nt:46, total layers:241\n",
      "*********************************** Start fine tune ***********************************\n",
      "Epoch 1/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 25.2748\n",
      "Epoch 00001: val_loss did not improve from 9.28259\n",
      "\n",
      "[20210302-1139-04] Learning rate for epoch 1 is 4.999999987376214e-07\n",
      "20/20 [==============================] - 14s 701ms/step - loss: 25.2748 - val_loss: 15.8516\n",
      "Epoch 2/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 23.2681\n",
      "Epoch 00002: val_loss did not improve from 9.28259\n",
      "\n",
      "[20210302-1139-09] Learning rate for epoch 2 is 5.044450517743826e-05\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 23.2681 - val_loss: 16.4526\n",
      "Epoch 3/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 18.6177\n",
      "Epoch 00003: val_loss did not improve from 9.28259\n",
      "\n",
      "[20210302-1139-29] Learning rate for epoch 3 is 0.00010028902033809572\n",
      "20/20 [==============================] - 17s 838ms/step - loss: 18.6177 - val_loss: 15.0549\n",
      "Epoch 4/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.6138\n",
      "Epoch 00004: val_loss did not improve from 9.28259\n",
      "\n",
      "[20210302-1139-34] Learning rate for epoch 4 is 0.00015003354928921908\n",
      "20/20 [==============================] - 1s 68ms/step - loss: 15.6138 - val_loss: 12.1984\n",
      "Epoch 5/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.4711\n",
      "Epoch 00005: val_loss did not improve from 9.28259\n",
      "\n",
      "[20210302-1139-39] Learning rate for epoch 5 is 0.0001996780774788931\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 14.4711 - val_loss: 10.1092\n",
      "Epoch 6/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.9768\n",
      "Epoch 00006: val_loss improved from 9.28259 to 8.74977, saving model to ./20210301-225844/heel_K9_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1139-45] Learning rate for epoch 6 is 0.000249222619459033\n",
      "20/20 [==============================] - 2s 92ms/step - loss: 13.9768 - val_loss: 8.7498\n",
      "Epoch 7/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.6620\n",
      "Epoch 00007: val_loss improved from 8.74977 to 7.29510, saving model to ./20210301-225844/heel_K9_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1139-50] Learning rate for epoch 7 is 0.0002986671752296388\n",
      "20/20 [==============================] - 2s 91ms/step - loss: 13.6620 - val_loss: 7.2951\n",
      "Epoch 8/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.0012\n",
      "Epoch 00008: val_loss did not improve from 7.29510\n",
      "\n",
      "[20210302-1139-55] Learning rate for epoch 8 is 0.00034801175934262574\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 13.0012 - val_loss: 7.5516\n",
      "Epoch 9/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 12.6048\n",
      "Epoch 00009: val_loss improved from 7.29510 to 6.53852, saving model to ./20210301-225844/heel_K9_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1140-00] Learning rate for epoch 9 is 0.00039725631359033287\n",
      "20/20 [==============================] - 2s 92ms/step - loss: 12.6048 - val_loss: 6.5385\n",
      "Epoch 10/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 12.2322\n",
      "Epoch 00010: val_loss did not improve from 6.53852\n",
      "\n",
      "[20210302-1140-05] Learning rate for epoch 10 is 0.0004464008961804211\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 12.2322 - val_loss: 7.2851\n",
      "Epoch 11/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.8957\n",
      "Epoch 00011: val_loss did not improve from 6.53852\n",
      "\n",
      "[20210302-1140-10] Learning rate for epoch 11 is 0.0004954455071128905\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 11.8957 - val_loss: 8.0178\n",
      "Epoch 12/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.4098\n",
      "Epoch 00012: val_loss did not improve from 6.53852\n",
      "\n",
      "[20210302-1140-15] Learning rate for epoch 12 is 0.0005443901172839105\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 11.4098 - val_loss: 6.6500\n",
      "Epoch 13/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.8468\n",
      "Epoch 00013: val_loss did not improve from 6.53852\n",
      "\n",
      "[20210302-1140-20] Learning rate for epoch 13 is 0.0005932347266934812\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 10.8468 - val_loss: 7.0665\n",
      "Epoch 14/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.1699\n",
      "Epoch 00014: val_loss did not improve from 6.53852\n",
      "\n",
      "[20210302-1140-25] Learning rate for epoch 14 is 0.0006419793353416026\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 10.1699 - val_loss: 10.1921\n",
      "Epoch 15/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.0529\n",
      "Epoch 00015: val_loss improved from 6.53852 to 5.14527, saving model to ./20210301-225844/heel_K9_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1140-30] Learning rate for epoch 15 is 0.0006906240014359355\n",
      "20/20 [==============================] - 2s 92ms/step - loss: 10.0529 - val_loss: 5.1453\n",
      "Epoch 16/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.7120\n",
      "Epoch 00016: val_loss did not improve from 5.14527\n",
      "\n",
      "[20210302-1140-35] Learning rate for epoch 16 is 0.0007391686085611582\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 9.7120 - val_loss: 6.0383\n",
      "Epoch 17/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.2421\n",
      "Epoch 00017: val_loss improved from 5.14527 to 4.21649, saving model to ./20210301-225844/heel_K9_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1140-40] Learning rate for epoch 17 is 0.0007876132731325924\n",
      "20/20 [==============================] - 2s 92ms/step - loss: 9.2421 - val_loss: 4.2165\n",
      "Epoch 18/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.1443\n",
      "Epoch 00018: val_loss did not improve from 4.21649\n",
      "\n",
      "[20210302-1140-45] Learning rate for epoch 18 is 0.0008359579369425774\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 9.1443 - val_loss: 8.0808\n",
      "Epoch 19/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.1027\n",
      "Epoch 00019: val_loss did not improve from 4.21649\n",
      "\n",
      "[20210302-1140-50] Learning rate for epoch 19 is 0.000884202599991113\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 9.1027 - val_loss: 5.9300\n",
      "Epoch 20/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.6595\n",
      "Epoch 00020: val_loss improved from 4.21649 to 4.21240, saving model to ./20210301-225844/heel_K9_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1140-55] Learning rate for epoch 20 is 0.0009323473204858601\n",
      "20/20 [==============================] - 2s 93ms/step - loss: 8.6595 - val_loss: 4.2124\n",
      "Epoch 21/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.7191\n",
      "Epoch 00021: val_loss did not improve from 4.21240\n",
      "\n",
      "[20210302-1141-00] Learning rate for epoch 21 is 0.000980391982011497\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 8.7191 - val_loss: 7.1511\n",
      "Epoch 22/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.7797\n",
      "Epoch 00022: val_loss improved from 4.21240 to 4.06017, saving model to ./20210301-225844/heel_K9_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1141-05] Learning rate for epoch 22 is 0.0010283367009833455\n",
      "20/20 [==============================] - 2s 92ms/step - loss: 8.7797 - val_loss: 4.0602\n",
      "Epoch 23/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.5215\n",
      "Epoch 00023: val_loss did not improve from 4.06017\n",
      "\n",
      "[20210302-1141-10] Learning rate for epoch 23 is 0.0010761814191937447\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 8.5215 - val_loss: 6.9525\n",
      "Epoch 24/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.9577\n",
      "Epoch 00024: val_loss did not improve from 4.06017\n",
      "\n",
      "[20210302-1141-15] Learning rate for epoch 24 is 0.0011239261366426945\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.9577 - val_loss: 5.6459\n",
      "Epoch 25/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7873\n",
      "Epoch 00025: val_loss did not improve from 4.06017\n",
      "\n",
      "[20210302-1141-20] Learning rate for epoch 25 is 0.001171570853330195\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.7873 - val_loss: 8.3538\n",
      "Epoch 26/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8855\n",
      "Epoch 00026: val_loss did not improve from 4.06017\n",
      "\n",
      "[20210302-1141-25] Learning rate for epoch 26 is 0.001219115569256246\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.8855 - val_loss: 4.4907\n",
      "Epoch 27/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5054\n",
      "Epoch 00027: val_loss did not improve from 4.06017\n",
      "\n",
      "[20210302-1141-29] Learning rate for epoch 27 is 0.0012665604008361697\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 7.5054 - val_loss: 4.4163\n",
      "Epoch 28/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5169\n",
      "Epoch 00028: val_loss did not improve from 4.06017\n",
      "\n",
      "[20210302-1141-34] Learning rate for epoch 28 is 0.0013139051152393222\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.5169 - val_loss: 6.8160\n",
      "Epoch 29/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7316\n",
      "Epoch 00029: val_loss did not improve from 4.06017\n",
      "\n",
      "[20210302-1141-39] Learning rate for epoch 29 is 0.0013611499452963471\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.7316 - val_loss: 22.9018\n",
      "Epoch 30/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3470\n",
      "Epoch 00030: val_loss did not improve from 4.06017\n",
      "\n",
      "[20210302-1141-44] Learning rate for epoch 30 is 0.001408294658176601\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.3470 - val_loss: 11.8173\n",
      "Epoch 31/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6336\n",
      "Epoch 00031: val_loss did not improve from 4.06017\n",
      "\n",
      "[20210302-1141-49] Learning rate for epoch 31 is 0.0014553394867107272\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.6336 - val_loss: 8.9739\n",
      "Epoch 32/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3167\n",
      "Epoch 00032: val_loss did not improve from 4.06017\n",
      "\n",
      "[20210302-1141-54] Learning rate for epoch 32 is 0.0015022843144834042\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 7.3167 - val_loss: 18.9569\n",
      "Epoch 33/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6044\n",
      "Epoch 00033: val_loss did not improve from 4.06017\n",
      "\n",
      "[20210302-1141-59] Learning rate for epoch 33 is 0.0015491291414946318\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.6044 - val_loss: 4.8759\n",
      "Epoch 34/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4125\n",
      "Epoch 00034: val_loss improved from 4.06017 to 3.66713, saving model to ./20210301-225844/heel_K9_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1142-04] Learning rate for epoch 34 is 0.00159587396774441\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 7.4125 - val_loss: 3.6671\n",
      "Epoch 35/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1743\n",
      "Epoch 00035: val_loss did not improve from 3.66713\n",
      "\n",
      "[20210302-1142-09] Learning rate for epoch 35 is 0.001642518793232739\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.1743 - val_loss: 7.7745\n",
      "Epoch 36/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4991\n",
      "Epoch 00036: val_loss improved from 3.66713 to 3.63937, saving model to ./20210301-225844/heel_K9_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1142-14] Learning rate for epoch 36 is 0.0016890636179596186\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 7.4991 - val_loss: 3.6394\n",
      "Epoch 37/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1713\n",
      "Epoch 00037: val_loss did not improve from 3.63937\n",
      "\n",
      "[20210302-1142-19] Learning rate for epoch 37 is 0.0017355084419250488\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 7.1713 - val_loss: 3.7748\n",
      "Epoch 38/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6248\n",
      "Epoch 00038: val_loss did not improve from 3.63937\n",
      "\n",
      "[20210302-1142-24] Learning rate for epoch 38 is 0.0017818533815443516\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.6248 - val_loss: 4.7700\n",
      "Epoch 39/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3039\n",
      "Epoch 00039: val_loss did not improve from 3.63937\n",
      "\n",
      "[20210302-1142-29] Learning rate for epoch 39 is 0.0018280982039868832\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.3039 - val_loss: 5.5833\n",
      "Epoch 40/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3308\n",
      "Epoch 00040: val_loss did not improve from 3.63937\n",
      "\n",
      "[20210302-1142-34] Learning rate for epoch 40 is 0.0018742431420832872\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.3308 - val_loss: 60.5387\n",
      "Epoch 41/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0488\n",
      "Epoch 00041: val_loss did not improve from 3.63937\n",
      "\n",
      "[20210302-1142-38] Learning rate for epoch 41 is 0.0019202879630029202\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.0488 - val_loss: 4.6269\n",
      "Epoch 42/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8183\n",
      "Epoch 00042: val_loss did not improve from 3.63937\n",
      "\n",
      "[20210302-1142-43] Learning rate for epoch 42 is 0.0019662328995764256\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.8183 - val_loss: 4.8410\n",
      "Epoch 43/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3946\n",
      "Epoch 00043: val_loss did not improve from 3.63937\n",
      "\n",
      "[20210302-1142-48] Learning rate for epoch 43 is 0.00201207771897316\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.3946 - val_loss: 6.1232\n",
      "Epoch 44/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5454\n",
      "Epoch 00044: val_loss did not improve from 3.63937\n",
      "\n",
      "[20210302-1142-53] Learning rate for epoch 44 is 0.0020578226540237665\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.5454 - val_loss: 4.5357\n",
      "Epoch 45/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4751\n",
      "Epoch 00045: val_loss did not improve from 3.63937\n",
      "\n",
      "[20210302-1142-58] Learning rate for epoch 45 is 0.0021034677047282457\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.4751 - val_loss: 5.3728\n",
      "Epoch 46/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3442\n",
      "Epoch 00046: val_loss did not improve from 3.63937\n",
      "\n",
      "[20210302-1143-02] Learning rate for epoch 46 is 0.002149012638255954\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.3442 - val_loss: 12.6412\n",
      "Epoch 47/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5285\n",
      "Epoch 00047: val_loss did not improve from 3.63937\n",
      "\n",
      "[20210302-1143-07] Learning rate for epoch 47 is 0.0021944576874375343\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.5285 - val_loss: 14.8002\n",
      "Epoch 48/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4912\n",
      "Epoch 00048: val_loss did not improve from 3.63937\n",
      "\n",
      "[20210302-1143-12] Learning rate for epoch 48 is 0.0022398026194423437\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.4912 - val_loss: 8.1928\n",
      "Epoch 49/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4342\n",
      "Epoch 00049: val_loss did not improve from 3.63937\n",
      "\n",
      "[20210302-1143-17] Learning rate for epoch 49 is 0.002285047434270382\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.4342 - val_loss: 6.9169\n",
      "Epoch 50/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1270\n",
      "Epoch 00050: val_loss did not improve from 3.63937\n",
      "\n",
      "[20210302-1143-22] Learning rate for epoch 50 is 0.0023301925975829363\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.1270 - val_loss: 4.9209\n",
      "Epoch 51/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0946\n",
      "Epoch 00051: val_loss did not improve from 3.63937\n",
      "\n",
      "[20210302-1143-26] Learning rate for epoch 51 is 0.002375237410888076\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.0946 - val_loss: 5.4900\n",
      "Epoch 52/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2081\n",
      "Epoch 00052: val_loss did not improve from 3.63937\n",
      "\n",
      "[20210302-1143-31] Learning rate for epoch 52 is 0.0024201825726777315\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 7.2081 - val_loss: 6.3559\n",
      "Epoch 53/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0161\n",
      "Epoch 00053: val_loss did not improve from 3.63937\n",
      "\n",
      "[20210302-1143-36] Learning rate for epoch 53 is 0.002465027617290616\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.0161 - val_loss: 5.7383\n",
      "Epoch 54/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4295\n",
      "Epoch 00054: val_loss did not improve from 3.63937\n",
      "\n",
      "[20210302-1143-41] Learning rate for epoch 54 is 0.0025097725447267294\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 7.4295 - val_loss: 5.3441\n",
      "Epoch 55/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2113\n",
      "Epoch 00055: val_loss did not improve from 3.63937\n",
      "\n",
      "[20210302-1143-46] Learning rate for epoch 55 is 0.0025544175878167152\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.2113 - val_loss: 5.8840\n",
      "Epoch 56/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0668\n",
      "Epoch 00056: val_loss did not improve from 3.63937\n",
      "\n",
      "[20210302-1143-51] Learning rate for epoch 56 is 0.00259896251372993\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 7.0668 - val_loss: 5.9701\n",
      "Epoch 57/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9887\n",
      "Epoch 00057: val_loss did not improve from 3.63937\n",
      "\n",
      "[20210302-1143-55] Learning rate for epoch 57 is 0.0026434077881276608\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.9887 - val_loss: 5.8026\n",
      "Epoch 58/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8737\n",
      "Epoch 00058: val_loss did not improve from 3.63937\n",
      "\n",
      "[20210302-1144-00] Learning rate for epoch 58 is 0.0026877527125179768\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.8737 - val_loss: 7.5741\n",
      "Epoch 59/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0069\n",
      "Epoch 00059: val_loss did not improve from 3.63937\n",
      "\n",
      "[20210302-1144-05] Learning rate for epoch 59 is 0.0027319977525621653\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.0069 - val_loss: 5.9347\n",
      "Epoch 60/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2830\n",
      "Epoch 00060: val_loss did not improve from 3.63937\n",
      "\n",
      "[20210302-1144-10] Learning rate for epoch 60 is 0.0027761429082602262\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 7.2830 - val_loss: 10.8900\n",
      "Epoch 61/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1874\n",
      "Epoch 00061: val_loss did not improve from 3.63937\n",
      "\n",
      "[20210302-1144-15] Learning rate for epoch 61 is 0.002820187946781516\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 7.1874 - val_loss: 13.5860\n",
      "Epoch 62/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0020\n",
      "Epoch 00062: val_loss did not improve from 3.63937\n",
      "\n",
      "[20210302-1144-19] Learning rate for epoch 62 is 0.0028641331009566784\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.0020 - val_loss: 10.3147\n",
      "Epoch 63/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3895\n",
      "Epoch 00063: val_loss did not improve from 3.63937\n",
      "\n",
      "[20210302-1144-24] Learning rate for epoch 63 is 0.0029079781379550695\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.3895 - val_loss: 25.8021\n",
      "Epoch 64/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0189\n",
      "Epoch 00064: val_loss did not improve from 3.63937\n",
      "\n",
      "[20210302-1144-29] Learning rate for epoch 64 is 0.002951723290607333\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.0189 - val_loss: 5.2867\n",
      "Epoch 65/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8426\n",
      "Epoch 00065: val_loss did not improve from 3.63937\n",
      "\n",
      "[20210302-1144-34] Learning rate for epoch 65 is 0.0029953685589134693\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.8426 - val_loss: 116.9484\n",
      "Epoch 66/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9078\n",
      "Epoch 00066: val_loss did not improve from 3.63937\n",
      "\n",
      "[20210302-1144-39] Learning rate for epoch 66 is 0.0030389137100428343\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.9078 - val_loss: 7.0204\n",
      "Epoch 67/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5791\n",
      "Epoch 00067: val_loss did not improve from 3.63937\n",
      "\n",
      "[20210302-1144-44] Learning rate for epoch 67 is 0.003082358743995428\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.5791 - val_loss: 4.0950\n",
      "Epoch 68/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1966\n",
      "Epoch 00068: val_loss did not improve from 3.63937\n",
      "\n",
      "[20210302-1144-48] Learning rate for epoch 68 is 0.0031257038936018944\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.1966 - val_loss: 31.0168\n",
      "Epoch 69/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6787\n",
      "Epoch 00069: val_loss did not improve from 3.63937\n",
      "\n",
      "[20210302-1144-53] Learning rate for epoch 69 is 0.003168949158862233\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.6787 - val_loss: 7.0935\n",
      "Epoch 70/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0188\n",
      "Epoch 00070: val_loss did not improve from 3.63937\n",
      "\n",
      "[20210302-1144-58] Learning rate for epoch 70 is 0.0032120943069458008\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.0188 - val_loss: 5.9413\n",
      "Epoch 71/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1624\n",
      "Epoch 00071: val_loss did not improve from 3.63937\n",
      "\n",
      "[20210302-1145-03] Learning rate for epoch 71 is 0.003255139570683241\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.1624 - val_loss: 4.3925\n",
      "Epoch 72/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7009\n",
      "Epoch 00072: val_loss improved from 3.63937 to 3.44034, saving model to ./20210301-225844/heel_K9_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1145-08] Learning rate for epoch 72 is 0.00329808471724391\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 6.7009 - val_loss: 3.4403\n",
      "Epoch 73/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7351\n",
      "Epoch 00073: val_loss did not improve from 3.44034\n",
      "\n",
      "[20210302-1145-13] Learning rate for epoch 73 is 0.0033409299794584513\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.7351 - val_loss: 3.4419\n",
      "Epoch 74/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8594\n",
      "Epoch 00074: val_loss did not improve from 3.44034\n",
      "\n",
      "[20210302-1145-18] Learning rate for epoch 74 is 0.0033836751244962215\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.8594 - val_loss: 3.6614\n",
      "Epoch 75/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5331\n",
      "Epoch 00075: val_loss did not improve from 3.44034\n",
      "\n",
      "[20210302-1145-23] Learning rate for epoch 75 is 0.0034263203851878643\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.5331 - val_loss: 3.6315\n",
      "Epoch 76/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6934\n",
      "Epoch 00076: val_loss did not improve from 3.44034\n",
      "\n",
      "[20210302-1145-28] Learning rate for epoch 76 is 0.003468865528702736\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.6934 - val_loss: 4.9078\n",
      "Epoch 77/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7286\n",
      "Epoch 00077: val_loss did not improve from 3.44034\n",
      "\n",
      "[20210302-1145-32] Learning rate for epoch 77 is 0.00351131078787148\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.7286 - val_loss: 4.6227\n",
      "Epoch 78/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7098\n",
      "Epoch 00078: val_loss did not improve from 3.44034\n",
      "\n",
      "[20210302-1145-37] Learning rate for epoch 78 is 0.0035536561626940966\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.7098 - val_loss: 3.9242\n",
      "Epoch 79/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3603\n",
      "Epoch 00079: val_loss did not improve from 3.44034\n",
      "\n",
      "[20210302-1145-42] Learning rate for epoch 79 is 0.003595901420339942\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.3603 - val_loss: 4.7588\n",
      "Epoch 80/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2708\n",
      "Epoch 00080: val_loss did not improve from 3.44034\n",
      "\n",
      "[20210302-1145-47] Learning rate for epoch 80 is 0.00363804679363966\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.2708 - val_loss: 6.6308\n",
      "Epoch 81/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7675\n",
      "Epoch 00081: val_loss did not improve from 3.44034\n",
      "\n",
      "[20210302-1145-52] Learning rate for epoch 81 is 0.0036800920497626066\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.7675 - val_loss: 107.0841\n",
      "Epoch 82/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0826\n",
      "Epoch 00082: val_loss did not improve from 3.44034\n",
      "\n",
      "[20210302-1145-57] Learning rate for epoch 82 is 0.003722037188708782\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.0826 - val_loss: 37.7561\n",
      "Epoch 83/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2711\n",
      "Epoch 00083: val_loss did not improve from 3.44034\n",
      "\n",
      "[20210302-1146-01] Learning rate for epoch 83 is 0.003763882676139474\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.2711 - val_loss: 100.0621\n",
      "Epoch 84/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9584\n",
      "Epoch 00084: val_loss did not improve from 3.44034\n",
      "\n",
      "[20210302-1146-06] Learning rate for epoch 84 is 0.0038056280463933945\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.9584 - val_loss: 99.7681\n",
      "Epoch 85/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7630\n",
      "Epoch 00085: val_loss did not improve from 3.44034\n",
      "\n",
      "[20210302-1146-11] Learning rate for epoch 85 is 0.003847273299470544\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.7630 - val_loss: 10.8852\n",
      "Epoch 86/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4317\n",
      "Epoch 00086: val_loss did not improve from 3.44034\n",
      "\n",
      "[20210302-1146-16] Learning rate for epoch 86 is 0.0038888186682015657\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.4317 - val_loss: 3.5956\n",
      "Epoch 87/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6081\n",
      "Epoch 00087: val_loss did not improve from 3.44034\n",
      "\n",
      "[20210302-1146-21] Learning rate for epoch 87 is 0.00393026415258646\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.6081 - val_loss: 3.4684\n",
      "Epoch 88/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7548\n",
      "Epoch 00088: val_loss did not improve from 3.44034\n",
      "\n",
      "[20210302-1146-25] Learning rate for epoch 88 is 0.00397160928696394\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.7548 - val_loss: 4.0892\n",
      "Epoch 89/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7057\n",
      "Epoch 00089: val_loss improved from 3.44034 to 3.40080, saving model to ./20210301-225844/heel_K9_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1146-31] Learning rate for epoch 89 is 0.004012854769825935\n",
      "20/20 [==============================] - 2s 91ms/step - loss: 6.7057 - val_loss: 3.4008\n",
      "Epoch 90/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7229\n",
      "Epoch 00090: val_loss did not improve from 3.40080\n",
      "\n",
      "[20210302-1146-36] Learning rate for epoch 90 is 0.00405400013551116\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.7229 - val_loss: 4.8806\n",
      "Epoch 91/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4096\n",
      "Epoch 00091: val_loss did not improve from 3.40080\n",
      "\n",
      "[20210302-1146-41] Learning rate for epoch 91 is 0.004095045384019613\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.4096 - val_loss: 4.5472\n",
      "Epoch 92/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1708\n",
      "Epoch 00092: val_loss did not improve from 3.40080\n",
      "\n",
      "[20210302-1146-45] Learning rate for epoch 92 is 0.004135990981012583\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.1708 - val_loss: 4.0218\n",
      "Epoch 93/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8060\n",
      "Epoch 00093: val_loss did not improve from 3.40080\n",
      "\n",
      "[20210302-1146-50] Learning rate for epoch 93 is 0.004176836460828781\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.8060 - val_loss: 5.4033\n",
      "Epoch 94/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2332\n",
      "Epoch 00094: val_loss did not improve from 3.40080\n",
      "\n",
      "[20210302-1146-55] Learning rate for epoch 94 is 0.004217581823468208\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.2332 - val_loss: 3.6147\n",
      "Epoch 95/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2427\n",
      "Epoch 00095: val_loss did not improve from 3.40080\n",
      "\n",
      "[20210302-1147-00] Learning rate for epoch 95 is 0.004258227068930864\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.2427 - val_loss: 4.8160\n",
      "Epoch 96/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6991\n",
      "Epoch 00096: val_loss did not improve from 3.40080\n",
      "\n",
      "[20210302-1147-05] Learning rate for epoch 96 is 0.0042987726628780365\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.6991 - val_loss: 4.4213\n",
      "Epoch 97/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3506\n",
      "Epoch 00097: val_loss did not improve from 3.40080\n",
      "\n",
      "[20210302-1147-10] Learning rate for epoch 97 is 0.0043392181396484375\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.3506 - val_loss: 4.5785\n",
      "Epoch 98/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4503\n",
      "Epoch 00098: val_loss did not improve from 3.40080\n",
      "\n",
      "[20210302-1147-14] Learning rate for epoch 98 is 0.004379563499242067\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.4503 - val_loss: 3.8141\n",
      "Epoch 99/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4240\n",
      "Epoch 00099: val_loss did not improve from 3.40080\n",
      "\n",
      "[20210302-1147-19] Learning rate for epoch 99 is 0.004419809207320213\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.4240 - val_loss: 3.6438\n",
      "Epoch 100/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2018\n",
      "Epoch 00100: val_loss did not improve from 3.40080\n",
      "\n",
      "[20210302-1147-24] Learning rate for epoch 100 is 0.004459954332560301\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.2018 - val_loss: 4.3212\n",
      "Epoch 101/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3167\n",
      "Epoch 00101: val_loss did not improve from 3.40080\n",
      "\n",
      "[20210302-1147-29] Learning rate for epoch 101 is 4.4999998749517545e-07\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.3167 - val_loss: 4.1404\n",
      "Epoch 102/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8132\n",
      "Epoch 00102: val_loss did not improve from 3.40080\n",
      "\n",
      "[20210302-1147-34] Learning rate for epoch 102 is 0.00018023152370005846\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.8132 - val_loss: 3.5042\n",
      "Epoch 103/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0898\n",
      "Epoch 00103: val_loss improved from 3.40080 to 3.26442, saving model to ./20210301-225844/heel_K9_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1147-39] Learning rate for epoch 103 is 0.000359613070031628\n",
      "20/20 [==============================] - 2s 93ms/step - loss: 6.0898 - val_loss: 3.2644\n",
      "Epoch 104/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2038\n",
      "Epoch 00104: val_loss did not improve from 3.26442\n",
      "\n",
      "[20210302-1147-44] Learning rate for epoch 104 is 0.0005385946715250611\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.2038 - val_loss: 3.3083\n",
      "Epoch 105/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2278\n",
      "Epoch 00105: val_loss did not improve from 3.26442\n",
      "\n",
      "[20210302-1147-49] Learning rate for epoch 105 is 0.0007171762990765274\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.2278 - val_loss: 3.3211\n",
      "Epoch 106/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2427\n",
      "Epoch 00106: val_loss did not improve from 3.26442\n",
      "\n",
      "[20210302-1147-54] Learning rate for epoch 106 is 0.0008953579817898571\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.2427 - val_loss: 3.4432\n",
      "Epoch 107/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6942\n",
      "Epoch 00107: val_loss did not improve from 3.26442\n",
      "\n",
      "[20210302-1147-59] Learning rate for epoch 107 is 0.0010731397196650505\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.6942 - val_loss: 3.7722\n",
      "Epoch 108/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4557\n",
      "Epoch 00108: val_loss did not improve from 3.26442\n",
      "\n",
      "[20210302-1148-04] Learning rate for epoch 108 is 0.0012505215127021074\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.4557 - val_loss: 4.5556\n",
      "Epoch 109/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2280\n",
      "Epoch 00109: val_loss did not improve from 3.26442\n",
      "\n",
      "[20210302-1148-08] Learning rate for epoch 109 is 0.001427503302693367\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.2280 - val_loss: 3.7521\n",
      "Epoch 110/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3772\n",
      "Epoch 00110: val_loss did not improve from 3.26442\n",
      "\n",
      "[20210302-1148-13] Learning rate for epoch 110 is 0.0016040850896388292\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.3772 - val_loss: 3.9754\n",
      "Epoch 111/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2714\n",
      "Epoch 00111: val_loss did not improve from 3.26442\n",
      "\n",
      "[20210302-1148-18] Learning rate for epoch 111 is 0.001780266989953816\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.2714 - val_loss: 4.5878\n",
      "Epoch 112/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7515\n",
      "Epoch 00112: val_loss did not improve from 3.26442\n",
      "\n",
      "[20210302-1148-23] Learning rate for epoch 112 is 0.0019560488872230053\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.7515 - val_loss: 6.1256\n",
      "Epoch 113/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8566\n",
      "Epoch 00113: val_loss did not improve from 3.26442\n",
      "\n",
      "[20210302-1148-28] Learning rate for epoch 113 is 0.002131430897861719\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.8566 - val_loss: 5.0758\n",
      "Epoch 114/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0631\n",
      "Epoch 00114: val_loss did not improve from 3.26442\n",
      "\n",
      "[20210302-1148-32] Learning rate for epoch 114 is 0.0023064129054546356\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.0631 - val_loss: 3.4449\n",
      "Epoch 115/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2491\n",
      "Epoch 00115: val_loss did not improve from 3.26442\n",
      "\n",
      "[20210302-1148-37] Learning rate for epoch 115 is 0.0024809949100017548\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.2491 - val_loss: 5.6644\n",
      "Epoch 116/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2205\n",
      "Epoch 00116: val_loss did not improve from 3.26442\n",
      "\n",
      "[20210302-1148-42] Learning rate for epoch 116 is 0.0026551769115030766\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.2205 - val_loss: 8.4368\n",
      "Epoch 117/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1984\n",
      "Epoch 00117: val_loss did not improve from 3.26442\n",
      "\n",
      "[20210302-1148-47] Learning rate for epoch 117 is 0.0028289591427892447\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.1984 - val_loss: 4.5378\n",
      "Epoch 118/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1664\n",
      "Epoch 00118: val_loss did not improve from 3.26442\n",
      "\n",
      "[20210302-1148-52] Learning rate for epoch 118 is 0.0030023413710296154\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.1664 - val_loss: 3.5800\n",
      "Epoch 119/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8779\n",
      "Epoch 00119: val_loss did not improve from 3.26442\n",
      "\n",
      "[20210302-1148-57] Learning rate for epoch 119 is 0.003175323596224189\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.8779 - val_loss: 3.7387\n",
      "Epoch 120/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3891\n",
      "Epoch 00120: val_loss did not improve from 3.26442\n",
      "\n",
      "[20210302-1149-01] Learning rate for epoch 120 is 0.003347905818372965\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.3891 - val_loss: 4.1853\n",
      "Epoch 121/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7808\n",
      "Epoch 00121: val_loss did not improve from 3.26442\n",
      "\n",
      "[20210302-1149-06] Learning rate for epoch 121 is 0.0035200880374759436\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.7808 - val_loss: 5.9287\n",
      "Epoch 122/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1191\n",
      "Epoch 00122: val_loss did not improve from 3.26442\n",
      "\n",
      "[20210302-1149-11] Learning rate for epoch 122 is 0.003691870253533125\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.1191 - val_loss: 4.4895\n",
      "Epoch 123/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8576\n",
      "Epoch 00123: val_loss did not improve from 3.26442\n",
      "\n",
      "[20210302-1149-16] Learning rate for epoch 123 is 0.0038632526993751526\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8576 - val_loss: 3.9446\n",
      "Epoch 124/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6214\n",
      "Epoch 00124: val_loss did not improve from 3.26442\n",
      "\n",
      "[20210302-1149-21] Learning rate for epoch 124 is 0.004034235142171383\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.6214 - val_loss: 3.6014\n",
      "Epoch 125/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7740\n",
      "Epoch 00125: val_loss did not improve from 3.26442\n",
      "\n",
      "[20210302-1149-26] Learning rate for epoch 125 is 0.004204817581921816\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.7740 - val_loss: 4.4247\n",
      "Epoch 126/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7730\n",
      "Epoch 00126: val_loss did not improve from 3.26442\n",
      "\n",
      "[20210302-1149-31] Learning rate for epoch 126 is 0.0043750000186264515\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.7730 - val_loss: 3.6428\n",
      "Epoch 127/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2121\n",
      "Epoch 00127: val_loss did not improve from 3.26442\n",
      "\n",
      "[20210302-1149-35] Learning rate for epoch 127 is 0.0041952175088226795\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.2121 - val_loss: 3.6672\n",
      "Epoch 128/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0044\n",
      "Epoch 00128: val_loss did not improve from 3.26442\n",
      "\n",
      "[20210302-1149-40] Learning rate for epoch 128 is 0.004015835002064705\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.0044 - val_loss: 4.5031\n",
      "Epoch 129/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9855\n",
      "Epoch 00129: val_loss did not improve from 3.26442\n",
      "\n",
      "[20210302-1149-45] Learning rate for epoch 129 is 0.003836852265521884\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.9855 - val_loss: 4.2651\n",
      "Epoch 130/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6006\n",
      "Epoch 00130: val_loss did not improve from 3.26442\n",
      "\n",
      "[20210302-1149-50] Learning rate for epoch 130 is 0.003658269764855504\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.6006 - val_loss: 6.5583\n",
      "Epoch 131/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0106\n",
      "Epoch 00131: val_loss did not improve from 3.26442\n",
      "\n",
      "[20210302-1149-55] Learning rate for epoch 131 is 0.003480087034404278\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.0106 - val_loss: 5.3418\n",
      "Epoch 132/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2902\n",
      "Epoch 00132: val_loss did not improve from 3.26442\n",
      "\n",
      "[20210302-1149-59] Learning rate for epoch 132 is 0.003302304306998849\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.2902 - val_loss: 4.5585\n",
      "Epoch 133/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8384\n",
      "Epoch 00133: val_loss did not improve from 3.26442\n",
      "\n",
      "[20210302-1150-04] Learning rate for epoch 133 is 0.0031249215826392174\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8384 - val_loss: 3.7551\n",
      "Epoch 134/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9450\n",
      "Epoch 00134: val_loss did not improve from 3.26442\n",
      "\n",
      "[20210302-1150-09] Learning rate for epoch 134 is 0.0029479386284947395\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.9450 - val_loss: 3.4825\n",
      "Epoch 135/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7507\n",
      "Epoch 00135: val_loss did not improve from 3.26442\n",
      "\n",
      "[20210302-1150-14] Learning rate for epoch 135 is 0.0027713559102267027\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.7507 - val_loss: 4.0093\n",
      "Epoch 136/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8245\n",
      "Epoch 00136: val_loss did not improve from 3.26442\n",
      "\n",
      "[20210302-1150-19] Learning rate for epoch 136 is 0.0025951729621738195\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.8245 - val_loss: 3.9647\n",
      "Epoch 137/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7310\n",
      "Epoch 00137: val_loss did not improve from 3.26442\n",
      "\n",
      "[20210302-1150-23] Learning rate for epoch 137 is 0.0024193900171667337\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.7310 - val_loss: 3.7770\n",
      "Epoch 138/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7843\n",
      "Epoch 00138: val_loss did not improve from 3.26442\n",
      "\n",
      "[20210302-1150-28] Learning rate for epoch 138 is 0.0022440070752054453\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.7843 - val_loss: 3.9228\n",
      "Epoch 139/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0519\n",
      "Epoch 00139: val_loss did not improve from 3.26442\n",
      "\n",
      "[20210302-1150-33] Learning rate for epoch 139 is 0.002069024136289954\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.0519 - val_loss: 4.0666\n",
      "Epoch 140/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5112\n",
      "Epoch 00140: val_loss did not improve from 3.26442\n",
      "\n",
      "[20210302-1150-38] Learning rate for epoch 140 is 0.0018944410840049386\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.5112 - val_loss: 3.7995\n",
      "Epoch 141/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1476\n",
      "Epoch 00141: val_loss did not improve from 3.26442\n",
      "\n",
      "[20210302-1150-43] Learning rate for epoch 141 is 0.0017202580347657204\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.1476 - val_loss: 4.2761\n",
      "Epoch 142/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7788\n",
      "Epoch 00142: val_loss did not improve from 3.26442\n",
      "\n",
      "[20210302-1150-48] Learning rate for epoch 142 is 0.0015464748721569777\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 5.7788 - val_loss: 3.3498\n",
      "Epoch 143/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9112\n",
      "Epoch 00143: val_loss did not improve from 3.26442\n",
      "\n",
      "[20210302-1150-52] Learning rate for epoch 143 is 0.0013730917125940323\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.9112 - val_loss: 3.4399\n",
      "Epoch 144/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8076\n",
      "Epoch 00144: val_loss did not improve from 3.26442\n",
      "\n",
      "[20210302-1150-57] Learning rate for epoch 144 is 0.0012001085560768843\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.8076 - val_loss: 3.2913\n",
      "Epoch 145/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8972\n",
      "Epoch 00145: val_loss did not improve from 3.26442\n",
      "\n",
      "[20210302-1151-02] Learning rate for epoch 145 is 0.0010275252861902118\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.8972 - val_loss: 3.5438\n",
      "Epoch 146/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4128\n",
      "Epoch 00146: val_loss improved from 3.26442 to 3.09094, saving model to ./20210301-225844/heel_K9_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1151-08] Learning rate for epoch 146 is 0.0008553420193493366\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 5.4128 - val_loss: 3.0909\n",
      "Epoch 147/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5155\n",
      "Epoch 00147: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1151-12] Learning rate for epoch 147 is 0.0006835586973465979\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.5155 - val_loss: 3.5474\n",
      "Epoch 148/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3604\n",
      "Epoch 00148: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1151-17] Learning rate for epoch 148 is 0.0005121753201819956\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.3604 - val_loss: 3.1558\n",
      "Epoch 149/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3072\n",
      "Epoch 00149: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1151-22] Learning rate for epoch 149 is 0.00034119191695936024\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.3072 - val_loss: 3.1211\n",
      "Epoch 150/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8665\n",
      "Epoch 00150: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1151-27] Learning rate for epoch 150 is 0.00017060847312677652\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.8665 - val_loss: 3.1130\n",
      "Epoch 151/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7344\n",
      "Epoch 00151: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1151-32] Learning rate for epoch 151 is 4.249999960848072e-07\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.7344 - val_loss: 3.1137\n",
      "Epoch 152/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6322\n",
      "Epoch 00152: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1151-36] Learning rate for epoch 152 is 0.00017020752420648932\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6322 - val_loss: 3.1037\n",
      "Epoch 153/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3644\n",
      "Epoch 00153: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1151-41] Learning rate for epoch 153 is 0.0003395900712348521\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.3644 - val_loss: 3.1755\n",
      "Epoch 154/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8020\n",
      "Epoch 00154: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1151-46] Learning rate for epoch 154 is 0.0005085726734250784\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.8020 - val_loss: 3.1823\n",
      "Epoch 155/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9381\n",
      "Epoch 00155: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1151-51] Learning rate for epoch 155 is 0.0006771553307771683\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.9381 - val_loss: 3.2491\n",
      "Epoch 156/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4034\n",
      "Epoch 00156: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1151-56] Learning rate for epoch 156 is 0.0008453379850834608\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.4034 - val_loss: 3.4100\n",
      "Epoch 157/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4676\n",
      "Epoch 00157: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1152-00] Learning rate for epoch 157 is 0.0010131207527592778\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.4676 - val_loss: 3.1377\n",
      "Epoch 158/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8295\n",
      "Epoch 00158: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1152-05] Learning rate for epoch 158 is 0.0011805035173892975\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.8295 - val_loss: 3.3656\n",
      "Epoch 159/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5928\n",
      "Epoch 00159: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1152-10] Learning rate for epoch 159 is 0.0013474862789735198\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.5928 - val_loss: 3.1996\n",
      "Epoch 160/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7834\n",
      "Epoch 00160: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1152-15] Learning rate for epoch 160 is 0.0015140691539272666\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.7834 - val_loss: 3.4848\n",
      "Epoch 161/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2308\n",
      "Epoch 00161: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1152-20] Learning rate for epoch 161 is 0.001680252025835216\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.2308 - val_loss: 3.5943\n",
      "Epoch 162/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9737\n",
      "Epoch 00162: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1152-24] Learning rate for epoch 162 is 0.0018460348946973681\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.9737 - val_loss: 3.8056\n",
      "Epoch 163/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9600\n",
      "Epoch 00163: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1152-29] Learning rate for epoch 163 is 0.0020114178769290447\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.9600 - val_loss: 4.2382\n",
      "Epoch 164/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0249\n",
      "Epoch 00164: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1152-34] Learning rate for epoch 164 is 0.0021764009725302458\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.0249 - val_loss: 9.4659\n",
      "Epoch 165/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8092\n",
      "Epoch 00165: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1152-39] Learning rate for epoch 165 is 0.002340983832255006\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.8092 - val_loss: 921.9188\n",
      "Epoch 166/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0898\n",
      "Epoch 00166: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1152-44] Learning rate for epoch 166 is 0.002505166921764612\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.0898 - val_loss: 501.3638\n",
      "Epoch 167/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5387\n",
      "Epoch 00167: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1152-48] Learning rate for epoch 167 is 0.002668950008228421\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.5387 - val_loss: 354.3106\n",
      "Epoch 168/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9716\n",
      "Epoch 00168: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1152-53] Learning rate for epoch 168 is 0.0028323333244770765\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.9716 - val_loss: 66.4079\n",
      "Epoch 169/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7323\n",
      "Epoch 00169: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1152-58] Learning rate for epoch 169 is 0.002995316404849291\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.7323 - val_loss: 13.0777\n",
      "Epoch 170/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6574\n",
      "Epoch 00170: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1153-03] Learning rate for epoch 170 is 0.0031578997150063515\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.6574 - val_loss: 27.0089\n",
      "Epoch 171/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2053\n",
      "Epoch 00171: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1153-08] Learning rate for epoch 171 is 0.0033200830221176147\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.2053 - val_loss: 5.9680\n",
      "Epoch 172/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9602\n",
      "Epoch 00172: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1153-13] Learning rate for epoch 172 is 0.0034818663261830807\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.9602 - val_loss: 6.5844\n",
      "Epoch 173/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8892\n",
      "Epoch 00173: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1153-18] Learning rate for epoch 173 is 0.0036432496272027493\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.8892 - val_loss: 5.5881\n",
      "Epoch 174/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9361\n",
      "Epoch 00174: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1153-22] Learning rate for epoch 174 is 0.003804233158007264\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.9361 - val_loss: 5.2302\n",
      "Epoch 175/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7439\n",
      "Epoch 00175: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1153-27] Learning rate for epoch 175 is 0.003964816685765982\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.7439 - val_loss: 10.8244\n",
      "Epoch 176/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6602\n",
      "Epoch 00176: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1153-32] Learning rate for epoch 176 is 0.004124999977648258\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6602 - val_loss: 10.2188\n",
      "Epoch 177/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7617\n",
      "Epoch 00177: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1153-37] Learning rate for epoch 177 is 0.003955216612666845\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.7617 - val_loss: 6.5107\n",
      "Epoch 178/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0985\n",
      "Epoch 00178: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1153-42] Learning rate for epoch 178 is 0.003785833017900586\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.0985 - val_loss: 5.5747\n",
      "Epoch 179/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3012\n",
      "Epoch 00179: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1153-47] Learning rate for epoch 179 is 0.0036168494261801243\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.3012 - val_loss: 4.9623\n",
      "Epoch 180/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6332\n",
      "Epoch 00180: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1153-51] Learning rate for epoch 180 is 0.003448265604674816\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.6332 - val_loss: 6.2587\n",
      "Epoch 181/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8929\n",
      "Epoch 00181: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1153-56] Learning rate for epoch 181 is 0.003280082019045949\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8929 - val_loss: 4.0068\n",
      "Epoch 182/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4373\n",
      "Epoch 00182: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1154-01] Learning rate for epoch 182 is 0.0031122982036322355\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.4373 - val_loss: 8.0071\n",
      "Epoch 183/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7395\n",
      "Epoch 00183: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1154-06] Learning rate for epoch 183 is 0.002944914624094963\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.7395 - val_loss: 7.3961\n",
      "Epoch 184/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7684\n",
      "Epoch 00184: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1154-11] Learning rate for epoch 184 is 0.0027779308147728443\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.7684 - val_loss: 10.2557\n",
      "Epoch 185/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8898\n",
      "Epoch 00185: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1154-16] Learning rate for epoch 185 is 0.0026113467756658792\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.8898 - val_loss: 7.1499\n",
      "Epoch 186/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6072\n",
      "Epoch 00186: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1154-20] Learning rate for epoch 186 is 0.002445162972435355\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.6072 - val_loss: 9.2713\n",
      "Epoch 187/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3781\n",
      "Epoch 00187: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1154-25] Learning rate for epoch 187 is 0.0022793791722506285\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.3781 - val_loss: 29.9871\n",
      "Epoch 188/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3106\n",
      "Epoch 00188: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1154-30] Learning rate for epoch 188 is 0.0021139951422810555\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.3106 - val_loss: 8.3127\n",
      "Epoch 189/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8871\n",
      "Epoch 00189: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1154-35] Learning rate for epoch 189 is 0.0019490111153572798\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.8871 - val_loss: 54.2326\n",
      "Epoch 190/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3738\n",
      "Epoch 00190: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1154-40] Learning rate for epoch 190 is 0.0017844270914793015\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.3738 - val_loss: 4.0370\n",
      "Epoch 191/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9393\n",
      "Epoch 00191: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1154-45] Learning rate for epoch 191 is 0.0016202429542317986\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.9393 - val_loss: 4.1485\n",
      "Epoch 192/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9615\n",
      "Epoch 00192: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1154-50] Learning rate for epoch 192 is 0.001456458936445415\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.9615 - val_loss: 3.6748\n",
      "Epoch 193/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5843\n",
      "Epoch 00193: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1154-55] Learning rate for epoch 193 is 0.001293074688874185\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5843 - val_loss: 3.2801\n",
      "Epoch 194/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8175\n",
      "Epoch 00194: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1155-00] Learning rate for epoch 194 is 0.0011300905607640743\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8175 - val_loss: 3.4092\n",
      "Epoch 195/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6962\n",
      "Epoch 00195: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1155-05] Learning rate for epoch 195 is 0.0009675062610767782\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6962 - val_loss: 3.2663\n",
      "Epoch 196/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5145\n",
      "Epoch 00196: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1155-09] Learning rate for epoch 196 is 0.0008053220226429403\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.5145 - val_loss: 3.4009\n",
      "Epoch 197/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9272\n",
      "Epoch 00197: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1155-14] Learning rate for epoch 197 is 0.0006435376708395779\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 5.9272 - val_loss: 3.2294\n",
      "Epoch 198/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9814\n",
      "Epoch 00198: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1155-20] Learning rate for epoch 198 is 0.0004821533220820129\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.9814 - val_loss: 3.1507\n",
      "Epoch 199/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4722\n",
      "Epoch 00199: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1155-24] Learning rate for epoch 199 is 0.0003211689181625843\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.4722 - val_loss: 3.1219\n",
      "Epoch 200/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6443\n",
      "Epoch 00200: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1155-29] Learning rate for epoch 200 is 0.00016058447363320738\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.6443 - val_loss: 3.1232\n",
      "Epoch 201/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7319\n",
      "Epoch 00201: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1155-34] Learning rate for epoch 201 is 4.0000000467443897e-07\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.7319 - val_loss: 3.1079\n",
      "Epoch 202/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7686\n",
      "Epoch 00202: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1155-39] Learning rate for epoch 202 is 0.0001601835247129202\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.7686 - val_loss: 3.1256\n",
      "Epoch 203/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4636\n",
      "Epoch 00203: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1155-44] Learning rate for epoch 203 is 0.00031956707243807614\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.4636 - val_loss: 3.1672\n",
      "Epoch 204/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6680\n",
      "Epoch 00204: val_loss did not improve from 3.09094\n",
      "\n",
      "[20210302-1155-49] Learning rate for epoch 204 is 0.00047855067532509565\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6680 - val_loss: 3.1172\n",
      "Epoch 205/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0439\n",
      "Epoch 00205: val_loss improved from 3.09094 to 3.06924, saving model to ./20210301-225844/heel_K9_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1155-54] Learning rate for epoch 205 is 0.0006371343042701483\n",
      "20/20 [==============================] - 2s 91ms/step - loss: 6.0439 - val_loss: 3.0692\n",
      "Epoch 206/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5409\n",
      "Epoch 00206: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1155-59] Learning rate for epoch 206 is 0.0007953179883770645\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.5409 - val_loss: 3.2705\n",
      "Epoch 207/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5242\n",
      "Epoch 00207: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1156-04] Learning rate for epoch 207 is 0.0009531017276458442\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.5242 - val_loss: 3.3553\n",
      "Epoch 208/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9320\n",
      "Epoch 00208: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1156-08] Learning rate for epoch 208 is 0.0011104855220764875\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.9320 - val_loss: 3.2267\n",
      "Epoch 209/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7101\n",
      "Epoch 00209: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1156-13] Learning rate for epoch 209 is 0.0012674692552536726\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.7101 - val_loss: 3.9963\n",
      "Epoch 210/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2662\n",
      "Epoch 00210: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1156-18] Learning rate for epoch 210 is 0.0014240531018003821\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.2662 - val_loss: 3.7946\n",
      "Epoch 211/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5403\n",
      "Epoch 00211: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1156-23] Learning rate for epoch 211 is 0.0015802369453012943\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.5403 - val_loss: 12.4179\n",
      "Epoch 212/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7869\n",
      "Epoch 00212: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1156-28] Learning rate for epoch 212 is 0.001736020902171731\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.7869 - val_loss: 19.8166\n",
      "Epoch 213/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5723\n",
      "Epoch 00213: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1156-33] Learning rate for epoch 213 is 0.0018914048559963703\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5723 - val_loss: 6.0526\n",
      "Epoch 214/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3907\n",
      "Epoch 00214: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1156-38] Learning rate for epoch 214 is 0.0020463888067752123\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.3907 - val_loss: 3.8187\n",
      "Epoch 215/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9275\n",
      "Epoch 00215: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1156-43] Learning rate for epoch 215 is 0.0022009729873389006\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.9275 - val_loss: 3.5591\n",
      "Epoch 216/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8573\n",
      "Epoch 00216: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1156-47] Learning rate for epoch 216 is 0.002355156932026148\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.8573 - val_loss: 4.0234\n",
      "Epoch 217/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8752\n",
      "Epoch 00217: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1156-52] Learning rate for epoch 217 is 0.0025089411064982414\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 5.8752 - val_loss: 97.1724\n",
      "Epoch 218/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6471\n",
      "Epoch 00218: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1156-57] Learning rate for epoch 218 is 0.0026623252779245377\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6471 - val_loss: 17.6316\n",
      "Epoch 219/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8988\n",
      "Epoch 00219: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1157-02] Learning rate for epoch 219 is 0.0028153094463050365\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8988 - val_loss: 19.1720\n",
      "Epoch 220/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4515\n",
      "Epoch 00220: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1157-07] Learning rate for epoch 220 is 0.002967893611639738\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.4515 - val_loss: 10.6972\n",
      "Epoch 221/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9447\n",
      "Epoch 00221: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1157-12] Learning rate for epoch 221 is 0.003120078006759286\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.9447 - val_loss: 54.6801\n",
      "Epoch 222/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2952\n",
      "Epoch 00222: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1157-16] Learning rate for epoch 222 is 0.0032718623988330364\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.2952 - val_loss: 6.0655\n",
      "Epoch 223/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8091\n",
      "Epoch 00223: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1157-21] Learning rate for epoch 223 is 0.0034232467878609896\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.8091 - val_loss: 9.7710\n",
      "Epoch 224/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8208\n",
      "Epoch 00224: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1157-26] Learning rate for epoch 224 is 0.0035742311738431454\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.8208 - val_loss: 40.1074\n",
      "Epoch 225/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9560\n",
      "Epoch 00225: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1157-31] Learning rate for epoch 225 is 0.003724815556779504\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.9560 - val_loss: 20.2805\n",
      "Epoch 226/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6671\n",
      "Epoch 00226: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1157-36] Learning rate for epoch 226 is 0.003874999936670065\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.6671 - val_loss: 6.1271\n",
      "Epoch 227/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0373\n",
      "Epoch 00227: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1157-41] Learning rate for epoch 227 is 0.0037152154836803675\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.0373 - val_loss: 7.1803\n",
      "Epoch 228/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0776\n",
      "Epoch 00228: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1157-46] Learning rate for epoch 228 is 0.0035558310337364674\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.0776 - val_loss: 3.9444\n",
      "Epoch 229/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3095\n",
      "Epoch 00229: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1157-50] Learning rate for epoch 229 is 0.003396846354007721\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.3095 - val_loss: 3.3059\n",
      "Epoch 230/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6303\n",
      "Epoch 00230: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1157-55] Learning rate for epoch 230 is 0.003238261677324772\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.6303 - val_loss: 4.3572\n",
      "Epoch 231/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8173\n",
      "Epoch 00231: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1158-00] Learning rate for epoch 231 is 0.00308007700368762\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.8173 - val_loss: 13.3333\n",
      "Epoch 232/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8522\n",
      "Epoch 00232: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1158-05] Learning rate for epoch 232 is 0.002922292333096266\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8522 - val_loss: 7.2627\n",
      "Epoch 233/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9371\n",
      "Epoch 00233: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1158-10] Learning rate for epoch 233 is 0.002764907432720065\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.9371 - val_loss: 4.0488\n",
      "Epoch 234/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4680\n",
      "Epoch 00234: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1158-20] Learning rate for epoch 234 is 0.0026079227682203054\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.4680 - val_loss: 6.4584\n",
      "Epoch 235/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9478\n",
      "Epoch 00235: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1158-25] Learning rate for epoch 235 is 0.0024513378739356995\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.9478 - val_loss: 5.7664\n",
      "Epoch 236/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1886\n",
      "Epoch 00236: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1158-30] Learning rate for epoch 236 is 0.002295152982696891\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.1886 - val_loss: 4.0118\n",
      "Epoch 237/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5538\n",
      "Epoch 00237: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1158-35] Learning rate for epoch 237 is 0.0021393680945038795\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 5.5538 - val_loss: 3.7180\n",
      "Epoch 238/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7477\n",
      "Epoch 00238: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1158-40] Learning rate for epoch 238 is 0.0019839832093566656\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.7477 - val_loss: 3.2675\n",
      "Epoch 239/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4553\n",
      "Epoch 00239: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1158-45] Learning rate for epoch 239 is 0.0018289980944246054\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 5.4553 - val_loss: 5.3254\n",
      "Epoch 240/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4382\n",
      "Epoch 00240: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1158-50] Learning rate for epoch 240 is 0.0016744130989536643\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 5.4382 - val_loss: 4.3290\n",
      "Epoch 241/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8407\n",
      "Epoch 00241: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1158-55] Learning rate for epoch 241 is 0.0015202279901131988\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.8407 - val_loss: 3.8304\n",
      "Epoch 242/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6847\n",
      "Epoch 00242: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1158-59] Learning rate for epoch 242 is 0.0013664428843185306\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6847 - val_loss: 3.1683\n",
      "Epoch 243/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4173\n",
      "Epoch 00243: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1159-04] Learning rate for epoch 243 is 0.0012130576651543379\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.4173 - val_loss: 3.3134\n",
      "Epoch 244/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3724\n",
      "Epoch 00244: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1159-09] Learning rate for epoch 244 is 0.0010600725654512644\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.3724 - val_loss: 3.5061\n",
      "Epoch 245/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3913\n",
      "Epoch 00245: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1159-14] Learning rate for epoch 245 is 0.0009074872941710055\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.3913 - val_loss: 3.5237\n",
      "Epoch 246/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6769\n",
      "Epoch 00246: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1159-19] Learning rate for epoch 246 is 0.0007553020259365439\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6769 - val_loss: 3.5568\n",
      "Epoch 247/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3402\n",
      "Epoch 00247: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1159-24] Learning rate for epoch 247 is 0.0006035167025402188\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.3402 - val_loss: 3.2410\n",
      "Epoch 248/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3589\n",
      "Epoch 00248: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1159-29] Learning rate for epoch 248 is 0.00045213132398203015\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.3589 - val_loss: 3.1916\n",
      "Epoch 249/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3288\n",
      "Epoch 00249: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1159-33] Learning rate for epoch 249 is 0.00030114591936580837\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.3288 - val_loss: 3.4050\n",
      "Epoch 250/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3413\n",
      "Epoch 00250: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1159-38] Learning rate for epoch 250 is 0.00015056047413963825\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 5.3413 - val_loss: 3.2648\n",
      "Epoch 251/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.0205\n",
      "Epoch 00251: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1159-43] Learning rate for epoch 251 is 3.7500001326407073e-07\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.0205 - val_loss: 3.2272\n",
      "Epoch 252/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6631\n",
      "Epoch 00252: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1159-48] Learning rate for epoch 252 is 0.00015015952521935105\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.6631 - val_loss: 3.1440\n",
      "Epoch 253/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2045\n",
      "Epoch 00253: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1159-53] Learning rate for epoch 253 is 0.0002995440736413002\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.2045 - val_loss: 3.4702\n",
      "Epoch 254/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7399\n",
      "Epoch 00254: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1159-58] Learning rate for epoch 254 is 0.0004485286772251129\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.7399 - val_loss: 3.4405\n",
      "Epoch 255/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5669\n",
      "Epoch 00255: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1200-03] Learning rate for epoch 255 is 0.0005971133359707892\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.5669 - val_loss: 3.4547\n",
      "Epoch 256/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3116\n",
      "Epoch 00256: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1200-08] Learning rate for epoch 256 is 0.0007452979916706681\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.3116 - val_loss: 3.2852\n",
      "Epoch 257/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5572\n",
      "Epoch 00257: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1200-13] Learning rate for epoch 257 is 0.0008930827025324106\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 5.5572 - val_loss: 3.7198\n",
      "Epoch 258/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5857\n",
      "Epoch 00258: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1200-17] Learning rate for epoch 258 is 0.0010404675267636776\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.5857 - val_loss: 3.7997\n",
      "Epoch 259/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6515\n",
      "Epoch 00259: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1200-22] Learning rate for epoch 259 is 0.0011874522315338254\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.6515 - val_loss: 3.6557\n",
      "Epoch 260/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5403\n",
      "Epoch 00260: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1200-27] Learning rate for epoch 260 is 0.0013340371660888195\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.5403 - val_loss: 3.2782\n",
      "Epoch 261/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4429\n",
      "Epoch 00261: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1200-32] Learning rate for epoch 261 is 0.0014802219811826944\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.4429 - val_loss: 3.3192\n",
      "Epoch 262/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9892\n",
      "Epoch 00262: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1200-37] Learning rate for epoch 262 is 0.0016260069096460938\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.9892 - val_loss: 5.5920\n",
      "Epoch 263/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8138\n",
      "Epoch 00263: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1200-42] Learning rate for epoch 263 is 0.001771391835063696\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.8138 - val_loss: 40.0593\n",
      "Epoch 264/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4537\n",
      "Epoch 00264: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1200-47] Learning rate for epoch 264 is 0.0019163768738508224\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 5.4537 - val_loss: 23.9266\n",
      "Epoch 265/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7346\n",
      "Epoch 00265: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1200-52] Learning rate for epoch 265 is 0.0020609619095921516\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.7346 - val_loss: 79.1203\n",
      "Epoch 266/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3584\n",
      "Epoch 00266: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1200-57] Learning rate for epoch 266 is 0.0022051469422876835\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.3584 - val_loss: 5.8088\n",
      "Epoch 267/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1198\n",
      "Epoch 00267: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1201-01] Learning rate for epoch 267 is 0.0023489322047680616\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.1198 - val_loss: 3.3371\n",
      "Epoch 268/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7140\n",
      "Epoch 00268: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1201-06] Learning rate for epoch 268 is 0.002492317231371999\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.7140 - val_loss: 3.4932\n",
      "Epoch 269/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8379\n",
      "Epoch 00269: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1201-11] Learning rate for epoch 269 is 0.0026353024877607822\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.8379 - val_loss: 4.8902\n",
      "Epoch 270/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8382\n",
      "Epoch 00270: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1201-16] Learning rate for epoch 270 is 0.0027778877411037683\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.8382 - val_loss: 4.2693\n",
      "Epoch 271/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0305\n",
      "Epoch 00271: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1201-21] Learning rate for epoch 271 is 0.002920072991400957\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.0305 - val_loss: 3.9917\n",
      "Epoch 272/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8440\n",
      "Epoch 00272: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1201-26] Learning rate for epoch 272 is 0.0030618582386523485\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.8440 - val_loss: 11.9291\n",
      "Epoch 273/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8815\n",
      "Epoch 00273: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1201-31] Learning rate for epoch 273 is 0.0032032437156885862\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8815 - val_loss: 16.0281\n",
      "Epoch 274/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7463\n",
      "Epoch 00274: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1201-35] Learning rate for epoch 274 is 0.0033442291896790266\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.7463 - val_loss: 31.1113\n",
      "Epoch 275/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3201\n",
      "Epoch 00275: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1201-40] Learning rate for epoch 275 is 0.003484814427793026\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.3201 - val_loss: 15.3511\n",
      "Epoch 276/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9714\n",
      "Epoch 00276: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1201-45] Learning rate for epoch 276 is 0.0036249998956918716\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.9714 - val_loss: 14.4059\n",
      "Epoch 277/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2927\n",
      "Epoch 00277: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1201-50] Learning rate for epoch 277 is 0.0034752145875245333\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.2927 - val_loss: 3.9310\n",
      "Epoch 278/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7168\n",
      "Epoch 00278: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1201-55] Learning rate for epoch 278 is 0.003325828816741705\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.7168 - val_loss: 3.9277\n",
      "Epoch 279/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6711\n",
      "Epoch 00279: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1202-00] Learning rate for epoch 279 is 0.0031768432818353176\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6711 - val_loss: 4.3861\n",
      "Epoch 280/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5546\n",
      "Epoch 00280: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1202-05] Learning rate for epoch 280 is 0.0030282577499747276\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.5546 - val_loss: 5.6868\n",
      "Epoch 281/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7959\n",
      "Epoch 00281: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1202-09] Learning rate for epoch 281 is 0.0028800719883292913\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.7959 - val_loss: 3.7921\n",
      "Epoch 282/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7650\n",
      "Epoch 00282: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1202-14] Learning rate for epoch 282 is 0.0027322862297296524\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.7650 - val_loss: 898.8389\n",
      "Epoch 283/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9002\n",
      "Epoch 00283: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1202-19] Learning rate for epoch 283 is 0.002584900474175811\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.9002 - val_loss: 223.3350\n",
      "Epoch 284/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0682\n",
      "Epoch 00284: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1202-24] Learning rate for epoch 284 is 0.0024379147216677666\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.0682 - val_loss: 17.9367\n",
      "Epoch 285/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5528\n",
      "Epoch 00285: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1202-29] Learning rate for epoch 285 is 0.0022913289722055197\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.5528 - val_loss: 6.5278\n",
      "Epoch 286/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5639\n",
      "Epoch 00286: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1202-34] Learning rate for epoch 286 is 0.0021451429929584265\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5639 - val_loss: 4.8250\n",
      "Epoch 287/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8544\n",
      "Epoch 00287: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1202-38] Learning rate for epoch 287 is 0.0019993570167571306\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.8544 - val_loss: 4.1118\n",
      "Epoch 288/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4580\n",
      "Epoch 00288: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1202-43] Learning rate for epoch 288 is 0.001853971160016954\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.4580 - val_loss: 3.4738\n",
      "Epoch 289/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2446\n",
      "Epoch 00289: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1202-48] Learning rate for epoch 289 is 0.001708985073491931\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.2446 - val_loss: 3.8380\n",
      "Epoch 290/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2619\n",
      "Epoch 00290: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1202-53] Learning rate for epoch 290 is 0.0015643991064280272\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.2619 - val_loss: 3.1101\n",
      "Epoch 291/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3339\n",
      "Epoch 00291: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1202-58] Learning rate for epoch 291 is 0.0014202130259945989\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.3339 - val_loss: 3.1904\n",
      "Epoch 292/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2434\n",
      "Epoch 00292: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1203-03] Learning rate for epoch 292 is 0.001276426832191646\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.2434 - val_loss: 3.5903\n",
      "Epoch 293/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7964\n",
      "Epoch 00293: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1203-07] Learning rate for epoch 293 is 0.0011330407578498125\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.7964 - val_loss: 3.1810\n",
      "Epoch 294/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3070\n",
      "Epoch 00294: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1203-12] Learning rate for epoch 294 is 0.0009900545701384544\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.3070 - val_loss: 3.1127\n",
      "Epoch 295/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2804\n",
      "Epoch 00295: val_loss did not improve from 3.06924\n",
      "\n",
      "[20210302-1203-17] Learning rate for epoch 295 is 0.0008474682690575719\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.2804 - val_loss: 3.1274\n",
      "Epoch 296/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4375\n",
      "Epoch 00296: val_loss improved from 3.06924 to 3.06092, saving model to ./20210301-225844/heel_K9_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1203-23] Learning rate for epoch 296 is 0.0007052819710224867\n",
      "20/20 [==============================] - 2s 92ms/step - loss: 5.4375 - val_loss: 3.0609\n",
      "Epoch 297/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 4.9777\n",
      "Epoch 00297: val_loss improved from 3.06092 to 3.06038, saving model to ./20210301-225844/heel_K9_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1203-28] Learning rate for epoch 297 is 0.0005634956760331988\n",
      "20/20 [==============================] - 2s 91ms/step - loss: 4.9777 - val_loss: 3.0604\n",
      "Epoch 298/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3259\n",
      "Epoch 00298: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1203-33] Learning rate for epoch 298 is 0.0004221093258820474\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.3259 - val_loss: 3.1317\n",
      "Epoch 299/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5444\n",
      "Epoch 00299: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1203-37] Learning rate for epoch 299 is 0.00028112292056903243\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.5444 - val_loss: 3.1053\n",
      "Epoch 300/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5208\n",
      "Epoch 00300: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1203-42] Learning rate for epoch 300 is 0.0001405364746460691\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.5208 - val_loss: 3.0890\n",
      "Epoch 301/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3630\n",
      "Epoch 00301: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1203-47] Learning rate for epoch 301 is 3.4999999343199306e-07\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.3630 - val_loss: 3.0795\n",
      "Epoch 302/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2977\n",
      "Epoch 00302: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1203-52] Learning rate for epoch 302 is 0.00014013552572578192\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.2977 - val_loss: 3.1142\n",
      "Epoch 303/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.0829\n",
      "Epoch 00303: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1203-57] Learning rate for epoch 303 is 0.00027952107484452426\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.0829 - val_loss: 3.1563\n",
      "Epoch 304/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1272\n",
      "Epoch 00304: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1204-02] Learning rate for epoch 304 is 0.0004185066791251302\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.1272 - val_loss: 3.1140\n",
      "Epoch 305/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2343\n",
      "Epoch 00305: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1204-06] Learning rate for epoch 305 is 0.0005570923094637692\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.2343 - val_loss: 3.1172\n",
      "Epoch 306/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4455\n",
      "Epoch 00306: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1204-11] Learning rate for epoch 306 is 0.0006952779949642718\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.4455 - val_loss: 3.1455\n",
      "Epoch 307/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2652\n",
      "Epoch 00307: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1204-16] Learning rate for epoch 307 is 0.0008330637356266379\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.2652 - val_loss: 3.1806\n",
      "Epoch 308/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3184\n",
      "Epoch 00308: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1204-21] Learning rate for epoch 308 is 0.0009704494732432067\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.3184 - val_loss: 3.2520\n",
      "Epoch 309/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5161\n",
      "Epoch 00309: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1204-26] Learning rate for epoch 309 is 0.0011074353242293\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.5161 - val_loss: 3.3960\n",
      "Epoch 310/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.0366\n",
      "Epoch 00310: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1204-31] Learning rate for epoch 310 is 0.001244021113961935\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.0366 - val_loss: 3.0781\n",
      "Epoch 311/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1954\n",
      "Epoch 00311: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1204-35] Learning rate for epoch 311 is 0.0013802070170640945\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 5.1954 - val_loss: 3.9794\n",
      "Epoch 312/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6226\n",
      "Epoch 00312: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1204-40] Learning rate for epoch 312 is 0.0015159929171204567\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6226 - val_loss: 3.4354\n",
      "Epoch 313/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3780\n",
      "Epoch 00313: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1204-45] Learning rate for epoch 313 is 0.0016513789305463433\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.3780 - val_loss: 4.2151\n",
      "Epoch 314/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7903\n",
      "Epoch 00314: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1204-50] Learning rate for epoch 314 is 0.0017863648245111108\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.7903 - val_loss: 3.5430\n",
      "Epoch 315/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.0208\n",
      "Epoch 00315: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1204-55] Learning rate for epoch 315 is 0.0019209509482607245\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.0208 - val_loss: 3.6489\n",
      "Epoch 316/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5178\n",
      "Epoch 00316: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1205-00] Learning rate for epoch 316 is 0.002055136952549219\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.5178 - val_loss: 4.1167\n",
      "Epoch 317/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8370\n",
      "Epoch 00317: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1205-05] Learning rate for epoch 317 is 0.002188923070207238\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.8370 - val_loss: 3.7475\n",
      "Epoch 318/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2190\n",
      "Epoch 00318: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1205-09] Learning rate for epoch 318 is 0.00232230918481946\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.2190 - val_loss: 3.8066\n",
      "Epoch 319/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4508\n",
      "Epoch 00319: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1205-14] Learning rate for epoch 319 is 0.002455295529216528\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.4508 - val_loss: 4.3378\n",
      "Epoch 320/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3323\n",
      "Epoch 00320: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1205-19] Learning rate for epoch 320 is 0.002587881637737155\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.3323 - val_loss: 3.0965\n",
      "Epoch 321/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5435\n",
      "Epoch 00321: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1205-24] Learning rate for epoch 321 is 0.0027200679760426283\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.5435 - val_loss: 3.2891\n",
      "Epoch 322/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5503\n",
      "Epoch 00322: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1205-29] Learning rate for epoch 322 is 0.0028518543113023043\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.5503 - val_loss: 3.0699\n",
      "Epoch 323/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2490\n",
      "Epoch 00323: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1205-34] Learning rate for epoch 323 is 0.002983240643516183\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.2490 - val_loss: 3.7483\n",
      "Epoch 324/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3951\n",
      "Epoch 00324: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1205-38] Learning rate for epoch 324 is 0.003114226972684264\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.3951 - val_loss: 4.3179\n",
      "Epoch 325/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0354\n",
      "Epoch 00325: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1205-43] Learning rate for epoch 325 is 0.0032448135316371918\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.0354 - val_loss: 3.6263\n",
      "Epoch 326/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1597\n",
      "Epoch 00326: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1205-48] Learning rate for epoch 326 is 0.003375000087544322\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.1597 - val_loss: 3.6455\n",
      "Epoch 327/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8485\n",
      "Epoch 00327: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1205-53] Learning rate for epoch 327 is 0.0032352134585380554\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8485 - val_loss: 5.5834\n",
      "Epoch 328/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2281\n",
      "Epoch 00328: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1205-58] Learning rate for epoch 328 is 0.003095826832577586\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.2281 - val_loss: 5.1733\n",
      "Epoch 329/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5565\n",
      "Epoch 00329: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1206-03] Learning rate for epoch 329 is 0.0029568402096629143\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.5565 - val_loss: 3.3550\n",
      "Epoch 330/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6153\n",
      "Epoch 00330: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1206-08] Learning rate for epoch 330 is 0.0028182535897940397\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.6153 - val_loss: 3.2077\n",
      "Epoch 331/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7018\n",
      "Epoch 00331: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1206-13] Learning rate for epoch 331 is 0.0026800669729709625\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.7018 - val_loss: 3.5003\n",
      "Epoch 332/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3456\n",
      "Epoch 00332: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1206-17] Learning rate for epoch 332 is 0.0025422803591936827\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.3456 - val_loss: 4.3014\n",
      "Epoch 333/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0840\n",
      "Epoch 00333: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1206-22] Learning rate for epoch 333 is 0.0024048935156315565\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.0840 - val_loss: 3.9905\n",
      "Epoch 334/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7414\n",
      "Epoch 00334: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1206-27] Learning rate for epoch 334 is 0.0022679066751152277\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.7414 - val_loss: 4.3458\n",
      "Epoch 335/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6013\n",
      "Epoch 00335: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1206-32] Learning rate for epoch 335 is 0.0021313198376446962\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.6013 - val_loss: 4.0168\n",
      "Epoch 336/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6773\n",
      "Epoch 00336: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1206-37] Learning rate for epoch 336 is 0.001995133003219962\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.6773 - val_loss: 3.3480\n",
      "Epoch 337/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4147\n",
      "Epoch 00337: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1206-42] Learning rate for epoch 337 is 0.0018593460554257035\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.4147 - val_loss: 3.5362\n",
      "Epoch 338/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4004\n",
      "Epoch 00338: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1206-47] Learning rate for epoch 338 is 0.0017239591106772423\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.4004 - val_loss: 3.4583\n",
      "Epoch 339/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2035\n",
      "Epoch 00339: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1206-51] Learning rate for epoch 339 is 0.0015889721689745784\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.2035 - val_loss: 3.2790\n",
      "Epoch 340/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1951\n",
      "Epoch 00340: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1206-56] Learning rate for epoch 340 is 0.00145438511390239\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.1951 - val_loss: 3.4137\n",
      "Epoch 341/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4197\n",
      "Epoch 00341: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1207-01] Learning rate for epoch 341 is 0.0013201979454606771\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.4197 - val_loss: 3.4583\n",
      "Epoch 342/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2795\n",
      "Epoch 00342: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1207-06] Learning rate for epoch 342 is 0.0011864108964800835\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.2795 - val_loss: 3.2607\n",
      "Epoch 343/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3540\n",
      "Epoch 00343: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1207-11] Learning rate for epoch 343 is 0.0010530237341299653\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.3540 - val_loss: 3.6047\n",
      "Epoch 344/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4286\n",
      "Epoch 00344: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1207-16] Learning rate for epoch 344 is 0.0009200365166179836\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.4286 - val_loss: 3.2111\n",
      "Epoch 345/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6183\n",
      "Epoch 00345: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1207-20] Learning rate for epoch 345 is 0.0007874493021517992\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6183 - val_loss: 3.3263\n",
      "Epoch 346/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 4.8819\n",
      "Epoch 00346: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1207-25] Learning rate for epoch 346 is 0.0006552619743160903\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 4.8819 - val_loss: 3.4346\n",
      "Epoch 347/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1019\n",
      "Epoch 00347: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1207-30] Learning rate for epoch 347 is 0.0005234747077338398\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.1019 - val_loss: 3.1518\n",
      "Epoch 348/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.0309\n",
      "Epoch 00348: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1207-35] Learning rate for epoch 348 is 0.0003920873277820647\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.0309 - val_loss: 3.2774\n",
      "Epoch 349/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2278\n",
      "Epoch 00349: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1207-40] Learning rate for epoch 349 is 0.0002610999217722565\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.2278 - val_loss: 3.1608\n",
      "Epoch 350/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5234\n",
      "Epoch 00350: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1207-45] Learning rate for epoch 350 is 0.00013051247515249997\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5234 - val_loss: 3.1728\n",
      "Epoch 351/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 4.8390\n",
      "Epoch 00351: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1207-49] Learning rate for epoch 351 is 3.250000020216248e-07\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 4.8390 - val_loss: 3.1697\n",
      "Epoch 352/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.0906\n",
      "Epoch 00352: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1207-54] Learning rate for epoch 352 is 0.00013011152623221278\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.0906 - val_loss: 3.0930\n",
      "Epoch 353/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2291\n",
      "Epoch 00353: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1207-59] Learning rate for epoch 353 is 0.00025949807604774833\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.2291 - val_loss: 3.2599\n",
      "Epoch 354/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2498\n",
      "Epoch 00354: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1208-04] Learning rate for epoch 354 is 0.00038848468102514744\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.2498 - val_loss: 3.1289\n",
      "Epoch 355/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4198\n",
      "Epoch 00355: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1208-09] Learning rate for epoch 355 is 0.0005170713411644101\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.4198 - val_loss: 3.1314\n",
      "Epoch 356/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6658\n",
      "Epoch 00356: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1208-14] Learning rate for epoch 356 is 0.0006452579982578754\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 5.6658 - val_loss: 3.2064\n",
      "Epoch 357/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1363\n",
      "Epoch 00357: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1208-18] Learning rate for epoch 357 is 0.0007730447105132043\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.1363 - val_loss: 3.3114\n",
      "Epoch 358/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.0436\n",
      "Epoch 00358: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1208-23] Learning rate for epoch 358 is 0.0009004314779303968\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.0436 - val_loss: 3.2103\n",
      "Epoch 359/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.0040\n",
      "Epoch 00359: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1208-28] Learning rate for epoch 359 is 0.0010274183005094528\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.0040 - val_loss: 3.2389\n",
      "Epoch 360/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3410\n",
      "Epoch 00360: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1208-33] Learning rate for epoch 360 is 0.0011540050618350506\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.3410 - val_loss: 3.4227\n",
      "Epoch 361/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.0686\n",
      "Epoch 00361: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1208-37] Learning rate for epoch 361 is 0.0012801920529454947\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.0686 - val_loss: 3.4140\n",
      "Epoch 362/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1719\n",
      "Epoch 00362: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1208-42] Learning rate for epoch 362 is 0.0014059789245948195\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.1719 - val_loss: 3.7403\n",
      "Epoch 363/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5566\n",
      "Epoch 00363: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1208-47] Learning rate for epoch 363 is 0.001531365909613669\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.5566 - val_loss: 3.5888\n",
      "Epoch 364/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8975\n",
      "Epoch 00364: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1208-52] Learning rate for epoch 364 is 0.001656352891586721\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.8975 - val_loss: 3.2196\n",
      "Epoch 365/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.0705\n",
      "Epoch 00365: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1208-57] Learning rate for epoch 365 is 0.0017809398705139756\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.0705 - val_loss: 3.4717\n",
      "Epoch 366/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6756\n",
      "Epoch 00366: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1209-02] Learning rate for epoch 366 is 0.0019051269628107548\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.6756 - val_loss: 3.1913\n",
      "Epoch 367/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5452\n",
      "Epoch 00367: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1209-06] Learning rate for epoch 367 is 0.0020289141684770584\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.5452 - val_loss: 3.3899\n",
      "Epoch 368/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3817\n",
      "Epoch 00368: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1209-11] Learning rate for epoch 368 is 0.0021523013710975647\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.3817 - val_loss: 3.4862\n",
      "Epoch 369/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7244\n",
      "Epoch 00369: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1209-16] Learning rate for epoch 369 is 0.0022752885706722736\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.7244 - val_loss: 4.6488\n",
      "Epoch 370/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7492\n",
      "Epoch 00370: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1209-21] Learning rate for epoch 370 is 0.0023978757672011852\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.7492 - val_loss: 5.0427\n",
      "Epoch 371/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7350\n",
      "Epoch 00371: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1209-26] Learning rate for epoch 371 is 0.0025200629606842995\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.7350 - val_loss: 3.4563\n",
      "Epoch 372/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5822\n",
      "Epoch 00372: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1209-30] Learning rate for epoch 372 is 0.00264185038395226\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 5.5822 - val_loss: 5.3627\n",
      "Epoch 373/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4777\n",
      "Epoch 00373: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1209-35] Learning rate for epoch 373 is 0.0027632375713437796\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.4777 - val_loss: 4.0022\n",
      "Epoch 374/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4640\n",
      "Epoch 00374: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1209-40] Learning rate for epoch 374 is 0.0028842249885201454\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.4640 - val_loss: 4.4824\n",
      "Epoch 375/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1797\n",
      "Epoch 00375: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1209-45] Learning rate for epoch 375 is 0.0030048126354813576\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.1797 - val_loss: 7.5504\n",
      "Epoch 376/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1657\n",
      "Epoch 00376: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1209-50] Learning rate for epoch 376 is 0.0031250000465661287\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.1657 - val_loss: 3.8875\n",
      "Epoch 377/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4395\n",
      "Epoch 00377: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1209-55] Learning rate for epoch 377 is 0.0029952125623822212\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.4395 - val_loss: 4.3077\n",
      "Epoch 378/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1207\n",
      "Epoch 00378: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1209-59] Learning rate for epoch 378 is 0.0028658248484134674\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.1207 - val_loss: 4.3046\n",
      "Epoch 379/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9880\n",
      "Epoch 00379: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1210-04] Learning rate for epoch 379 is 0.0027368373703211546\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.9880 - val_loss: 4.1148\n",
      "Epoch 380/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5266\n",
      "Epoch 00380: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1210-09] Learning rate for epoch 380 is 0.0026082496624439955\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.5266 - val_loss: 3.2718\n",
      "Epoch 381/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4618\n",
      "Epoch 00381: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1210-14] Learning rate for epoch 381 is 0.0024800619576126337\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.4618 - val_loss: 3.5625\n",
      "Epoch 382/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4432\n",
      "Epoch 00382: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1210-19] Learning rate for epoch 382 is 0.0023522742558270693\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.4432 - val_loss: 4.3965\n",
      "Epoch 383/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1639\n",
      "Epoch 00383: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1210-24] Learning rate for epoch 383 is 0.002224886557087302\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.1639 - val_loss: 3.4192\n",
      "Epoch 384/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1135\n",
      "Epoch 00384: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1210-28] Learning rate for epoch 384 is 0.002097898628562689\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.1135 - val_loss: 3.6188\n",
      "Epoch 385/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3264\n",
      "Epoch 00385: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1210-33] Learning rate for epoch 385 is 0.0019713109359145164\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.3264 - val_loss: 4.4226\n",
      "Epoch 386/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3931\n",
      "Epoch 00386: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1210-38] Learning rate for epoch 386 is 0.0018451230134814978\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 5.3931 - val_loss: 4.7850\n",
      "Epoch 387/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6548\n",
      "Epoch 00387: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1210-43] Learning rate for epoch 387 is 0.0017193350940942764\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.6548 - val_loss: 3.9506\n",
      "Epoch 388/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4302\n",
      "Epoch 00388: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1210-48] Learning rate for epoch 388 is 0.0015939471777528524\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.4302 - val_loss: 4.1441\n",
      "Epoch 389/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4665\n",
      "Epoch 00389: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1210-53] Learning rate for epoch 389 is 0.001468959148041904\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.4665 - val_loss: 3.3073\n",
      "Epoch 390/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4685\n",
      "Epoch 00390: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1210-58] Learning rate for epoch 390 is 0.0013443711213767529\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.4685 - val_loss: 3.2764\n",
      "Epoch 391/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 4.8804\n",
      "Epoch 00391: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1211-02] Learning rate for epoch 391 is 0.0012201829813420773\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 4.8804 - val_loss: 3.1638\n",
      "Epoch 392/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3814\n",
      "Epoch 00392: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1211-07] Learning rate for epoch 392 is 0.001096394844353199\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.3814 - val_loss: 3.3239\n",
      "Epoch 393/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 4.9861\n",
      "Epoch 00393: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1211-12] Learning rate for epoch 393 is 0.0009730067104101181\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 4.9861 - val_loss: 3.2065\n",
      "Epoch 394/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 4.8472\n",
      "Epoch 00394: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1211-17] Learning rate for epoch 394 is 0.0008500185213051736\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 4.8472 - val_loss: 3.2092\n",
      "Epoch 395/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4728\n",
      "Epoch 00395: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1211-22] Learning rate for epoch 395 is 0.0007274302770383656\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.4728 - val_loss: 3.1625\n",
      "Epoch 396/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2419\n",
      "Epoch 00396: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1211-27] Learning rate for epoch 396 is 0.000605241977609694\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.2419 - val_loss: 3.0912\n",
      "Epoch 397/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4325\n",
      "Epoch 00397: val_loss did not improve from 3.06038\n",
      "\n",
      "[20210302-1211-31] Learning rate for epoch 397 is 0.00048345368122681975\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.4325 - val_loss: 3.2557\n",
      "K= 10\n",
      "Check cache in memory:Y True\n",
      "Check keypoints process01: Tensor(\"args_1:0\", shape=(2,), dtype=int64) (2,) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Check keypoints process02: Tensor(\"Reshape:0\", shape=(1, 2), dtype=int64) (1, 2) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Check keypoints process03: Tensor(\"PyFunc:1\", dtype=float32, device=/job:localhost/replica:0/task:0)\n",
      "Check keypoints process04: Tensor(\"Reshape_1:0\", shape=(2,), dtype=float32)\n",
      "Check augment :Y True\n",
      "Check ds cache[True] and augment[True]\n",
      "Check cache in memory:Y True\n",
      "Check augment :N False\n",
      "Check ds cache[True] and augment[False]\n",
      "best_model_name: ./20210301-225844/heel_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "base_model.trainable :  False\n",
      "\n",
      "[20210302-1211-34] Learning rate for epoch 1 is 0.00047855067532509565\n",
      "Epoch 1/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 106.6690\n",
      "Epoch 00001: val_loss improved from inf to 73.36321, saving model to ./20210301-225844/heel_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 5s 266ms/step - loss: 104.5471 - val_loss: 73.3632\n",
      "\n",
      "[20210302-1211-49] Learning rate for epoch 2 is 0.00047855067532509565\n",
      "Epoch 2/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 42.2708\n",
      "Epoch 00002: val_loss improved from 73.36321 to 29.31214, saving model to ./20210301-225844/heel_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 41.6496 - val_loss: 29.3121\n",
      "\n",
      "[20210302-1211-53] Learning rate for epoch 3 is 0.00047855067532509565\n",
      "Epoch 3/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 23.3790\n",
      "Epoch 00003: val_loss improved from 29.31214 to 23.45413, saving model to ./20210301-225844/heel_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 7s 348ms/step - loss: 23.3790 - val_loss: 23.4541\n",
      "\n",
      "[20210302-1212-04] Learning rate for epoch 4 is 0.00047855067532509565\n",
      "Epoch 4/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 20.7838\n",
      "Epoch 00004: val_loss improved from 23.45413 to 21.39502, saving model to ./20210301-225844/heel_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 20.6887 - val_loss: 21.3950\n",
      "\n",
      "[20210302-1212-08] Learning rate for epoch 5 is 0.00047855067532509565\n",
      "Epoch 5/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 19.8380\n",
      "Epoch 00005: val_loss improved from 21.39502 to 21.35930, saving model to ./20210301-225844/heel_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 19.8380 - val_loss: 21.3593\n",
      "\n",
      "[20210302-1212-13] Learning rate for epoch 6 is 0.00047855067532509565\n",
      "Epoch 6/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 18.7818\n",
      "Epoch 00006: val_loss improved from 21.35930 to 21.07877, saving model to ./20210301-225844/heel_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 18.7818 - val_loss: 21.0788\n",
      "\n",
      "[20210302-1212-17] Learning rate for epoch 7 is 0.00047855067532509565\n",
      "Epoch 7/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 18.5022\n",
      "Epoch 00007: val_loss improved from 21.07877 to 20.81675, saving model to ./20210301-225844/heel_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 18.5022 - val_loss: 20.8167\n",
      "\n",
      "[20210302-1212-21] Learning rate for epoch 8 is 0.00047855067532509565\n",
      "Epoch 8/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 18.2453\n",
      "Epoch 00008: val_loss improved from 20.81675 to 20.11096, saving model to ./20210301-225844/heel_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 18.3526 - val_loss: 20.1110\n",
      "\n",
      "[20210302-1212-25] Learning rate for epoch 9 is 0.00047855067532509565\n",
      "Epoch 9/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 17.3912\n",
      "Epoch 00009: val_loss improved from 20.11096 to 20.03256, saving model to ./20210301-225844/heel_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 17.4392 - val_loss: 20.0326\n",
      "\n",
      "[20210302-1212-30] Learning rate for epoch 10 is 0.00047855067532509565\n",
      "Epoch 10/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 17.5497\n",
      "Epoch 00010: val_loss improved from 20.03256 to 19.53547, saving model to ./20210301-225844/heel_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 17.5367 - val_loss: 19.5355\n",
      "\n",
      "[20210302-1212-34] Learning rate for epoch 11 is 0.00047855067532509565\n",
      "Epoch 11/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 17.7570\n",
      "Epoch 00011: val_loss improved from 19.53547 to 18.43569, saving model to ./20210301-225844/heel_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 17.7570 - val_loss: 18.4357\n",
      "\n",
      "[20210302-1212-38] Learning rate for epoch 12 is 0.00047855067532509565\n",
      "Epoch 12/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 17.1602\n",
      "Epoch 00012: val_loss did not improve from 18.43569\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 17.2643 - val_loss: 18.9059\n",
      "\n",
      "[20210302-1212-42] Learning rate for epoch 13 is 0.00047855067532509565\n",
      "Epoch 13/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 17.8896\n",
      "Epoch 00013: val_loss improved from 18.43569 to 18.36469, saving model to ./20210301-225844/heel_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 17.7697 - val_loss: 18.3647\n",
      "\n",
      "[20210302-1212-47] Learning rate for epoch 14 is 0.00047855067532509565\n",
      "Epoch 14/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 17.9025\n",
      "Epoch 00014: val_loss did not improve from 18.36469\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 17.9612 - val_loss: 18.5717\n",
      "\n",
      "[20210302-1212-51] Learning rate for epoch 15 is 0.00047855067532509565\n",
      "Epoch 15/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 16.9468\n",
      "Epoch 00015: val_loss improved from 18.36469 to 16.60265, saving model to ./20210301-225844/heel_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 17.0204 - val_loss: 16.6026\n",
      "\n",
      "[20210302-1212-55] Learning rate for epoch 16 is 0.00047855067532509565\n",
      "Epoch 16/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 17.0763\n",
      "Epoch 00016: val_loss improved from 16.60265 to 15.78724, saving model to ./20210301-225844/heel_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.9790 - val_loss: 15.7872\n",
      "\n",
      "[20210302-1212-59] Learning rate for epoch 17 is 0.00047855067532509565\n",
      "Epoch 17/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 17.0382\n",
      "Epoch 00017: val_loss improved from 15.78724 to 15.03389, saving model to ./20210301-225844/heel_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 17.0011 - val_loss: 15.0339\n",
      "\n",
      "[20210302-1213-04] Learning rate for epoch 18 is 0.00047855067532509565\n",
      "Epoch 18/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.4546\n",
      "Epoch 00018: val_loss did not improve from 15.03389\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.5326 - val_loss: 15.4751\n",
      "\n",
      "[20210302-1213-08] Learning rate for epoch 19 is 0.00047855067532509565\n",
      "Epoch 19/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.9606\n",
      "Epoch 00019: val_loss improved from 15.03389 to 14.34232, saving model to ./20210301-225844/heel_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 15.8008 - val_loss: 14.3423\n",
      "\n",
      "[20210302-1213-12] Learning rate for epoch 20 is 0.00047855067532509565\n",
      "Epoch 20/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.2324\n",
      "Epoch 00020: val_loss did not improve from 14.34232\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.1910 - val_loss: 14.6610\n",
      "\n",
      "[20210302-1213-16] Learning rate for epoch 21 is 0.00047855067532509565\n",
      "Epoch 21/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 17.1013\n",
      "Epoch 00021: val_loss did not improve from 14.34232\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 17.1013 - val_loss: 14.9958\n",
      "\n",
      "[20210302-1213-20] Learning rate for epoch 22 is 0.00047855067532509565\n",
      "Epoch 22/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.6200\n",
      "Epoch 00022: val_loss did not improve from 14.34232\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.7191 - val_loss: 15.6346\n",
      "\n",
      "[20210302-1213-24] Learning rate for epoch 23 is 0.00047855067532509565\n",
      "Epoch 23/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.6222\n",
      "Epoch 00023: val_loss improved from 14.34232 to 12.77786, saving model to ./20210301-225844/heel_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 16.6222 - val_loss: 12.7779\n",
      "\n",
      "[20210302-1213-28] Learning rate for epoch 24 is 0.00047855067532509565\n",
      "Epoch 24/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.7291\n",
      "Epoch 00024: val_loss improved from 12.77786 to 12.10046, saving model to ./20210301-225844/heel_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.6267 - val_loss: 12.1005\n",
      "\n",
      "[20210302-1213-32] Learning rate for epoch 25 is 0.00047855067532509565\n",
      "Epoch 25/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.8130\n",
      "Epoch 00025: val_loss improved from 12.10046 to 12.03743, saving model to ./20210301-225844/heel_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 16.8130 - val_loss: 12.0374\n",
      "\n",
      "[20210302-1213-37] Learning rate for epoch 26 is 0.00047855067532509565\n",
      "Epoch 26/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.1691\n",
      "Epoch 00026: val_loss improved from 12.03743 to 11.77701, saving model to ./20210301-225844/heel_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.1399 - val_loss: 11.7770\n",
      "\n",
      "[20210302-1213-41] Learning rate for epoch 27 is 0.00047855067532509565\n",
      "Epoch 27/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 15.8593\n",
      "Epoch 00027: val_loss did not improve from 11.77701\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 15.8252 - val_loss: 12.8525\n",
      "\n",
      "[20210302-1213-45] Learning rate for epoch 28 is 0.00047855067532509565\n",
      "Epoch 28/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.2574\n",
      "Epoch 00028: val_loss improved from 11.77701 to 11.51141, saving model to ./20210301-225844/heel_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 16.1439 - val_loss: 11.5114\n",
      "\n",
      "[20210302-1213-49] Learning rate for epoch 29 is 0.00047855067532509565\n",
      "Epoch 29/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 16.5273\n",
      "Epoch 00029: val_loss improved from 11.51141 to 11.16866, saving model to ./20210301-225844/heel_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 16.4353 - val_loss: 11.1687\n",
      "\n",
      "[20210302-1213-54] Learning rate for epoch 30 is 0.00047855067532509565\n",
      "Epoch 30/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 16.4515\n",
      "Epoch 00030: val_loss did not improve from 11.16866\n",
      "20/20 [==============================] - 0s 24ms/step - loss: 16.4355 - val_loss: 12.0293\n",
      "\n",
      "[20210302-1213-58] Learning rate for epoch 31 is 0.00047855067532509565\n",
      "Epoch 31/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.4160\n",
      "Epoch 00031: val_loss improved from 11.16866 to 11.01660, saving model to ./20210301-225844/heel_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.4160 - val_loss: 11.0166\n",
      "\n",
      "[20210302-1214-02] Learning rate for epoch 32 is 0.00047855067532509565\n",
      "Epoch 32/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 15.9692\n",
      "Epoch 00032: val_loss did not improve from 11.01660\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 16.0371 - val_loss: 11.9202\n",
      "\n",
      "[20210302-1214-06] Learning rate for epoch 33 is 0.00047855067532509565\n",
      "Epoch 33/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.0989\n",
      "Epoch 00033: val_loss improved from 11.01660 to 10.00423, saving model to ./20210301-225844/heel_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 16.0989 - val_loss: 10.0042\n",
      "\n",
      "[20210302-1214-10] Learning rate for epoch 34 is 0.00047855067532509565\n",
      "Epoch 34/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.2041\n",
      "Epoch 00034: val_loss did not improve from 10.00423\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.1938 - val_loss: 10.7628\n",
      "\n",
      "[20210302-1214-14] Learning rate for epoch 35 is 0.00047855067532509565\n",
      "Epoch 35/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.7729\n",
      "Epoch 00035: val_loss did not improve from 10.00423\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.7729 - val_loss: 10.9144\n",
      "\n",
      "[20210302-1214-18] Learning rate for epoch 36 is 0.00047855067532509565\n",
      "Epoch 36/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.6714\n",
      "Epoch 00036: val_loss did not improve from 10.00423\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.6714 - val_loss: 11.9631\n",
      "\n",
      "[20210302-1214-22] Learning rate for epoch 37 is 0.00047855067532509565\n",
      "Epoch 37/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 16.0680\n",
      "Epoch 00037: val_loss did not improve from 10.00423\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.0807 - val_loss: 11.3712\n",
      "\n",
      "[20210302-1214-26] Learning rate for epoch 38 is 0.00047855067532509565\n",
      "Epoch 38/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 16.1824\n",
      "Epoch 00038: val_loss improved from 10.00423 to 9.77413, saving model to ./20210301-225844/heel_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 16.1754 - val_loss: 9.7741\n",
      "\n",
      "[20210302-1214-30] Learning rate for epoch 39 is 0.00047855067532509565\n",
      "Epoch 39/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.0314\n",
      "Epoch 00039: val_loss improved from 9.77413 to 9.25309, saving model to ./20210301-225844/heel_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.0702 - val_loss: 9.2531\n",
      "\n",
      "[20210302-1214-35] Learning rate for epoch 40 is 0.00047855067532509565\n",
      "Epoch 40/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.9008\n",
      "Epoch 00040: val_loss did not improve from 9.25309\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 15.9008 - val_loss: 9.5060\n",
      "\n",
      "[20210302-1214-39] Learning rate for epoch 41 is 0.00047855067532509565\n",
      "Epoch 41/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.1649\n",
      "Epoch 00041: val_loss did not improve from 9.25309\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.0628 - val_loss: 9.8038\n",
      "\n",
      "[20210302-1214-43] Learning rate for epoch 42 is 0.00047855067532509565\n",
      "Epoch 42/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.7253\n",
      "Epoch 00042: val_loss did not improve from 9.25309\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.7253 - val_loss: 10.3925\n",
      "\n",
      "[20210302-1214-47] Learning rate for epoch 43 is 0.00047855067532509565\n",
      "Epoch 43/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 15.7427\n",
      "Epoch 00043: val_loss did not improve from 9.25309\n",
      "20/20 [==============================] - 0s 24ms/step - loss: 15.6498 - val_loss: 11.8908\n",
      "\n",
      "[20210302-1214-51] Learning rate for epoch 44 is 0.00047855067532509565\n",
      "Epoch 44/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.2027\n",
      "Epoch 00044: val_loss did not improve from 9.25309\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.0865 - val_loss: 11.5228\n",
      "\n",
      "[20210302-1214-54] Learning rate for epoch 45 is 0.00047855067532509565\n",
      "Epoch 45/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 16.2104\n",
      "Epoch 00045: val_loss improved from 9.25309 to 9.25135, saving model to ./20210301-225844/heel_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 16.0930 - val_loss: 9.2513\n",
      "\n",
      "[20210302-1214-59] Learning rate for epoch 46 is 0.00047855067532509565\n",
      "Epoch 46/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 15.7432\n",
      "Epoch 00046: val_loss did not improve from 9.25135\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 15.8971 - val_loss: 11.1784\n",
      "\n",
      "[20210302-1215-03] Learning rate for epoch 47 is 0.00047855067532509565\n",
      "Epoch 47/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.5767\n",
      "Epoch 00047: val_loss did not improve from 9.25135\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.5767 - val_loss: 10.0232\n",
      "\n",
      "[20210302-1215-07] Learning rate for epoch 48 is 0.00047855067532509565\n",
      "Epoch 48/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 15.5602\n",
      "Epoch 00048: val_loss improved from 9.25135 to 9.03796, saving model to ./20210301-225844/heel_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 15.6179 - val_loss: 9.0380\n",
      "\n",
      "[20210302-1215-11] Learning rate for epoch 49 is 0.00047855067532509565\n",
      "Epoch 49/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.9675\n",
      "Epoch 00049: val_loss did not improve from 9.03796\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.9819 - val_loss: 10.3011\n",
      "\n",
      "[20210302-1215-15] Learning rate for epoch 50 is 0.00047855067532509565\n",
      "Epoch 50/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.9033\n",
      "Epoch 00050: val_loss did not improve from 9.03796\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.8912 - val_loss: 11.9122\n",
      "\n",
      "[20210302-1215-19] Learning rate for epoch 51 is 0.00047855067532509565\n",
      "Epoch 51/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.8724\n",
      "Epoch 00051: val_loss did not improve from 9.03796\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.8724 - val_loss: 11.0415\n",
      "\n",
      "[20210302-1215-23] Learning rate for epoch 52 is 0.00047855067532509565\n",
      "Epoch 52/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.9270\n",
      "Epoch 00052: val_loss did not improve from 9.03796\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 15.8649 - val_loss: 9.9812\n",
      "\n",
      "[20210302-1215-27] Learning rate for epoch 53 is 0.00047855067532509565\n",
      "Epoch 53/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.3425\n",
      "Epoch 00053: val_loss did not improve from 9.03796\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.3425 - val_loss: 9.9766\n",
      "\n",
      "[20210302-1215-31] Learning rate for epoch 54 is 0.00047855067532509565\n",
      "Epoch 54/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 15.9655\n",
      "Epoch 00054: val_loss did not improve from 9.03796\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.9789 - val_loss: 9.8884\n",
      "\n",
      "[20210302-1215-35] Learning rate for epoch 55 is 0.00047855067532509565\n",
      "Epoch 55/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.8496\n",
      "Epoch 00055: val_loss did not improve from 9.03796\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.6996 - val_loss: 10.4851\n",
      "\n",
      "[20210302-1215-39] Learning rate for epoch 56 is 0.00047855067532509565\n",
      "Epoch 56/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.8376\n",
      "Epoch 00056: val_loss did not improve from 9.03796\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.8342 - val_loss: 10.3149\n",
      "\n",
      "[20210302-1215-43] Learning rate for epoch 57 is 0.00047855067532509565\n",
      "Epoch 57/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.5973\n",
      "Epoch 00057: val_loss did not improve from 9.03796\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.5239 - val_loss: 10.8411\n",
      "\n",
      "[20210302-1215-47] Learning rate for epoch 58 is 0.00047855067532509565\n",
      "Epoch 58/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.4437\n",
      "Epoch 00058: val_loss improved from 9.03796 to 9.02533, saving model to ./20210301-225844/heel_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 15.4437 - val_loss: 9.0253\n",
      "\n",
      "[20210302-1215-51] Learning rate for epoch 59 is 0.00047855067532509565\n",
      "Epoch 59/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.8098\n",
      "Epoch 00059: val_loss did not improve from 9.02533\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.8098 - val_loss: 9.9544\n",
      "\n",
      "[20210302-1215-55] Learning rate for epoch 60 is 0.00047855067532509565\n",
      "Epoch 60/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.7574\n",
      "Epoch 00060: val_loss did not improve from 9.02533\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 15.7711 - val_loss: 9.2166\n",
      "\n",
      "[20210302-1215-59] Learning rate for epoch 61 is 0.00047855067532509565\n",
      "Epoch 61/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.3924\n",
      "Epoch 00061: val_loss did not improve from 9.02533\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.3924 - val_loss: 9.2682\n",
      "\n",
      "[20210302-1216-03] Learning rate for epoch 62 is 0.00047855067532509565\n",
      "Epoch 62/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.5489\n",
      "Epoch 00062: val_loss did not improve from 9.02533\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.6179 - val_loss: 10.1634\n",
      "\n",
      "[20210302-1216-07] Learning rate for epoch 63 is 0.00047855067532509565\n",
      "Epoch 63/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.8022\n",
      "Epoch 00063: val_loss did not improve from 9.02533\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.8052 - val_loss: 9.8474\n",
      "\n",
      "[20210302-1216-11] Learning rate for epoch 64 is 0.00047855067532509565\n",
      "Epoch 64/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.3933\n",
      "Epoch 00064: val_loss did not improve from 9.02533\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.3933 - val_loss: 9.9287\n",
      "\n",
      "[20210302-1216-15] Learning rate for epoch 65 is 0.00047855067532509565\n",
      "Epoch 65/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.4345\n",
      "Epoch 00065: val_loss did not improve from 9.02533\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.4472 - val_loss: 10.8652\n",
      "\n",
      "[20210302-1216-19] Learning rate for epoch 66 is 0.00047855067532509565\n",
      "Epoch 66/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.5555\n",
      "Epoch 00066: val_loss did not improve from 9.02533\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.4701 - val_loss: 9.9629\n",
      "\n",
      "[20210302-1216-23] Learning rate for epoch 67 is 0.00047855067532509565\n",
      "Epoch 67/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.6150\n",
      "Epoch 00067: val_loss did not improve from 9.02533\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.5908 - val_loss: 9.5169\n",
      "\n",
      "[20210302-1216-27] Learning rate for epoch 68 is 0.00047855067532509565\n",
      "Epoch 68/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.3912\n",
      "Epoch 00068: val_loss did not improve from 9.02533\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.4719 - val_loss: 10.8039\n",
      "\n",
      "[20210302-1216-31] Learning rate for epoch 69 is 0.00047855067532509565\n",
      "Epoch 69/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.9278\n",
      "Epoch 00069: val_loss did not improve from 9.02533\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.8538 - val_loss: 10.2036\n",
      "\n",
      "[20210302-1216-35] Learning rate for epoch 70 is 0.00047855067532509565\n",
      "Epoch 70/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.9901\n",
      "Epoch 00070: val_loss did not improve from 9.02533\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.9901 - val_loss: 10.5235\n",
      "\n",
      "[20210302-1216-39] Learning rate for epoch 71 is 0.00047855067532509565\n",
      "Epoch 71/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.5877\n",
      "Epoch 00071: val_loss did not improve from 9.02533\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.5877 - val_loss: 10.0172\n",
      "\n",
      "[20210302-1216-43] Learning rate for epoch 72 is 0.00047855067532509565\n",
      "Epoch 72/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.2822\n",
      "Epoch 00072: val_loss did not improve from 9.02533\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 15.4804 - val_loss: 9.4530\n",
      "\n",
      "[20210302-1216-47] Learning rate for epoch 73 is 0.00047855067532509565\n",
      "Epoch 73/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.7840\n",
      "Epoch 00073: val_loss did not improve from 9.02533\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.7665 - val_loss: 10.5181\n",
      "\n",
      "[20210302-1216-51] Learning rate for epoch 74 is 0.00047855067532509565\n",
      "Epoch 74/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 15.6470\n",
      "Epoch 00074: val_loss did not improve from 9.02533\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 15.7872 - val_loss: 9.5475\n",
      "\n",
      "[20210302-1216-55] Learning rate for epoch 75 is 0.00047855067532509565\n",
      "Epoch 75/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.2438\n",
      "Epoch 00075: val_loss did not improve from 9.02533\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.2424 - val_loss: 11.0849\n",
      "\n",
      "[20210302-1216-58] Learning rate for epoch 76 is 0.00047855067532509565\n",
      "Epoch 76/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.1649\n",
      "Epoch 00076: val_loss did not improve from 9.02533\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 15.1897 - val_loss: 10.0766\n",
      "\n",
      "[20210302-1217-02] Learning rate for epoch 77 is 0.00047855067532509565\n",
      "Epoch 77/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.7270\n",
      "Epoch 00077: val_loss did not improve from 9.02533\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.7270 - val_loss: 9.9559\n",
      "\n",
      "[20210302-1217-06] Learning rate for epoch 78 is 0.00047855067532509565\n",
      "Epoch 78/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.2479\n",
      "Epoch 00078: val_loss did not improve from 9.02533\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.2789 - val_loss: 9.4863\n",
      "\n",
      "*********************************** Start fine tune ***********************************\n",
      "tt: 195, nt:46, total layers:241\n",
      "*********************************** Start fine tune ***********************************\n",
      "Epoch 1/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 24.6530\n",
      "Epoch 00001: val_loss did not improve from 9.02533\n",
      "\n",
      "[20210302-1217-41] Learning rate for epoch 1 is 4.999999987376214e-07\n",
      "20/20 [==============================] - 14s 691ms/step - loss: 24.6530 - val_loss: 15.8791\n",
      "Epoch 2/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 23.1189\n",
      "Epoch 00002: val_loss did not improve from 9.02533\n",
      "\n",
      "[20210302-1217-46] Learning rate for epoch 2 is 5.044450517743826e-05\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 23.1189 - val_loss: 16.0656\n",
      "Epoch 3/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 18.3605\n",
      "Epoch 00003: val_loss did not improve from 9.02533\n",
      "\n",
      "[20210302-1218-06] Learning rate for epoch 3 is 0.00010028902033809572\n",
      "20/20 [==============================] - 17s 840ms/step - loss: 18.3605 - val_loss: 13.8758\n",
      "Epoch 4/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.8879\n",
      "Epoch 00004: val_loss did not improve from 9.02533\n",
      "\n",
      "[20210302-1218-11] Learning rate for epoch 4 is 0.00015003354928921908\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 15.8879 - val_loss: 11.0187\n",
      "Epoch 5/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.7130\n",
      "Epoch 00005: val_loss did not improve from 9.02533\n",
      "\n",
      "[20210302-1218-16] Learning rate for epoch 5 is 0.0001996780774788931\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 14.7130 - val_loss: 9.9456\n",
      "Epoch 6/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.7551\n",
      "Epoch 00006: val_loss improved from 9.02533 to 8.65649, saving model to ./20210301-225844/heel_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1218-21] Learning rate for epoch 6 is 0.000249222619459033\n",
      "20/20 [==============================] - 2s 89ms/step - loss: 13.7551 - val_loss: 8.6565\n",
      "Epoch 7/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.5641\n",
      "Epoch 00007: val_loss improved from 8.65649 to 8.59475, saving model to ./20210301-225844/heel_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1218-26] Learning rate for epoch 7 is 0.0002986671752296388\n",
      "20/20 [==============================] - 2s 89ms/step - loss: 13.5641 - val_loss: 8.5947\n",
      "Epoch 8/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.2564\n",
      "Epoch 00008: val_loss did not improve from 8.59475\n",
      "\n",
      "[20210302-1218-31] Learning rate for epoch 8 is 0.00034801175934262574\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 13.2564 - val_loss: 9.2340\n",
      "Epoch 9/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 12.4348\n",
      "Epoch 00009: val_loss improved from 8.59475 to 7.17889, saving model to ./20210301-225844/heel_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1218-37] Learning rate for epoch 9 is 0.00039725631359033287\n",
      "20/20 [==============================] - 2s 89ms/step - loss: 12.4348 - val_loss: 7.1789\n",
      "Epoch 10/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 12.9368\n",
      "Epoch 00010: val_loss did not improve from 7.17889\n",
      "\n",
      "[20210302-1218-42] Learning rate for epoch 10 is 0.0004464008961804211\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 12.9368 - val_loss: 8.9522\n",
      "Epoch 11/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.6989\n",
      "Epoch 00011: val_loss did not improve from 7.17889\n",
      "\n",
      "[20210302-1218-46] Learning rate for epoch 11 is 0.0004954455071128905\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 11.6989 - val_loss: 10.6232\n",
      "Epoch 12/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 12.0971\n",
      "Epoch 00012: val_loss did not improve from 7.17889\n",
      "\n",
      "[20210302-1218-51] Learning rate for epoch 12 is 0.0005443901172839105\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 12.0971 - val_loss: 8.3225\n",
      "Epoch 13/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.6244\n",
      "Epoch 00013: val_loss improved from 7.17889 to 6.18399, saving model to ./20210301-225844/heel_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1218-57] Learning rate for epoch 13 is 0.0005932347266934812\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 11.6244 - val_loss: 6.1840\n",
      "Epoch 14/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.3192\n",
      "Epoch 00014: val_loss did not improve from 6.18399\n",
      "\n",
      "[20210302-1219-02] Learning rate for epoch 14 is 0.0006419793353416026\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 11.3192 - val_loss: 7.7525\n",
      "Epoch 15/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.6575\n",
      "Epoch 00015: val_loss did not improve from 6.18399\n",
      "\n",
      "[20210302-1219-06] Learning rate for epoch 15 is 0.0006906240014359355\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 10.6575 - val_loss: 6.5367\n",
      "Epoch 16/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.9481\n",
      "Epoch 00016: val_loss did not improve from 6.18399\n",
      "\n",
      "[20210302-1219-11] Learning rate for epoch 16 is 0.0007391686085611582\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 9.9481 - val_loss: 8.3078\n",
      "Epoch 17/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.5133\n",
      "Epoch 00017: val_loss did not improve from 6.18399\n",
      "\n",
      "[20210302-1219-16] Learning rate for epoch 17 is 0.0007876132731325924\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 9.5133 - val_loss: 7.2350\n",
      "Epoch 18/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.3737\n",
      "Epoch 00018: val_loss did not improve from 6.18399\n",
      "\n",
      "[20210302-1219-21] Learning rate for epoch 18 is 0.0008359579369425774\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 9.3737 - val_loss: 7.6204\n",
      "Epoch 19/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.8935\n",
      "Epoch 00019: val_loss improved from 6.18399 to 4.65238, saving model to ./20210301-225844/heel_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1219-26] Learning rate for epoch 19 is 0.000884202599991113\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 8.8935 - val_loss: 4.6524\n",
      "Epoch 20/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.3388\n",
      "Epoch 00020: val_loss improved from 4.65238 to 4.16690, saving model to ./20210301-225844/heel_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1219-31] Learning rate for epoch 20 is 0.0009323473204858601\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 8.3388 - val_loss: 4.1669\n",
      "Epoch 21/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.8766\n",
      "Epoch 00021: val_loss did not improve from 4.16690\n",
      "\n",
      "[20210302-1219-36] Learning rate for epoch 21 is 0.000980391982011497\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 8.8766 - val_loss: 5.4770\n",
      "Epoch 22/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.3184\n",
      "Epoch 00022: val_loss did not improve from 4.16690\n",
      "\n",
      "[20210302-1219-41] Learning rate for epoch 22 is 0.0010283367009833455\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 8.3184 - val_loss: 6.4592\n",
      "Epoch 23/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8635\n",
      "Epoch 00023: val_loss did not improve from 4.16690\n",
      "\n",
      "[20210302-1219-46] Learning rate for epoch 23 is 0.0010761814191937447\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.8635 - val_loss: 4.3756\n",
      "Epoch 24/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.9093\n",
      "Epoch 00024: val_loss improved from 4.16690 to 3.97419, saving model to ./20210301-225844/heel_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1219-51] Learning rate for epoch 24 is 0.0011239261366426945\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 7.9093 - val_loss: 3.9742\n",
      "Epoch 25/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5316\n",
      "Epoch 00025: val_loss improved from 3.97419 to 3.77241, saving model to ./20210301-225844/heel_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1219-56] Learning rate for epoch 25 is 0.001171570853330195\n",
      "20/20 [==============================] - 2s 92ms/step - loss: 7.5316 - val_loss: 3.7724\n",
      "Epoch 26/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8050\n",
      "Epoch 00026: val_loss did not improve from 3.77241\n",
      "\n",
      "[20210302-1220-01] Learning rate for epoch 26 is 0.001219115569256246\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 7.8050 - val_loss: 6.2293\n",
      "Epoch 27/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8497\n",
      "Epoch 00027: val_loss did not improve from 3.77241\n",
      "\n",
      "[20210302-1220-06] Learning rate for epoch 27 is 0.0012665604008361697\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.8497 - val_loss: 6.1051\n",
      "Epoch 28/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6333\n",
      "Epoch 00028: val_loss improved from 3.77241 to 3.76812, saving model to ./20210301-225844/heel_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1220-11] Learning rate for epoch 28 is 0.0013139051152393222\n",
      "20/20 [==============================] - 2s 92ms/step - loss: 7.6333 - val_loss: 3.7681\n",
      "Epoch 29/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5275\n",
      "Epoch 00029: val_loss did not improve from 3.76812\n",
      "\n",
      "[20210302-1220-16] Learning rate for epoch 29 is 0.0013611499452963471\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.5275 - val_loss: 3.9645\n",
      "Epoch 30/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6776\n",
      "Epoch 00030: val_loss did not improve from 3.76812\n",
      "\n",
      "[20210302-1220-21] Learning rate for epoch 30 is 0.001408294658176601\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.6776 - val_loss: 4.7440\n",
      "Epoch 31/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3498\n",
      "Epoch 00031: val_loss did not improve from 3.76812\n",
      "\n",
      "[20210302-1220-26] Learning rate for epoch 31 is 0.0014553394867107272\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.3498 - val_loss: 4.1341\n",
      "Epoch 32/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6118\n",
      "Epoch 00032: val_loss did not improve from 3.76812\n",
      "\n",
      "[20210302-1220-31] Learning rate for epoch 32 is 0.0015022843144834042\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.6118 - val_loss: 4.6414\n",
      "Epoch 33/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5076\n",
      "Epoch 00033: val_loss did not improve from 3.76812\n",
      "\n",
      "[20210302-1220-35] Learning rate for epoch 33 is 0.0015491291414946318\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.5076 - val_loss: 3.8366\n",
      "Epoch 34/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3001\n",
      "Epoch 00034: val_loss did not improve from 3.76812\n",
      "\n",
      "[20210302-1220-40] Learning rate for epoch 34 is 0.00159587396774441\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 7.3001 - val_loss: 3.9164\n",
      "Epoch 35/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7324\n",
      "Epoch 00035: val_loss did not improve from 3.76812\n",
      "\n",
      "[20210302-1220-45] Learning rate for epoch 35 is 0.001642518793232739\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.7324 - val_loss: 4.4585\n",
      "Epoch 36/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9802\n",
      "Epoch 00036: val_loss did not improve from 3.76812\n",
      "\n",
      "[20210302-1220-50] Learning rate for epoch 36 is 0.0016890636179596186\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.9802 - val_loss: 4.9178\n",
      "Epoch 37/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9288\n",
      "Epoch 00037: val_loss did not improve from 3.76812\n",
      "\n",
      "[20210302-1220-55] Learning rate for epoch 37 is 0.0017355084419250488\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.9288 - val_loss: 4.2164\n",
      "Epoch 38/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2037\n",
      "Epoch 00038: val_loss did not improve from 3.76812\n",
      "\n",
      "[20210302-1221-00] Learning rate for epoch 38 is 0.0017818533815443516\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 7.2037 - val_loss: 5.1847\n",
      "Epoch 39/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4232\n",
      "Epoch 00039: val_loss did not improve from 3.76812\n",
      "\n",
      "[20210302-1221-04] Learning rate for epoch 39 is 0.0018280982039868832\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.4232 - val_loss: 5.5593\n",
      "Epoch 40/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3330\n",
      "Epoch 00040: val_loss did not improve from 3.76812\n",
      "\n",
      "[20210302-1221-09] Learning rate for epoch 40 is 0.0018742431420832872\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.3330 - val_loss: 4.6106\n",
      "Epoch 41/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9015\n",
      "Epoch 00041: val_loss did not improve from 3.76812\n",
      "\n",
      "[20210302-1221-14] Learning rate for epoch 41 is 0.0019202879630029202\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.9015 - val_loss: 4.1028\n",
      "Epoch 42/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4020\n",
      "Epoch 00042: val_loss did not improve from 3.76812\n",
      "\n",
      "[20210302-1221-19] Learning rate for epoch 42 is 0.0019662328995764256\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.4020 - val_loss: 5.7846\n",
      "Epoch 43/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4872\n",
      "Epoch 00043: val_loss did not improve from 3.76812\n",
      "\n",
      "[20210302-1221-24] Learning rate for epoch 43 is 0.00201207771897316\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.4872 - val_loss: 3.9438\n",
      "Epoch 44/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6072\n",
      "Epoch 00044: val_loss did not improve from 3.76812\n",
      "\n",
      "[20210302-1221-28] Learning rate for epoch 44 is 0.0020578226540237665\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.6072 - val_loss: 3.9698\n",
      "Epoch 45/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0995\n",
      "Epoch 00045: val_loss did not improve from 3.76812\n",
      "\n",
      "[20210302-1221-33] Learning rate for epoch 45 is 0.0021034677047282457\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.0995 - val_loss: 4.8981\n",
      "Epoch 46/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0141\n",
      "Epoch 00046: val_loss did not improve from 3.76812\n",
      "\n",
      "[20210302-1221-38] Learning rate for epoch 46 is 0.002149012638255954\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 7.0141 - val_loss: 5.6601\n",
      "Epoch 47/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6238\n",
      "Epoch 00047: val_loss did not improve from 3.76812\n",
      "\n",
      "[20210302-1221-43] Learning rate for epoch 47 is 0.0021944576874375343\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 7.6238 - val_loss: 14.0619\n",
      "Epoch 48/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8567\n",
      "Epoch 00048: val_loss did not improve from 3.76812\n",
      "\n",
      "[20210302-1221-48] Learning rate for epoch 48 is 0.0022398026194423437\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.8567 - val_loss: 14.7142\n",
      "Epoch 49/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8324\n",
      "Epoch 00049: val_loss did not improve from 3.76812\n",
      "\n",
      "[20210302-1221-52] Learning rate for epoch 49 is 0.002285047434270382\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.8324 - val_loss: 5.7682\n",
      "Epoch 50/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0768\n",
      "Epoch 00050: val_loss did not improve from 3.76812\n",
      "\n",
      "[20210302-1221-57] Learning rate for epoch 50 is 0.0023301925975829363\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.0768 - val_loss: 6.1745\n",
      "Epoch 51/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8986\n",
      "Epoch 00051: val_loss did not improve from 3.76812\n",
      "\n",
      "[20210302-1222-02] Learning rate for epoch 51 is 0.002375237410888076\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.8986 - val_loss: 19.0490\n",
      "Epoch 52/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0734\n",
      "Epoch 00052: val_loss did not improve from 3.76812\n",
      "\n",
      "[20210302-1222-07] Learning rate for epoch 52 is 0.0024201825726777315\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.0734 - val_loss: 9.9829\n",
      "Epoch 53/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7637\n",
      "Epoch 00053: val_loss did not improve from 3.76812\n",
      "\n",
      "[20210302-1222-12] Learning rate for epoch 53 is 0.002465027617290616\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.7637 - val_loss: 4.5907\n",
      "Epoch 54/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0687\n",
      "Epoch 00054: val_loss did not improve from 3.76812\n",
      "\n",
      "[20210302-1222-16] Learning rate for epoch 54 is 0.0025097725447267294\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.0687 - val_loss: 4.7590\n",
      "Epoch 55/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8435\n",
      "Epoch 00055: val_loss did not improve from 3.76812\n",
      "\n",
      "[20210302-1222-21] Learning rate for epoch 55 is 0.0025544175878167152\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.8435 - val_loss: 5.7280\n",
      "Epoch 56/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5560\n",
      "Epoch 00056: val_loss did not improve from 3.76812\n",
      "\n",
      "[20210302-1222-26] Learning rate for epoch 56 is 0.00259896251372993\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.5560 - val_loss: 5.0505\n",
      "Epoch 57/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1796\n",
      "Epoch 00057: val_loss did not improve from 3.76812\n",
      "\n",
      "[20210302-1222-31] Learning rate for epoch 57 is 0.0026434077881276608\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.1796 - val_loss: 4.5037\n",
      "Epoch 58/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5444\n",
      "Epoch 00058: val_loss did not improve from 3.76812\n",
      "\n",
      "[20210302-1222-36] Learning rate for epoch 58 is 0.0026877527125179768\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.5444 - val_loss: 3.9296\n",
      "Epoch 59/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8506\n",
      "Epoch 00059: val_loss did not improve from 3.76812\n",
      "\n",
      "[20210302-1222-40] Learning rate for epoch 59 is 0.0027319977525621653\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.8506 - val_loss: 7.3048\n",
      "Epoch 60/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8393\n",
      "Epoch 00060: val_loss did not improve from 3.76812\n",
      "\n",
      "[20210302-1222-45] Learning rate for epoch 60 is 0.0027761429082602262\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.8393 - val_loss: 6.8694\n",
      "Epoch 61/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1671\n",
      "Epoch 00061: val_loss did not improve from 3.76812\n",
      "\n",
      "[20210302-1222-50] Learning rate for epoch 61 is 0.002820187946781516\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.1671 - val_loss: 7.5124\n",
      "Epoch 62/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.0952\n",
      "Epoch 00062: val_loss did not improve from 3.76812\n",
      "\n",
      "[20210302-1222-55] Learning rate for epoch 62 is 0.0028641331009566784\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 8.0952 - val_loss: 12.2591\n",
      "Epoch 63/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3691\n",
      "Epoch 00063: val_loss did not improve from 3.76812\n",
      "\n",
      "[20210302-1223-00] Learning rate for epoch 63 is 0.0029079781379550695\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.3691 - val_loss: 9.1702\n",
      "Epoch 64/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5673\n",
      "Epoch 00064: val_loss did not improve from 3.76812\n",
      "\n",
      "[20210302-1223-04] Learning rate for epoch 64 is 0.002951723290607333\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.5673 - val_loss: 5.9221\n",
      "Epoch 65/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0834\n",
      "Epoch 00065: val_loss did not improve from 3.76812\n",
      "\n",
      "[20210302-1223-09] Learning rate for epoch 65 is 0.0029953685589134693\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 7.0834 - val_loss: 4.7928\n",
      "Epoch 66/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1265\n",
      "Epoch 00066: val_loss did not improve from 3.76812\n",
      "\n",
      "[20210302-1223-14] Learning rate for epoch 66 is 0.0030389137100428343\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.1265 - val_loss: 6.8251\n",
      "Epoch 67/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3571\n",
      "Epoch 00067: val_loss did not improve from 3.76812\n",
      "\n",
      "[20210302-1223-19] Learning rate for epoch 67 is 0.003082358743995428\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.3571 - val_loss: 5.4748\n",
      "Epoch 68/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9534\n",
      "Epoch 00068: val_loss did not improve from 3.76812\n",
      "\n",
      "[20210302-1223-24] Learning rate for epoch 68 is 0.0031257038936018944\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.9534 - val_loss: 5.0172\n",
      "Epoch 69/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7045\n",
      "Epoch 00069: val_loss did not improve from 3.76812\n",
      "\n",
      "[20210302-1223-28] Learning rate for epoch 69 is 0.003168949158862233\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.7045 - val_loss: 5.7549\n",
      "Epoch 70/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1515\n",
      "Epoch 00070: val_loss did not improve from 3.76812\n",
      "\n",
      "[20210302-1223-33] Learning rate for epoch 70 is 0.0032120943069458008\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.1515 - val_loss: 5.6363\n",
      "Epoch 71/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1295\n",
      "Epoch 00071: val_loss did not improve from 3.76812\n",
      "\n",
      "[20210302-1223-38] Learning rate for epoch 71 is 0.003255139570683241\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.1295 - val_loss: 7.2490\n",
      "Epoch 72/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3406\n",
      "Epoch 00072: val_loss did not improve from 3.76812\n",
      "\n",
      "[20210302-1223-43] Learning rate for epoch 72 is 0.00329808471724391\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.3406 - val_loss: 7.5595\n",
      "Epoch 73/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8269\n",
      "Epoch 00073: val_loss did not improve from 3.76812\n",
      "\n",
      "[20210302-1223-48] Learning rate for epoch 73 is 0.0033409299794584513\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.8269 - val_loss: 3.8621\n",
      "Epoch 74/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6571\n",
      "Epoch 00074: val_loss did not improve from 3.76812\n",
      "\n",
      "[20210302-1223-53] Learning rate for epoch 74 is 0.0033836751244962215\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.6571 - val_loss: 4.6066\n",
      "Epoch 75/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8763\n",
      "Epoch 00075: val_loss improved from 3.76812 to 3.62345, saving model to ./20210301-225844/heel_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1223-58] Learning rate for epoch 75 is 0.0034263203851878643\n",
      "20/20 [==============================] - 2s 91ms/step - loss: 6.8763 - val_loss: 3.6234\n",
      "Epoch 76/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0094\n",
      "Epoch 00076: val_loss improved from 3.62345 to 3.52899, saving model to ./20210301-225844/heel_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1224-03] Learning rate for epoch 76 is 0.003468865528702736\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 7.0094 - val_loss: 3.5290\n",
      "Epoch 77/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8648\n",
      "Epoch 00077: val_loss did not improve from 3.52899\n",
      "\n",
      "[20210302-1224-08] Learning rate for epoch 77 is 0.00351131078787148\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.8648 - val_loss: 3.6191\n",
      "Epoch 78/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9872\n",
      "Epoch 00078: val_loss did not improve from 3.52899\n",
      "\n",
      "[20210302-1224-13] Learning rate for epoch 78 is 0.0035536561626940966\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.9872 - val_loss: 4.6398\n",
      "Epoch 79/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8696\n",
      "Epoch 00079: val_loss improved from 3.52899 to 3.48310, saving model to ./20210301-225844/heel_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1224-18] Learning rate for epoch 79 is 0.003595901420339942\n",
      "20/20 [==============================] - 2s 91ms/step - loss: 6.8696 - val_loss: 3.4831\n",
      "Epoch 80/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2027\n",
      "Epoch 00080: val_loss did not improve from 3.48310\n",
      "\n",
      "[20210302-1224-23] Learning rate for epoch 80 is 0.00363804679363966\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.2027 - val_loss: 4.2798\n",
      "Epoch 81/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6792\n",
      "Epoch 00081: val_loss did not improve from 3.48310\n",
      "\n",
      "[20210302-1224-28] Learning rate for epoch 81 is 0.0036800920497626066\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.6792 - val_loss: 4.3140\n",
      "Epoch 82/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7912\n",
      "Epoch 00082: val_loss did not improve from 3.48310\n",
      "\n",
      "[20210302-1224-33] Learning rate for epoch 82 is 0.003722037188708782\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.7912 - val_loss: 6.8397\n",
      "Epoch 83/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3617\n",
      "Epoch 00083: val_loss did not improve from 3.48310\n",
      "\n",
      "[20210302-1224-38] Learning rate for epoch 83 is 0.003763882676139474\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 7.3617 - val_loss: 6.5609\n",
      "Epoch 84/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7116\n",
      "Epoch 00084: val_loss did not improve from 3.48310\n",
      "\n",
      "[20210302-1224-43] Learning rate for epoch 84 is 0.0038056280463933945\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.7116 - val_loss: 4.6918\n",
      "Epoch 85/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7071\n",
      "Epoch 00085: val_loss did not improve from 3.48310\n",
      "\n",
      "[20210302-1224-48] Learning rate for epoch 85 is 0.003847273299470544\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.7071 - val_loss: 4.8440\n",
      "Epoch 86/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0655\n",
      "Epoch 00086: val_loss did not improve from 3.48310\n",
      "\n",
      "[20210302-1224-53] Learning rate for epoch 86 is 0.0038888186682015657\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.0655 - val_loss: 3.6545\n",
      "Epoch 87/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3299\n",
      "Epoch 00087: val_loss did not improve from 3.48310\n",
      "\n",
      "[20210302-1224-57] Learning rate for epoch 87 is 0.00393026415258646\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.3299 - val_loss: 3.6109\n",
      "Epoch 88/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1740\n",
      "Epoch 00088: val_loss did not improve from 3.48310\n",
      "\n",
      "[20210302-1225-02] Learning rate for epoch 88 is 0.00397160928696394\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.1740 - val_loss: 3.7020\n",
      "Epoch 89/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5753\n",
      "Epoch 00089: val_loss did not improve from 3.48310\n",
      "\n",
      "[20210302-1225-07] Learning rate for epoch 89 is 0.004012854769825935\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.5753 - val_loss: 4.9975\n",
      "Epoch 90/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5507\n",
      "Epoch 00090: val_loss did not improve from 3.48310\n",
      "\n",
      "[20210302-1225-12] Learning rate for epoch 90 is 0.00405400013551116\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.5507 - val_loss: 4.5210\n",
      "Epoch 91/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0312\n",
      "Epoch 00091: val_loss did not improve from 3.48310\n",
      "\n",
      "[20210302-1225-17] Learning rate for epoch 91 is 0.004095045384019613\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 7.0312 - val_loss: 5.6854\n",
      "Epoch 92/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5914\n",
      "Epoch 00092: val_loss did not improve from 3.48310\n",
      "\n",
      "[20210302-1225-22] Learning rate for epoch 92 is 0.004135990981012583\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.5914 - val_loss: 4.7641\n",
      "Epoch 93/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8950\n",
      "Epoch 00093: val_loss did not improve from 3.48310\n",
      "\n",
      "[20210302-1225-26] Learning rate for epoch 93 is 0.004176836460828781\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.8950 - val_loss: 4.8021\n",
      "Epoch 94/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5222\n",
      "Epoch 00094: val_loss did not improve from 3.48310\n",
      "\n",
      "[20210302-1225-31] Learning rate for epoch 94 is 0.004217581823468208\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.5222 - val_loss: 5.6725\n",
      "Epoch 95/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5098\n",
      "Epoch 00095: val_loss did not improve from 3.48310\n",
      "\n",
      "[20210302-1225-36] Learning rate for epoch 95 is 0.004258227068930864\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.5098 - val_loss: 5.3468\n",
      "Epoch 96/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4805\n",
      "Epoch 00096: val_loss did not improve from 3.48310\n",
      "\n",
      "[20210302-1225-41] Learning rate for epoch 96 is 0.0042987726628780365\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.4805 - val_loss: 4.3710\n",
      "Epoch 97/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3601\n",
      "Epoch 00097: val_loss did not improve from 3.48310\n",
      "\n",
      "[20210302-1225-46] Learning rate for epoch 97 is 0.0043392181396484375\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.3601 - val_loss: 3.6969\n",
      "Epoch 98/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6096\n",
      "Epoch 00098: val_loss did not improve from 3.48310\n",
      "\n",
      "[20210302-1225-50] Learning rate for epoch 98 is 0.004379563499242067\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.6096 - val_loss: 5.0525\n",
      "Epoch 99/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1889\n",
      "Epoch 00099: val_loss did not improve from 3.48310\n",
      "\n",
      "[20210302-1225-55] Learning rate for epoch 99 is 0.004419809207320213\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.1889 - val_loss: 4.7300\n",
      "Epoch 100/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3144\n",
      "Epoch 00100: val_loss did not improve from 3.48310\n",
      "\n",
      "[20210302-1226-00] Learning rate for epoch 100 is 0.004459954332560301\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.3144 - val_loss: 4.9792\n",
      "Epoch 101/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3152\n",
      "Epoch 00101: val_loss did not improve from 3.48310\n",
      "\n",
      "[20210302-1226-05] Learning rate for epoch 101 is 4.4999998749517545e-07\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.3152 - val_loss: 4.4279\n",
      "Epoch 102/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9536\n",
      "Epoch 00102: val_loss did not improve from 3.48310\n",
      "\n",
      "[20210302-1226-09] Learning rate for epoch 102 is 0.00018023152370005846\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.9536 - val_loss: 3.9123\n",
      "Epoch 103/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4333\n",
      "Epoch 00103: val_loss did not improve from 3.48310\n",
      "\n",
      "[20210302-1226-14] Learning rate for epoch 103 is 0.000359613070031628\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.4333 - val_loss: 4.0073\n",
      "Epoch 104/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0652\n",
      "Epoch 00104: val_loss did not improve from 3.48310\n",
      "\n",
      "[20210302-1226-19] Learning rate for epoch 104 is 0.0005385946715250611\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.0652 - val_loss: 4.5428\n",
      "Epoch 105/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3202\n",
      "Epoch 00105: val_loss improved from 3.48310 to 3.25046, saving model to ./20210301-225844/heel_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1226-24] Learning rate for epoch 105 is 0.0007171762990765274\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 6.3202 - val_loss: 3.2505\n",
      "Epoch 106/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8112\n",
      "Epoch 00106: val_loss did not improve from 3.25046\n",
      "\n",
      "[20210302-1226-29] Learning rate for epoch 106 is 0.0008953579817898571\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.8112 - val_loss: 3.6644\n",
      "Epoch 107/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0505\n",
      "Epoch 00107: val_loss did not improve from 3.25046\n",
      "\n",
      "[20210302-1226-34] Learning rate for epoch 107 is 0.0010731397196650505\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.0505 - val_loss: 3.5296\n",
      "Epoch 108/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9812\n",
      "Epoch 00108: val_loss did not improve from 3.25046\n",
      "\n",
      "[20210302-1226-39] Learning rate for epoch 108 is 0.0012505215127021074\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.9812 - val_loss: 3.2762\n",
      "Epoch 109/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1558\n",
      "Epoch 00109: val_loss did not improve from 3.25046\n",
      "\n",
      "[20210302-1226-43] Learning rate for epoch 109 is 0.001427503302693367\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.1558 - val_loss: 3.2903\n",
      "Epoch 110/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1117\n",
      "Epoch 00110: val_loss did not improve from 3.25046\n",
      "\n",
      "[20210302-1226-48] Learning rate for epoch 110 is 0.0016040850896388292\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.1117 - val_loss: 3.5635\n",
      "Epoch 111/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2742\n",
      "Epoch 00111: val_loss did not improve from 3.25046\n",
      "\n",
      "[20210302-1226-53] Learning rate for epoch 111 is 0.001780266989953816\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.2742 - val_loss: 3.6618\n",
      "Epoch 112/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8184\n",
      "Epoch 00112: val_loss did not improve from 3.25046\n",
      "\n",
      "[20210302-1226-58] Learning rate for epoch 112 is 0.0019560488872230053\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.8184 - val_loss: 3.5759\n",
      "Epoch 113/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8991\n",
      "Epoch 00113: val_loss did not improve from 3.25046\n",
      "\n",
      "[20210302-1227-03] Learning rate for epoch 113 is 0.002131430897861719\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8991 - val_loss: 3.3409\n",
      "Epoch 114/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0623\n",
      "Epoch 00114: val_loss improved from 3.25046 to 3.21020, saving model to ./20210301-225844/heel_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1227-08] Learning rate for epoch 114 is 0.0023064129054546356\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 6.0623 - val_loss: 3.2102\n",
      "Epoch 115/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0116\n",
      "Epoch 00115: val_loss did not improve from 3.21020\n",
      "\n",
      "[20210302-1227-13] Learning rate for epoch 115 is 0.0024809949100017548\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.0116 - val_loss: 6.0863\n",
      "Epoch 116/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8667\n",
      "Epoch 00116: val_loss did not improve from 3.21020\n",
      "\n",
      "[20210302-1227-18] Learning rate for epoch 116 is 0.0026551769115030766\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.8667 - val_loss: 3.9468\n",
      "Epoch 117/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1569\n",
      "Epoch 00117: val_loss did not improve from 3.21020\n",
      "\n",
      "[20210302-1227-22] Learning rate for epoch 117 is 0.0028289591427892447\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.1569 - val_loss: 6.9736\n",
      "Epoch 118/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6072\n",
      "Epoch 00118: val_loss did not improve from 3.21020\n",
      "\n",
      "[20210302-1227-27] Learning rate for epoch 118 is 0.0030023413710296154\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.6072 - val_loss: 4.3903\n",
      "Epoch 119/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2138\n",
      "Epoch 00119: val_loss did not improve from 3.21020\n",
      "\n",
      "[20210302-1227-32] Learning rate for epoch 119 is 0.003175323596224189\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.2138 - val_loss: 5.4171\n",
      "Epoch 120/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5758\n",
      "Epoch 00120: val_loss did not improve from 3.21020\n",
      "\n",
      "[20210302-1227-37] Learning rate for epoch 120 is 0.003347905818372965\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.5758 - val_loss: 3.6257\n",
      "Epoch 121/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6340\n",
      "Epoch 00121: val_loss did not improve from 3.21020\n",
      "\n",
      "[20210302-1227-42] Learning rate for epoch 121 is 0.0035200880374759436\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.6340 - val_loss: 3.4880\n",
      "Epoch 122/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0691\n",
      "Epoch 00122: val_loss did not improve from 3.21020\n",
      "\n",
      "[20210302-1227-47] Learning rate for epoch 122 is 0.003691870253533125\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.0691 - val_loss: 5.2153\n",
      "Epoch 123/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5306\n",
      "Epoch 00123: val_loss did not improve from 3.21020\n",
      "\n",
      "[20210302-1227-51] Learning rate for epoch 123 is 0.0038632526993751526\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.5306 - val_loss: 3.9402\n",
      "Epoch 124/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3229\n",
      "Epoch 00124: val_loss did not improve from 3.21020\n",
      "\n",
      "[20210302-1227-56] Learning rate for epoch 124 is 0.004034235142171383\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.3229 - val_loss: 4.7282\n",
      "Epoch 125/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5878\n",
      "Epoch 00125: val_loss did not improve from 3.21020\n",
      "\n",
      "[20210302-1228-01] Learning rate for epoch 125 is 0.004204817581921816\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.5878 - val_loss: 4.1619\n",
      "Epoch 126/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4573\n",
      "Epoch 00126: val_loss did not improve from 3.21020\n",
      "\n",
      "[20210302-1228-06] Learning rate for epoch 126 is 0.0043750000186264515\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.4573 - val_loss: 4.8840\n",
      "Epoch 127/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3410\n",
      "Epoch 00127: val_loss did not improve from 3.21020\n",
      "\n",
      "[20210302-1228-11] Learning rate for epoch 127 is 0.0041952175088226795\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.3410 - val_loss: 4.0898\n",
      "Epoch 128/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3965\n",
      "Epoch 00128: val_loss did not improve from 3.21020\n",
      "\n",
      "[20210302-1228-15] Learning rate for epoch 128 is 0.004015835002064705\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.3965 - val_loss: 3.9426\n",
      "Epoch 129/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5227\n",
      "Epoch 00129: val_loss did not improve from 3.21020\n",
      "\n",
      "[20210302-1228-20] Learning rate for epoch 129 is 0.003836852265521884\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.5227 - val_loss: 3.7149\n",
      "Epoch 130/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4943\n",
      "Epoch 00130: val_loss did not improve from 3.21020\n",
      "\n",
      "[20210302-1228-25] Learning rate for epoch 130 is 0.003658269764855504\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.4943 - val_loss: 3.3294\n",
      "Epoch 131/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1991\n",
      "Epoch 00131: val_loss did not improve from 3.21020\n",
      "\n",
      "[20210302-1228-30] Learning rate for epoch 131 is 0.003480087034404278\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.1991 - val_loss: 3.7849\n",
      "Epoch 132/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0779\n",
      "Epoch 00132: val_loss did not improve from 3.21020\n",
      "\n",
      "[20210302-1228-35] Learning rate for epoch 132 is 0.003302304306998849\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.0779 - val_loss: 3.5875\n",
      "Epoch 133/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2671\n",
      "Epoch 00133: val_loss did not improve from 3.21020\n",
      "\n",
      "[20210302-1228-39] Learning rate for epoch 133 is 0.0031249215826392174\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.2671 - val_loss: 3.5457\n",
      "Epoch 134/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9308\n",
      "Epoch 00134: val_loss did not improve from 3.21020\n",
      "\n",
      "[20210302-1228-44] Learning rate for epoch 134 is 0.0029479386284947395\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.9308 - val_loss: 4.1804\n",
      "Epoch 135/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9907\n",
      "Epoch 00135: val_loss did not improve from 3.21020\n",
      "\n",
      "[20210302-1228-49] Learning rate for epoch 135 is 0.0027713559102267027\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.9907 - val_loss: 3.4161\n",
      "Epoch 136/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4524\n",
      "Epoch 00136: val_loss did not improve from 3.21020\n",
      "\n",
      "[20210302-1228-54] Learning rate for epoch 136 is 0.0025951729621738195\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.4524 - val_loss: 3.5760\n",
      "Epoch 137/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6514\n",
      "Epoch 00137: val_loss improved from 3.21020 to 3.17193, saving model to ./20210301-225844/heel_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1228-59] Learning rate for epoch 137 is 0.0024193900171667337\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 5.6514 - val_loss: 3.1719\n",
      "Epoch 138/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9190\n",
      "Epoch 00138: val_loss did not improve from 3.17193\n",
      "\n",
      "[20210302-1229-04] Learning rate for epoch 138 is 0.0022440070752054453\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.9190 - val_loss: 3.2895\n",
      "Epoch 139/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1102\n",
      "Epoch 00139: val_loss did not improve from 3.17193\n",
      "\n",
      "[20210302-1229-09] Learning rate for epoch 139 is 0.002069024136289954\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.1102 - val_loss: 3.3571\n",
      "Epoch 140/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1309\n",
      "Epoch 00140: val_loss did not improve from 3.17193\n",
      "\n",
      "[20210302-1229-14] Learning rate for epoch 140 is 0.0018944410840049386\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.1309 - val_loss: 3.6846\n",
      "Epoch 141/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9567\n",
      "Epoch 00141: val_loss did not improve from 3.17193\n",
      "\n",
      "[20210302-1229-19] Learning rate for epoch 141 is 0.0017202580347657204\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.9567 - val_loss: 3.3099\n",
      "Epoch 142/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1259\n",
      "Epoch 00142: val_loss did not improve from 3.17193\n",
      "\n",
      "[20210302-1229-23] Learning rate for epoch 142 is 0.0015464748721569777\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.1259 - val_loss: 3.3476\n",
      "Epoch 143/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3666\n",
      "Epoch 00143: val_loss improved from 3.17193 to 3.14004, saving model to ./20210301-225844/heel_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1229-29] Learning rate for epoch 143 is 0.0013730917125940323\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 6.3666 - val_loss: 3.1400\n",
      "Epoch 144/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5407\n",
      "Epoch 00144: val_loss did not improve from 3.14004\n",
      "\n",
      "[20210302-1229-34] Learning rate for epoch 144 is 0.0012001085560768843\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.5407 - val_loss: 3.8834\n",
      "Epoch 145/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8779\n",
      "Epoch 00145: val_loss did not improve from 3.14004\n",
      "\n",
      "[20210302-1229-38] Learning rate for epoch 145 is 0.0010275252861902118\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.8779 - val_loss: 3.4924\n",
      "Epoch 146/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8816\n",
      "Epoch 00146: val_loss did not improve from 3.14004\n",
      "\n",
      "[20210302-1229-43] Learning rate for epoch 146 is 0.0008553420193493366\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8816 - val_loss: 3.3597\n",
      "Epoch 147/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0877\n",
      "Epoch 00147: val_loss improved from 3.14004 to 3.06957, saving model to ./20210301-225844/heel_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1229-49] Learning rate for epoch 147 is 0.0006835586973465979\n",
      "20/20 [==============================] - 2s 87ms/step - loss: 6.0877 - val_loss: 3.0696\n",
      "Epoch 148/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6673\n",
      "Epoch 00148: val_loss improved from 3.06957 to 3.01858, saving model to ./20210301-225844/heel_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1229-54] Learning rate for epoch 148 is 0.0005121753201819956\n",
      "20/20 [==============================] - 2s 91ms/step - loss: 5.6673 - val_loss: 3.0186\n",
      "Epoch 149/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8984\n",
      "Epoch 00149: val_loss did not improve from 3.01858\n",
      "\n",
      "[20210302-1229-59] Learning rate for epoch 149 is 0.00034119191695936024\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.8984 - val_loss: 3.0310\n",
      "Epoch 150/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5988\n",
      "Epoch 00150: val_loss did not improve from 3.01858\n",
      "\n",
      "[20210302-1230-04] Learning rate for epoch 150 is 0.00017060847312677652\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.5988 - val_loss: 3.0485\n",
      "Epoch 151/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7939\n",
      "Epoch 00151: val_loss did not improve from 3.01858\n",
      "\n",
      "[20210302-1230-08] Learning rate for epoch 151 is 4.249999960848072e-07\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.7939 - val_loss: 3.0717\n",
      "Epoch 152/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6975\n",
      "Epoch 00152: val_loss improved from 3.01858 to 3.00764, saving model to ./20210301-225844/heel_K10_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1230-14] Learning rate for epoch 152 is 0.00017020752420648932\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 5.6975 - val_loss: 3.0076\n",
      "Epoch 153/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4640\n",
      "Epoch 00153: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1230-19] Learning rate for epoch 153 is 0.0003395900712348521\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.4640 - val_loss: 3.0158\n",
      "Epoch 154/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6288\n",
      "Epoch 00154: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1230-24] Learning rate for epoch 154 is 0.0005085726734250784\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.6288 - val_loss: 3.2227\n",
      "Epoch 155/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5136\n",
      "Epoch 00155: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1230-28] Learning rate for epoch 155 is 0.0006771553307771683\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5136 - val_loss: 3.5234\n",
      "Epoch 156/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6064\n",
      "Epoch 00156: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1230-33] Learning rate for epoch 156 is 0.0008453379850834608\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.6064 - val_loss: 3.0428\n",
      "Epoch 157/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8594\n",
      "Epoch 00157: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1230-38] Learning rate for epoch 157 is 0.0010131207527592778\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.8594 - val_loss: 3.0332\n",
      "Epoch 158/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6500\n",
      "Epoch 00158: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1230-43] Learning rate for epoch 158 is 0.0011805035173892975\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.6500 - val_loss: 3.3757\n",
      "Epoch 159/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9361\n",
      "Epoch 00159: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1230-48] Learning rate for epoch 159 is 0.0013474862789735198\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.9361 - val_loss: 3.4771\n",
      "Epoch 160/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8465\n",
      "Epoch 00160: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1230-53] Learning rate for epoch 160 is 0.0015140691539272666\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.8465 - val_loss: 3.1721\n",
      "Epoch 161/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8449\n",
      "Epoch 00161: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1230-57] Learning rate for epoch 161 is 0.001680252025835216\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.8449 - val_loss: 3.4397\n",
      "Epoch 162/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5431\n",
      "Epoch 00162: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1231-02] Learning rate for epoch 162 is 0.0018460348946973681\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5431 - val_loss: 4.8467\n",
      "Epoch 163/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8565\n",
      "Epoch 00163: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1231-07] Learning rate for epoch 163 is 0.0020114178769290447\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8565 - val_loss: 4.0144\n",
      "Epoch 164/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2442\n",
      "Epoch 00164: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1231-12] Learning rate for epoch 164 is 0.0021764009725302458\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.2442 - val_loss: 3.5770\n",
      "Epoch 165/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5940\n",
      "Epoch 00165: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1231-17] Learning rate for epoch 165 is 0.002340983832255006\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.5940 - val_loss: 3.3334\n",
      "Epoch 166/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9202\n",
      "Epoch 00166: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1231-22] Learning rate for epoch 166 is 0.002505166921764612\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.9202 - val_loss: 5.5279\n",
      "Epoch 167/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9148\n",
      "Epoch 00167: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1231-26] Learning rate for epoch 167 is 0.002668950008228421\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.9148 - val_loss: 21.5537\n",
      "Epoch 168/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1172\n",
      "Epoch 00168: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1231-31] Learning rate for epoch 168 is 0.0028323333244770765\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.1172 - val_loss: 47.0388\n",
      "Epoch 169/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5199\n",
      "Epoch 00169: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1231-36] Learning rate for epoch 169 is 0.002995316404849291\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5199 - val_loss: 33.2378\n",
      "Epoch 170/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8764\n",
      "Epoch 00170: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1231-41] Learning rate for epoch 170 is 0.0031578997150063515\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.8764 - val_loss: 13.8168\n",
      "Epoch 171/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0811\n",
      "Epoch 00171: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1231-46] Learning rate for epoch 171 is 0.0033200830221176147\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.0811 - val_loss: 51.7595\n",
      "Epoch 172/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0884\n",
      "Epoch 00172: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1231-51] Learning rate for epoch 172 is 0.0034818663261830807\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.0884 - val_loss: 4.8641\n",
      "Epoch 173/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9703\n",
      "Epoch 00173: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1231-55] Learning rate for epoch 173 is 0.0036432496272027493\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.9703 - val_loss: 5.1299\n",
      "Epoch 174/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0169\n",
      "Epoch 00174: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1232-00] Learning rate for epoch 174 is 0.003804233158007264\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.0169 - val_loss: 3.8291\n",
      "Epoch 175/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8229\n",
      "Epoch 00175: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1232-05] Learning rate for epoch 175 is 0.003964816685765982\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.8229 - val_loss: 4.4318\n",
      "Epoch 176/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9116\n",
      "Epoch 00176: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1232-10] Learning rate for epoch 176 is 0.004124999977648258\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.9116 - val_loss: 3.6010\n",
      "Epoch 177/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0549\n",
      "Epoch 00177: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1232-15] Learning rate for epoch 177 is 0.003955216612666845\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.0549 - val_loss: 4.3323\n",
      "Epoch 178/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3845\n",
      "Epoch 00178: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1232-20] Learning rate for epoch 178 is 0.003785833017900586\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.3845 - val_loss: 3.7918\n",
      "Epoch 179/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2455\n",
      "Epoch 00179: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1232-24] Learning rate for epoch 179 is 0.0036168494261801243\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.2455 - val_loss: 6.1523\n",
      "Epoch 180/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6430\n",
      "Epoch 00180: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1232-29] Learning rate for epoch 180 is 0.003448265604674816\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.6430 - val_loss: 3.8593\n",
      "Epoch 181/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4079\n",
      "Epoch 00181: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1232-34] Learning rate for epoch 181 is 0.003280082019045949\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.4079 - val_loss: 4.9876\n",
      "Epoch 182/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1157\n",
      "Epoch 00182: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1232-39] Learning rate for epoch 182 is 0.0031122982036322355\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.1157 - val_loss: 4.5892\n",
      "Epoch 183/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8568\n",
      "Epoch 00183: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1232-44] Learning rate for epoch 183 is 0.002944914624094963\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.8568 - val_loss: 3.8448\n",
      "Epoch 184/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6582\n",
      "Epoch 00184: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1232-49] Learning rate for epoch 184 is 0.0027779308147728443\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 5.6582 - val_loss: 3.6017\n",
      "Epoch 185/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8820\n",
      "Epoch 00185: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1232-53] Learning rate for epoch 185 is 0.0026113467756658792\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.8820 - val_loss: 3.5332\n",
      "Epoch 186/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4435\n",
      "Epoch 00186: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1232-58] Learning rate for epoch 186 is 0.002445162972435355\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.4435 - val_loss: 4.7157\n",
      "Epoch 187/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8688\n",
      "Epoch 00187: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1233-03] Learning rate for epoch 187 is 0.0022793791722506285\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.8688 - val_loss: 3.2229\n",
      "Epoch 188/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5411\n",
      "Epoch 00188: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1233-08] Learning rate for epoch 188 is 0.0021139951422810555\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.5411 - val_loss: 3.3653\n",
      "Epoch 189/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5066\n",
      "Epoch 00189: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1233-13] Learning rate for epoch 189 is 0.0019490111153572798\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.5066 - val_loss: 4.3082\n",
      "Epoch 190/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5739\n",
      "Epoch 00190: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1233-17] Learning rate for epoch 190 is 0.0017844270914793015\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5739 - val_loss: 4.1922\n",
      "Epoch 191/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4713\n",
      "Epoch 00191: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1233-22] Learning rate for epoch 191 is 0.0016202429542317986\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.4713 - val_loss: 3.2503\n",
      "Epoch 192/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6595\n",
      "Epoch 00192: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1233-27] Learning rate for epoch 192 is 0.001456458936445415\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.6595 - val_loss: 3.3490\n",
      "Epoch 193/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4998\n",
      "Epoch 00193: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1233-32] Learning rate for epoch 193 is 0.001293074688874185\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.4998 - val_loss: 3.5254\n",
      "Epoch 194/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4191\n",
      "Epoch 00194: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1233-37] Learning rate for epoch 194 is 0.0011300905607640743\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.4191 - val_loss: 3.6534\n",
      "Epoch 195/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8317\n",
      "Epoch 00195: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1233-41] Learning rate for epoch 195 is 0.0009675062610767782\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.8317 - val_loss: 3.2925\n",
      "Epoch 196/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4995\n",
      "Epoch 00196: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1233-46] Learning rate for epoch 196 is 0.0008053220226429403\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.4995 - val_loss: 3.2608\n",
      "Epoch 197/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3711\n",
      "Epoch 00197: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1233-51] Learning rate for epoch 197 is 0.0006435376708395779\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.3711 - val_loss: 3.4324\n",
      "Epoch 198/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1374\n",
      "Epoch 00198: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1233-56] Learning rate for epoch 198 is 0.0004821533220820129\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.1374 - val_loss: 3.4314\n",
      "Epoch 199/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3769\n",
      "Epoch 00199: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1234-01] Learning rate for epoch 199 is 0.0003211689181625843\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.3769 - val_loss: 3.2883\n",
      "Epoch 200/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4919\n",
      "Epoch 00200: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1234-06] Learning rate for epoch 200 is 0.00016058447363320738\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.4919 - val_loss: 3.1877\n",
      "Epoch 201/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4482\n",
      "Epoch 00201: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1234-10] Learning rate for epoch 201 is 4.0000000467443897e-07\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.4482 - val_loss: 3.1839\n",
      "Epoch 202/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3088\n",
      "Epoch 00202: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1234-15] Learning rate for epoch 202 is 0.0001601835247129202\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.3088 - val_loss: 3.2138\n",
      "Epoch 203/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5426\n",
      "Epoch 00203: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1234-20] Learning rate for epoch 203 is 0.00031956707243807614\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.5426 - val_loss: 3.1451\n",
      "Epoch 204/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5624\n",
      "Epoch 00204: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1234-25] Learning rate for epoch 204 is 0.00047855067532509565\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.5624 - val_loss: 3.1512\n",
      "Epoch 205/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1859\n",
      "Epoch 00205: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1234-30] Learning rate for epoch 205 is 0.0006371343042701483\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.1859 - val_loss: 3.5065\n",
      "Epoch 206/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8000\n",
      "Epoch 00206: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1234-34] Learning rate for epoch 206 is 0.0007953179883770645\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.8000 - val_loss: 3.7461\n",
      "Epoch 207/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3121\n",
      "Epoch 00207: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1234-39] Learning rate for epoch 207 is 0.0009531017276458442\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.3121 - val_loss: 3.5346\n",
      "Epoch 208/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8695\n",
      "Epoch 00208: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1234-44] Learning rate for epoch 208 is 0.0011104855220764875\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.8695 - val_loss: 3.3051\n",
      "Epoch 209/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4135\n",
      "Epoch 00209: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1234-49] Learning rate for epoch 209 is 0.0012674692552536726\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.4135 - val_loss: 3.4879\n",
      "Epoch 210/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0033\n",
      "Epoch 00210: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1234-54] Learning rate for epoch 210 is 0.0014240531018003821\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.0033 - val_loss: 3.8510\n",
      "Epoch 211/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4150\n",
      "Epoch 00211: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1234-59] Learning rate for epoch 211 is 0.0015802369453012943\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.4150 - val_loss: 3.4093\n",
      "Epoch 212/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0334\n",
      "Epoch 00212: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1235-04] Learning rate for epoch 212 is 0.001736020902171731\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.0334 - val_loss: 4.1179\n",
      "Epoch 213/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0006\n",
      "Epoch 00213: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1235-09] Learning rate for epoch 213 is 0.0018914048559963703\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.0006 - val_loss: 5.1320\n",
      "Epoch 214/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6278\n",
      "Epoch 00214: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1235-13] Learning rate for epoch 214 is 0.0020463888067752123\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.6278 - val_loss: 3.2381\n",
      "Epoch 215/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5162\n",
      "Epoch 00215: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1235-18] Learning rate for epoch 215 is 0.0022009729873389006\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.5162 - val_loss: 3.2845\n",
      "Epoch 216/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8620\n",
      "Epoch 00216: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1235-23] Learning rate for epoch 216 is 0.002355156932026148\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.8620 - val_loss: 3.9902\n",
      "Epoch 217/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6536\n",
      "Epoch 00217: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1235-28] Learning rate for epoch 217 is 0.0025089411064982414\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.6536 - val_loss: 4.1844\n",
      "Epoch 218/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8363\n",
      "Epoch 00218: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1235-38] Learning rate for epoch 218 is 0.0026623252779245377\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 5.8363 - val_loss: 4.8493\n",
      "Epoch 219/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3658\n",
      "Epoch 00219: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1235-43] Learning rate for epoch 219 is 0.0028153094463050365\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.3658 - val_loss: 3.3054\n",
      "Epoch 220/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9287\n",
      "Epoch 00220: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1235-48] Learning rate for epoch 220 is 0.002967893611639738\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.9287 - val_loss: 4.3444\n",
      "Epoch 221/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5497\n",
      "Epoch 00221: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1235-53] Learning rate for epoch 221 is 0.003120078006759286\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.5497 - val_loss: 3.7808\n",
      "Epoch 222/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6267\n",
      "Epoch 00222: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1235-58] Learning rate for epoch 222 is 0.0032718623988330364\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.6267 - val_loss: 3.7691\n",
      "Epoch 223/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5195\n",
      "Epoch 00223: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1236-03] Learning rate for epoch 223 is 0.0034232467878609896\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.5195 - val_loss: 5.0927\n",
      "Epoch 224/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2877\n",
      "Epoch 00224: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1236-07] Learning rate for epoch 224 is 0.0035742311738431454\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.2877 - val_loss: 6.1068\n",
      "Epoch 225/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6746\n",
      "Epoch 00225: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1236-12] Learning rate for epoch 225 is 0.003724815556779504\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.6746 - val_loss: 4.3397\n",
      "Epoch 226/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8134\n",
      "Epoch 00226: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1236-17] Learning rate for epoch 226 is 0.003874999936670065\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.8134 - val_loss: 4.3656\n",
      "Epoch 227/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7779\n",
      "Epoch 00227: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1236-22] Learning rate for epoch 227 is 0.0037152154836803675\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 5.7779 - val_loss: 3.8243\n",
      "Epoch 228/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7822\n",
      "Epoch 00228: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1236-27] Learning rate for epoch 228 is 0.0035558310337364674\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.7822 - val_loss: 4.2493\n",
      "Epoch 229/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0573\n",
      "Epoch 00229: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1236-32] Learning rate for epoch 229 is 0.003396846354007721\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.0573 - val_loss: 5.1530\n",
      "Epoch 230/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0831\n",
      "Epoch 00230: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1236-37] Learning rate for epoch 230 is 0.003238261677324772\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 6.0831 - val_loss: 3.2864\n",
      "Epoch 231/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8418\n",
      "Epoch 00231: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1236-41] Learning rate for epoch 231 is 0.00308007700368762\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.8418 - val_loss: 3.8771\n",
      "Epoch 232/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2637\n",
      "Epoch 00232: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1236-46] Learning rate for epoch 232 is 0.002922292333096266\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.2637 - val_loss: 3.3156\n",
      "Epoch 233/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1258\n",
      "Epoch 00233: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1236-51] Learning rate for epoch 233 is 0.002764907432720065\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.1258 - val_loss: 3.9645\n",
      "Epoch 234/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6894\n",
      "Epoch 00234: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1236-56] Learning rate for epoch 234 is 0.0026079227682203054\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6894 - val_loss: 3.5158\n",
      "Epoch 235/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8132\n",
      "Epoch 00235: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1237-01] Learning rate for epoch 235 is 0.0024513378739356995\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.8132 - val_loss: 3.0966\n",
      "Epoch 236/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6733\n",
      "Epoch 00236: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1237-06] Learning rate for epoch 236 is 0.002295152982696891\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.6733 - val_loss: 3.9120\n",
      "Epoch 237/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5790\n",
      "Epoch 00237: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1237-10] Learning rate for epoch 237 is 0.0021393680945038795\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 5.5790 - val_loss: 3.8981\n",
      "Epoch 238/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4122\n",
      "Epoch 00238: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1237-15] Learning rate for epoch 238 is 0.0019839832093566656\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.4122 - val_loss: 4.0173\n",
      "Epoch 239/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5981\n",
      "Epoch 00239: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1237-20] Learning rate for epoch 239 is 0.0018289980944246054\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.5981 - val_loss: 3.1681\n",
      "Epoch 240/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7001\n",
      "Epoch 00240: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1237-25] Learning rate for epoch 240 is 0.0016744130989536643\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.7001 - val_loss: 3.1833\n",
      "Epoch 241/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4317\n",
      "Epoch 00241: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1237-30] Learning rate for epoch 241 is 0.0015202279901131988\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.4317 - val_loss: 3.1618\n",
      "Epoch 242/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4937\n",
      "Epoch 00242: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1237-35] Learning rate for epoch 242 is 0.0013664428843185306\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.4937 - val_loss: 3.4962\n",
      "Epoch 243/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7606\n",
      "Epoch 00243: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1237-39] Learning rate for epoch 243 is 0.0012130576651543379\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.7606 - val_loss: 3.7602\n",
      "Epoch 244/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9682\n",
      "Epoch 00244: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1237-44] Learning rate for epoch 244 is 0.0010600725654512644\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.9682 - val_loss: 3.2034\n",
      "Epoch 245/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6078\n",
      "Epoch 00245: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1237-49] Learning rate for epoch 245 is 0.0009074872941710055\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6078 - val_loss: 3.4015\n",
      "Epoch 246/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2641\n",
      "Epoch 00246: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1237-54] Learning rate for epoch 246 is 0.0007553020259365439\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.2641 - val_loss: 3.2178\n",
      "Epoch 247/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2805\n",
      "Epoch 00247: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1237-59] Learning rate for epoch 247 is 0.0006035167025402188\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.2805 - val_loss: 3.0716\n",
      "Epoch 248/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4768\n",
      "Epoch 00248: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1238-04] Learning rate for epoch 248 is 0.00045213132398203015\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.4768 - val_loss: 3.1683\n",
      "Epoch 249/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6360\n",
      "Epoch 00249: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1238-08] Learning rate for epoch 249 is 0.00030114591936580837\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6360 - val_loss: 3.1211\n",
      "Epoch 250/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2489\n",
      "Epoch 00250: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1238-13] Learning rate for epoch 250 is 0.00015056047413963825\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.2489 - val_loss: 3.1621\n",
      "Epoch 251/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4206\n",
      "Epoch 00251: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1238-18] Learning rate for epoch 251 is 3.7500001326407073e-07\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.4206 - val_loss: 3.1578\n",
      "Epoch 252/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5184\n",
      "Epoch 00252: val_loss did not improve from 3.00764\n",
      "\n",
      "[20210302-1238-23] Learning rate for epoch 252 is 0.00015015952521935105\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.5184 - val_loss: 3.0819\n",
      "K= 11\n",
      "Check cache in memory:Y True\n",
      "Check keypoints process01: Tensor(\"args_1:0\", shape=(2,), dtype=int64) (2,) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Check keypoints process02: Tensor(\"Reshape:0\", shape=(1, 2), dtype=int64) (1, 2) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Check keypoints process03: Tensor(\"PyFunc:1\", dtype=float32, device=/job:localhost/replica:0/task:0)\n",
      "Check keypoints process04: Tensor(\"Reshape_1:0\", shape=(2,), dtype=float32)\n",
      "Check augment :Y True\n",
      "Check ds cache[True] and augment[True]\n",
      "Check cache in memory:Y True\n",
      "Check augment :N False\n",
      "Check ds cache[True] and augment[False]\n",
      "best_model_name: ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "base_model.trainable :  False\n",
      "\n",
      "[20210302-1238-26] Learning rate for epoch 1 is 0.00047855067532509565\n",
      "Epoch 1/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 107.0009\n",
      "Epoch 00001: val_loss improved from inf to 76.52335, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 5s 267ms/step - loss: 104.6819 - val_loss: 76.5233\n",
      "\n",
      "[20210302-1238-41] Learning rate for epoch 2 is 0.00047855067532509565\n",
      "Epoch 2/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 42.3638\n",
      "Epoch 00002: val_loss improved from 76.52335 to 27.14904, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 41.7738 - val_loss: 27.1490\n",
      "\n",
      "[20210302-1238-45] Learning rate for epoch 3 is 0.00047855067532509565\n",
      "Epoch 3/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 24.1653\n",
      "Epoch 00003: val_loss improved from 27.14904 to 23.01050, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 8s 404ms/step - loss: 24.1653 - val_loss: 23.0105\n",
      "\n",
      "[20210302-1238-57] Learning rate for epoch 4 is 0.00047855067532509565\n",
      "Epoch 4/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 20.4844\n",
      "Epoch 00004: val_loss improved from 23.01050 to 21.49036, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 20.4429 - val_loss: 21.4904\n",
      "\n",
      "[20210302-1239-01] Learning rate for epoch 5 is 0.00047855067532509565\n",
      "Epoch 5/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 19.6598\n",
      "Epoch 00005: val_loss improved from 21.49036 to 20.64693, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 19.6598 - val_loss: 20.6469\n",
      "\n",
      "[20210302-1239-06] Learning rate for epoch 6 is 0.00047855067532509565\n",
      "Epoch 6/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 18.6769\n",
      "Epoch 00006: val_loss improved from 20.64693 to 19.63069, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 18.7452 - val_loss: 19.6307\n",
      "\n",
      "[20210302-1239-10] Learning rate for epoch 7 is 0.00047855067532509565\n",
      "Epoch 7/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 18.2986\n",
      "Epoch 00007: val_loss improved from 19.63069 to 18.78313, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 18.2986 - val_loss: 18.7831\n",
      "\n",
      "[20210302-1239-14] Learning rate for epoch 8 is 0.00047855067532509565\n",
      "Epoch 8/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 18.4709\n",
      "Epoch 00008: val_loss improved from 18.78313 to 18.50227, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 18.3661 - val_loss: 18.5023\n",
      "\n",
      "[20210302-1239-19] Learning rate for epoch 9 is 0.00047855067532509565\n",
      "Epoch 9/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 17.8301\n",
      "Epoch 00009: val_loss improved from 18.50227 to 18.17037, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 17.8301 - val_loss: 18.1704\n",
      "\n",
      "[20210302-1239-23] Learning rate for epoch 10 is 0.00047855067532509565\n",
      "Epoch 10/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 17.0929\n",
      "Epoch 00010: val_loss improved from 18.17037 to 17.56750, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 17.0929 - val_loss: 17.5675\n",
      "\n",
      "[20210302-1239-27] Learning rate for epoch 11 is 0.00047855067532509565\n",
      "Epoch 11/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 17.4788\n",
      "Epoch 00011: val_loss improved from 17.56750 to 17.43079, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 17.4324 - val_loss: 17.4308\n",
      "\n",
      "[20210302-1239-32] Learning rate for epoch 12 is 0.00047855067532509565\n",
      "Epoch 12/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 17.5925\n",
      "Epoch 00012: val_loss did not improve from 17.43079\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 17.5625 - val_loss: 17.4924\n",
      "\n",
      "[20210302-1239-36] Learning rate for epoch 13 is 0.00047855067532509565\n",
      "Epoch 13/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 18.0163\n",
      "Epoch 00013: val_loss improved from 17.43079 to 16.74666, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 18.1471 - val_loss: 16.7467\n",
      "\n",
      "[20210302-1239-40] Learning rate for epoch 14 is 0.00047855067532509565\n",
      "Epoch 14/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 17.3178\n",
      "Epoch 00014: val_loss improved from 16.74666 to 16.29617, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 17.1354 - val_loss: 16.2962\n",
      "\n",
      "[20210302-1239-44] Learning rate for epoch 15 is 0.00047855067532509565\n",
      "Epoch 15/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 17.0439\n",
      "Epoch 00015: val_loss improved from 16.29617 to 15.80782, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 17.0439 - val_loss: 15.8078\n",
      "\n",
      "[20210302-1239-49] Learning rate for epoch 16 is 0.00047855067532509565\n",
      "Epoch 16/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 17.6025\n",
      "Epoch 00016: val_loss improved from 15.80782 to 15.17400, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 17.6025 - val_loss: 15.1740\n",
      "\n",
      "[20210302-1239-53] Learning rate for epoch 17 is 0.00047855067532509565\n",
      "Epoch 17/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 17.0425\n",
      "Epoch 00017: val_loss improved from 15.17400 to 14.58238, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 16.9876 - val_loss: 14.5824\n",
      "\n",
      "[20210302-1239-58] Learning rate for epoch 18 is 0.00047855067532509565\n",
      "Epoch 18/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.4528\n",
      "Epoch 00018: val_loss improved from 14.58238 to 14.38262, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 16.4839 - val_loss: 14.3826\n",
      "\n",
      "[20210302-1240-02] Learning rate for epoch 19 is 0.00047855067532509565\n",
      "Epoch 19/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 17.0037\n",
      "Epoch 00019: val_loss improved from 14.38262 to 13.87895, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 17.0037 - val_loss: 13.8790\n",
      "\n",
      "[20210302-1240-06] Learning rate for epoch 20 is 0.00047855067532509565\n",
      "Epoch 20/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.7504\n",
      "Epoch 00020: val_loss improved from 13.87895 to 13.60464, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 16.8163 - val_loss: 13.6046\n",
      "\n",
      "[20210302-1240-11] Learning rate for epoch 21 is 0.00047855067532509565\n",
      "Epoch 21/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.5185\n",
      "Epoch 00021: val_loss improved from 13.60464 to 13.09681, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 16.5828 - val_loss: 13.0968\n",
      "\n",
      "[20210302-1240-15] Learning rate for epoch 22 is 0.00047855067532509565\n",
      "Epoch 22/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.8042\n",
      "Epoch 00022: val_loss improved from 13.09681 to 12.91421, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 15.9237 - val_loss: 12.9142\n",
      "\n",
      "[20210302-1240-19] Learning rate for epoch 23 is 0.00047855067532509565\n",
      "Epoch 23/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.1202\n",
      "Epoch 00023: val_loss improved from 12.91421 to 11.94515, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 16.1202 - val_loss: 11.9452\n",
      "\n",
      "[20210302-1240-24] Learning rate for epoch 24 is 0.00047855067532509565\n",
      "Epoch 24/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.5927\n",
      "Epoch 00024: val_loss improved from 11.94515 to 11.93664, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 16.6900 - val_loss: 11.9366\n",
      "\n",
      "[20210302-1240-28] Learning rate for epoch 25 is 0.00047855067532509565\n",
      "Epoch 25/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 17.2269\n",
      "Epoch 00025: val_loss did not improve from 11.93664\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 17.2269 - val_loss: 12.2596\n",
      "\n",
      "[20210302-1240-32] Learning rate for epoch 26 is 0.00047855067532509565\n",
      "Epoch 26/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.8949\n",
      "Epoch 00026: val_loss did not improve from 11.93664\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.8949 - val_loss: 12.4396\n",
      "\n",
      "[20210302-1240-36] Learning rate for epoch 27 is 0.00047855067532509565\n",
      "Epoch 27/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.7712\n",
      "Epoch 00027: val_loss improved from 11.93664 to 11.26335, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 16.7812 - val_loss: 11.2634\n",
      "\n",
      "[20210302-1240-40] Learning rate for epoch 28 is 0.00047855067532509565\n",
      "Epoch 28/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.8480\n",
      "Epoch 00028: val_loss improved from 11.26335 to 11.05737, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 15.8927 - val_loss: 11.0574\n",
      "\n",
      "[20210302-1240-45] Learning rate for epoch 29 is 0.00047855067532509565\n",
      "Epoch 29/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.0811\n",
      "Epoch 00029: val_loss did not improve from 11.05737\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.0811 - val_loss: 11.3348\n",
      "\n",
      "[20210302-1240-49] Learning rate for epoch 30 is 0.00047855067532509565\n",
      "Epoch 30/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.7049\n",
      "Epoch 00030: val_loss improved from 11.05737 to 10.55135, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 15.7111 - val_loss: 10.5513\n",
      "\n",
      "[20210302-1240-53] Learning rate for epoch 31 is 0.00047855067532509565\n",
      "Epoch 31/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.1598\n",
      "Epoch 00031: val_loss did not improve from 10.55135\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.1598 - val_loss: 11.3497\n",
      "\n",
      "[20210302-1240-57] Learning rate for epoch 32 is 0.00047855067532509565\n",
      "Epoch 32/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.1946\n",
      "Epoch 00032: val_loss improved from 10.55135 to 10.54741, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 16.2648 - val_loss: 10.5474\n",
      "\n",
      "[20210302-1241-02] Learning rate for epoch 33 is 0.00047855067532509565\n",
      "Epoch 33/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.2495\n",
      "Epoch 00033: val_loss improved from 10.54741 to 10.21686, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 16.2495 - val_loss: 10.2169\n",
      "\n",
      "[20210302-1241-06] Learning rate for epoch 34 is 0.00047855067532509565\n",
      "Epoch 34/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.2643\n",
      "Epoch 00034: val_loss did not improve from 10.21686\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.1537 - val_loss: 10.3693\n",
      "\n",
      "[20210302-1241-10] Learning rate for epoch 35 is 0.00047855067532509565\n",
      "Epoch 35/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.8580\n",
      "Epoch 00035: val_loss did not improve from 10.21686\n",
      "20/20 [==============================] - 0s 24ms/step - loss: 15.8714 - val_loss: 10.4274\n",
      "\n",
      "[20210302-1241-14] Learning rate for epoch 36 is 0.00047855067532509565\n",
      "Epoch 36/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.8946\n",
      "Epoch 00036: val_loss did not improve from 10.21686\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.8456 - val_loss: 10.5537\n",
      "\n",
      "[20210302-1241-18] Learning rate for epoch 37 is 0.00047855067532509565\n",
      "Epoch 37/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.3515\n",
      "Epoch 00037: val_loss improved from 10.21686 to 10.18277, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.1869 - val_loss: 10.1828\n",
      "\n",
      "[20210302-1241-22] Learning rate for epoch 38 is 0.00047855067532509565\n",
      "Epoch 38/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.2122\n",
      "Epoch 00038: val_loss did not improve from 10.18277\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.1423 - val_loss: 11.0395\n",
      "\n",
      "[20210302-1241-26] Learning rate for epoch 39 is 0.00047855067532509565\n",
      "Epoch 39/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.7212\n",
      "Epoch 00039: val_loss improved from 10.18277 to 10.02500, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 15.7154 - val_loss: 10.0250\n",
      "\n",
      "[20210302-1241-31] Learning rate for epoch 40 is 0.00047855067532509565\n",
      "Epoch 40/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.4436\n",
      "Epoch 00040: val_loss did not improve from 10.02500\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.4063 - val_loss: 11.7140\n",
      "\n",
      "[20210302-1241-35] Learning rate for epoch 41 is 0.00047855067532509565\n",
      "Epoch 41/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 16.2059\n",
      "Epoch 00041: val_loss did not improve from 10.02500\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.2430 - val_loss: 13.2760\n",
      "\n",
      "[20210302-1241-38] Learning rate for epoch 42 is 0.00047855067532509565\n",
      "Epoch 42/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.2629\n",
      "Epoch 00042: val_loss did not improve from 10.02500\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.2420 - val_loss: 13.5333\n",
      "\n",
      "[20210302-1241-42] Learning rate for epoch 43 is 0.00047855067532509565\n",
      "Epoch 43/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.2250\n",
      "Epoch 00043: val_loss did not improve from 10.02500\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.3017 - val_loss: 11.4531\n",
      "\n",
      "[20210302-1241-46] Learning rate for epoch 44 is 0.00047855067532509565\n",
      "Epoch 44/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.9599\n",
      "Epoch 00044: val_loss did not improve from 10.02500\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.9630 - val_loss: 10.0717\n",
      "\n",
      "[20210302-1241-50] Learning rate for epoch 45 is 0.00047855067532509565\n",
      "Epoch 45/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.4529\n",
      "Epoch 00045: val_loss did not improve from 10.02500\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.4529 - val_loss: 10.0443\n",
      "\n",
      "[20210302-1241-54] Learning rate for epoch 46 is 0.00047855067532509565\n",
      "Epoch 46/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.1586\n",
      "Epoch 00046: val_loss did not improve from 10.02500\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.1586 - val_loss: 12.2181\n",
      "\n",
      "[20210302-1241-58] Learning rate for epoch 47 is 0.00047855067532509565\n",
      "Epoch 47/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.7730\n",
      "Epoch 00047: val_loss did not improve from 10.02500\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.7730 - val_loss: 11.8973\n",
      "\n",
      "[20210302-1242-02] Learning rate for epoch 48 is 0.00047855067532509565\n",
      "Epoch 48/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.8185\n",
      "Epoch 00048: val_loss did not improve from 10.02500\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.8185 - val_loss: 11.1834\n",
      "\n",
      "[20210302-1242-06] Learning rate for epoch 49 is 0.00047855067532509565\n",
      "Epoch 49/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.1398\n",
      "Epoch 00049: val_loss improved from 10.02500 to 9.57263, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.1480 - val_loss: 9.5726\n",
      "\n",
      "[20210302-1242-11] Learning rate for epoch 50 is 0.00047855067532509565\n",
      "Epoch 50/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.9363\n",
      "Epoch 00050: val_loss did not improve from 9.57263\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.8928 - val_loss: 9.9513\n",
      "\n",
      "[20210302-1242-15] Learning rate for epoch 51 is 0.00047855067532509565\n",
      "Epoch 51/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.9277\n",
      "Epoch 00051: val_loss did not improve from 9.57263\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.9514 - val_loss: 10.6288\n",
      "\n",
      "[20210302-1242-19] Learning rate for epoch 52 is 0.00047855067532509565\n",
      "Epoch 52/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.5728\n",
      "Epoch 00052: val_loss did not improve from 9.57263\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.7038 - val_loss: 10.8215\n",
      "\n",
      "[20210302-1242-23] Learning rate for epoch 53 is 0.00047855067532509565\n",
      "Epoch 53/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.7648\n",
      "Epoch 00053: val_loss did not improve from 9.57263\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.7841 - val_loss: 11.7893\n",
      "\n",
      "[20210302-1242-27] Learning rate for epoch 54 is 0.00047855067532509565\n",
      "Epoch 54/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.6275\n",
      "Epoch 00054: val_loss did not improve from 9.57263\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.4974 - val_loss: 12.7640\n",
      "\n",
      "[20210302-1242-31] Learning rate for epoch 55 is 0.00047855067532509565\n",
      "Epoch 55/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.0986\n",
      "Epoch 00055: val_loss did not improve from 9.57263\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.1416 - val_loss: 11.2018\n",
      "\n",
      "[20210302-1242-34] Learning rate for epoch 56 is 0.00047855067532509565\n",
      "Epoch 56/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.8337\n",
      "Epoch 00056: val_loss did not improve from 9.57263\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.8226 - val_loss: 10.0891\n",
      "\n",
      "[20210302-1242-38] Learning rate for epoch 57 is 0.00047855067532509565\n",
      "Epoch 57/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.3838\n",
      "Epoch 00057: val_loss did not improve from 9.57263\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.4136 - val_loss: 11.0010\n",
      "\n",
      "[20210302-1242-42] Learning rate for epoch 58 is 0.00047855067532509565\n",
      "Epoch 58/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.5118\n",
      "Epoch 00058: val_loss did not improve from 9.57263\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.4417 - val_loss: 11.3003\n",
      "\n",
      "[20210302-1242-46] Learning rate for epoch 59 is 0.00047855067532509565\n",
      "Epoch 59/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.4867\n",
      "Epoch 00059: val_loss did not improve from 9.57263\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 15.4692 - val_loss: 10.5552\n",
      "\n",
      "[20210302-1242-50] Learning rate for epoch 60 is 0.00047855067532509565\n",
      "Epoch 60/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.3055\n",
      "Epoch 00060: val_loss did not improve from 9.57263\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.3246 - val_loss: 11.1674\n",
      "\n",
      "[20210302-1242-55] Learning rate for epoch 61 is 0.00047855067532509565\n",
      "Epoch 61/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.2915\n",
      "Epoch 00061: val_loss did not improve from 9.57263\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 15.2658 - val_loss: 11.4456\n",
      "\n",
      "[20210302-1242-59] Learning rate for epoch 62 is 0.00047855067532509565\n",
      "Epoch 62/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 14.9737\n",
      "Epoch 00062: val_loss did not improve from 9.57263\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.0943 - val_loss: 10.4527\n",
      "\n",
      "[20210302-1243-03] Learning rate for epoch 63 is 0.00047855067532509565\n",
      "Epoch 63/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 14.9968\n",
      "Epoch 00063: val_loss did not improve from 9.57263\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 15.0154 - val_loss: 9.8262\n",
      "\n",
      "[20210302-1243-07] Learning rate for epoch 64 is 0.00047855067532509565\n",
      "Epoch 64/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 15.6356\n",
      "Epoch 00064: val_loss did not improve from 9.57263\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 15.5610 - val_loss: 9.8966\n",
      "\n",
      "[20210302-1243-11] Learning rate for epoch 65 is 0.00047855067532509565\n",
      "Epoch 65/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.4138\n",
      "Epoch 00065: val_loss did not improve from 9.57263\n",
      "20/20 [==============================] - 0s 24ms/step - loss: 15.4153 - val_loss: 10.3354\n",
      "\n",
      "[20210302-1243-15] Learning rate for epoch 66 is 0.00047855067532509565\n",
      "Epoch 66/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 15.6269\n",
      "Epoch 00066: val_loss improved from 9.57263 to 9.53878, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 15.5678 - val_loss: 9.5388\n",
      "\n",
      "[20210302-1243-19] Learning rate for epoch 67 is 0.00047855067532509565\n",
      "Epoch 67/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.0436\n",
      "Epoch 00067: val_loss did not improve from 9.53878\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.0436 - val_loss: 10.0520\n",
      "\n",
      "[20210302-1243-23] Learning rate for epoch 68 is 0.00047855067532509565\n",
      "Epoch 68/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 15.4964\n",
      "Epoch 00068: val_loss improved from 9.53878 to 9.53803, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 15.5219 - val_loss: 9.5380\n",
      "\n",
      "[20210302-1243-27] Learning rate for epoch 69 is 0.00047855067532509565\n",
      "Epoch 69/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.1824\n",
      "Epoch 00069: val_loss did not improve from 9.53803\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.1824 - val_loss: 9.5626\n",
      "\n",
      "[20210302-1243-31] Learning rate for epoch 70 is 0.00047855067532509565\n",
      "Epoch 70/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.5629\n",
      "Epoch 00070: val_loss did not improve from 9.53803\n",
      "20/20 [==============================] - 0s 24ms/step - loss: 15.5841 - val_loss: 10.1292\n",
      "\n",
      "[20210302-1243-35] Learning rate for epoch 71 is 0.00047855067532509565\n",
      "Epoch 71/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.3453\n",
      "Epoch 00071: val_loss did not improve from 9.53803\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.3453 - val_loss: 10.0578\n",
      "\n",
      "[20210302-1243-39] Learning rate for epoch 72 is 0.00047855067532509565\n",
      "Epoch 72/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.4398\n",
      "Epoch 00072: val_loss did not improve from 9.53803\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.4398 - val_loss: 11.2226\n",
      "\n",
      "[20210302-1243-43] Learning rate for epoch 73 is 0.00047855067532509565\n",
      "Epoch 73/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.7215\n",
      "Epoch 00073: val_loss did not improve from 9.53803\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.7215 - val_loss: 9.5837\n",
      "\n",
      "[20210302-1243-47] Learning rate for epoch 74 is 0.00047855067532509565\n",
      "Epoch 74/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.1577\n",
      "Epoch 00074: val_loss did not improve from 9.53803\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.1474 - val_loss: 10.2841\n",
      "\n",
      "[20210302-1243-51] Learning rate for epoch 75 is 0.00047855067532509565\n",
      "Epoch 75/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.1067\n",
      "Epoch 00075: val_loss did not improve from 9.53803\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.0609 - val_loss: 10.2893\n",
      "\n",
      "[20210302-1243-55] Learning rate for epoch 76 is 0.00047855067532509565\n",
      "Epoch 76/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 14.8420\n",
      "Epoch 00076: val_loss did not improve from 9.53803\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 14.9195 - val_loss: 10.1276\n",
      "\n",
      "[20210302-1243-59] Learning rate for epoch 77 is 0.00047855067532509565\n",
      "Epoch 77/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.3677\n",
      "Epoch 00077: val_loss did not improve from 9.53803\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 15.3353 - val_loss: 9.8303\n",
      "\n",
      "[20210302-1244-03] Learning rate for epoch 78 is 0.00047855067532509565\n",
      "Epoch 78/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.7734\n",
      "Epoch 00078: val_loss did not improve from 9.53803\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.6493 - val_loss: 9.9618\n",
      "\n",
      "[20210302-1244-07] Learning rate for epoch 79 is 0.00047855067532509565\n",
      "Epoch 79/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.3519\n",
      "Epoch 00079: val_loss did not improve from 9.53803\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.2752 - val_loss: 10.2102\n",
      "\n",
      "[20210302-1244-11] Learning rate for epoch 80 is 0.00047855067532509565\n",
      "Epoch 80/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.5137\n",
      "Epoch 00080: val_loss improved from 9.53803 to 9.52673, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 15.5253 - val_loss: 9.5267\n",
      "\n",
      "[20210302-1244-15] Learning rate for epoch 81 is 0.00047855067532509565\n",
      "Epoch 81/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.5366\n",
      "Epoch 00081: val_loss did not improve from 9.52673\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 15.6036 - val_loss: 12.3820\n",
      "\n",
      "[20210302-1244-19] Learning rate for epoch 82 is 0.00047855067532509565\n",
      "Epoch 82/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.5920\n",
      "Epoch 00082: val_loss did not improve from 9.52673\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 15.5920 - val_loss: 12.3041\n",
      "\n",
      "[20210302-1244-23] Learning rate for epoch 83 is 0.00047855067532509565\n",
      "Epoch 83/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.9512\n",
      "Epoch 00083: val_loss did not improve from 9.52673\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.9202 - val_loss: 11.4782\n",
      "\n",
      "[20210302-1244-27] Learning rate for epoch 84 is 0.00047855067532509565\n",
      "Epoch 84/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.6429\n",
      "Epoch 00084: val_loss improved from 9.52673 to 9.48195, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 15.6429 - val_loss: 9.4820\n",
      "\n",
      "[20210302-1244-31] Learning rate for epoch 85 is 0.00047855067532509565\n",
      "Epoch 85/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 15.6520\n",
      "Epoch 00085: val_loss did not improve from 9.48195\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 15.7867 - val_loss: 10.0712\n",
      "\n",
      "[20210302-1244-35] Learning rate for epoch 86 is 0.00047855067532509565\n",
      "Epoch 86/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 15.7412\n",
      "Epoch 00086: val_loss improved from 9.48195 to 9.20664, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 15.5929 - val_loss: 9.2066\n",
      "\n",
      "[20210302-1244-40] Learning rate for epoch 87 is 0.00047855067532509565\n",
      "Epoch 87/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.9126\n",
      "Epoch 00087: val_loss did not improve from 9.20664\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.8981 - val_loss: 9.6813\n",
      "\n",
      "[20210302-1244-44] Learning rate for epoch 88 is 0.00047855067532509565\n",
      "Epoch 88/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 16.0613\n",
      "Epoch 00088: val_loss did not improve from 9.20664\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 15.9005 - val_loss: 9.8959\n",
      "\n",
      "[20210302-1244-48] Learning rate for epoch 89 is 0.00047855067532509565\n",
      "Epoch 89/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.5929\n",
      "Epoch 00089: val_loss did not improve from 9.20664\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 15.5785 - val_loss: 11.4483\n",
      "\n",
      "[20210302-1244-52] Learning rate for epoch 90 is 0.00047855067532509565\n",
      "Epoch 90/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.4773\n",
      "Epoch 00090: val_loss did not improve from 9.20664\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 15.5095 - val_loss: 9.9539\n",
      "\n",
      "[20210302-1244-56] Learning rate for epoch 91 is 0.00047855067532509565\n",
      "Epoch 91/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 15.6397\n",
      "Epoch 00091: val_loss did not improve from 9.20664\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.6509 - val_loss: 10.3040\n",
      "\n",
      "[20210302-1245-00] Learning rate for epoch 92 is 0.00047855067532509565\n",
      "Epoch 92/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.3948\n",
      "Epoch 00092: val_loss did not improve from 9.20664\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 15.4155 - val_loss: 9.4511\n",
      "\n",
      "[20210302-1245-04] Learning rate for epoch 93 is 0.00047855067532509565\n",
      "Epoch 93/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.5585\n",
      "Epoch 00093: val_loss did not improve from 9.20664\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.5585 - val_loss: 10.1193\n",
      "\n",
      "[20210302-1245-08] Learning rate for epoch 94 is 0.00047855067532509565\n",
      "Epoch 94/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.2793\n",
      "Epoch 00094: val_loss did not improve from 9.20664\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.3412 - val_loss: 10.1255\n",
      "\n",
      "[20210302-1245-12] Learning rate for epoch 95 is 0.00047855067532509565\n",
      "Epoch 95/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.6376\n",
      "Epoch 00095: val_loss did not improve from 9.20664\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.6376 - val_loss: 9.4331\n",
      "\n",
      "[20210302-1245-16] Learning rate for epoch 96 is 0.00047855067532509565\n",
      "Epoch 96/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.1573\n",
      "Epoch 00096: val_loss did not improve from 9.20664\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.1573 - val_loss: 9.4879\n",
      "\n",
      "[20210302-1245-20] Learning rate for epoch 97 is 0.00047855067532509565\n",
      "Epoch 97/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.5080\n",
      "Epoch 00097: val_loss did not improve from 9.20664\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.4991 - val_loss: 10.2529\n",
      "\n",
      "[20210302-1245-23] Learning rate for epoch 98 is 0.00047855067532509565\n",
      "Epoch 98/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.3662\n",
      "Epoch 00098: val_loss did not improve from 9.20664\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.3500 - val_loss: 9.9534\n",
      "\n",
      "[20210302-1245-27] Learning rate for epoch 99 is 0.00047855067532509565\n",
      "Epoch 99/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.8376\n",
      "Epoch 00099: val_loss did not improve from 9.20664\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.7597 - val_loss: 10.0879\n",
      "\n",
      "[20210302-1245-31] Learning rate for epoch 100 is 0.00047855067532509565\n",
      "Epoch 100/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.0383\n",
      "Epoch 00100: val_loss did not improve from 9.20664\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.0383 - val_loss: 11.8413\n",
      "\n",
      "[20210302-1245-35] Learning rate for epoch 101 is 0.00047855067532509565\n",
      "Epoch 101/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.3635\n",
      "Epoch 00101: val_loss did not improve from 9.20664\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.4510 - val_loss: 10.8837\n",
      "\n",
      "[20210302-1245-39] Learning rate for epoch 102 is 0.00047855067532509565\n",
      "Epoch 102/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.2325\n",
      "Epoch 00102: val_loss did not improve from 9.20664\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.2939 - val_loss: 10.0900\n",
      "\n",
      "[20210302-1245-43] Learning rate for epoch 103 is 0.00047855067532509565\n",
      "Epoch 103/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.3831\n",
      "Epoch 00103: val_loss did not improve from 9.20664\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 15.3673 - val_loss: 9.6276\n",
      "\n",
      "[20210302-1245-47] Learning rate for epoch 104 is 0.00047855067532509565\n",
      "Epoch 104/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.0543\n",
      "Epoch 00104: val_loss did not improve from 9.20664\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.1212 - val_loss: 10.7167\n",
      "\n",
      "[20210302-1245-51] Learning rate for epoch 105 is 0.00047855067532509565\n",
      "Epoch 105/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.7799\n",
      "Epoch 00105: val_loss did not improve from 9.20664\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.7093 - val_loss: 9.4854\n",
      "\n",
      "[20210302-1245-55] Learning rate for epoch 106 is 0.00047855067532509565\n",
      "Epoch 106/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.5668\n",
      "Epoch 00106: val_loss did not improve from 9.20664\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.5668 - val_loss: 11.1984\n",
      "\n",
      "*********************************** Start fine tune ***********************************\n",
      "tt: 195, nt:46, total layers:241\n",
      "*********************************** Start fine tune ***********************************\n",
      "Epoch 1/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 24.7205\n",
      "Epoch 00001: val_loss did not improve from 9.20664\n",
      "\n",
      "[20210302-1246-30] Learning rate for epoch 1 is 4.999999987376214e-07\n",
      "20/20 [==============================] - 14s 695ms/step - loss: 24.7205 - val_loss: 14.6683\n",
      "Epoch 2/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 23.2458\n",
      "Epoch 00002: val_loss did not improve from 9.20664\n",
      "\n",
      "[20210302-1246-35] Learning rate for epoch 2 is 5.044450517743826e-05\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 23.2458 - val_loss: 14.6405\n",
      "Epoch 3/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 18.2651\n",
      "Epoch 00003: val_loss did not improve from 9.20664\n",
      "\n",
      "[20210302-1246-56] Learning rate for epoch 3 is 0.00010028902033809572\n",
      "20/20 [==============================] - 17s 875ms/step - loss: 18.2651 - val_loss: 11.1681\n",
      "Epoch 4/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.2059\n",
      "Epoch 00004: val_loss did not improve from 9.20664\n",
      "\n",
      "[20210302-1247-01] Learning rate for epoch 4 is 0.00015003354928921908\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 15.2059 - val_loss: 12.5507\n",
      "Epoch 5/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.5392\n",
      "Epoch 00005: val_loss improved from 9.20664 to 8.95733, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1247-06] Learning rate for epoch 5 is 0.0001996780774788931\n",
      "20/20 [==============================] - 2s 94ms/step - loss: 14.5392 - val_loss: 8.9573\n",
      "Epoch 6/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.9720\n",
      "Epoch 00006: val_loss did not improve from 8.95733\n",
      "\n",
      "[20210302-1247-11] Learning rate for epoch 6 is 0.000249222619459033\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 13.9720 - val_loss: 9.7492\n",
      "Epoch 7/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.0472\n",
      "Epoch 00007: val_loss did not improve from 8.95733\n",
      "\n",
      "[20210302-1247-16] Learning rate for epoch 7 is 0.0002986671752296388\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 13.0472 - val_loss: 9.8395\n",
      "Epoch 8/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.2194\n",
      "Epoch 00008: val_loss did not improve from 8.95733\n",
      "\n",
      "[20210302-1247-21] Learning rate for epoch 8 is 0.00034801175934262574\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 13.2194 - val_loss: 9.6821\n",
      "Epoch 9/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.1191\n",
      "Epoch 00009: val_loss did not improve from 8.95733\n",
      "\n",
      "[20210302-1247-26] Learning rate for epoch 9 is 0.00039725631359033287\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 13.1191 - val_loss: 11.7977\n",
      "Epoch 10/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 12.4443\n",
      "Epoch 00010: val_loss did not improve from 8.95733\n",
      "\n",
      "[20210302-1247-30] Learning rate for epoch 10 is 0.0004464008961804211\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 12.4443 - val_loss: 11.9159\n",
      "Epoch 11/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 12.1522\n",
      "Epoch 00011: val_loss did not improve from 8.95733\n",
      "\n",
      "[20210302-1247-35] Learning rate for epoch 11 is 0.0004954455071128905\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 12.1522 - val_loss: 9.2371\n",
      "Epoch 12/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.7145\n",
      "Epoch 00012: val_loss improved from 8.95733 to 7.23210, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1247-41] Learning rate for epoch 12 is 0.0005443901172839105\n",
      "20/20 [==============================] - 2s 91ms/step - loss: 11.7145 - val_loss: 7.2321\n",
      "Epoch 13/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.6150\n",
      "Epoch 00013: val_loss did not improve from 7.23210\n",
      "\n",
      "[20210302-1247-45] Learning rate for epoch 13 is 0.0005932347266934812\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 11.6150 - val_loss: 7.8125\n",
      "Epoch 14/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.3873\n",
      "Epoch 00014: val_loss did not improve from 7.23210\n",
      "\n",
      "[20210302-1247-50] Learning rate for epoch 14 is 0.0006419793353416026\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 11.3873 - val_loss: 8.6845\n",
      "Epoch 15/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.7368\n",
      "Epoch 00015: val_loss improved from 7.23210 to 6.81884, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1247-56] Learning rate for epoch 15 is 0.0006906240014359355\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 10.7368 - val_loss: 6.8188\n",
      "Epoch 16/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.2622\n",
      "Epoch 00016: val_loss improved from 6.81884 to 6.40052, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1248-01] Learning rate for epoch 16 is 0.0007391686085611582\n",
      "20/20 [==============================] - 2s 91ms/step - loss: 10.2622 - val_loss: 6.4005\n",
      "Epoch 17/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.1824\n",
      "Epoch 00017: val_loss did not improve from 6.40052\n",
      "\n",
      "[20210302-1248-06] Learning rate for epoch 17 is 0.0007876132731325924\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 9.1824 - val_loss: 6.5950\n",
      "Epoch 18/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.1457\n",
      "Epoch 00018: val_loss did not improve from 6.40052\n",
      "\n",
      "[20210302-1248-11] Learning rate for epoch 18 is 0.0008359579369425774\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 9.1457 - val_loss: 8.6842\n",
      "Epoch 19/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.6799\n",
      "Epoch 00019: val_loss did not improve from 6.40052\n",
      "\n",
      "[20210302-1248-16] Learning rate for epoch 19 is 0.000884202599991113\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 8.6799 - val_loss: 9.2927\n",
      "Epoch 20/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.4768\n",
      "Epoch 00020: val_loss did not improve from 6.40052\n",
      "\n",
      "[20210302-1248-20] Learning rate for epoch 20 is 0.0009323473204858601\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 8.4768 - val_loss: 20.4802\n",
      "Epoch 21/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.2670\n",
      "Epoch 00021: val_loss did not improve from 6.40052\n",
      "\n",
      "[20210302-1248-25] Learning rate for epoch 21 is 0.000980391982011497\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 8.2670 - val_loss: 39.3847\n",
      "Epoch 22/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.1247\n",
      "Epoch 00022: val_loss improved from 6.40052 to 6.08975, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1248-31] Learning rate for epoch 22 is 0.0010283367009833455\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 8.1247 - val_loss: 6.0897\n",
      "Epoch 23/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.9811\n",
      "Epoch 00023: val_loss improved from 6.08975 to 4.01484, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1248-36] Learning rate for epoch 23 is 0.0010761814191937447\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 7.9811 - val_loss: 4.0148\n",
      "Epoch 24/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8692\n",
      "Epoch 00024: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1248-41] Learning rate for epoch 24 is 0.0011239261366426945\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.8692 - val_loss: 4.9326\n",
      "Epoch 25/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.2978\n",
      "Epoch 00025: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1248-46] Learning rate for epoch 25 is 0.001171570853330195\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 8.2978 - val_loss: 5.7488\n",
      "Epoch 26/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.2138\n",
      "Epoch 00026: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1248-51] Learning rate for epoch 26 is 0.001219115569256246\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 8.2138 - val_loss: 11.8650\n",
      "Epoch 27/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8474\n",
      "Epoch 00027: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1248-55] Learning rate for epoch 27 is 0.0012665604008361697\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.8474 - val_loss: 9.1016\n",
      "Epoch 28/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6537\n",
      "Epoch 00028: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1249-00] Learning rate for epoch 28 is 0.0013139051152393222\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.6537 - val_loss: 27.2452\n",
      "Epoch 29/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5653\n",
      "Epoch 00029: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1249-05] Learning rate for epoch 29 is 0.0013611499452963471\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.5653 - val_loss: 24.2157\n",
      "Epoch 30/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4848\n",
      "Epoch 00030: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1249-10] Learning rate for epoch 30 is 0.001408294658176601\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.4848 - val_loss: 9.2997\n",
      "Epoch 31/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5134\n",
      "Epoch 00031: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1249-15] Learning rate for epoch 31 is 0.0014553394867107272\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.5134 - val_loss: 13.9733\n",
      "Epoch 32/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3202\n",
      "Epoch 00032: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1249-20] Learning rate for epoch 32 is 0.0015022843144834042\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.3202 - val_loss: 10.3870\n",
      "Epoch 33/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0644\n",
      "Epoch 00033: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1249-24] Learning rate for epoch 33 is 0.0015491291414946318\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.0644 - val_loss: 7.1113\n",
      "Epoch 34/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1830\n",
      "Epoch 00034: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1249-29] Learning rate for epoch 34 is 0.00159587396774441\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.1830 - val_loss: 6.2468\n",
      "Epoch 35/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4402\n",
      "Epoch 00035: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1249-34] Learning rate for epoch 35 is 0.001642518793232739\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.4402 - val_loss: 7.0996\n",
      "Epoch 36/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1265\n",
      "Epoch 00036: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1249-39] Learning rate for epoch 36 is 0.0016890636179596186\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.1265 - val_loss: 9.7357\n",
      "Epoch 37/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5276\n",
      "Epoch 00037: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1249-44] Learning rate for epoch 37 is 0.0017355084419250488\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.5276 - val_loss: 11.1535\n",
      "Epoch 38/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9989\n",
      "Epoch 00038: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1249-48] Learning rate for epoch 38 is 0.0017818533815443516\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.9989 - val_loss: 16.2836\n",
      "Epoch 39/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4330\n",
      "Epoch 00039: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1249-53] Learning rate for epoch 39 is 0.0018280982039868832\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.4330 - val_loss: 18.3609\n",
      "Epoch 40/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2662\n",
      "Epoch 00040: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1249-58] Learning rate for epoch 40 is 0.0018742431420832872\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 7.2662 - val_loss: 18.1036\n",
      "Epoch 41/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1474\n",
      "Epoch 00041: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1250-03] Learning rate for epoch 41 is 0.0019202879630029202\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.1474 - val_loss: 5.6330\n",
      "Epoch 42/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4545\n",
      "Epoch 00042: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1250-08] Learning rate for epoch 42 is 0.0019662328995764256\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 7.4545 - val_loss: 32.7114\n",
      "Epoch 43/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5022\n",
      "Epoch 00043: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1250-13] Learning rate for epoch 43 is 0.00201207771897316\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.5022 - val_loss: 257.4583\n",
      "Epoch 44/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2007\n",
      "Epoch 00044: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1250-18] Learning rate for epoch 44 is 0.0020578226540237665\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 7.2007 - val_loss: 89.6076\n",
      "Epoch 45/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8315\n",
      "Epoch 00045: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1250-22] Learning rate for epoch 45 is 0.0021034677047282457\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.8315 - val_loss: 5.0894\n",
      "Epoch 46/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0513\n",
      "Epoch 00046: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1250-27] Learning rate for epoch 46 is 0.002149012638255954\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 7.0513 - val_loss: 7.1731\n",
      "Epoch 47/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0537\n",
      "Epoch 00047: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1250-32] Learning rate for epoch 47 is 0.0021944576874375343\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.0537 - val_loss: 6.8719\n",
      "Epoch 48/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9917\n",
      "Epoch 00048: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1250-37] Learning rate for epoch 48 is 0.0022398026194423437\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.9917 - val_loss: 5.2406\n",
      "Epoch 49/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1766\n",
      "Epoch 00049: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1250-42] Learning rate for epoch 49 is 0.002285047434270382\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 7.1766 - val_loss: 14.4547\n",
      "Epoch 50/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1391\n",
      "Epoch 00050: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1250-47] Learning rate for epoch 50 is 0.0023301925975829363\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 7.1391 - val_loss: 14.7978\n",
      "Epoch 51/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7676\n",
      "Epoch 00051: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1250-52] Learning rate for epoch 51 is 0.002375237410888076\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.7676 - val_loss: 10.1313\n",
      "Epoch 52/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9960\n",
      "Epoch 00052: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1250-56] Learning rate for epoch 52 is 0.0024201825726777315\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.9960 - val_loss: 5.8701\n",
      "Epoch 53/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1305\n",
      "Epoch 00053: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1251-01] Learning rate for epoch 53 is 0.002465027617290616\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.1305 - val_loss: 6.7092\n",
      "Epoch 54/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9414\n",
      "Epoch 00054: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1251-06] Learning rate for epoch 54 is 0.0025097725447267294\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.9414 - val_loss: 5.5007\n",
      "Epoch 55/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6057\n",
      "Epoch 00055: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1251-11] Learning rate for epoch 55 is 0.0025544175878167152\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.6057 - val_loss: 5.1068\n",
      "Epoch 56/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7004\n",
      "Epoch 00056: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1251-16] Learning rate for epoch 56 is 0.00259896251372993\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.7004 - val_loss: 4.8352\n",
      "Epoch 57/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0293\n",
      "Epoch 00057: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1251-21] Learning rate for epoch 57 is 0.0026434077881276608\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.0293 - val_loss: 5.9053\n",
      "Epoch 58/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2735\n",
      "Epoch 00058: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1251-25] Learning rate for epoch 58 is 0.0026877527125179768\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.2735 - val_loss: 4.9764\n",
      "Epoch 59/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5294\n",
      "Epoch 00059: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1251-30] Learning rate for epoch 59 is 0.0027319977525621653\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.5294 - val_loss: 5.3668\n",
      "Epoch 60/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4320\n",
      "Epoch 00060: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1251-35] Learning rate for epoch 60 is 0.0027761429082602262\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.4320 - val_loss: 4.2941\n",
      "Epoch 61/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2905\n",
      "Epoch 00061: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1251-40] Learning rate for epoch 61 is 0.002820187946781516\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.2905 - val_loss: 5.2481\n",
      "Epoch 62/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0239\n",
      "Epoch 00062: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1251-45] Learning rate for epoch 62 is 0.0028641331009566784\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.0239 - val_loss: 6.1342\n",
      "Epoch 63/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4326\n",
      "Epoch 00063: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1251-50] Learning rate for epoch 63 is 0.0029079781379550695\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.4326 - val_loss: 6.1193\n",
      "Epoch 64/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1921\n",
      "Epoch 00064: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1251-54] Learning rate for epoch 64 is 0.002951723290607333\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.1921 - val_loss: 5.9943\n",
      "Epoch 65/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4278\n",
      "Epoch 00065: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1251-59] Learning rate for epoch 65 is 0.0029953685589134693\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.4278 - val_loss: 6.8896\n",
      "Epoch 66/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4935\n",
      "Epoch 00066: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1252-04] Learning rate for epoch 66 is 0.0030389137100428343\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.4935 - val_loss: 6.2792\n",
      "Epoch 67/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8837\n",
      "Epoch 00067: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1252-09] Learning rate for epoch 67 is 0.003082358743995428\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.8837 - val_loss: 15.3792\n",
      "Epoch 68/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8967\n",
      "Epoch 00068: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1252-14] Learning rate for epoch 68 is 0.0031257038936018944\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.8967 - val_loss: 10.3710\n",
      "Epoch 69/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1927\n",
      "Epoch 00069: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1252-19] Learning rate for epoch 69 is 0.003168949158862233\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 7.1927 - val_loss: 7.4832\n",
      "Epoch 70/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0252\n",
      "Epoch 00070: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1252-23] Learning rate for epoch 70 is 0.0032120943069458008\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.0252 - val_loss: 5.5213\n",
      "Epoch 71/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5966\n",
      "Epoch 00071: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1252-28] Learning rate for epoch 71 is 0.003255139570683241\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.5966 - val_loss: 7.0042\n",
      "Epoch 72/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8751\n",
      "Epoch 00072: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1252-33] Learning rate for epoch 72 is 0.00329808471724391\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.8751 - val_loss: 4.5405\n",
      "Epoch 73/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8310\n",
      "Epoch 00073: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1252-38] Learning rate for epoch 73 is 0.0033409299794584513\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.8310 - val_loss: 5.2573\n",
      "Epoch 74/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6101\n",
      "Epoch 00074: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1252-43] Learning rate for epoch 74 is 0.0033836751244962215\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.6101 - val_loss: 4.3663\n",
      "Epoch 75/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7727\n",
      "Epoch 00075: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1252-48] Learning rate for epoch 75 is 0.0034263203851878643\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.7727 - val_loss: 4.6474\n",
      "Epoch 76/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6977\n",
      "Epoch 00076: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1252-52] Learning rate for epoch 76 is 0.003468865528702736\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.6977 - val_loss: 4.1830\n",
      "Epoch 77/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1050\n",
      "Epoch 00077: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1252-57] Learning rate for epoch 77 is 0.00351131078787148\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 7.1050 - val_loss: 6.1686\n",
      "Epoch 78/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3294\n",
      "Epoch 00078: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1253-02] Learning rate for epoch 78 is 0.0035536561626940966\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.3294 - val_loss: 5.7018\n",
      "Epoch 79/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7189\n",
      "Epoch 00079: val_loss did not improve from 4.01484\n",
      "\n",
      "[20210302-1253-07] Learning rate for epoch 79 is 0.003595901420339942\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.7189 - val_loss: 5.7930\n",
      "Epoch 80/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6968\n",
      "Epoch 00080: val_loss improved from 4.01484 to 3.73419, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1253-12] Learning rate for epoch 80 is 0.00363804679363966\n",
      "20/20 [==============================] - 2s 91ms/step - loss: 6.6968 - val_loss: 3.7342\n",
      "Epoch 81/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7319\n",
      "Epoch 00081: val_loss did not improve from 3.73419\n",
      "\n",
      "[20210302-1253-17] Learning rate for epoch 81 is 0.0036800920497626066\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.7319 - val_loss: 7.5367\n",
      "Epoch 82/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1967\n",
      "Epoch 00082: val_loss did not improve from 3.73419\n",
      "\n",
      "[20210302-1253-22] Learning rate for epoch 82 is 0.003722037188708782\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.1967 - val_loss: 4.1021\n",
      "Epoch 83/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2842\n",
      "Epoch 00083: val_loss did not improve from 3.73419\n",
      "\n",
      "[20210302-1253-27] Learning rate for epoch 83 is 0.003763882676139474\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.2842 - val_loss: 6.3299\n",
      "Epoch 84/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3327\n",
      "Epoch 00084: val_loss did not improve from 3.73419\n",
      "\n",
      "[20210302-1253-32] Learning rate for epoch 84 is 0.0038056280463933945\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.3327 - val_loss: 5.3450\n",
      "Epoch 85/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8087\n",
      "Epoch 00085: val_loss did not improve from 3.73419\n",
      "\n",
      "[20210302-1253-36] Learning rate for epoch 85 is 0.003847273299470544\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.8087 - val_loss: 18.1984\n",
      "Epoch 86/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.5263\n",
      "Epoch 00086: val_loss did not improve from 3.73419\n",
      "\n",
      "[20210302-1253-41] Learning rate for epoch 86 is 0.0038888186682015657\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 8.5263 - val_loss: 15.9319\n",
      "Epoch 87/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2842\n",
      "Epoch 00087: val_loss did not improve from 3.73419\n",
      "\n",
      "[20210302-1253-46] Learning rate for epoch 87 is 0.00393026415258646\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.2842 - val_loss: 4.6768\n",
      "Epoch 88/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7060\n",
      "Epoch 00088: val_loss did not improve from 3.73419\n",
      "\n",
      "[20210302-1253-51] Learning rate for epoch 88 is 0.00397160928696394\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.7060 - val_loss: 7.3368\n",
      "Epoch 89/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5458\n",
      "Epoch 00089: val_loss did not improve from 3.73419\n",
      "\n",
      "[20210302-1253-56] Learning rate for epoch 89 is 0.004012854769825935\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.5458 - val_loss: 4.7553\n",
      "Epoch 90/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4699\n",
      "Epoch 00090: val_loss did not improve from 3.73419\n",
      "\n",
      "[20210302-1254-00] Learning rate for epoch 90 is 0.00405400013551116\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.4699 - val_loss: 4.0802\n",
      "Epoch 91/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5708\n",
      "Epoch 00091: val_loss improved from 3.73419 to 3.73069, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1254-06] Learning rate for epoch 91 is 0.004095045384019613\n",
      "20/20 [==============================] - 2s 91ms/step - loss: 6.5708 - val_loss: 3.7307\n",
      "Epoch 92/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5306\n",
      "Epoch 00092: val_loss did not improve from 3.73069\n",
      "\n",
      "[20210302-1254-11] Learning rate for epoch 92 is 0.004135990981012583\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.5306 - val_loss: 5.6326\n",
      "Epoch 93/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8778\n",
      "Epoch 00093: val_loss did not improve from 3.73069\n",
      "\n",
      "[20210302-1254-15] Learning rate for epoch 93 is 0.004176836460828781\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.8778 - val_loss: 5.5964\n",
      "Epoch 94/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7399\n",
      "Epoch 00094: val_loss did not improve from 3.73069\n",
      "\n",
      "[20210302-1254-20] Learning rate for epoch 94 is 0.004217581823468208\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.7399 - val_loss: 4.1986\n",
      "Epoch 95/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4439\n",
      "Epoch 00095: val_loss did not improve from 3.73069\n",
      "\n",
      "[20210302-1254-25] Learning rate for epoch 95 is 0.004258227068930864\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.4439 - val_loss: 4.9932\n",
      "Epoch 96/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7032\n",
      "Epoch 00096: val_loss did not improve from 3.73069\n",
      "\n",
      "[20210302-1254-30] Learning rate for epoch 96 is 0.0042987726628780365\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.7032 - val_loss: 4.6178\n",
      "Epoch 97/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0886\n",
      "Epoch 00097: val_loss did not improve from 3.73069\n",
      "\n",
      "[20210302-1254-35] Learning rate for epoch 97 is 0.0043392181396484375\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.0886 - val_loss: 5.3983\n",
      "Epoch 98/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1868\n",
      "Epoch 00098: val_loss did not improve from 3.73069\n",
      "\n",
      "[20210302-1254-39] Learning rate for epoch 98 is 0.004379563499242067\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.1868 - val_loss: 5.7613\n",
      "Epoch 99/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2465\n",
      "Epoch 00099: val_loss did not improve from 3.73069\n",
      "\n",
      "[20210302-1254-44] Learning rate for epoch 99 is 0.004419809207320213\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.2465 - val_loss: 5.1431\n",
      "Epoch 100/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9430\n",
      "Epoch 00100: val_loss did not improve from 3.73069\n",
      "\n",
      "[20210302-1254-49] Learning rate for epoch 100 is 0.004459954332560301\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.9430 - val_loss: 4.3454\n",
      "Epoch 101/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9061\n",
      "Epoch 00101: val_loss did not improve from 3.73069\n",
      "\n",
      "[20210302-1254-54] Learning rate for epoch 101 is 4.4999998749517545e-07\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.9061 - val_loss: 4.0105\n",
      "Epoch 102/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2529\n",
      "Epoch 00102: val_loss did not improve from 3.73069\n",
      "\n",
      "[20210302-1254-59] Learning rate for epoch 102 is 0.00018023152370005846\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.2529 - val_loss: 4.0498\n",
      "Epoch 103/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1546\n",
      "Epoch 00103: val_loss improved from 3.73069 to 3.69996, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1255-04] Learning rate for epoch 103 is 0.000359613070031628\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 6.1546 - val_loss: 3.7000\n",
      "Epoch 104/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4137\n",
      "Epoch 00104: val_loss did not improve from 3.69996\n",
      "\n",
      "[20210302-1255-09] Learning rate for epoch 104 is 0.0005385946715250611\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.4137 - val_loss: 3.7076\n",
      "Epoch 105/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0819\n",
      "Epoch 00105: val_loss did not improve from 3.69996\n",
      "\n",
      "[20210302-1255-14] Learning rate for epoch 105 is 0.0007171762990765274\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.0819 - val_loss: 3.9533\n",
      "Epoch 106/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0931\n",
      "Epoch 00106: val_loss did not improve from 3.69996\n",
      "\n",
      "[20210302-1255-18] Learning rate for epoch 106 is 0.0008953579817898571\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.0931 - val_loss: 3.9502\n",
      "Epoch 107/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1072\n",
      "Epoch 00107: val_loss did not improve from 3.69996\n",
      "\n",
      "[20210302-1255-23] Learning rate for epoch 107 is 0.0010731397196650505\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.1072 - val_loss: 3.7384\n",
      "Epoch 108/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2905\n",
      "Epoch 00108: val_loss did not improve from 3.69996\n",
      "\n",
      "[20210302-1255-28] Learning rate for epoch 108 is 0.0012505215127021074\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.2905 - val_loss: 4.3781\n",
      "Epoch 109/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1560\n",
      "Epoch 00109: val_loss did not improve from 3.69996\n",
      "\n",
      "[20210302-1255-33] Learning rate for epoch 109 is 0.001427503302693367\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.1560 - val_loss: 3.9095\n",
      "Epoch 110/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4353\n",
      "Epoch 00110: val_loss did not improve from 3.69996\n",
      "\n",
      "[20210302-1255-38] Learning rate for epoch 110 is 0.0016040850896388292\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.4353 - val_loss: 4.0528\n",
      "Epoch 111/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9443\n",
      "Epoch 00111: val_loss did not improve from 3.69996\n",
      "\n",
      "[20210302-1255-43] Learning rate for epoch 111 is 0.001780266989953816\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.9443 - val_loss: 4.0460\n",
      "Epoch 112/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3816\n",
      "Epoch 00112: val_loss did not improve from 3.69996\n",
      "\n",
      "[20210302-1255-47] Learning rate for epoch 112 is 0.0019560488872230053\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.3816 - val_loss: 4.9725\n",
      "Epoch 113/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0540\n",
      "Epoch 00113: val_loss did not improve from 3.69996\n",
      "\n",
      "[20210302-1255-52] Learning rate for epoch 113 is 0.002131430897861719\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.0540 - val_loss: 4.1263\n",
      "Epoch 114/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1081\n",
      "Epoch 00114: val_loss did not improve from 3.69996\n",
      "\n",
      "[20210302-1255-57] Learning rate for epoch 114 is 0.0023064129054546356\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.1081 - val_loss: 4.2082\n",
      "Epoch 115/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9050\n",
      "Epoch 00115: val_loss did not improve from 3.69996\n",
      "\n",
      "[20210302-1256-02] Learning rate for epoch 115 is 0.0024809949100017548\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.9050 - val_loss: 4.2729\n",
      "Epoch 116/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1537\n",
      "Epoch 00116: val_loss did not improve from 3.69996\n",
      "\n",
      "[20210302-1256-07] Learning rate for epoch 116 is 0.0026551769115030766\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.1537 - val_loss: 4.9088\n",
      "Epoch 117/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9965\n",
      "Epoch 00117: val_loss did not improve from 3.69996\n",
      "\n",
      "[20210302-1256-12] Learning rate for epoch 117 is 0.0028289591427892447\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.9965 - val_loss: 4.1557\n",
      "Epoch 118/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0440\n",
      "Epoch 00118: val_loss improved from 3.69996 to 3.61719, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1256-17] Learning rate for epoch 118 is 0.0030023413710296154\n",
      "20/20 [==============================] - 2s 91ms/step - loss: 6.0440 - val_loss: 3.6172\n",
      "Epoch 119/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6451\n",
      "Epoch 00119: val_loss did not improve from 3.61719\n",
      "\n",
      "[20210302-1256-22] Learning rate for epoch 119 is 0.003175323596224189\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.6451 - val_loss: 3.6913\n",
      "Epoch 120/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9181\n",
      "Epoch 00120: val_loss did not improve from 3.61719\n",
      "\n",
      "[20210302-1256-27] Learning rate for epoch 120 is 0.003347905818372965\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.9181 - val_loss: 4.7081\n",
      "Epoch 121/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3485\n",
      "Epoch 00121: val_loss did not improve from 3.61719\n",
      "\n",
      "[20210302-1256-31] Learning rate for epoch 121 is 0.0035200880374759436\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.3485 - val_loss: 4.7352\n",
      "Epoch 122/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0170\n",
      "Epoch 00122: val_loss did not improve from 3.61719\n",
      "\n",
      "[20210302-1256-36] Learning rate for epoch 122 is 0.003691870253533125\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.0170 - val_loss: 5.5793\n",
      "Epoch 123/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3320\n",
      "Epoch 00123: val_loss did not improve from 3.61719\n",
      "\n",
      "[20210302-1256-41] Learning rate for epoch 123 is 0.0038632526993751526\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.3320 - val_loss: 4.9771\n",
      "Epoch 124/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4136\n",
      "Epoch 00124: val_loss did not improve from 3.61719\n",
      "\n",
      "[20210302-1256-46] Learning rate for epoch 124 is 0.004034235142171383\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.4136 - val_loss: 7.1213\n",
      "Epoch 125/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3370\n",
      "Epoch 00125: val_loss did not improve from 3.61719\n",
      "\n",
      "[20210302-1256-51] Learning rate for epoch 125 is 0.004204817581921816\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.3370 - val_loss: 4.4033\n",
      "Epoch 126/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9068\n",
      "Epoch 00126: val_loss did not improve from 3.61719\n",
      "\n",
      "[20210302-1256-56] Learning rate for epoch 126 is 0.0043750000186264515\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.9068 - val_loss: 5.6199\n",
      "Epoch 127/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2408\n",
      "Epoch 00127: val_loss did not improve from 3.61719\n",
      "\n",
      "[20210302-1257-00] Learning rate for epoch 127 is 0.0041952175088226795\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.2408 - val_loss: 4.1298\n",
      "Epoch 128/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8455\n",
      "Epoch 00128: val_loss did not improve from 3.61719\n",
      "\n",
      "[20210302-1257-05] Learning rate for epoch 128 is 0.004015835002064705\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.8455 - val_loss: 6.2630\n",
      "Epoch 129/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1143\n",
      "Epoch 00129: val_loss did not improve from 3.61719\n",
      "\n",
      "[20210302-1257-10] Learning rate for epoch 129 is 0.003836852265521884\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.1143 - val_loss: 3.9938\n",
      "Epoch 130/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2172\n",
      "Epoch 00130: val_loss did not improve from 3.61719\n",
      "\n",
      "[20210302-1257-15] Learning rate for epoch 130 is 0.003658269764855504\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.2172 - val_loss: 4.2814\n",
      "Epoch 131/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1703\n",
      "Epoch 00131: val_loss did not improve from 3.61719\n",
      "\n",
      "[20210302-1257-20] Learning rate for epoch 131 is 0.003480087034404278\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.1703 - val_loss: 6.8328\n",
      "Epoch 132/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2860\n",
      "Epoch 00132: val_loss did not improve from 3.61719\n",
      "\n",
      "[20210302-1257-25] Learning rate for epoch 132 is 0.003302304306998849\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.2860 - val_loss: 6.8316\n",
      "Epoch 133/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2857\n",
      "Epoch 00133: val_loss did not improve from 3.61719\n",
      "\n",
      "[20210302-1257-29] Learning rate for epoch 133 is 0.0031249215826392174\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.2857 - val_loss: 4.3451\n",
      "Epoch 134/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4477\n",
      "Epoch 00134: val_loss did not improve from 3.61719\n",
      "\n",
      "[20210302-1257-34] Learning rate for epoch 134 is 0.0029479386284947395\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.4477 - val_loss: 4.5286\n",
      "Epoch 135/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9852\n",
      "Epoch 00135: val_loss did not improve from 3.61719\n",
      "\n",
      "[20210302-1257-39] Learning rate for epoch 135 is 0.0027713559102267027\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.9852 - val_loss: 5.1507\n",
      "Epoch 136/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0415\n",
      "Epoch 00136: val_loss did not improve from 3.61719\n",
      "\n",
      "[20210302-1257-44] Learning rate for epoch 136 is 0.0025951729621738195\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.0415 - val_loss: 5.1996\n",
      "Epoch 137/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6761\n",
      "Epoch 00137: val_loss did not improve from 3.61719\n",
      "\n",
      "[20210302-1257-49] Learning rate for epoch 137 is 0.0024193900171667337\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.6761 - val_loss: 4.5971\n",
      "Epoch 138/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9976\n",
      "Epoch 00138: val_loss did not improve from 3.61719\n",
      "\n",
      "[20210302-1257-54] Learning rate for epoch 138 is 0.0022440070752054453\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.9976 - val_loss: 4.1709\n",
      "Epoch 139/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7105\n",
      "Epoch 00139: val_loss did not improve from 3.61719\n",
      "\n",
      "[20210302-1257-58] Learning rate for epoch 139 is 0.002069024136289954\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.7105 - val_loss: 4.1910\n",
      "Epoch 140/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8013\n",
      "Epoch 00140: val_loss did not improve from 3.61719\n",
      "\n",
      "[20210302-1258-03] Learning rate for epoch 140 is 0.0018944410840049386\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.8013 - val_loss: 3.9853\n",
      "Epoch 141/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9136\n",
      "Epoch 00141: val_loss did not improve from 3.61719\n",
      "\n",
      "[20210302-1258-08] Learning rate for epoch 141 is 0.0017202580347657204\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.9136 - val_loss: 3.6395\n",
      "Epoch 142/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0559\n",
      "Epoch 00142: val_loss did not improve from 3.61719\n",
      "\n",
      "[20210302-1258-13] Learning rate for epoch 142 is 0.0015464748721569777\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.0559 - val_loss: 4.1550\n",
      "Epoch 143/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7003\n",
      "Epoch 00143: val_loss did not improve from 3.61719\n",
      "\n",
      "[20210302-1258-18] Learning rate for epoch 143 is 0.0013730917125940323\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.7003 - val_loss: 3.9289\n",
      "Epoch 144/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0862\n",
      "Epoch 00144: val_loss did not improve from 3.61719\n",
      "\n",
      "[20210302-1258-23] Learning rate for epoch 144 is 0.0012001085560768843\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.0862 - val_loss: 4.1932\n",
      "Epoch 145/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9922\n",
      "Epoch 00145: val_loss did not improve from 3.61719\n",
      "\n",
      "[20210302-1258-28] Learning rate for epoch 145 is 0.0010275252861902118\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 5.9922 - val_loss: 4.0110\n",
      "Epoch 146/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5487\n",
      "Epoch 00146: val_loss did not improve from 3.61719\n",
      "\n",
      "[20210302-1258-32] Learning rate for epoch 146 is 0.0008553420193493366\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.5487 - val_loss: 3.8528\n",
      "Epoch 147/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5288\n",
      "Epoch 00147: val_loss did not improve from 3.61719\n",
      "\n",
      "[20210302-1258-37] Learning rate for epoch 147 is 0.0006835586973465979\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 5.5288 - val_loss: 3.8075\n",
      "Epoch 148/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8771\n",
      "Epoch 00148: val_loss did not improve from 3.61719\n",
      "\n",
      "[20210302-1258-42] Learning rate for epoch 148 is 0.0005121753201819956\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8771 - val_loss: 3.7234\n",
      "Epoch 149/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7254\n",
      "Epoch 00149: val_loss did not improve from 3.61719\n",
      "\n",
      "[20210302-1258-47] Learning rate for epoch 149 is 0.00034119191695936024\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.7254 - val_loss: 3.6633\n",
      "Epoch 150/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0319\n",
      "Epoch 00150: val_loss did not improve from 3.61719\n",
      "\n",
      "[20210302-1258-52] Learning rate for epoch 150 is 0.00017060847312677652\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.0319 - val_loss: 3.7366\n",
      "Epoch 151/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8470\n",
      "Epoch 00151: val_loss did not improve from 3.61719\n",
      "\n",
      "[20210302-1258-57] Learning rate for epoch 151 is 4.249999960848072e-07\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.8470 - val_loss: 3.7163\n",
      "Epoch 152/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5516\n",
      "Epoch 00152: val_loss did not improve from 3.61719\n",
      "\n",
      "[20210302-1259-01] Learning rate for epoch 152 is 0.00017020752420648932\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.5516 - val_loss: 3.6293\n",
      "Epoch 153/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7617\n",
      "Epoch 00153: val_loss improved from 3.61719 to 3.59655, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1259-07] Learning rate for epoch 153 is 0.0003395900712348521\n",
      "20/20 [==============================] - 2s 93ms/step - loss: 5.7617 - val_loss: 3.5966\n",
      "Epoch 154/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9684\n",
      "Epoch 00154: val_loss did not improve from 3.59655\n",
      "\n",
      "[20210302-1259-12] Learning rate for epoch 154 is 0.0005085726734250784\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.9684 - val_loss: 3.7297\n",
      "Epoch 155/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7794\n",
      "Epoch 00155: val_loss did not improve from 3.59655\n",
      "\n",
      "[20210302-1259-17] Learning rate for epoch 155 is 0.0006771553307771683\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.7794 - val_loss: 3.6944\n",
      "Epoch 156/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8683\n",
      "Epoch 00156: val_loss did not improve from 3.59655\n",
      "\n",
      "[20210302-1259-21] Learning rate for epoch 156 is 0.0008453379850834608\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8683 - val_loss: 3.7064\n",
      "Epoch 157/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7322\n",
      "Epoch 00157: val_loss did not improve from 3.59655\n",
      "\n",
      "[20210302-1259-26] Learning rate for epoch 157 is 0.0010131207527592778\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.7322 - val_loss: 3.7919\n",
      "Epoch 158/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8103\n",
      "Epoch 00158: val_loss did not improve from 3.59655\n",
      "\n",
      "[20210302-1259-31] Learning rate for epoch 158 is 0.0011805035173892975\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8103 - val_loss: 3.8026\n",
      "Epoch 159/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6242\n",
      "Epoch 00159: val_loss improved from 3.59655 to 3.49769, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1259-36] Learning rate for epoch 159 is 0.0013474862789735198\n",
      "20/20 [==============================] - 2s 89ms/step - loss: 5.6242 - val_loss: 3.4977\n",
      "Epoch 160/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8522\n",
      "Epoch 00160: val_loss did not improve from 3.49769\n",
      "\n",
      "[20210302-1259-41] Learning rate for epoch 160 is 0.0015140691539272666\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8522 - val_loss: 3.7466\n",
      "Epoch 161/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2981\n",
      "Epoch 00161: val_loss did not improve from 3.49769\n",
      "\n",
      "[20210302-1259-46] Learning rate for epoch 161 is 0.001680252025835216\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.2981 - val_loss: 4.2614\n",
      "Epoch 162/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.4935\n",
      "Epoch 00162: val_loss did not improve from 3.49769\n",
      "\n",
      "[20210302-1259-51] Learning rate for epoch 162 is 0.0018460348946973681\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 8.4935 - val_loss: 8.3156\n",
      "Epoch 163/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1123\n",
      "Epoch 00163: val_loss did not improve from 3.49769\n",
      "\n",
      "[20210302-1259-56] Learning rate for epoch 163 is 0.0020114178769290447\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.1123 - val_loss: 10.0686\n",
      "Epoch 164/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1860\n",
      "Epoch 00164: val_loss did not improve from 3.49769\n",
      "\n",
      "[20210302-1300-00] Learning rate for epoch 164 is 0.0021764009725302458\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.1860 - val_loss: 9.0283\n",
      "Epoch 165/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9729\n",
      "Epoch 00165: val_loss did not improve from 3.49769\n",
      "\n",
      "[20210302-1300-05] Learning rate for epoch 165 is 0.002340983832255006\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.9729 - val_loss: 5.8095\n",
      "Epoch 166/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8697\n",
      "Epoch 00166: val_loss did not improve from 3.49769\n",
      "\n",
      "[20210302-1300-10] Learning rate for epoch 166 is 0.002505166921764612\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.8697 - val_loss: 6.0847\n",
      "Epoch 167/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8157\n",
      "Epoch 00167: val_loss did not improve from 3.49769\n",
      "\n",
      "[20210302-1300-15] Learning rate for epoch 167 is 0.002668950008228421\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.8157 - val_loss: 5.5252\n",
      "Epoch 168/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1684\n",
      "Epoch 00168: val_loss did not improve from 3.49769\n",
      "\n",
      "[20210302-1300-20] Learning rate for epoch 168 is 0.0028323333244770765\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.1684 - val_loss: 4.1834\n",
      "Epoch 169/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6850\n",
      "Epoch 00169: val_loss did not improve from 3.49769\n",
      "\n",
      "[20210302-1300-25] Learning rate for epoch 169 is 0.002995316404849291\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6850 - val_loss: 4.1497\n",
      "Epoch 170/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3441\n",
      "Epoch 00170: val_loss did not improve from 3.49769\n",
      "\n",
      "[20210302-1300-29] Learning rate for epoch 170 is 0.0031578997150063515\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.3441 - val_loss: 4.3111\n",
      "Epoch 171/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2061\n",
      "Epoch 00171: val_loss did not improve from 3.49769\n",
      "\n",
      "[20210302-1300-34] Learning rate for epoch 171 is 0.0033200830221176147\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.2061 - val_loss: 3.8338\n",
      "Epoch 172/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8771\n",
      "Epoch 00172: val_loss improved from 3.49769 to 3.48355, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1300-40] Learning rate for epoch 172 is 0.0034818663261830807\n",
      "20/20 [==============================] - 2s 91ms/step - loss: 5.8771 - val_loss: 3.4836\n",
      "Epoch 173/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4011\n",
      "Epoch 00173: val_loss did not improve from 3.48355\n",
      "\n",
      "[20210302-1300-44] Learning rate for epoch 173 is 0.0036432496272027493\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.4011 - val_loss: 3.9154\n",
      "Epoch 174/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5943\n",
      "Epoch 00174: val_loss did not improve from 3.48355\n",
      "\n",
      "[20210302-1300-49] Learning rate for epoch 174 is 0.003804233158007264\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.5943 - val_loss: 4.0265\n",
      "Epoch 175/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8441\n",
      "Epoch 00175: val_loss did not improve from 3.48355\n",
      "\n",
      "[20210302-1300-54] Learning rate for epoch 175 is 0.003964816685765982\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.8441 - val_loss: 4.2993\n",
      "Epoch 176/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1117\n",
      "Epoch 00176: val_loss did not improve from 3.48355\n",
      "\n",
      "[20210302-1300-59] Learning rate for epoch 176 is 0.004124999977648258\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.1117 - val_loss: 8.0554\n",
      "Epoch 177/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5445\n",
      "Epoch 00177: val_loss did not improve from 3.48355\n",
      "\n",
      "[20210302-1301-04] Learning rate for epoch 177 is 0.003955216612666845\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.5445 - val_loss: 3.9207\n",
      "Epoch 178/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0464\n",
      "Epoch 00178: val_loss did not improve from 3.48355\n",
      "\n",
      "[20210302-1301-09] Learning rate for epoch 178 is 0.003785833017900586\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.0464 - val_loss: 6.0559\n",
      "Epoch 179/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8122\n",
      "Epoch 00179: val_loss did not improve from 3.48355\n",
      "\n",
      "[20210302-1301-14] Learning rate for epoch 179 is 0.0036168494261801243\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.8122 - val_loss: 4.7982\n",
      "Epoch 180/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0664\n",
      "Epoch 00180: val_loss did not improve from 3.48355\n",
      "\n",
      "[20210302-1301-18] Learning rate for epoch 180 is 0.003448265604674816\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.0664 - val_loss: 3.8078\n",
      "Epoch 181/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6288\n",
      "Epoch 00181: val_loss did not improve from 3.48355\n",
      "\n",
      "[20210302-1301-23] Learning rate for epoch 181 is 0.003280082019045949\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6288 - val_loss: 4.0371\n",
      "Epoch 182/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5949\n",
      "Epoch 00182: val_loss did not improve from 3.48355\n",
      "\n",
      "[20210302-1301-28] Learning rate for epoch 182 is 0.0031122982036322355\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.5949 - val_loss: 4.6461\n",
      "Epoch 183/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2329\n",
      "Epoch 00183: val_loss did not improve from 3.48355\n",
      "\n",
      "[20210302-1301-33] Learning rate for epoch 183 is 0.002944914624094963\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.2329 - val_loss: 3.5052\n",
      "Epoch 184/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0572\n",
      "Epoch 00184: val_loss did not improve from 3.48355\n",
      "\n",
      "[20210302-1301-38] Learning rate for epoch 184 is 0.0027779308147728443\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 7.0572 - val_loss: 3.6031\n",
      "Epoch 185/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0502\n",
      "Epoch 00185: val_loss did not improve from 3.48355\n",
      "\n",
      "[20210302-1301-42] Learning rate for epoch 185 is 0.0026113467756658792\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.0502 - val_loss: 3.7851\n",
      "Epoch 186/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2862\n",
      "Epoch 00186: val_loss improved from 3.48355 to 3.39279, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1301-48] Learning rate for epoch 186 is 0.002445162972435355\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 6.2862 - val_loss: 3.3928\n",
      "Epoch 187/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7597\n",
      "Epoch 00187: val_loss did not improve from 3.39279\n",
      "\n",
      "[20210302-1301-53] Learning rate for epoch 187 is 0.0022793791722506285\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.7597 - val_loss: 3.9619\n",
      "Epoch 188/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1223\n",
      "Epoch 00188: val_loss did not improve from 3.39279\n",
      "\n",
      "[20210302-1301-58] Learning rate for epoch 188 is 0.0021139951422810555\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.1223 - val_loss: 3.4446\n",
      "Epoch 189/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5889\n",
      "Epoch 00189: val_loss did not improve from 3.39279\n",
      "\n",
      "[20210302-1302-02] Learning rate for epoch 189 is 0.0019490111153572798\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.5889 - val_loss: 3.4227\n",
      "Epoch 190/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0059\n",
      "Epoch 00190: val_loss did not improve from 3.39279\n",
      "\n",
      "[20210302-1302-07] Learning rate for epoch 190 is 0.0017844270914793015\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.0059 - val_loss: 3.8604\n",
      "Epoch 191/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6385\n",
      "Epoch 00191: val_loss did not improve from 3.39279\n",
      "\n",
      "[20210302-1302-12] Learning rate for epoch 191 is 0.0016202429542317986\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.6385 - val_loss: 3.4581\n",
      "Epoch 192/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7109\n",
      "Epoch 00192: val_loss did not improve from 3.39279\n",
      "\n",
      "[20210302-1302-17] Learning rate for epoch 192 is 0.001456458936445415\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.7109 - val_loss: 3.9736\n",
      "Epoch 193/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9892\n",
      "Epoch 00193: val_loss did not improve from 3.39279\n",
      "\n",
      "[20210302-1302-22] Learning rate for epoch 193 is 0.001293074688874185\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.9892 - val_loss: 4.6488\n",
      "Epoch 194/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3398\n",
      "Epoch 00194: val_loss did not improve from 3.39279\n",
      "\n",
      "[20210302-1302-26] Learning rate for epoch 194 is 0.0011300905607640743\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.3398 - val_loss: 3.4273\n",
      "Epoch 195/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7809\n",
      "Epoch 00195: val_loss did not improve from 3.39279\n",
      "\n",
      "[20210302-1302-31] Learning rate for epoch 195 is 0.0009675062610767782\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.7809 - val_loss: 3.5539\n",
      "Epoch 196/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5564\n",
      "Epoch 00196: val_loss did not improve from 3.39279\n",
      "\n",
      "[20210302-1302-36] Learning rate for epoch 196 is 0.0008053220226429403\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.5564 - val_loss: 3.6251\n",
      "Epoch 197/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4517\n",
      "Epoch 00197: val_loss did not improve from 3.39279\n",
      "\n",
      "[20210302-1302-41] Learning rate for epoch 197 is 0.0006435376708395779\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.4517 - val_loss: 3.5202\n",
      "Epoch 198/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7915\n",
      "Epoch 00198: val_loss did not improve from 3.39279\n",
      "\n",
      "[20210302-1302-46] Learning rate for epoch 198 is 0.0004821533220820129\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.7915 - val_loss: 3.5300\n",
      "Epoch 199/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6939\n",
      "Epoch 00199: val_loss did not improve from 3.39279\n",
      "\n",
      "[20210302-1302-51] Learning rate for epoch 199 is 0.0003211689181625843\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6939 - val_loss: 3.5309\n",
      "Epoch 200/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4742\n",
      "Epoch 00200: val_loss did not improve from 3.39279\n",
      "\n",
      "[20210302-1302-55] Learning rate for epoch 200 is 0.00016058447363320738\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.4742 - val_loss: 3.5069\n",
      "Epoch 201/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6227\n",
      "Epoch 00201: val_loss did not improve from 3.39279\n",
      "\n",
      "[20210302-1303-00] Learning rate for epoch 201 is 4.0000000467443897e-07\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.6227 - val_loss: 3.5143\n",
      "Epoch 202/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5159\n",
      "Epoch 00202: val_loss did not improve from 3.39279\n",
      "\n",
      "[20210302-1303-05] Learning rate for epoch 202 is 0.0001601835247129202\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.5159 - val_loss: 3.5443\n",
      "Epoch 203/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3200\n",
      "Epoch 00203: val_loss did not improve from 3.39279\n",
      "\n",
      "[20210302-1303-10] Learning rate for epoch 203 is 0.00031956707243807614\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.3200 - val_loss: 3.5662\n",
      "Epoch 204/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5178\n",
      "Epoch 00204: val_loss did not improve from 3.39279\n",
      "\n",
      "[20210302-1303-15] Learning rate for epoch 204 is 0.00047855067532509565\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.5178 - val_loss: 3.7532\n",
      "Epoch 205/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7429\n",
      "Epoch 00205: val_loss did not improve from 3.39279\n",
      "\n",
      "[20210302-1303-19] Learning rate for epoch 205 is 0.0006371343042701483\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.7429 - val_loss: 3.4962\n",
      "Epoch 206/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8480\n",
      "Epoch 00206: val_loss did not improve from 3.39279\n",
      "\n",
      "[20210302-1303-24] Learning rate for epoch 206 is 0.0007953179883770645\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.8480 - val_loss: 4.3170\n",
      "Epoch 207/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7115\n",
      "Epoch 00207: val_loss did not improve from 3.39279\n",
      "\n",
      "[20210302-1303-29] Learning rate for epoch 207 is 0.0009531017276458442\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.7115 - val_loss: 3.9333\n",
      "Epoch 208/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8963\n",
      "Epoch 00208: val_loss did not improve from 3.39279\n",
      "\n",
      "[20210302-1303-34] Learning rate for epoch 208 is 0.0011104855220764875\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.8963 - val_loss: 3.7370\n",
      "Epoch 209/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4155\n",
      "Epoch 00209: val_loss did not improve from 3.39279\n",
      "\n",
      "[20210302-1303-39] Learning rate for epoch 209 is 0.0012674692552536726\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.4155 - val_loss: 3.7909\n",
      "Epoch 210/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0999\n",
      "Epoch 00210: val_loss did not improve from 3.39279\n",
      "\n",
      "[20210302-1303-44] Learning rate for epoch 210 is 0.0014240531018003821\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.0999 - val_loss: 4.3866\n",
      "Epoch 211/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9146\n",
      "Epoch 00211: val_loss did not improve from 3.39279\n",
      "\n",
      "[20210302-1303-48] Learning rate for epoch 211 is 0.0015802369453012943\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.9146 - val_loss: 4.5585\n",
      "Epoch 212/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0830\n",
      "Epoch 00212: val_loss did not improve from 3.39279\n",
      "\n",
      "[20210302-1303-53] Learning rate for epoch 212 is 0.001736020902171731\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.0830 - val_loss: 3.9591\n",
      "Epoch 213/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6164\n",
      "Epoch 00213: val_loss did not improve from 3.39279\n",
      "\n",
      "[20210302-1303-58] Learning rate for epoch 213 is 0.0018914048559963703\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.6164 - val_loss: 4.7171\n",
      "Epoch 214/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1033\n",
      "Epoch 00214: val_loss did not improve from 3.39279\n",
      "\n",
      "[20210302-1304-03] Learning rate for epoch 214 is 0.0020463888067752123\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.1033 - val_loss: 3.4741\n",
      "Epoch 215/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5350\n",
      "Epoch 00215: val_loss did not improve from 3.39279\n",
      "\n",
      "[20210302-1304-08] Learning rate for epoch 215 is 0.0022009729873389006\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.5350 - val_loss: 3.7466\n",
      "Epoch 216/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3746\n",
      "Epoch 00216: val_loss did not improve from 3.39279\n",
      "\n",
      "[20210302-1304-13] Learning rate for epoch 216 is 0.002355156932026148\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.3746 - val_loss: 3.8193\n",
      "Epoch 217/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8526\n",
      "Epoch 00217: val_loss improved from 3.39279 to 3.32682, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1304-18] Learning rate for epoch 217 is 0.0025089411064982414\n",
      "20/20 [==============================] - 2s 92ms/step - loss: 5.8526 - val_loss: 3.3268\n",
      "Epoch 218/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3166\n",
      "Epoch 00218: val_loss did not improve from 3.32682\n",
      "\n",
      "[20210302-1304-23] Learning rate for epoch 218 is 0.0026623252779245377\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.3166 - val_loss: 3.7297\n",
      "Epoch 219/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7144\n",
      "Epoch 00219: val_loss did not improve from 3.32682\n",
      "\n",
      "[20210302-1304-27] Learning rate for epoch 219 is 0.0028153094463050365\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.7144 - val_loss: 3.5608\n",
      "Epoch 220/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8730\n",
      "Epoch 00220: val_loss did not improve from 3.32682\n",
      "\n",
      "[20210302-1304-32] Learning rate for epoch 220 is 0.002967893611639738\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8730 - val_loss: 4.3507\n",
      "Epoch 221/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9873\n",
      "Epoch 00221: val_loss did not improve from 3.32682\n",
      "\n",
      "[20210302-1304-37] Learning rate for epoch 221 is 0.003120078006759286\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.9873 - val_loss: 4.3609\n",
      "Epoch 222/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7099\n",
      "Epoch 00222: val_loss did not improve from 3.32682\n",
      "\n",
      "[20210302-1304-42] Learning rate for epoch 222 is 0.0032718623988330364\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.7099 - val_loss: 3.4017\n",
      "Epoch 223/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6249\n",
      "Epoch 00223: val_loss did not improve from 3.32682\n",
      "\n",
      "[20210302-1304-47] Learning rate for epoch 223 is 0.0034232467878609896\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6249 - val_loss: 5.1519\n",
      "Epoch 224/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5083\n",
      "Epoch 00224: val_loss did not improve from 3.32682\n",
      "\n",
      "[20210302-1304-52] Learning rate for epoch 224 is 0.0035742311738431454\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.5083 - val_loss: 5.5024\n",
      "Epoch 225/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2707\n",
      "Epoch 00225: val_loss did not improve from 3.32682\n",
      "\n",
      "[20210302-1304-57] Learning rate for epoch 225 is 0.003724815556779504\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.2707 - val_loss: 4.1819\n",
      "Epoch 226/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7898\n",
      "Epoch 00226: val_loss did not improve from 3.32682\n",
      "\n",
      "[20210302-1305-01] Learning rate for epoch 226 is 0.003874999936670065\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.7898 - val_loss: 5.8963\n",
      "Epoch 227/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5446\n",
      "Epoch 00227: val_loss did not improve from 3.32682\n",
      "\n",
      "[20210302-1305-06] Learning rate for epoch 227 is 0.0037152154836803675\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.5446 - val_loss: 3.8923\n",
      "Epoch 228/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6055\n",
      "Epoch 00228: val_loss did not improve from 3.32682\n",
      "\n",
      "[20210302-1305-11] Learning rate for epoch 228 is 0.0035558310337364674\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.6055 - val_loss: 4.6011\n",
      "Epoch 229/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7732\n",
      "Epoch 00229: val_loss did not improve from 3.32682\n",
      "\n",
      "[20210302-1305-16] Learning rate for epoch 229 is 0.003396846354007721\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.7732 - val_loss: 3.5517\n",
      "Epoch 230/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0179\n",
      "Epoch 00230: val_loss did not improve from 3.32682\n",
      "\n",
      "[20210302-1305-21] Learning rate for epoch 230 is 0.003238261677324772\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.0179 - val_loss: 3.8108\n",
      "Epoch 231/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6526\n",
      "Epoch 00231: val_loss did not improve from 3.32682\n",
      "\n",
      "[20210302-1305-26] Learning rate for epoch 231 is 0.00308007700368762\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6526 - val_loss: 3.3514\n",
      "Epoch 232/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9409\n",
      "Epoch 00232: val_loss did not improve from 3.32682\n",
      "\n",
      "[20210302-1305-30] Learning rate for epoch 232 is 0.002922292333096266\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.9409 - val_loss: 4.0931\n",
      "Epoch 233/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6130\n",
      "Epoch 00233: val_loss did not improve from 3.32682\n",
      "\n",
      "[20210302-1305-35] Learning rate for epoch 233 is 0.002764907432720065\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.6130 - val_loss: 3.8841\n",
      "Epoch 234/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7871\n",
      "Epoch 00234: val_loss did not improve from 3.32682\n",
      "\n",
      "[20210302-1305-40] Learning rate for epoch 234 is 0.0026079227682203054\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.7871 - val_loss: 3.7295\n",
      "Epoch 235/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4370\n",
      "Epoch 00235: val_loss did not improve from 3.32682\n",
      "\n",
      "[20210302-1305-45] Learning rate for epoch 235 is 0.0024513378739356995\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.4370 - val_loss: 3.8174\n",
      "Epoch 236/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7106\n",
      "Epoch 00236: val_loss did not improve from 3.32682\n",
      "\n",
      "[20210302-1305-50] Learning rate for epoch 236 is 0.002295152982696891\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.7106 - val_loss: 4.0103\n",
      "Epoch 237/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9010\n",
      "Epoch 00237: val_loss did not improve from 3.32682\n",
      "\n",
      "[20210302-1305-54] Learning rate for epoch 237 is 0.0021393680945038795\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.9010 - val_loss: 4.4524\n",
      "Epoch 238/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9410\n",
      "Epoch 00238: val_loss did not improve from 3.32682\n",
      "\n",
      "[20210302-1305-59] Learning rate for epoch 238 is 0.0019839832093566656\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.9410 - val_loss: 4.1155\n",
      "Epoch 239/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7743\n",
      "Epoch 00239: val_loss did not improve from 3.32682\n",
      "\n",
      "[20210302-1306-04] Learning rate for epoch 239 is 0.0018289980944246054\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.7743 - val_loss: 4.7158\n",
      "Epoch 240/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6061\n",
      "Epoch 00240: val_loss did not improve from 3.32682\n",
      "\n",
      "[20210302-1306-09] Learning rate for epoch 240 is 0.0016744130989536643\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.6061 - val_loss: 5.5742\n",
      "Epoch 241/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6134\n",
      "Epoch 00241: val_loss did not improve from 3.32682\n",
      "\n",
      "[20210302-1306-14] Learning rate for epoch 241 is 0.0015202279901131988\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.6134 - val_loss: 3.8843\n",
      "Epoch 242/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7225\n",
      "Epoch 00242: val_loss did not improve from 3.32682\n",
      "\n",
      "[20210302-1306-18] Learning rate for epoch 242 is 0.0013664428843185306\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.7225 - val_loss: 3.5958\n",
      "Epoch 243/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3953\n",
      "Epoch 00243: val_loss did not improve from 3.32682\n",
      "\n",
      "[20210302-1306-23] Learning rate for epoch 243 is 0.0012130576651543379\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.3953 - val_loss: 3.4229\n",
      "Epoch 244/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1752\n",
      "Epoch 00244: val_loss did not improve from 3.32682\n",
      "\n",
      "[20210302-1306-28] Learning rate for epoch 244 is 0.0010600725654512644\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.1752 - val_loss: 3.3671\n",
      "Epoch 245/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3752\n",
      "Epoch 00245: val_loss did not improve from 3.32682\n",
      "\n",
      "[20210302-1306-33] Learning rate for epoch 245 is 0.0009074872941710055\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.3752 - val_loss: 3.3334\n",
      "Epoch 246/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4788\n",
      "Epoch 00246: val_loss improved from 3.32682 to 3.31580, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1306-38] Learning rate for epoch 246 is 0.0007553020259365439\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 5.4788 - val_loss: 3.3158\n",
      "Epoch 247/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1031\n",
      "Epoch 00247: val_loss did not improve from 3.31580\n",
      "\n",
      "[20210302-1306-43] Learning rate for epoch 247 is 0.0006035167025402188\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.1031 - val_loss: 3.4237\n",
      "Epoch 248/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1039\n",
      "Epoch 00248: val_loss did not improve from 3.31580\n",
      "\n",
      "[20210302-1306-48] Learning rate for epoch 248 is 0.00045213132398203015\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.1039 - val_loss: 3.3745\n",
      "Epoch 249/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6492\n",
      "Epoch 00249: val_loss did not improve from 3.31580\n",
      "\n",
      "[20210302-1306-52] Learning rate for epoch 249 is 0.00030114591936580837\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.6492 - val_loss: 3.3867\n",
      "Epoch 250/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5479\n",
      "Epoch 00250: val_loss did not improve from 3.31580\n",
      "\n",
      "[20210302-1306-57] Learning rate for epoch 250 is 0.00015056047413963825\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.5479 - val_loss: 3.3866\n",
      "Epoch 251/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3566\n",
      "Epoch 00251: val_loss did not improve from 3.31580\n",
      "\n",
      "[20210302-1307-02] Learning rate for epoch 251 is 3.7500001326407073e-07\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.3566 - val_loss: 3.3930\n",
      "Epoch 252/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4566\n",
      "Epoch 00252: val_loss did not improve from 3.31580\n",
      "\n",
      "[20210302-1307-07] Learning rate for epoch 252 is 0.00015015952521935105\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.4566 - val_loss: 3.3867\n",
      "Epoch 253/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6656\n",
      "Epoch 00253: val_loss did not improve from 3.31580\n",
      "\n",
      "[20210302-1307-12] Learning rate for epoch 253 is 0.0002995440736413002\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.6656 - val_loss: 3.3239\n",
      "Epoch 254/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8051\n",
      "Epoch 00254: val_loss did not improve from 3.31580\n",
      "\n",
      "[20210302-1307-17] Learning rate for epoch 254 is 0.0004485286772251129\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.8051 - val_loss: 3.5267\n",
      "Epoch 255/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2972\n",
      "Epoch 00255: val_loss did not improve from 3.31580\n",
      "\n",
      "[20210302-1307-21] Learning rate for epoch 255 is 0.0005971133359707892\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.2972 - val_loss: 3.5596\n",
      "Epoch 256/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4129\n",
      "Epoch 00256: val_loss did not improve from 3.31580\n",
      "\n",
      "[20210302-1307-26] Learning rate for epoch 256 is 0.0007452979916706681\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.4129 - val_loss: 3.7097\n",
      "Epoch 257/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2755\n",
      "Epoch 00257: val_loss did not improve from 3.31580\n",
      "\n",
      "[20210302-1307-31] Learning rate for epoch 257 is 0.0008930827025324106\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.2755 - val_loss: 3.6712\n",
      "Epoch 258/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0877\n",
      "Epoch 00258: val_loss did not improve from 3.31580\n",
      "\n",
      "[20210302-1307-36] Learning rate for epoch 258 is 0.0010404675267636776\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.0877 - val_loss: 3.7023\n",
      "Epoch 259/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6423\n",
      "Epoch 00259: val_loss did not improve from 3.31580\n",
      "\n",
      "[20210302-1307-41] Learning rate for epoch 259 is 0.0011874522315338254\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6423 - val_loss: 3.6496\n",
      "Epoch 260/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2456\n",
      "Epoch 00260: val_loss did not improve from 3.31580\n",
      "\n",
      "[20210302-1307-45] Learning rate for epoch 260 is 0.0013340371660888195\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.2456 - val_loss: 3.6375\n",
      "Epoch 261/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3331\n",
      "Epoch 00261: val_loss did not improve from 3.31580\n",
      "\n",
      "[20210302-1307-50] Learning rate for epoch 261 is 0.0014802219811826944\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.3331 - val_loss: 3.9522\n",
      "Epoch 262/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3614\n",
      "Epoch 00262: val_loss did not improve from 3.31580\n",
      "\n",
      "[20210302-1307-55] Learning rate for epoch 262 is 0.0016260069096460938\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.3614 - val_loss: 3.8734\n",
      "Epoch 263/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5733\n",
      "Epoch 00263: val_loss did not improve from 3.31580\n",
      "\n",
      "[20210302-1308-00] Learning rate for epoch 263 is 0.001771391835063696\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5733 - val_loss: 4.0141\n",
      "Epoch 264/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2042\n",
      "Epoch 00264: val_loss did not improve from 3.31580\n",
      "\n",
      "[20210302-1308-05] Learning rate for epoch 264 is 0.0019163768738508224\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.2042 - val_loss: 3.5564\n",
      "Epoch 265/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.0018\n",
      "Epoch 00265: val_loss did not improve from 3.31580\n",
      "\n",
      "[20210302-1308-09] Learning rate for epoch 265 is 0.0020609619095921516\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.0018 - val_loss: 4.4801\n",
      "Epoch 266/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6401\n",
      "Epoch 00266: val_loss did not improve from 3.31580\n",
      "\n",
      "[20210302-1308-14] Learning rate for epoch 266 is 0.0022051469422876835\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6401 - val_loss: 3.5567\n",
      "Epoch 267/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4274\n",
      "Epoch 00267: val_loss improved from 3.31580 to 3.20673, saving model to ./20210301-225844/heel_K11_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1308-20] Learning rate for epoch 267 is 0.0023489322047680616\n",
      "20/20 [==============================] - 2s 91ms/step - loss: 5.4274 - val_loss: 3.2067\n",
      "Epoch 268/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4158\n",
      "Epoch 00268: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1308-25] Learning rate for epoch 268 is 0.002492317231371999\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.4158 - val_loss: 3.4934\n",
      "Epoch 269/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3408\n",
      "Epoch 00269: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1308-30] Learning rate for epoch 269 is 0.0026353024877607822\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 5.3408 - val_loss: 3.5258\n",
      "Epoch 270/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6173\n",
      "Epoch 00270: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1308-34] Learning rate for epoch 270 is 0.0027778877411037683\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.6173 - val_loss: 4.4644\n",
      "Epoch 271/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6275\n",
      "Epoch 00271: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1308-39] Learning rate for epoch 271 is 0.002920072991400957\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6275 - val_loss: 3.8277\n",
      "Epoch 272/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7646\n",
      "Epoch 00272: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1308-44] Learning rate for epoch 272 is 0.0030618582386523485\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.7646 - val_loss: 4.1419\n",
      "Epoch 273/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4939\n",
      "Epoch 00273: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1308-49] Learning rate for epoch 273 is 0.0032032437156885862\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.4939 - val_loss: 4.7884\n",
      "Epoch 274/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2297\n",
      "Epoch 00274: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1308-54] Learning rate for epoch 274 is 0.0033442291896790266\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.2297 - val_loss: 5.8507\n",
      "Epoch 275/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1248\n",
      "Epoch 00275: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1308-58] Learning rate for epoch 275 is 0.003484814427793026\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.1248 - val_loss: 3.9647\n",
      "Epoch 276/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8645\n",
      "Epoch 00276: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1309-03] Learning rate for epoch 276 is 0.0036249998956918716\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.8645 - val_loss: 4.9575\n",
      "Epoch 277/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5738\n",
      "Epoch 00277: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1309-08] Learning rate for epoch 277 is 0.0034752145875245333\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5738 - val_loss: 6.0831\n",
      "Epoch 278/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6335\n",
      "Epoch 00278: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1309-13] Learning rate for epoch 278 is 0.003325828816741705\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.6335 - val_loss: 5.3871\n",
      "Epoch 279/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8140\n",
      "Epoch 00279: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1309-18] Learning rate for epoch 279 is 0.0031768432818353176\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8140 - val_loss: 4.0776\n",
      "Epoch 280/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6255\n",
      "Epoch 00280: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1309-23] Learning rate for epoch 280 is 0.0030282577499747276\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.6255 - val_loss: 4.2831\n",
      "Epoch 281/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4554\n",
      "Epoch 00281: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1309-27] Learning rate for epoch 281 is 0.0028800719883292913\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.4554 - val_loss: 4.8508\n",
      "Epoch 282/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4316\n",
      "Epoch 00282: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1309-32] Learning rate for epoch 282 is 0.0027322862297296524\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.4316 - val_loss: 3.8209\n",
      "Epoch 283/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.0067\n",
      "Epoch 00283: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1309-37] Learning rate for epoch 283 is 0.002584900474175811\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.0067 - val_loss: 3.8049\n",
      "Epoch 284/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5215\n",
      "Epoch 00284: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1309-42] Learning rate for epoch 284 is 0.0024379147216677666\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.5215 - val_loss: 3.7958\n",
      "Epoch 285/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6929\n",
      "Epoch 00285: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1309-47] Learning rate for epoch 285 is 0.0022913289722055197\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6929 - val_loss: 4.0367\n",
      "Epoch 286/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4342\n",
      "Epoch 00286: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1309-51] Learning rate for epoch 286 is 0.0021451429929584265\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.4342 - val_loss: 3.7507\n",
      "Epoch 287/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3635\n",
      "Epoch 00287: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1309-56] Learning rate for epoch 287 is 0.0019993570167571306\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.3635 - val_loss: 3.5834\n",
      "Epoch 288/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0050\n",
      "Epoch 00288: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1310-01] Learning rate for epoch 288 is 0.001853971160016954\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.0050 - val_loss: 3.6346\n",
      "Epoch 289/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4231\n",
      "Epoch 00289: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1310-06] Learning rate for epoch 289 is 0.001708985073491931\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.4231 - val_loss: 3.9187\n",
      "Epoch 290/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5242\n",
      "Epoch 00290: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1310-11] Learning rate for epoch 290 is 0.0015643991064280272\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.5242 - val_loss: 3.5321\n",
      "Epoch 291/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3083\n",
      "Epoch 00291: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1310-15] Learning rate for epoch 291 is 0.0014202130259945989\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.3083 - val_loss: 3.9328\n",
      "Epoch 292/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7493\n",
      "Epoch 00292: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1310-20] Learning rate for epoch 292 is 0.001276426832191646\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.7493 - val_loss: 3.4221\n",
      "Epoch 293/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9362\n",
      "Epoch 00293: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1310-25] Learning rate for epoch 293 is 0.0011330407578498125\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.9362 - val_loss: 3.6961\n",
      "Epoch 294/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4713\n",
      "Epoch 00294: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1310-30] Learning rate for epoch 294 is 0.0009900545701384544\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.4713 - val_loss: 3.9803\n",
      "Epoch 295/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.0909\n",
      "Epoch 00295: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1310-35] Learning rate for epoch 295 is 0.0008474682690575719\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 5.0909 - val_loss: 3.5101\n",
      "Epoch 296/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2461\n",
      "Epoch 00296: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1310-40] Learning rate for epoch 296 is 0.0007052819710224867\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.2461 - val_loss: 3.7554\n",
      "Epoch 297/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2779\n",
      "Epoch 00297: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1310-45] Learning rate for epoch 297 is 0.0005634956760331988\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.2779 - val_loss: 3.6772\n",
      "Epoch 298/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 4.9785\n",
      "Epoch 00298: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1310-49] Learning rate for epoch 298 is 0.0004221093258820474\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 4.9785 - val_loss: 3.5983\n",
      "Epoch 299/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1500\n",
      "Epoch 00299: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1310-54] Learning rate for epoch 299 is 0.00028112292056903243\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.1500 - val_loss: 3.4994\n",
      "Epoch 300/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0304\n",
      "Epoch 00300: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1310-59] Learning rate for epoch 300 is 0.0001405364746460691\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.0304 - val_loss: 3.4472\n",
      "Epoch 301/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 4.9285\n",
      "Epoch 00301: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1311-04] Learning rate for epoch 301 is 3.4999999343199306e-07\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 4.9285 - val_loss: 3.4309\n",
      "Epoch 302/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1963\n",
      "Epoch 00302: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1311-09] Learning rate for epoch 302 is 0.00014013552572578192\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.1963 - val_loss: 3.4457\n",
      "Epoch 303/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6894\n",
      "Epoch 00303: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1311-13] Learning rate for epoch 303 is 0.00027952107484452426\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.6894 - val_loss: 3.4501\n",
      "Epoch 304/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2387\n",
      "Epoch 00304: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1311-18] Learning rate for epoch 304 is 0.0004185066791251302\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.2387 - val_loss: 3.3888\n",
      "Epoch 305/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1317\n",
      "Epoch 00305: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1311-23] Learning rate for epoch 305 is 0.0005570923094637692\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.1317 - val_loss: 3.9746\n",
      "Epoch 306/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 4.9882\n",
      "Epoch 00306: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1311-28] Learning rate for epoch 306 is 0.0006952779949642718\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 4.9882 - val_loss: 3.8060\n",
      "Epoch 307/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 4.9507\n",
      "Epoch 00307: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1311-33] Learning rate for epoch 307 is 0.0008330637356266379\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 4.9507 - val_loss: 3.5977\n",
      "Epoch 308/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1093\n",
      "Epoch 00308: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1311-37] Learning rate for epoch 308 is 0.0009704494732432067\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.1093 - val_loss: 3.5218\n",
      "Epoch 309/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2453\n",
      "Epoch 00309: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1311-42] Learning rate for epoch 309 is 0.0011074353242293\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.2453 - val_loss: 3.9044\n",
      "Epoch 310/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3588\n",
      "Epoch 00310: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1311-47] Learning rate for epoch 310 is 0.001244021113961935\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.3588 - val_loss: 3.4375\n",
      "Epoch 311/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3942\n",
      "Epoch 00311: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1311-52] Learning rate for epoch 311 is 0.0013802070170640945\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.3942 - val_loss: 3.8393\n",
      "Epoch 312/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7818\n",
      "Epoch 00312: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1311-57] Learning rate for epoch 312 is 0.0015159929171204567\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.7818 - val_loss: 4.6483\n",
      "Epoch 313/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3816\n",
      "Epoch 00313: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1312-01] Learning rate for epoch 313 is 0.0016513789305463433\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.3816 - val_loss: 5.2599\n",
      "Epoch 314/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3772\n",
      "Epoch 00314: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1312-06] Learning rate for epoch 314 is 0.0017863648245111108\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.3772 - val_loss: 4.9616\n",
      "Epoch 315/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3995\n",
      "Epoch 00315: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1312-11] Learning rate for epoch 315 is 0.0019209509482607245\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.3995 - val_loss: 4.3908\n",
      "Epoch 316/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4970\n",
      "Epoch 00316: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1312-16] Learning rate for epoch 316 is 0.002055136952549219\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.4970 - val_loss: 4.9359\n",
      "Epoch 317/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6184\n",
      "Epoch 00317: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1312-21] Learning rate for epoch 317 is 0.002188923070207238\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.6184 - val_loss: 3.7545\n",
      "Epoch 318/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2808\n",
      "Epoch 00318: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1312-25] Learning rate for epoch 318 is 0.00232230918481946\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.2808 - val_loss: 3.4562\n",
      "Epoch 319/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7503\n",
      "Epoch 00319: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1312-30] Learning rate for epoch 319 is 0.002455295529216528\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.7503 - val_loss: 4.4217\n",
      "Epoch 320/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4712\n",
      "Epoch 00320: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1312-35] Learning rate for epoch 320 is 0.002587881637737155\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.4712 - val_loss: 3.7906\n",
      "Epoch 321/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6785\n",
      "Epoch 00321: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1312-40] Learning rate for epoch 321 is 0.0027200679760426283\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6785 - val_loss: 3.8903\n",
      "Epoch 322/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4430\n",
      "Epoch 00322: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1312-45] Learning rate for epoch 322 is 0.0028518543113023043\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.4430 - val_loss: 3.7388\n",
      "Epoch 323/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2173\n",
      "Epoch 00323: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1312-55] Learning rate for epoch 323 is 0.002983240643516183\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.2173 - val_loss: 4.7290\n",
      "Epoch 324/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6483\n",
      "Epoch 00324: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1313-00] Learning rate for epoch 324 is 0.003114226972684264\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.6483 - val_loss: 4.6586\n",
      "Epoch 325/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4199\n",
      "Epoch 00325: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1313-05] Learning rate for epoch 325 is 0.0032448135316371918\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.4199 - val_loss: 4.0718\n",
      "Epoch 326/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7111\n",
      "Epoch 00326: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1313-10] Learning rate for epoch 326 is 0.003375000087544322\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.7111 - val_loss: 3.9285\n",
      "Epoch 327/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4536\n",
      "Epoch 00327: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1313-15] Learning rate for epoch 327 is 0.0032352134585380554\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.4536 - val_loss: 4.1257\n",
      "Epoch 328/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9270\n",
      "Epoch 00328: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1313-20] Learning rate for epoch 328 is 0.003095826832577586\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.9270 - val_loss: 4.7395\n",
      "Epoch 329/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3560\n",
      "Epoch 00329: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1313-25] Learning rate for epoch 329 is 0.0029568402096629143\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.3560 - val_loss: 4.5957\n",
      "Epoch 330/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2758\n",
      "Epoch 00330: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1313-30] Learning rate for epoch 330 is 0.0028182535897940397\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 5.2758 - val_loss: 3.7060\n",
      "Epoch 331/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3241\n",
      "Epoch 00331: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1313-34] Learning rate for epoch 331 is 0.0026800669729709625\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.3241 - val_loss: 4.6280\n",
      "Epoch 332/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1000\n",
      "Epoch 00332: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1313-39] Learning rate for epoch 332 is 0.0025422803591936827\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.1000 - val_loss: 4.8879\n",
      "Epoch 333/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9420\n",
      "Epoch 00333: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1313-44] Learning rate for epoch 333 is 0.0024048935156315565\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.9420 - val_loss: 3.9249\n",
      "Epoch 334/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8312\n",
      "Epoch 00334: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1313-49] Learning rate for epoch 334 is 0.0022679066751152277\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8312 - val_loss: 3.8285\n",
      "Epoch 335/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6148\n",
      "Epoch 00335: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1313-54] Learning rate for epoch 335 is 0.0021313198376446962\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6148 - val_loss: 4.3250\n",
      "Epoch 336/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8837\n",
      "Epoch 00336: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1313-59] Learning rate for epoch 336 is 0.001995133003219962\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.8837 - val_loss: 6.1630\n",
      "Epoch 337/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5485\n",
      "Epoch 00337: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1314-04] Learning rate for epoch 337 is 0.0018593460554257035\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.5485 - val_loss: 3.8224\n",
      "Epoch 338/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6588\n",
      "Epoch 00338: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1314-08] Learning rate for epoch 338 is 0.0017239591106772423\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.6588 - val_loss: 3.7964\n",
      "Epoch 339/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3702\n",
      "Epoch 00339: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1314-13] Learning rate for epoch 339 is 0.0015889721689745784\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.3702 - val_loss: 3.5739\n",
      "Epoch 340/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2854\n",
      "Epoch 00340: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1314-18] Learning rate for epoch 340 is 0.00145438511390239\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.2854 - val_loss: 5.0058\n",
      "Epoch 341/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8174\n",
      "Epoch 00341: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1314-23] Learning rate for epoch 341 is 0.0013201979454606771\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8174 - val_loss: 3.4115\n",
      "Epoch 342/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2956\n",
      "Epoch 00342: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1314-28] Learning rate for epoch 342 is 0.0011864108964800835\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.2956 - val_loss: 4.3625\n",
      "Epoch 343/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8685\n",
      "Epoch 00343: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1314-33] Learning rate for epoch 343 is 0.0010530237341299653\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.8685 - val_loss: 3.6111\n",
      "Epoch 344/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5082\n",
      "Epoch 00344: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1314-37] Learning rate for epoch 344 is 0.0009200365166179836\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.5082 - val_loss: 3.6652\n",
      "Epoch 345/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6234\n",
      "Epoch 00345: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1314-42] Learning rate for epoch 345 is 0.0007874493021517992\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6234 - val_loss: 3.7029\n",
      "Epoch 346/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3550\n",
      "Epoch 00346: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1314-47] Learning rate for epoch 346 is 0.0006552619743160903\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.3550 - val_loss: 3.6890\n",
      "Epoch 347/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.0582\n",
      "Epoch 00347: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1314-52] Learning rate for epoch 347 is 0.0005234747077338398\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.0582 - val_loss: 3.7830\n",
      "Epoch 348/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7096\n",
      "Epoch 00348: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1314-57] Learning rate for epoch 348 is 0.0003920873277820647\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.7096 - val_loss: 3.6272\n",
      "Epoch 349/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1924\n",
      "Epoch 00349: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1315-02] Learning rate for epoch 349 is 0.0002610999217722565\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.1924 - val_loss: 3.5731\n",
      "Epoch 350/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2955\n",
      "Epoch 00350: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1315-07] Learning rate for epoch 350 is 0.00013051247515249997\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.2955 - val_loss: 3.5442\n",
      "Epoch 351/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4970\n",
      "Epoch 00351: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1315-11] Learning rate for epoch 351 is 3.250000020216248e-07\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.4970 - val_loss: 3.5201\n",
      "Epoch 352/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3901\n",
      "Epoch 00352: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1315-16] Learning rate for epoch 352 is 0.00013011152623221278\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.3901 - val_loss: 3.4860\n",
      "Epoch 353/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1350\n",
      "Epoch 00353: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1315-21] Learning rate for epoch 353 is 0.00025949807604774833\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 5.1350 - val_loss: 3.3768\n",
      "Epoch 354/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1962\n",
      "Epoch 00354: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1315-26] Learning rate for epoch 354 is 0.00038848468102514744\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.1962 - val_loss: 3.4797\n",
      "Epoch 355/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5770\n",
      "Epoch 00355: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1315-31] Learning rate for epoch 355 is 0.0005170713411644101\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.5770 - val_loss: 3.8225\n",
      "Epoch 356/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.0624\n",
      "Epoch 00356: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1315-36] Learning rate for epoch 356 is 0.0006452579982578754\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.0624 - val_loss: 3.4644\n",
      "Epoch 357/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1188\n",
      "Epoch 00357: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1315-41] Learning rate for epoch 357 is 0.0007730447105132043\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.1188 - val_loss: 3.5723\n",
      "Epoch 358/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 4.9688\n",
      "Epoch 00358: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1315-45] Learning rate for epoch 358 is 0.0009004314779303968\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 4.9688 - val_loss: 3.4058\n",
      "Epoch 359/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 4.9486\n",
      "Epoch 00359: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1315-50] Learning rate for epoch 359 is 0.0010274183005094528\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 4.9486 - val_loss: 3.7605\n",
      "Epoch 360/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3173\n",
      "Epoch 00360: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1315-55] Learning rate for epoch 360 is 0.0011540050618350506\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.3173 - val_loss: 3.6198\n",
      "Epoch 361/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1542\n",
      "Epoch 00361: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1316-00] Learning rate for epoch 361 is 0.0012801920529454947\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.1542 - val_loss: 3.5442\n",
      "Epoch 362/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3695\n",
      "Epoch 00362: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1316-05] Learning rate for epoch 362 is 0.0014059789245948195\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.3695 - val_loss: 6.8542\n",
      "Epoch 363/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4080\n",
      "Epoch 00363: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1316-10] Learning rate for epoch 363 is 0.001531365909613669\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.4080 - val_loss: 4.8742\n",
      "Epoch 364/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3730\n",
      "Epoch 00364: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1316-15] Learning rate for epoch 364 is 0.001656352891586721\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 5.3730 - val_loss: 4.3734\n",
      "Epoch 365/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3828\n",
      "Epoch 00365: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1316-19] Learning rate for epoch 365 is 0.0017809398705139756\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.3828 - val_loss: 4.0839\n",
      "Epoch 366/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2681\n",
      "Epoch 00366: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1316-24] Learning rate for epoch 366 is 0.0019051269628107548\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.2681 - val_loss: 3.8734\n",
      "Epoch 367/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4418\n",
      "Epoch 00367: val_loss did not improve from 3.20673\n",
      "\n",
      "[20210302-1316-29] Learning rate for epoch 367 is 0.0020289141684770584\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.4418 - val_loss: 7.6400\n",
      "K= 12\n",
      "Check cache in memory:Y True\n",
      "Check keypoints process01: Tensor(\"args_1:0\", shape=(2,), dtype=int64) (2,) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Check keypoints process02: Tensor(\"Reshape:0\", shape=(1, 2), dtype=int64) (1, 2) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Check keypoints process03: Tensor(\"PyFunc:1\", dtype=float32, device=/job:localhost/replica:0/task:0)\n",
      "Check keypoints process04: Tensor(\"Reshape_1:0\", shape=(2,), dtype=float32)\n",
      "Check augment :Y True\n",
      "Check ds cache[True] and augment[True]\n",
      "Check cache in memory:Y True\n",
      "Check augment :N False\n",
      "Check ds cache[True] and augment[False]\n",
      "best_model_name: ./20210301-225844/heel_K12_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "base_model.trainable :  False\n",
      "\n",
      "[20210302-1316-32] Learning rate for epoch 1 is 0.00047855067532509565\n",
      "Epoch 1/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 106.4094\n",
      "Epoch 00001: val_loss improved from inf to 73.80949, saving model to ./20210301-225844/heel_K12_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 5s 267ms/step - loss: 104.1923 - val_loss: 73.8095\n",
      "\n",
      "[20210302-1316-47] Learning rate for epoch 2 is 0.00047855067532509565\n",
      "Epoch 2/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 42.1575\n",
      "Epoch 00002: val_loss improved from 73.80949 to 28.16975, saving model to ./20210301-225844/heel_K12_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 41.4827 - val_loss: 28.1698\n",
      "\n",
      "[20210302-1316-52] Learning rate for epoch 3 is 0.00047855067532509565\n",
      "Epoch 3/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 23.4356\n",
      "Epoch 00003: val_loss improved from 28.16975 to 23.63674, saving model to ./20210301-225844/heel_K12_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 8s 402ms/step - loss: 23.4356 - val_loss: 23.6367\n",
      "\n",
      "[20210302-1317-03] Learning rate for epoch 4 is 0.00047855067532509565\n",
      "Epoch 4/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 20.9404\n",
      "Epoch 00004: val_loss improved from 23.63674 to 22.36638, saving model to ./20210301-225844/heel_K12_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 20.8982 - val_loss: 22.3664\n",
      "\n",
      "[20210302-1317-07] Learning rate for epoch 5 is 0.00047855067532509565\n",
      "Epoch 5/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 19.0182\n",
      "Epoch 00005: val_loss improved from 22.36638 to 21.11424, saving model to ./20210301-225844/heel_K12_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 19.0182 - val_loss: 21.1142\n",
      "\n",
      "[20210302-1317-12] Learning rate for epoch 6 is 0.00047855067532509565\n",
      "Epoch 6/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 19.1648\n",
      "Epoch 00006: val_loss did not improve from 21.11424\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 19.1613 - val_loss: 21.2151\n",
      "\n",
      "[20210302-1317-16] Learning rate for epoch 7 is 0.00047855067532509565\n",
      "Epoch 7/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 18.6602\n",
      "Epoch 00007: val_loss improved from 21.11424 to 20.40200, saving model to ./20210301-225844/heel_K12_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 18.5896 - val_loss: 20.4020\n",
      "\n",
      "[20210302-1317-20] Learning rate for epoch 8 is 0.00047855067532509565\n",
      "Epoch 8/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 18.3505\n",
      "Epoch 00008: val_loss improved from 20.40200 to 19.80307, saving model to ./20210301-225844/heel_K12_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 18.3196 - val_loss: 19.8031\n",
      "\n",
      "[20210302-1317-25] Learning rate for epoch 9 is 0.00047855067532509565\n",
      "Epoch 9/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 18.0662\n",
      "Epoch 00009: val_loss improved from 19.80307 to 19.53657, saving model to ./20210301-225844/heel_K12_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 17.9520 - val_loss: 19.5366\n",
      "\n",
      "[20210302-1317-29] Learning rate for epoch 10 is 0.00047855067532509565\n",
      "Epoch 10/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 17.9231\n",
      "Epoch 00010: val_loss did not improve from 19.53657\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 17.9986 - val_loss: 19.6416\n",
      "\n",
      "[20210302-1317-33] Learning rate for epoch 11 is 0.00047855067532509565\n",
      "Epoch 11/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 17.6289\n",
      "Epoch 00011: val_loss improved from 19.53657 to 18.48814, saving model to ./20210301-225844/heel_K12_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 17.6680 - val_loss: 18.4881\n",
      "\n",
      "[20210302-1317-37] Learning rate for epoch 12 is 0.00047855067532509565\n",
      "Epoch 12/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 17.6508\n",
      "Epoch 00012: val_loss improved from 18.48814 to 17.14157, saving model to ./20210301-225844/heel_K12_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 17.6252 - val_loss: 17.1416\n",
      "\n",
      "[20210302-1317-42] Learning rate for epoch 13 is 0.00047855067532509565\n",
      "Epoch 13/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 17.7060\n",
      "Epoch 00013: val_loss improved from 17.14157 to 16.96558, saving model to ./20210301-225844/heel_K12_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 17.5100 - val_loss: 16.9656\n",
      "\n",
      "[20210302-1317-46] Learning rate for epoch 14 is 0.00047855067532509565\n",
      "Epoch 14/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 17.1997\n",
      "Epoch 00014: val_loss did not improve from 16.96558\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 17.1762 - val_loss: 16.9996\n",
      "\n",
      "[20210302-1317-50] Learning rate for epoch 15 is 0.00047855067532509565\n",
      "Epoch 15/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.8770\n",
      "Epoch 00015: val_loss improved from 16.96558 to 16.30367, saving model to ./20210301-225844/heel_K12_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 16.8770 - val_loss: 16.3037\n",
      "\n",
      "[20210302-1317-54] Learning rate for epoch 16 is 0.00047855067532509565\n",
      "Epoch 16/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 17.2306\n",
      "Epoch 00016: val_loss improved from 16.30367 to 15.47965, saving model to ./20210301-225844/heel_K12_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 17.1650 - val_loss: 15.4797\n",
      "\n",
      "[20210302-1317-58] Learning rate for epoch 17 is 0.00047855067532509565\n",
      "Epoch 17/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.6342\n",
      "Epoch 00017: val_loss did not improve from 15.47965\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.6342 - val_loss: 15.6577\n",
      "\n",
      "[20210302-1318-02] Learning rate for epoch 18 is 0.00047855067532509565\n",
      "Epoch 18/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.6447\n",
      "Epoch 00018: val_loss improved from 15.47965 to 14.79525, saving model to ./20210301-225844/heel_K12_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 16.7118 - val_loss: 14.7952\n",
      "\n",
      "[20210302-1318-07] Learning rate for epoch 19 is 0.00047855067532509565\n",
      "Epoch 19/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.8708\n",
      "Epoch 00019: val_loss improved from 14.79525 to 14.05701, saving model to ./20210301-225844/heel_K12_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 16.8708 - val_loss: 14.0570\n",
      "\n",
      "[20210302-1318-11] Learning rate for epoch 20 is 0.00047855067532509565\n",
      "Epoch 20/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.8421\n",
      "Epoch 00020: val_loss improved from 14.05701 to 13.30303, saving model to ./20210301-225844/heel_K12_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 16.8421 - val_loss: 13.3030\n",
      "\n",
      "[20210302-1318-15] Learning rate for epoch 21 is 0.00047855067532509565\n",
      "Epoch 21/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.4956\n",
      "Epoch 00021: val_loss improved from 13.30303 to 13.20655, saving model to ./20210301-225844/heel_K12_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.4641 - val_loss: 13.2065\n",
      "\n",
      "[20210302-1318-20] Learning rate for epoch 22 is 0.00047855067532509565\n",
      "Epoch 22/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 16.8298\n",
      "Epoch 00022: val_loss improved from 13.20655 to 12.51653, saving model to ./20210301-225844/heel_K12_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 16.8414 - val_loss: 12.5165\n",
      "\n",
      "[20210302-1318-24] Learning rate for epoch 23 is 0.00047855067532509565\n",
      "Epoch 23/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 16.8492\n",
      "Epoch 00023: val_loss did not improve from 12.51653\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 16.6483 - val_loss: 12.7501\n",
      "\n",
      "[20210302-1318-28] Learning rate for epoch 24 is 0.00047855067532509565\n",
      "Epoch 24/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.5711\n",
      "Epoch 00024: val_loss improved from 12.51653 to 12.16292, saving model to ./20210301-225844/heel_K12_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 16.5711 - val_loss: 12.1629\n",
      "\n",
      "[20210302-1318-32] Learning rate for epoch 25 is 0.00047855067532509565\n",
      "Epoch 25/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 16.1868\n",
      "Epoch 00025: val_loss improved from 12.16292 to 11.48579, saving model to ./20210301-225844/heel_K12_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 16.3165 - val_loss: 11.4858\n",
      "\n",
      "[20210302-1318-37] Learning rate for epoch 26 is 0.00047855067532509565\n",
      "Epoch 26/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 17.0310\n",
      "Epoch 00026: val_loss improved from 11.48579 to 11.46662, saving model to ./20210301-225844/heel_K12_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 17.0614 - val_loss: 11.4666\n",
      "\n",
      "[20210302-1318-41] Learning rate for epoch 27 is 0.00047855067532509565\n",
      "Epoch 27/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.2818\n",
      "Epoch 00027: val_loss improved from 11.46662 to 11.36523, saving model to ./20210301-225844/heel_K12_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.2818 - val_loss: 11.3652\n",
      "\n",
      "[20210302-1318-45] Learning rate for epoch 28 is 0.00047855067532509565\n",
      "Epoch 28/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 16.3461\n",
      "Epoch 00028: val_loss improved from 11.36523 to 10.42363, saving model to ./20210301-225844/heel_K12_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 16.2740 - val_loss: 10.4236\n",
      "\n",
      "[20210302-1318-50] Learning rate for epoch 29 is 0.00047855067532509565\n",
      "Epoch 29/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 16.1504\n",
      "Epoch 00029: val_loss did not improve from 10.42363\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.2350 - val_loss: 10.7449\n",
      "\n",
      "[20210302-1318-54] Learning rate for epoch 30 is 0.00047855067532509565\n",
      "Epoch 30/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 16.6022\n",
      "Epoch 00030: val_loss improved from 10.42363 to 9.80086, saving model to ./20210301-225844/heel_K12_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.4421 - val_loss: 9.8009\n",
      "\n",
      "[20210302-1318-58] Learning rate for epoch 31 is 0.00047855067532509565\n",
      "Epoch 31/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 16.2168\n",
      "Epoch 00031: val_loss did not improve from 9.80086\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.1666 - val_loss: 9.8264\n",
      "\n",
      "[20210302-1319-02] Learning rate for epoch 32 is 0.00047855067532509565\n",
      "Epoch 32/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.3581\n",
      "Epoch 00032: val_loss improved from 9.80086 to 9.75031, saving model to ./20210301-225844/heel_K12_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.3581 - val_loss: 9.7503\n",
      "\n",
      "[20210302-1319-06] Learning rate for epoch 33 is 0.00047855067532509565\n",
      "Epoch 33/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 16.4905\n",
      "Epoch 00033: val_loss did not improve from 9.75031\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.3447 - val_loss: 11.4204\n",
      "\n",
      "[20210302-1319-10] Learning rate for epoch 34 is 0.00047855067532509565\n",
      "Epoch 34/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.4549\n",
      "Epoch 00034: val_loss did not improve from 9.75031\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.3690 - val_loss: 11.8268\n",
      "\n",
      "[20210302-1319-14] Learning rate for epoch 35 is 0.00047855067532509565\n",
      "Epoch 35/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 16.8733\n",
      "Epoch 00035: val_loss did not improve from 9.75031\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.9080 - val_loss: 11.7177\n",
      "\n",
      "[20210302-1319-18] Learning rate for epoch 36 is 0.00047855067532509565\n",
      "Epoch 36/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.3776\n",
      "Epoch 00036: val_loss did not improve from 9.75031\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 16.3776 - val_loss: 10.2656\n",
      "\n",
      "[20210302-1319-22] Learning rate for epoch 37 is 0.00047855067532509565\n",
      "Epoch 37/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.0639\n",
      "Epoch 00037: val_loss improved from 9.75031 to 9.63413, saving model to ./20210301-225844/heel_K12_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.0639 - val_loss: 9.6341\n",
      "\n",
      "[20210302-1319-26] Learning rate for epoch 38 is 0.00047855067532509565\n",
      "Epoch 38/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.4534\n",
      "Epoch 00038: val_loss improved from 9.63413 to 9.34350, saving model to ./20210301-225844/heel_K12_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 16.4308 - val_loss: 9.3435\n",
      "\n",
      "[20210302-1319-31] Learning rate for epoch 39 is 0.00047855067532509565\n",
      "Epoch 39/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.7525\n",
      "Epoch 00039: val_loss did not improve from 9.34350\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.7535 - val_loss: 9.5144\n",
      "\n",
      "[20210302-1319-35] Learning rate for epoch 40 is 0.00047855067532509565\n",
      "Epoch 40/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.2512\n",
      "Epoch 00040: val_loss did not improve from 9.34350\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.2512 - val_loss: 11.7328\n",
      "\n",
      "[20210302-1319-39] Learning rate for epoch 41 is 0.00047855067532509565\n",
      "Epoch 41/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.2775\n",
      "Epoch 00041: val_loss did not improve from 9.34350\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 16.2775 - val_loss: 10.2075\n",
      "\n",
      "[20210302-1319-43] Learning rate for epoch 42 is 0.00047855067532509565\n",
      "Epoch 42/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 16.2002\n",
      "Epoch 00042: val_loss did not improve from 9.34350\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 16.2425 - val_loss: 13.2506\n",
      "\n",
      "[20210302-1319-47] Learning rate for epoch 43 is 0.00047855067532509565\n",
      "Epoch 43/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.5484\n",
      "Epoch 00043: val_loss did not improve from 9.34350\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.4700 - val_loss: 9.4601\n",
      "\n",
      "[20210302-1319-50] Learning rate for epoch 44 is 0.00047855067532509565\n",
      "Epoch 44/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.4631\n",
      "Epoch 00044: val_loss improved from 9.34350 to 8.43840, saving model to ./20210301-225844/heel_K12_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 15.6536 - val_loss: 8.4384\n",
      "\n",
      "[20210302-1319-55] Learning rate for epoch 45 is 0.00047855067532509565\n",
      "Epoch 45/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 16.0159\n",
      "Epoch 00045: val_loss did not improve from 8.43840\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 16.1609 - val_loss: 8.5732\n",
      "\n",
      "[20210302-1319-59] Learning rate for epoch 46 is 0.00047855067532509565\n",
      "Epoch 46/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.9652\n",
      "Epoch 00046: val_loss did not improve from 8.43840\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 15.9652 - val_loss: 8.7311\n",
      "\n",
      "[20210302-1320-03] Learning rate for epoch 47 is 0.00047855067532509565\n",
      "Epoch 47/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.6063\n",
      "Epoch 00047: val_loss did not improve from 8.43840\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.6401 - val_loss: 10.2997\n",
      "\n",
      "[20210302-1320-07] Learning rate for epoch 48 is 0.00047855067532509565\n",
      "Epoch 48/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 16.1989\n",
      "Epoch 00048: val_loss did not improve from 8.43840\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 16.3083 - val_loss: 11.6925\n",
      "\n",
      "[20210302-1320-11] Learning rate for epoch 49 is 0.00047855067532509565\n",
      "Epoch 49/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.2693\n",
      "Epoch 00049: val_loss did not improve from 8.43840\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.2693 - val_loss: 10.3771\n",
      "\n",
      "[20210302-1320-15] Learning rate for epoch 50 is 0.00047855067532509565\n",
      "Epoch 50/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.6940\n",
      "Epoch 00050: val_loss did not improve from 8.43840\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.5232 - val_loss: 8.8533\n",
      "\n",
      "[20210302-1320-19] Learning rate for epoch 51 is 0.00047855067532509565\n",
      "Epoch 51/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.0135\n",
      "Epoch 00051: val_loss did not improve from 8.43840\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.0135 - val_loss: 9.6572\n",
      "\n",
      "[20210302-1320-23] Learning rate for epoch 52 is 0.00047855067532509565\n",
      "Epoch 52/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 16.2921\n",
      "Epoch 00052: val_loss did not improve from 8.43840\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.2337 - val_loss: 9.3799\n",
      "\n",
      "[20210302-1320-27] Learning rate for epoch 53 is 0.00047855067532509565\n",
      "Epoch 53/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 16.2778\n",
      "Epoch 00053: val_loss did not improve from 8.43840\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 16.2174 - val_loss: 9.4111\n",
      "\n",
      "[20210302-1320-31] Learning rate for epoch 54 is 0.00047855067532509565\n",
      "Epoch 54/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.4662\n",
      "Epoch 00054: val_loss did not improve from 8.43840\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.6369 - val_loss: 8.7493\n",
      "\n",
      "[20210302-1320-34] Learning rate for epoch 55 is 0.00047855067532509565\n",
      "Epoch 55/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.8062\n",
      "Epoch 00055: val_loss did not improve from 8.43840\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.8062 - val_loss: 9.1686\n",
      "\n",
      "[20210302-1320-38] Learning rate for epoch 56 is 0.00047855067532509565\n",
      "Epoch 56/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.7152\n",
      "Epoch 00056: val_loss did not improve from 8.43840\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 15.6522 - val_loss: 9.4515\n",
      "\n",
      "[20210302-1320-42] Learning rate for epoch 57 is 0.00047855067532509565\n",
      "Epoch 57/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.1546\n",
      "Epoch 00057: val_loss did not improve from 8.43840\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.2027 - val_loss: 8.7910\n",
      "\n",
      "[20210302-1320-46] Learning rate for epoch 58 is 0.00047855067532509565\n",
      "Epoch 58/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 16.2436\n",
      "Epoch 00058: val_loss did not improve from 8.43840\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.3161 - val_loss: 8.5944\n",
      "\n",
      "[20210302-1320-50] Learning rate for epoch 59 is 0.00047855067532509565\n",
      "Epoch 59/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.9233\n",
      "Epoch 00059: val_loss did not improve from 8.43840\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.8035 - val_loss: 9.1231\n",
      "\n",
      "[20210302-1320-54] Learning rate for epoch 60 is 0.00047855067532509565\n",
      "Epoch 60/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.9896\n",
      "Epoch 00060: val_loss did not improve from 8.43840\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.9896 - val_loss: 9.8850\n",
      "\n",
      "[20210302-1320-58] Learning rate for epoch 61 is 0.00047855067532509565\n",
      "Epoch 61/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.8186\n",
      "Epoch 00061: val_loss did not improve from 8.43840\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.8186 - val_loss: 11.3245\n",
      "\n",
      "[20210302-1321-02] Learning rate for epoch 62 is 0.00047855067532509565\n",
      "Epoch 62/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.0855\n",
      "Epoch 00062: val_loss did not improve from 8.43840\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.0570 - val_loss: 8.6405\n",
      "\n",
      "[20210302-1321-06] Learning rate for epoch 63 is 0.00047855067532509565\n",
      "Epoch 63/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.7558\n",
      "Epoch 00063: val_loss did not improve from 8.43840\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.7617 - val_loss: 11.3633\n",
      "\n",
      "[20210302-1321-10] Learning rate for epoch 64 is 0.00047855067532509565\n",
      "Epoch 64/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.9979\n",
      "Epoch 00064: val_loss did not improve from 8.43840\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 15.9301 - val_loss: 9.7640\n",
      "\n",
      "*********************************** Start fine tune ***********************************\n",
      "tt: 195, nt:46, total layers:241\n",
      "*********************************** Start fine tune ***********************************\n",
      "Epoch 1/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 25.5810\n",
      "Epoch 00001: val_loss did not improve from 8.43840\n",
      "\n",
      "[20210302-1321-45] Learning rate for epoch 1 is 4.999999987376214e-07\n",
      "20/20 [==============================] - 14s 692ms/step - loss: 25.5810 - val_loss: 13.9280\n",
      "Epoch 2/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 22.9790\n",
      "Epoch 00002: val_loss did not improve from 8.43840\n",
      "\n",
      "[20210302-1321-49] Learning rate for epoch 2 is 5.044450517743826e-05\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 22.9790 - val_loss: 14.8689\n",
      "Epoch 3/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 17.9863\n",
      "Epoch 00003: val_loss did not improve from 8.43840\n",
      "\n",
      "[20210302-1322-10] Learning rate for epoch 3 is 0.00010028902033809572\n",
      "20/20 [==============================] - 17s 874ms/step - loss: 17.9863 - val_loss: 12.3524\n",
      "Epoch 4/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.4282\n",
      "Epoch 00004: val_loss did not improve from 8.43840\n",
      "\n",
      "[20210302-1322-15] Learning rate for epoch 4 is 0.00015003354928921908\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 15.4282 - val_loss: 11.6056\n",
      "Epoch 5/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.4297\n",
      "Epoch 00005: val_loss did not improve from 8.43840\n",
      "\n",
      "[20210302-1322-20] Learning rate for epoch 5 is 0.0001996780774788931\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 14.4297 - val_loss: 12.2592\n",
      "Epoch 6/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.6223\n",
      "Epoch 00006: val_loss did not improve from 8.43840\n",
      "\n",
      "[20210302-1322-25] Learning rate for epoch 6 is 0.000249222619459033\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 13.6223 - val_loss: 8.5250\n",
      "Epoch 7/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.2777\n",
      "Epoch 00007: val_loss did not improve from 8.43840\n",
      "\n",
      "[20210302-1322-30] Learning rate for epoch 7 is 0.0002986671752296388\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 13.2777 - val_loss: 8.9249\n",
      "Epoch 8/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 12.6797\n",
      "Epoch 00008: val_loss did not improve from 8.43840\n",
      "\n",
      "[20210302-1322-35] Learning rate for epoch 8 is 0.00034801175934262574\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 12.6797 - val_loss: 9.5615\n",
      "Epoch 9/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 12.9289\n",
      "Epoch 00009: val_loss did not improve from 8.43840\n",
      "\n",
      "[20210302-1322-40] Learning rate for epoch 9 is 0.00039725631359033287\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 12.9289 - val_loss: 8.6138\n",
      "Epoch 10/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 12.4135\n",
      "Epoch 00010: val_loss improved from 8.43840 to 8.35916, saving model to ./20210301-225844/heel_K12_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1322-45] Learning rate for epoch 10 is 0.0004464008961804211\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 12.4135 - val_loss: 8.3592\n",
      "Epoch 11/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 12.2511\n",
      "Epoch 00011: val_loss improved from 8.35916 to 8.28173, saving model to ./20210301-225844/heel_K12_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1322-50] Learning rate for epoch 11 is 0.0004954455071128905\n",
      "20/20 [==============================] - 2s 89ms/step - loss: 12.2511 - val_loss: 8.2817\n",
      "Epoch 12/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.6008\n",
      "Epoch 00012: val_loss did not improve from 8.28173\n",
      "\n",
      "[20210302-1322-55] Learning rate for epoch 12 is 0.0005443901172839105\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 11.6008 - val_loss: 8.7533\n",
      "Epoch 13/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.5473\n",
      "Epoch 00013: val_loss did not improve from 8.28173\n",
      "\n",
      "[20210302-1323-00] Learning rate for epoch 13 is 0.0005932347266934812\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 11.5473 - val_loss: 9.5888\n",
      "Epoch 14/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.0960\n",
      "Epoch 00014: val_loss improved from 8.28173 to 8.06825, saving model to ./20210301-225844/heel_K12_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1323-06] Learning rate for epoch 14 is 0.0006419793353416026\n",
      "20/20 [==============================] - 2s 92ms/step - loss: 11.0960 - val_loss: 8.0682\n",
      "Epoch 15/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.6963\n",
      "Epoch 00015: val_loss improved from 8.06825 to 6.67152, saving model to ./20210301-225844/heel_K12_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1323-11] Learning rate for epoch 15 is 0.0006906240014359355\n",
      "20/20 [==============================] - 2s 92ms/step - loss: 10.6963 - val_loss: 6.6715\n",
      "Epoch 16/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.0018\n",
      "Epoch 00016: val_loss improved from 6.67152 to 5.66806, saving model to ./20210301-225844/heel_K12_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1323-17] Learning rate for epoch 16 is 0.0007391686085611582\n",
      "20/20 [==============================] - 2s 96ms/step - loss: 10.0018 - val_loss: 5.6681\n",
      "Epoch 17/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.1335\n",
      "Epoch 00017: val_loss improved from 5.66806 to 5.39226, saving model to ./20210301-225844/heel_K12_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1323-22] Learning rate for epoch 17 is 0.0007876132731325924\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 9.1335 - val_loss: 5.3923\n",
      "Epoch 18/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.0857\n",
      "Epoch 00018: val_loss did not improve from 5.39226\n",
      "\n",
      "[20210302-1323-27] Learning rate for epoch 18 is 0.0008359579369425774\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 9.0857 - val_loss: 5.5784\n",
      "Epoch 19/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.2296\n",
      "Epoch 00019: val_loss improved from 5.39226 to 5.22365, saving model to ./20210301-225844/heel_K12_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1323-32] Learning rate for epoch 19 is 0.000884202599991113\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 9.2296 - val_loss: 5.2236\n",
      "Epoch 20/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.7185\n",
      "Epoch 00020: val_loss improved from 5.22365 to 4.61627, saving model to ./20210301-225844/heel_K12_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1323-38] Learning rate for epoch 20 is 0.0009323473204858601\n",
      "20/20 [==============================] - 2s 92ms/step - loss: 8.7185 - val_loss: 4.6163\n",
      "Epoch 21/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.3582\n",
      "Epoch 00021: val_loss improved from 4.61627 to 4.06272, saving model to ./20210301-225844/heel_K12_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1323-43] Learning rate for epoch 21 is 0.000980391982011497\n",
      "20/20 [==============================] - 2s 93ms/step - loss: 8.3582 - val_loss: 4.0627\n",
      "Epoch 22/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.2377\n",
      "Epoch 00022: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1323-48] Learning rate for epoch 22 is 0.0010283367009833455\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 8.2377 - val_loss: 6.0974\n",
      "Epoch 23/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.9946\n",
      "Epoch 00023: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1323-53] Learning rate for epoch 23 is 0.0010761814191937447\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.9946 - val_loss: 5.2829\n",
      "Epoch 24/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.1541\n",
      "Epoch 00024: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1323-57] Learning rate for epoch 24 is 0.0011239261366426945\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 8.1541 - val_loss: 6.0126\n",
      "Epoch 25/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8751\n",
      "Epoch 00025: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1324-02] Learning rate for epoch 25 is 0.001171570853330195\n",
      "20/20 [==============================] - 1s 68ms/step - loss: 7.8751 - val_loss: 6.8984\n",
      "Epoch 26/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7883\n",
      "Epoch 00026: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1324-07] Learning rate for epoch 26 is 0.001219115569256246\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.7883 - val_loss: 8.3811\n",
      "Epoch 27/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1816\n",
      "Epoch 00027: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1324-12] Learning rate for epoch 27 is 0.0012665604008361697\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 7.1816 - val_loss: 8.3636\n",
      "Epoch 28/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3815\n",
      "Epoch 00028: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1324-17] Learning rate for epoch 28 is 0.0013139051152393222\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.3815 - val_loss: 4.6396\n",
      "Epoch 29/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1553\n",
      "Epoch 00029: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1324-22] Learning rate for epoch 29 is 0.0013611499452963471\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.1553 - val_loss: 4.8822\n",
      "Epoch 30/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.9683\n",
      "Epoch 00030: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1324-27] Learning rate for epoch 30 is 0.001408294658176601\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.9683 - val_loss: 6.1517\n",
      "Epoch 31/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7825\n",
      "Epoch 00031: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1324-31] Learning rate for epoch 31 is 0.0014553394867107272\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.7825 - val_loss: 6.0501\n",
      "Epoch 32/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7362\n",
      "Epoch 00032: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1324-36] Learning rate for epoch 32 is 0.0015022843144834042\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.7362 - val_loss: 4.7063\n",
      "Epoch 33/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3536\n",
      "Epoch 00033: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1324-41] Learning rate for epoch 33 is 0.0015491291414946318\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.3536 - val_loss: 4.7169\n",
      "Epoch 34/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5235\n",
      "Epoch 00034: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1324-46] Learning rate for epoch 34 is 0.00159587396774441\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 7.5235 - val_loss: 5.8022\n",
      "Epoch 35/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3018\n",
      "Epoch 00035: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1324-51] Learning rate for epoch 35 is 0.001642518793232739\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 7.3018 - val_loss: 5.0511\n",
      "Epoch 36/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2615\n",
      "Epoch 00036: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1324-56] Learning rate for epoch 36 is 0.0016890636179596186\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.2615 - val_loss: 5.3365\n",
      "Epoch 37/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3534\n",
      "Epoch 00037: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1325-00] Learning rate for epoch 37 is 0.0017355084419250488\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.3534 - val_loss: 5.9831\n",
      "Epoch 38/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1253\n",
      "Epoch 00038: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1325-05] Learning rate for epoch 38 is 0.0017818533815443516\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.1253 - val_loss: 6.0433\n",
      "Epoch 39/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3602\n",
      "Epoch 00039: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1325-10] Learning rate for epoch 39 is 0.0018280982039868832\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.3602 - val_loss: 7.7502\n",
      "Epoch 40/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6274\n",
      "Epoch 00040: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1325-15] Learning rate for epoch 40 is 0.0018742431420832872\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.6274 - val_loss: 7.8872\n",
      "Epoch 41/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.9830\n",
      "Epoch 00041: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1325-20] Learning rate for epoch 41 is 0.0019202879630029202\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.9830 - val_loss: 7.2658\n",
      "Epoch 42/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2123\n",
      "Epoch 00042: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1325-25] Learning rate for epoch 42 is 0.0019662328995764256\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 7.2123 - val_loss: 6.3906\n",
      "Epoch 43/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5316\n",
      "Epoch 00043: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1325-29] Learning rate for epoch 43 is 0.00201207771897316\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.5316 - val_loss: 6.8308\n",
      "Epoch 44/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9738\n",
      "Epoch 00044: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1325-34] Learning rate for epoch 44 is 0.0020578226540237665\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.9738 - val_loss: 8.5642\n",
      "Epoch 45/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7472\n",
      "Epoch 00045: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1325-39] Learning rate for epoch 45 is 0.0021034677047282457\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.7472 - val_loss: 7.4157\n",
      "Epoch 46/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5905\n",
      "Epoch 00046: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1325-44] Learning rate for epoch 46 is 0.002149012638255954\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.5905 - val_loss: 5.7474\n",
      "Epoch 47/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7350\n",
      "Epoch 00047: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1325-49] Learning rate for epoch 47 is 0.0021944576874375343\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.7350 - val_loss: 5.2818\n",
      "Epoch 48/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0150\n",
      "Epoch 00048: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1325-54] Learning rate for epoch 48 is 0.0022398026194423437\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.0150 - val_loss: 20.6183\n",
      "Epoch 49/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9443\n",
      "Epoch 00049: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1325-59] Learning rate for epoch 49 is 0.002285047434270382\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.9443 - val_loss: 11.8000\n",
      "Epoch 50/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9239\n",
      "Epoch 00050: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1326-03] Learning rate for epoch 50 is 0.0023301925975829363\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.9239 - val_loss: 7.8990\n",
      "Epoch 51/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8338\n",
      "Epoch 00051: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1326-08] Learning rate for epoch 51 is 0.002375237410888076\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.8338 - val_loss: 26.1112\n",
      "Epoch 52/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4506\n",
      "Epoch 00052: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1326-13] Learning rate for epoch 52 is 0.0024201825726777315\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 7.4506 - val_loss: 7.6422\n",
      "Epoch 53/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9281\n",
      "Epoch 00053: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1326-18] Learning rate for epoch 53 is 0.002465027617290616\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.9281 - val_loss: 6.5410\n",
      "Epoch 54/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8146\n",
      "Epoch 00054: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1326-23] Learning rate for epoch 54 is 0.0025097725447267294\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.8146 - val_loss: 6.4659\n",
      "Epoch 55/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6115\n",
      "Epoch 00055: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1326-28] Learning rate for epoch 55 is 0.0025544175878167152\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.6115 - val_loss: 6.8634\n",
      "Epoch 56/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9385\n",
      "Epoch 00056: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1326-32] Learning rate for epoch 56 is 0.00259896251372993\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.9385 - val_loss: 4.8329\n",
      "Epoch 57/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7979\n",
      "Epoch 00057: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1326-37] Learning rate for epoch 57 is 0.0026434077881276608\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.7979 - val_loss: 7.9898\n",
      "Epoch 58/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1860\n",
      "Epoch 00058: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1326-42] Learning rate for epoch 58 is 0.0026877527125179768\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.1860 - val_loss: 26.6669\n",
      "Epoch 59/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8922\n",
      "Epoch 00059: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1326-47] Learning rate for epoch 59 is 0.0027319977525621653\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.8922 - val_loss: 25.6795\n",
      "Epoch 60/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7650\n",
      "Epoch 00060: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1326-52] Learning rate for epoch 60 is 0.0027761429082602262\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.7650 - val_loss: 14.3454\n",
      "Epoch 61/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5943\n",
      "Epoch 00061: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1326-56] Learning rate for epoch 61 is 0.002820187946781516\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.5943 - val_loss: 5.8653\n",
      "Epoch 62/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7120\n",
      "Epoch 00062: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1327-01] Learning rate for epoch 62 is 0.0028641331009566784\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.7120 - val_loss: 8.0131\n",
      "Epoch 63/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9755\n",
      "Epoch 00063: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1327-06] Learning rate for epoch 63 is 0.0029079781379550695\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.9755 - val_loss: 7.5257\n",
      "Epoch 64/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8224\n",
      "Epoch 00064: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1327-11] Learning rate for epoch 64 is 0.002951723290607333\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.8224 - val_loss: 23.7527\n",
      "Epoch 65/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2670\n",
      "Epoch 00065: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1327-16] Learning rate for epoch 65 is 0.0029953685589134693\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.2670 - val_loss: 174.3824\n",
      "Epoch 66/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7805\n",
      "Epoch 00066: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1327-21] Learning rate for epoch 66 is 0.0030389137100428343\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 6.7805 - val_loss: 117.6259\n",
      "Epoch 67/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4244\n",
      "Epoch 00067: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1327-25] Learning rate for epoch 67 is 0.003082358743995428\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.4244 - val_loss: 11.0077\n",
      "Epoch 68/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0555\n",
      "Epoch 00068: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1327-30] Learning rate for epoch 68 is 0.0031257038936018944\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 7.0555 - val_loss: 10.7452\n",
      "Epoch 69/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7418\n",
      "Epoch 00069: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1327-35] Learning rate for epoch 69 is 0.003168949158862233\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.7418 - val_loss: 260.4986\n",
      "Epoch 70/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8981\n",
      "Epoch 00070: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1327-40] Learning rate for epoch 70 is 0.0032120943069458008\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.8981 - val_loss: 73.7312\n",
      "Epoch 71/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3214\n",
      "Epoch 00071: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1327-45] Learning rate for epoch 71 is 0.003255139570683241\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 7.3214 - val_loss: 32.2799\n",
      "Epoch 72/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8499\n",
      "Epoch 00072: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1327-50] Learning rate for epoch 72 is 0.00329808471724391\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.8499 - val_loss: 30.7455\n",
      "Epoch 73/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8388\n",
      "Epoch 00073: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1327-55] Learning rate for epoch 73 is 0.0033409299794584513\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.8388 - val_loss: 8.3929\n",
      "Epoch 74/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7816\n",
      "Epoch 00074: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1327-59] Learning rate for epoch 74 is 0.0033836751244962215\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.7816 - val_loss: 6.0478\n",
      "Epoch 75/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8020\n",
      "Epoch 00075: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1328-04] Learning rate for epoch 75 is 0.0034263203851878643\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.8020 - val_loss: 17.6658\n",
      "Epoch 76/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0568\n",
      "Epoch 00076: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1328-09] Learning rate for epoch 76 is 0.003468865528702736\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.0568 - val_loss: 12.5345\n",
      "Epoch 77/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9191\n",
      "Epoch 00077: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1328-14] Learning rate for epoch 77 is 0.00351131078787148\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.9191 - val_loss: 4.2076\n",
      "Epoch 78/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8572\n",
      "Epoch 00078: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1328-19] Learning rate for epoch 78 is 0.0035536561626940966\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.8572 - val_loss: 5.8842\n",
      "Epoch 79/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8923\n",
      "Epoch 00079: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1328-23] Learning rate for epoch 79 is 0.003595901420339942\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.8923 - val_loss: 8.7674\n",
      "Epoch 80/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5307\n",
      "Epoch 00080: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1328-28] Learning rate for epoch 80 is 0.00363804679363966\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.5307 - val_loss: 5.8103\n",
      "Epoch 81/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5540\n",
      "Epoch 00081: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1328-33] Learning rate for epoch 81 is 0.0036800920497626066\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 7.5540 - val_loss: 4.2290\n",
      "Epoch 82/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3557\n",
      "Epoch 00082: val_loss did not improve from 4.06272\n",
      "\n",
      "[20210302-1328-38] Learning rate for epoch 82 is 0.003722037188708782\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.3557 - val_loss: 4.1340\n",
      "Epoch 83/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5519\n",
      "Epoch 00083: val_loss improved from 4.06272 to 3.85758, saving model to ./20210301-225844/heel_K12_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1328-43] Learning rate for epoch 83 is 0.003763882676139474\n",
      "20/20 [==============================] - 2s 89ms/step - loss: 6.5519 - val_loss: 3.8576\n",
      "Epoch 84/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8231\n",
      "Epoch 00084: val_loss did not improve from 3.85758\n",
      "\n",
      "[20210302-1328-48] Learning rate for epoch 84 is 0.0038056280463933945\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.8231 - val_loss: 5.4972\n",
      "Epoch 85/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6787\n",
      "Epoch 00085: val_loss did not improve from 3.85758\n",
      "\n",
      "[20210302-1328-53] Learning rate for epoch 85 is 0.003847273299470544\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.6787 - val_loss: 4.0510\n",
      "Epoch 86/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1461\n",
      "Epoch 00086: val_loss did not improve from 3.85758\n",
      "\n",
      "[20210302-1328-58] Learning rate for epoch 86 is 0.0038888186682015657\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 7.1461 - val_loss: 6.4135\n",
      "Epoch 87/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6190\n",
      "Epoch 00087: val_loss did not improve from 3.85758\n",
      "\n",
      "[20210302-1329-02] Learning rate for epoch 87 is 0.00393026415258646\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.6190 - val_loss: 7.0542\n",
      "Epoch 88/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0170\n",
      "Epoch 00088: val_loss did not improve from 3.85758\n",
      "\n",
      "[20210302-1329-07] Learning rate for epoch 88 is 0.00397160928696394\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.0170 - val_loss: 4.0479\n",
      "Epoch 89/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8984\n",
      "Epoch 00089: val_loss did not improve from 3.85758\n",
      "\n",
      "[20210302-1329-12] Learning rate for epoch 89 is 0.004012854769825935\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.8984 - val_loss: 378.1147\n",
      "Epoch 90/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6982\n",
      "Epoch 00090: val_loss did not improve from 3.85758\n",
      "\n",
      "[20210302-1329-17] Learning rate for epoch 90 is 0.00405400013551116\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.6982 - val_loss: 1143.8156\n",
      "Epoch 91/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1723\n",
      "Epoch 00091: val_loss did not improve from 3.85758\n",
      "\n",
      "[20210302-1329-22] Learning rate for epoch 91 is 0.004095045384019613\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.1723 - val_loss: 1682.7218\n",
      "Epoch 92/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3993\n",
      "Epoch 00092: val_loss did not improve from 3.85758\n",
      "\n",
      "[20210302-1329-27] Learning rate for epoch 92 is 0.004135990981012583\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.3993 - val_loss: 2234.5947\n",
      "Epoch 93/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2444\n",
      "Epoch 00093: val_loss did not improve from 3.85758\n",
      "\n",
      "[20210302-1329-32] Learning rate for epoch 93 is 0.004176836460828781\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.2444 - val_loss: 186.8113\n",
      "Epoch 94/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0921\n",
      "Epoch 00094: val_loss did not improve from 3.85758\n",
      "\n",
      "[20210302-1329-36] Learning rate for epoch 94 is 0.004217581823468208\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.0921 - val_loss: 5.2561\n",
      "Epoch 95/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5526\n",
      "Epoch 00095: val_loss did not improve from 3.85758\n",
      "\n",
      "[20210302-1329-41] Learning rate for epoch 95 is 0.004258227068930864\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.5526 - val_loss: 6.1514\n",
      "Epoch 96/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5568\n",
      "Epoch 00096: val_loss did not improve from 3.85758\n",
      "\n",
      "[20210302-1329-46] Learning rate for epoch 96 is 0.0042987726628780365\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.5568 - val_loss: 5.0825\n",
      "Epoch 97/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3975\n",
      "Epoch 00097: val_loss did not improve from 3.85758\n",
      "\n",
      "[20210302-1329-51] Learning rate for epoch 97 is 0.0043392181396484375\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.3975 - val_loss: 4.9713\n",
      "Epoch 98/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0554\n",
      "Epoch 00098: val_loss did not improve from 3.85758\n",
      "\n",
      "[20210302-1329-56] Learning rate for epoch 98 is 0.004379563499242067\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.0554 - val_loss: 6.7470\n",
      "Epoch 99/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3084\n",
      "Epoch 00099: val_loss did not improve from 3.85758\n",
      "\n",
      "[20210302-1330-00] Learning rate for epoch 99 is 0.004419809207320213\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.3084 - val_loss: 269.7151\n",
      "Epoch 100/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7218\n",
      "Epoch 00100: val_loss did not improve from 3.85758\n",
      "\n",
      "[20210302-1330-05] Learning rate for epoch 100 is 0.004459954332560301\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.7218 - val_loss: 340.4132\n",
      "Epoch 101/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6939\n",
      "Epoch 00101: val_loss did not improve from 3.85758\n",
      "\n",
      "[20210302-1330-10] Learning rate for epoch 101 is 4.4999998749517545e-07\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.6939 - val_loss: 127.5274\n",
      "Epoch 102/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3463\n",
      "Epoch 00102: val_loss did not improve from 3.85758\n",
      "\n",
      "[20210302-1330-15] Learning rate for epoch 102 is 0.00018023152370005846\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.3463 - val_loss: 26.4740\n",
      "Epoch 103/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3218\n",
      "Epoch 00103: val_loss did not improve from 3.85758\n",
      "\n",
      "[20210302-1330-20] Learning rate for epoch 103 is 0.000359613070031628\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.3218 - val_loss: 7.7898\n",
      "Epoch 104/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6324\n",
      "Epoch 00104: val_loss did not improve from 3.85758\n",
      "\n",
      "[20210302-1330-25] Learning rate for epoch 104 is 0.0005385946715250611\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.6324 - val_loss: 5.5472\n",
      "Epoch 105/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0685\n",
      "Epoch 00105: val_loss did not improve from 3.85758\n",
      "\n",
      "[20210302-1330-30] Learning rate for epoch 105 is 0.0007171762990765274\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.0685 - val_loss: 6.2488\n",
      "Epoch 106/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7325\n",
      "Epoch 00106: val_loss did not improve from 3.85758\n",
      "\n",
      "[20210302-1330-34] Learning rate for epoch 106 is 0.0008953579817898571\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.7325 - val_loss: 4.5233\n",
      "Epoch 107/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6592\n",
      "Epoch 00107: val_loss did not improve from 3.85758\n",
      "\n",
      "[20210302-1330-39] Learning rate for epoch 107 is 0.0010731397196650505\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.6592 - val_loss: 4.8883\n",
      "Epoch 108/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6826\n",
      "Epoch 00108: val_loss did not improve from 3.85758\n",
      "\n",
      "[20210302-1330-44] Learning rate for epoch 108 is 0.0012505215127021074\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.6826 - val_loss: 4.1788\n",
      "Epoch 109/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0947\n",
      "Epoch 00109: val_loss improved from 3.85758 to 3.48816, saving model to ./20210301-225844/heel_K12_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1330-49] Learning rate for epoch 109 is 0.001427503302693367\n",
      "20/20 [==============================] - 2s 91ms/step - loss: 6.0947 - val_loss: 3.4882\n",
      "Epoch 110/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1486\n",
      "Epoch 00110: val_loss did not improve from 3.48816\n",
      "\n",
      "[20210302-1330-54] Learning rate for epoch 110 is 0.0016040850896388292\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.1486 - val_loss: 4.1121\n",
      "Epoch 111/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2878\n",
      "Epoch 00111: val_loss did not improve from 3.48816\n",
      "\n",
      "[20210302-1330-59] Learning rate for epoch 111 is 0.001780266989953816\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.2878 - val_loss: 4.7054\n",
      "Epoch 112/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0129\n",
      "Epoch 00112: val_loss did not improve from 3.48816\n",
      "\n",
      "[20210302-1331-04] Learning rate for epoch 112 is 0.0019560488872230053\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.0129 - val_loss: 3.5540\n",
      "Epoch 113/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0962\n",
      "Epoch 00113: val_loss did not improve from 3.48816\n",
      "\n",
      "[20210302-1331-09] Learning rate for epoch 113 is 0.002131430897861719\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.0962 - val_loss: 3.6811\n",
      "Epoch 114/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0765\n",
      "Epoch 00114: val_loss did not improve from 3.48816\n",
      "\n",
      "[20210302-1331-14] Learning rate for epoch 114 is 0.0023064129054546356\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.0765 - val_loss: 5.6473\n",
      "Epoch 115/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7218\n",
      "Epoch 00115: val_loss did not improve from 3.48816\n",
      "\n",
      "[20210302-1331-18] Learning rate for epoch 115 is 0.0024809949100017548\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.7218 - val_loss: 4.8502\n",
      "Epoch 116/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1947\n",
      "Epoch 00116: val_loss did not improve from 3.48816\n",
      "\n",
      "[20210302-1331-23] Learning rate for epoch 116 is 0.0026551769115030766\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.1947 - val_loss: 3.6561\n",
      "Epoch 117/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8606\n",
      "Epoch 00117: val_loss did not improve from 3.48816\n",
      "\n",
      "[20210302-1331-28] Learning rate for epoch 117 is 0.0028289591427892447\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8606 - val_loss: 29.8647\n",
      "Epoch 118/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4615\n",
      "Epoch 00118: val_loss did not improve from 3.48816\n",
      "\n",
      "[20210302-1331-33] Learning rate for epoch 118 is 0.0030023413710296154\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.4615 - val_loss: 25.6118\n",
      "Epoch 119/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0647\n",
      "Epoch 00119: val_loss did not improve from 3.48816\n",
      "\n",
      "[20210302-1331-38] Learning rate for epoch 119 is 0.003175323596224189\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.0647 - val_loss: 5.1113\n",
      "Epoch 120/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3812\n",
      "Epoch 00120: val_loss did not improve from 3.48816\n",
      "\n",
      "[20210302-1331-43] Learning rate for epoch 120 is 0.003347905818372965\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.3812 - val_loss: 3.8346\n",
      "Epoch 121/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4563\n",
      "Epoch 00121: val_loss did not improve from 3.48816\n",
      "\n",
      "[20210302-1331-48] Learning rate for epoch 121 is 0.0035200880374759436\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 5.4563 - val_loss: 3.7212\n",
      "Epoch 122/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8227\n",
      "Epoch 00122: val_loss did not improve from 3.48816\n",
      "\n",
      "[20210302-1331-52] Learning rate for epoch 122 is 0.003691870253533125\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.8227 - val_loss: 4.8162\n",
      "Epoch 123/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8759\n",
      "Epoch 00123: val_loss did not improve from 3.48816\n",
      "\n",
      "[20210302-1331-57] Learning rate for epoch 123 is 0.0038632526993751526\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8759 - val_loss: 21.2252\n",
      "Epoch 124/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2810\n",
      "Epoch 00124: val_loss did not improve from 3.48816\n",
      "\n",
      "[20210302-1332-02] Learning rate for epoch 124 is 0.004034235142171383\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.2810 - val_loss: 13.2160\n",
      "Epoch 125/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9203\n",
      "Epoch 00125: val_loss did not improve from 3.48816\n",
      "\n",
      "[20210302-1332-07] Learning rate for epoch 125 is 0.004204817581921816\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.9203 - val_loss: 33.0983\n",
      "Epoch 126/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9268\n",
      "Epoch 00126: val_loss did not improve from 3.48816\n",
      "\n",
      "[20210302-1332-12] Learning rate for epoch 126 is 0.0043750000186264515\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.9268 - val_loss: 18.9093\n",
      "Epoch 127/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5399\n",
      "Epoch 00127: val_loss did not improve from 3.48816\n",
      "\n",
      "[20210302-1332-16] Learning rate for epoch 127 is 0.0041952175088226795\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.5399 - val_loss: 7.1936\n",
      "Epoch 128/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0138\n",
      "Epoch 00128: val_loss did not improve from 3.48816\n",
      "\n",
      "[20210302-1332-21] Learning rate for epoch 128 is 0.004015835002064705\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.0138 - val_loss: 4.2240\n",
      "Epoch 129/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6682\n",
      "Epoch 00129: val_loss did not improve from 3.48816\n",
      "\n",
      "[20210302-1332-26] Learning rate for epoch 129 is 0.003836852265521884\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.6682 - val_loss: 4.5095\n",
      "Epoch 130/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2009\n",
      "Epoch 00130: val_loss did not improve from 3.48816\n",
      "\n",
      "[20210302-1332-31] Learning rate for epoch 130 is 0.003658269764855504\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.2009 - val_loss: 5.2058\n",
      "Epoch 131/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8868\n",
      "Epoch 00131: val_loss did not improve from 3.48816\n",
      "\n",
      "[20210302-1332-36] Learning rate for epoch 131 is 0.003480087034404278\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.8868 - val_loss: 4.7738\n",
      "Epoch 132/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6470\n",
      "Epoch 00132: val_loss did not improve from 3.48816\n",
      "\n",
      "[20210302-1332-41] Learning rate for epoch 132 is 0.003302304306998849\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.6470 - val_loss: 3.9676\n",
      "Epoch 133/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1125\n",
      "Epoch 00133: val_loss did not improve from 3.48816\n",
      "\n",
      "[20210302-1332-46] Learning rate for epoch 133 is 0.0031249215826392174\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 6.1125 - val_loss: 3.5144\n",
      "Epoch 134/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2704\n",
      "Epoch 00134: val_loss did not improve from 3.48816\n",
      "\n",
      "[20210302-1332-50] Learning rate for epoch 134 is 0.0029479386284947395\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.2704 - val_loss: 4.5565\n",
      "Epoch 135/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7410\n",
      "Epoch 00135: val_loss did not improve from 3.48816\n",
      "\n",
      "[20210302-1332-55] Learning rate for epoch 135 is 0.0027713559102267027\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.7410 - val_loss: 3.5990\n",
      "Epoch 136/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7878\n",
      "Epoch 00136: val_loss did not improve from 3.48816\n",
      "\n",
      "[20210302-1333-00] Learning rate for epoch 136 is 0.0025951729621738195\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.7878 - val_loss: 3.6324\n",
      "Epoch 137/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6065\n",
      "Epoch 00137: val_loss did not improve from 3.48816\n",
      "\n",
      "[20210302-1333-05] Learning rate for epoch 137 is 0.0024193900171667337\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.6065 - val_loss: 3.5321\n",
      "Epoch 138/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3166\n",
      "Epoch 00138: val_loss did not improve from 3.48816\n",
      "\n",
      "[20210302-1333-10] Learning rate for epoch 138 is 0.0022440070752054453\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.3166 - val_loss: 3.5782\n",
      "Epoch 139/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0000\n",
      "Epoch 00139: val_loss improved from 3.48816 to 3.43820, saving model to ./20210301-225844/heel_K12_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1333-15] Learning rate for epoch 139 is 0.002069024136289954\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 6.0000 - val_loss: 3.4382\n",
      "Epoch 140/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6573\n",
      "Epoch 00140: val_loss improved from 3.43820 to 3.40146, saving model to ./20210301-225844/heel_K12_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1333-20] Learning rate for epoch 140 is 0.0018944410840049386\n",
      "20/20 [==============================] - 2s 93ms/step - loss: 5.6573 - val_loss: 3.4015\n",
      "Epoch 141/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6631\n",
      "Epoch 00141: val_loss did not improve from 3.40146\n",
      "\n",
      "[20210302-1333-25] Learning rate for epoch 141 is 0.0017202580347657204\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.6631 - val_loss: 3.8233\n",
      "Epoch 142/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2964\n",
      "Epoch 00142: val_loss did not improve from 3.40146\n",
      "\n",
      "[20210302-1333-30] Learning rate for epoch 142 is 0.0015464748721569777\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.2964 - val_loss: 4.1467\n",
      "Epoch 143/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6864\n",
      "Epoch 00143: val_loss did not improve from 3.40146\n",
      "\n",
      "[20210302-1333-35] Learning rate for epoch 143 is 0.0013730917125940323\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.6864 - val_loss: 3.8391\n",
      "Epoch 144/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5880\n",
      "Epoch 00144: val_loss did not improve from 3.40146\n",
      "\n",
      "[20210302-1333-40] Learning rate for epoch 144 is 0.0012001085560768843\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5880 - val_loss: 3.5072\n",
      "Epoch 145/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8239\n",
      "Epoch 00145: val_loss did not improve from 3.40146\n",
      "\n",
      "[20210302-1333-45] Learning rate for epoch 145 is 0.0010275252861902118\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.8239 - val_loss: 3.8889\n",
      "Epoch 146/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1433\n",
      "Epoch 00146: val_loss did not improve from 3.40146\n",
      "\n",
      "[20210302-1333-49] Learning rate for epoch 146 is 0.0008553420193493366\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.1433 - val_loss: 3.5927\n",
      "Epoch 147/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6492\n",
      "Epoch 00147: val_loss did not improve from 3.40146\n",
      "\n",
      "[20210302-1333-54] Learning rate for epoch 147 is 0.0006835586973465979\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.6492 - val_loss: 3.6470\n",
      "Epoch 148/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9307\n",
      "Epoch 00148: val_loss did not improve from 3.40146\n",
      "\n",
      "[20210302-1333-59] Learning rate for epoch 148 is 0.0005121753201819956\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.9307 - val_loss: 3.6939\n",
      "Epoch 149/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8685\n",
      "Epoch 00149: val_loss did not improve from 3.40146\n",
      "\n",
      "[20210302-1334-04] Learning rate for epoch 149 is 0.00034119191695936024\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.8685 - val_loss: 3.5169\n",
      "Epoch 150/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7616\n",
      "Epoch 00150: val_loss did not improve from 3.40146\n",
      "\n",
      "[20210302-1334-09] Learning rate for epoch 150 is 0.00017060847312677652\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.7616 - val_loss: 3.5299\n",
      "Epoch 151/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4109\n",
      "Epoch 00151: val_loss did not improve from 3.40146\n",
      "\n",
      "[20210302-1334-13] Learning rate for epoch 151 is 4.249999960848072e-07\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.4109 - val_loss: 3.5311\n",
      "Epoch 152/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9664\n",
      "Epoch 00152: val_loss did not improve from 3.40146\n",
      "\n",
      "[20210302-1334-18] Learning rate for epoch 152 is 0.00017020752420648932\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.9664 - val_loss: 3.5499\n",
      "Epoch 153/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5753\n",
      "Epoch 00153: val_loss did not improve from 3.40146\n",
      "\n",
      "[20210302-1334-23] Learning rate for epoch 153 is 0.0003395900712348521\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.5753 - val_loss: 3.6803\n",
      "Epoch 154/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2653\n",
      "Epoch 00154: val_loss did not improve from 3.40146\n",
      "\n",
      "[20210302-1334-28] Learning rate for epoch 154 is 0.0005085726734250784\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.2653 - val_loss: 3.6414\n",
      "Epoch 155/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8906\n",
      "Epoch 00155: val_loss did not improve from 3.40146\n",
      "\n",
      "[20210302-1334-33] Learning rate for epoch 155 is 0.0006771553307771683\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.8906 - val_loss: 3.4852\n",
      "Epoch 156/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8517\n",
      "Epoch 00156: val_loss did not improve from 3.40146\n",
      "\n",
      "[20210302-1334-38] Learning rate for epoch 156 is 0.0008453379850834608\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.8517 - val_loss: 3.4976\n",
      "Epoch 157/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6182\n",
      "Epoch 00157: val_loss did not improve from 3.40146\n",
      "\n",
      "[20210302-1334-42] Learning rate for epoch 157 is 0.0010131207527592778\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6182 - val_loss: 3.7089\n",
      "Epoch 158/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6404\n",
      "Epoch 00158: val_loss did not improve from 3.40146\n",
      "\n",
      "[20210302-1334-47] Learning rate for epoch 158 is 0.0011805035173892975\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.6404 - val_loss: 3.5171\n",
      "Epoch 159/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9474\n",
      "Epoch 00159: val_loss did not improve from 3.40146\n",
      "\n",
      "[20210302-1334-52] Learning rate for epoch 159 is 0.0013474862789735198\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 5.9474 - val_loss: 3.4597\n",
      "Epoch 160/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5907\n",
      "Epoch 00160: val_loss did not improve from 3.40146\n",
      "\n",
      "[20210302-1334-57] Learning rate for epoch 160 is 0.0015140691539272666\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5907 - val_loss: 3.5742\n",
      "Epoch 161/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7521\n",
      "Epoch 00161: val_loss did not improve from 3.40146\n",
      "\n",
      "[20210302-1335-02] Learning rate for epoch 161 is 0.001680252025835216\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.7521 - val_loss: 3.5518\n",
      "Epoch 162/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3084\n",
      "Epoch 00162: val_loss did not improve from 3.40146\n",
      "\n",
      "[20210302-1335-07] Learning rate for epoch 162 is 0.0018460348946973681\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.3084 - val_loss: 3.5599\n",
      "Epoch 163/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9170\n",
      "Epoch 00163: val_loss did not improve from 3.40146\n",
      "\n",
      "[20210302-1335-11] Learning rate for epoch 163 is 0.0020114178769290447\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.9170 - val_loss: 5.2614\n",
      "Epoch 164/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8848\n",
      "Epoch 00164: val_loss did not improve from 3.40146\n",
      "\n",
      "[20210302-1335-16] Learning rate for epoch 164 is 0.0021764009725302458\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8848 - val_loss: 3.8941\n",
      "Epoch 165/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5180\n",
      "Epoch 00165: val_loss did not improve from 3.40146\n",
      "\n",
      "[20210302-1335-21] Learning rate for epoch 165 is 0.002340983832255006\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5180 - val_loss: 4.2722\n",
      "Epoch 166/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5427\n",
      "Epoch 00166: val_loss improved from 3.40146 to 3.34693, saving model to ./20210301-225844/heel_K12_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1335-27] Learning rate for epoch 166 is 0.002505166921764612\n",
      "20/20 [==============================] - 2s 92ms/step - loss: 5.5427 - val_loss: 3.3469\n",
      "Epoch 167/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7195\n",
      "Epoch 00167: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1335-32] Learning rate for epoch 167 is 0.002668950008228421\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.7195 - val_loss: 4.6197\n",
      "Epoch 168/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0080\n",
      "Epoch 00168: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1335-36] Learning rate for epoch 168 is 0.0028323333244770765\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.0080 - val_loss: 5.0857\n",
      "Epoch 169/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7231\n",
      "Epoch 00169: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1335-41] Learning rate for epoch 169 is 0.002995316404849291\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.7231 - val_loss: 8.2766\n",
      "Epoch 170/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7576\n",
      "Epoch 00170: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1335-46] Learning rate for epoch 170 is 0.0031578997150063515\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.7576 - val_loss: 7.6068\n",
      "Epoch 171/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8937\n",
      "Epoch 00171: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1335-51] Learning rate for epoch 171 is 0.0033200830221176147\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.8937 - val_loss: 4.1525\n",
      "Epoch 172/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5327\n",
      "Epoch 00172: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1335-56] Learning rate for epoch 172 is 0.0034818663261830807\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.5327 - val_loss: 4.2061\n",
      "Epoch 173/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1508\n",
      "Epoch 00173: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1336-01] Learning rate for epoch 173 is 0.0036432496272027493\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.1508 - val_loss: 6.8202\n",
      "Epoch 174/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2713\n",
      "Epoch 00174: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1336-05] Learning rate for epoch 174 is 0.003804233158007264\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.2713 - val_loss: 4.3409\n",
      "Epoch 175/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3441\n",
      "Epoch 00175: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1336-10] Learning rate for epoch 175 is 0.003964816685765982\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.3441 - val_loss: 4.1210\n",
      "Epoch 176/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8723\n",
      "Epoch 00176: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1336-15] Learning rate for epoch 176 is 0.004124999977648258\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8723 - val_loss: 3.9784\n",
      "Epoch 177/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4900\n",
      "Epoch 00177: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1336-20] Learning rate for epoch 177 is 0.003955216612666845\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.4900 - val_loss: 3.9643\n",
      "Epoch 178/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7883\n",
      "Epoch 00178: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1336-25] Learning rate for epoch 178 is 0.003785833017900586\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.7883 - val_loss: 5.7602\n",
      "Epoch 179/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6040\n",
      "Epoch 00179: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1336-30] Learning rate for epoch 179 is 0.0036168494261801243\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.6040 - val_loss: 4.9966\n",
      "Epoch 180/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0817\n",
      "Epoch 00180: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1336-34] Learning rate for epoch 180 is 0.003448265604674816\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.0817 - val_loss: 6.2597\n",
      "Epoch 181/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9038\n",
      "Epoch 00181: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1336-39] Learning rate for epoch 181 is 0.003280082019045949\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.9038 - val_loss: 5.8393\n",
      "Epoch 182/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9188\n",
      "Epoch 00182: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1336-44] Learning rate for epoch 182 is 0.0031122982036322355\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.9188 - val_loss: 3.9778\n",
      "Epoch 183/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2582\n",
      "Epoch 00183: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1336-49] Learning rate for epoch 183 is 0.002944914624094963\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.2582 - val_loss: 5.4498\n",
      "Epoch 184/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6926\n",
      "Epoch 00184: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1336-54] Learning rate for epoch 184 is 0.0027779308147728443\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.6926 - val_loss: 3.6863\n",
      "Epoch 185/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8929\n",
      "Epoch 00185: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1336-59] Learning rate for epoch 185 is 0.0026113467756658792\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.8929 - val_loss: 4.6763\n",
      "Epoch 186/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4203\n",
      "Epoch 00186: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1337-03] Learning rate for epoch 186 is 0.002445162972435355\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.4203 - val_loss: 3.3803\n",
      "Epoch 187/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0731\n",
      "Epoch 00187: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1337-08] Learning rate for epoch 187 is 0.0022793791722506285\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.0731 - val_loss: 4.1075\n",
      "Epoch 188/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4197\n",
      "Epoch 00188: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1337-13] Learning rate for epoch 188 is 0.0021139951422810555\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.4197 - val_loss: 4.1346\n",
      "Epoch 189/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5009\n",
      "Epoch 00189: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1337-18] Learning rate for epoch 189 is 0.0019490111153572798\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.5009 - val_loss: 3.9919\n",
      "Epoch 190/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7946\n",
      "Epoch 00190: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1337-23] Learning rate for epoch 190 is 0.0017844270914793015\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 5.7946 - val_loss: 4.1328\n",
      "Epoch 191/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9383\n",
      "Epoch 00191: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1337-28] Learning rate for epoch 191 is 0.0016202429542317986\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.9383 - val_loss: 4.0279\n",
      "Epoch 192/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3402\n",
      "Epoch 00192: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1337-32] Learning rate for epoch 192 is 0.001456458936445415\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.3402 - val_loss: 3.5698\n",
      "Epoch 193/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6116\n",
      "Epoch 00193: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1337-37] Learning rate for epoch 193 is 0.001293074688874185\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.6116 - val_loss: 3.4351\n",
      "Epoch 194/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5074\n",
      "Epoch 00194: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1337-42] Learning rate for epoch 194 is 0.0011300905607640743\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5074 - val_loss: 3.4905\n",
      "Epoch 195/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0213\n",
      "Epoch 00195: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1337-47] Learning rate for epoch 195 is 0.0009675062610767782\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.0213 - val_loss: 3.7866\n",
      "Epoch 196/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6900\n",
      "Epoch 00196: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1337-52] Learning rate for epoch 196 is 0.0008053220226429403\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.6900 - val_loss: 4.3281\n",
      "Epoch 197/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9485\n",
      "Epoch 00197: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1337-57] Learning rate for epoch 197 is 0.0006435376708395779\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.9485 - val_loss: 3.6411\n",
      "Epoch 198/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4043\n",
      "Epoch 00198: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1338-02] Learning rate for epoch 198 is 0.0004821533220820129\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.4043 - val_loss: 3.6004\n",
      "Epoch 199/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6410\n",
      "Epoch 00199: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1338-06] Learning rate for epoch 199 is 0.0003211689181625843\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6410 - val_loss: 3.5569\n",
      "Epoch 200/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4329\n",
      "Epoch 00200: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1338-11] Learning rate for epoch 200 is 0.00016058447363320738\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.4329 - val_loss: 3.4642\n",
      "Epoch 201/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3119\n",
      "Epoch 00201: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1338-16] Learning rate for epoch 201 is 4.0000000467443897e-07\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.3119 - val_loss: 3.4749\n",
      "Epoch 202/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6593\n",
      "Epoch 00202: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1338-21] Learning rate for epoch 202 is 0.0001601835247129202\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 5.6593 - val_loss: 3.4784\n",
      "Epoch 203/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6596\n",
      "Epoch 00203: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1338-26] Learning rate for epoch 203 is 0.00031956707243807614\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.6596 - val_loss: 3.5785\n",
      "Epoch 204/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2539\n",
      "Epoch 00204: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1338-31] Learning rate for epoch 204 is 0.00047855067532509565\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 5.2539 - val_loss: 3.5513\n",
      "Epoch 205/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6842\n",
      "Epoch 00205: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1338-35] Learning rate for epoch 205 is 0.0006371343042701483\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.6842 - val_loss: 3.5729\n",
      "Epoch 206/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2435\n",
      "Epoch 00206: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1338-40] Learning rate for epoch 206 is 0.0007953179883770645\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.2435 - val_loss: 3.4769\n",
      "Epoch 207/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7512\n",
      "Epoch 00207: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1338-45] Learning rate for epoch 207 is 0.0009531017276458442\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.7512 - val_loss: 3.4361\n",
      "Epoch 208/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2984\n",
      "Epoch 00208: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1338-50] Learning rate for epoch 208 is 0.0011104855220764875\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.2984 - val_loss: 3.6181\n",
      "Epoch 209/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6241\n",
      "Epoch 00209: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1338-55] Learning rate for epoch 209 is 0.0012674692552536726\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.6241 - val_loss: 4.1241\n",
      "Epoch 210/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2931\n",
      "Epoch 00210: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1339-00] Learning rate for epoch 210 is 0.0014240531018003821\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.2931 - val_loss: 3.4819\n",
      "Epoch 211/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4011\n",
      "Epoch 00211: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1339-04] Learning rate for epoch 211 is 0.0015802369453012943\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.4011 - val_loss: 5.0949\n",
      "Epoch 212/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8932\n",
      "Epoch 00212: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1339-09] Learning rate for epoch 212 is 0.001736020902171731\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.8932 - val_loss: 3.9418\n",
      "Epoch 213/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8552\n",
      "Epoch 00213: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1339-14] Learning rate for epoch 213 is 0.0018914048559963703\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.8552 - val_loss: 3.7715\n",
      "Epoch 214/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7886\n",
      "Epoch 00214: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1339-19] Learning rate for epoch 214 is 0.0020463888067752123\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.7886 - val_loss: 4.0340\n",
      "Epoch 215/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8135\n",
      "Epoch 00215: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1339-24] Learning rate for epoch 215 is 0.0022009729873389006\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 5.8135 - val_loss: 3.8638\n",
      "Epoch 216/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3048\n",
      "Epoch 00216: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1339-29] Learning rate for epoch 216 is 0.002355156932026148\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.3048 - val_loss: 4.2801\n",
      "Epoch 217/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5476\n",
      "Epoch 00217: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1339-34] Learning rate for epoch 217 is 0.0025089411064982414\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.5476 - val_loss: 4.2804\n",
      "Epoch 218/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7349\n",
      "Epoch 00218: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1339-38] Learning rate for epoch 218 is 0.0026623252779245377\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.7349 - val_loss: 4.6380\n",
      "Epoch 219/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5744\n",
      "Epoch 00219: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1339-43] Learning rate for epoch 219 is 0.0028153094463050365\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5744 - val_loss: 4.7131\n",
      "Epoch 220/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3886\n",
      "Epoch 00220: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1339-48] Learning rate for epoch 220 is 0.002967893611639738\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.3886 - val_loss: 4.3630\n",
      "Epoch 221/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9823\n",
      "Epoch 00221: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1339-53] Learning rate for epoch 221 is 0.003120078006759286\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.9823 - val_loss: 5.7498\n",
      "Epoch 222/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9626\n",
      "Epoch 00222: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1339-58] Learning rate for epoch 222 is 0.0032718623988330364\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.9626 - val_loss: 5.2758\n",
      "Epoch 223/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5711\n",
      "Epoch 00223: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1340-03] Learning rate for epoch 223 is 0.0034232467878609896\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5711 - val_loss: 4.1304\n",
      "Epoch 224/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0562\n",
      "Epoch 00224: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1340-08] Learning rate for epoch 224 is 0.0035742311738431454\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.0562 - val_loss: 4.7628\n",
      "Epoch 225/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6035\n",
      "Epoch 00225: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1340-12] Learning rate for epoch 225 is 0.003724815556779504\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.6035 - val_loss: 3.5064\n",
      "Epoch 226/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6710\n",
      "Epoch 00226: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1340-17] Learning rate for epoch 226 is 0.003874999936670065\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.6710 - val_loss: 4.7956\n",
      "Epoch 227/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3371\n",
      "Epoch 00227: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1340-22] Learning rate for epoch 227 is 0.0037152154836803675\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.3371 - val_loss: 4.1915\n",
      "Epoch 228/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1727\n",
      "Epoch 00228: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1340-27] Learning rate for epoch 228 is 0.0035558310337364674\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.1727 - val_loss: 5.0745\n",
      "Epoch 229/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9232\n",
      "Epoch 00229: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1340-32] Learning rate for epoch 229 is 0.003396846354007721\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.9232 - val_loss: 5.3288\n",
      "Epoch 230/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8552\n",
      "Epoch 00230: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1340-36] Learning rate for epoch 230 is 0.003238261677324772\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.8552 - val_loss: 3.9499\n",
      "Epoch 231/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6439\n",
      "Epoch 00231: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1340-41] Learning rate for epoch 231 is 0.00308007700368762\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6439 - val_loss: 3.9132\n",
      "Epoch 232/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1177\n",
      "Epoch 00232: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1340-46] Learning rate for epoch 232 is 0.002922292333096266\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.1177 - val_loss: 4.1374\n",
      "Epoch 233/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9594\n",
      "Epoch 00233: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1340-51] Learning rate for epoch 233 is 0.002764907432720065\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.9594 - val_loss: 6.2430\n",
      "Epoch 234/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6346\n",
      "Epoch 00234: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1340-56] Learning rate for epoch 234 is 0.0026079227682203054\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.6346 - val_loss: 4.7895\n",
      "Epoch 235/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0381\n",
      "Epoch 00235: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1341-01] Learning rate for epoch 235 is 0.0024513378739356995\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.0381 - val_loss: 5.0789\n",
      "Epoch 236/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8490\n",
      "Epoch 00236: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1341-05] Learning rate for epoch 236 is 0.002295152982696891\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.8490 - val_loss: 6.0507\n",
      "Epoch 237/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6161\n",
      "Epoch 00237: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1341-10] Learning rate for epoch 237 is 0.0021393680945038795\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 6.6161 - val_loss: 4.8379\n",
      "Epoch 238/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5850\n",
      "Epoch 00238: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1341-15] Learning rate for epoch 238 is 0.0019839832093566656\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.5850 - val_loss: 5.0561\n",
      "Epoch 239/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4011\n",
      "Epoch 00239: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1341-20] Learning rate for epoch 239 is 0.0018289980944246054\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.4011 - val_loss: 3.9093\n",
      "Epoch 240/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4420\n",
      "Epoch 00240: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1341-25] Learning rate for epoch 240 is 0.0016744130989536643\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.4420 - val_loss: 5.1241\n",
      "Epoch 241/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6894\n",
      "Epoch 00241: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1341-30] Learning rate for epoch 241 is 0.0015202279901131988\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.6894 - val_loss: 3.6267\n",
      "Epoch 242/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4945\n",
      "Epoch 00242: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1341-34] Learning rate for epoch 242 is 0.0013664428843185306\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.4945 - val_loss: 3.6773\n",
      "Epoch 243/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5412\n",
      "Epoch 00243: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1341-39] Learning rate for epoch 243 is 0.0012130576651543379\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.5412 - val_loss: 3.7046\n",
      "Epoch 244/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6119\n",
      "Epoch 00244: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1341-44] Learning rate for epoch 244 is 0.0010600725654512644\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 5.6119 - val_loss: 3.4480\n",
      "Epoch 245/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3016\n",
      "Epoch 00245: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1341-49] Learning rate for epoch 245 is 0.0009074872941710055\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.3016 - val_loss: 3.4671\n",
      "Epoch 246/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5214\n",
      "Epoch 00246: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1341-54] Learning rate for epoch 246 is 0.0007553020259365439\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.5214 - val_loss: 3.9464\n",
      "Epoch 247/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7158\n",
      "Epoch 00247: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1341-59] Learning rate for epoch 247 is 0.0006035167025402188\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.7158 - val_loss: 3.8301\n",
      "Epoch 248/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4885\n",
      "Epoch 00248: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1342-04] Learning rate for epoch 248 is 0.00045213132398203015\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.4885 - val_loss: 3.4160\n",
      "Epoch 249/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.0843\n",
      "Epoch 00249: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1342-08] Learning rate for epoch 249 is 0.00030114591936580837\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.0843 - val_loss: 3.4541\n",
      "Epoch 250/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1989\n",
      "Epoch 00250: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1342-13] Learning rate for epoch 250 is 0.00015056047413963825\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.1989 - val_loss: 3.4825\n",
      "Epoch 251/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1395\n",
      "Epoch 00251: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1342-18] Learning rate for epoch 251 is 3.7500001326407073e-07\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 5.1395 - val_loss: 3.4844\n",
      "Epoch 252/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 4.9846\n",
      "Epoch 00252: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1342-23] Learning rate for epoch 252 is 0.00015015952521935105\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 4.9846 - val_loss: 3.5005\n",
      "Epoch 253/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4139\n",
      "Epoch 00253: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1342-28] Learning rate for epoch 253 is 0.0002995440736413002\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.4139 - val_loss: 3.5386\n",
      "Epoch 254/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4478\n",
      "Epoch 00254: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1342-32] Learning rate for epoch 254 is 0.0004485286772251129\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.4478 - val_loss: 3.6911\n",
      "Epoch 255/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.0493\n",
      "Epoch 00255: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1342-37] Learning rate for epoch 255 is 0.0005971133359707892\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.0493 - val_loss: 4.0640\n",
      "Epoch 256/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3358\n",
      "Epoch 00256: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1342-42] Learning rate for epoch 256 is 0.0007452979916706681\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.3358 - val_loss: 3.3909\n",
      "Epoch 257/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3889\n",
      "Epoch 00257: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1342-47] Learning rate for epoch 257 is 0.0008930827025324106\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.3889 - val_loss: 3.6826\n",
      "Epoch 258/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6242\n",
      "Epoch 00258: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1342-52] Learning rate for epoch 258 is 0.0010404675267636776\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 5.6242 - val_loss: 3.7876\n",
      "Epoch 259/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4684\n",
      "Epoch 00259: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1342-57] Learning rate for epoch 259 is 0.0011874522315338254\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.4684 - val_loss: 3.6388\n",
      "Epoch 260/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2673\n",
      "Epoch 00260: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1343-02] Learning rate for epoch 260 is 0.0013340371660888195\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 5.2673 - val_loss: 3.5265\n",
      "Epoch 261/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3364\n",
      "Epoch 00261: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1343-06] Learning rate for epoch 261 is 0.0014802219811826944\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.3364 - val_loss: 4.1844\n",
      "Epoch 262/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5709\n",
      "Epoch 00262: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1343-11] Learning rate for epoch 262 is 0.0016260069096460938\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.5709 - val_loss: 3.5010\n",
      "Epoch 263/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7468\n",
      "Epoch 00263: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1343-16] Learning rate for epoch 263 is 0.001771391835063696\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.7468 - val_loss: 3.8609\n",
      "Epoch 264/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3974\n",
      "Epoch 00264: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1343-21] Learning rate for epoch 264 is 0.0019163768738508224\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.3974 - val_loss: 4.0325\n",
      "Epoch 265/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8537\n",
      "Epoch 00265: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1343-26] Learning rate for epoch 265 is 0.0020609619095921516\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.8537 - val_loss: 4.2878\n",
      "Epoch 266/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2884\n",
      "Epoch 00266: val_loss did not improve from 3.34693\n",
      "\n",
      "[20210302-1343-31] Learning rate for epoch 266 is 0.0022051469422876835\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.2884 - val_loss: 3.9396\n",
      "K= 13\n",
      "Check cache in memory:Y True\n",
      "Check keypoints process01: Tensor(\"args_1:0\", shape=(2,), dtype=int64) (2,) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Check keypoints process02: Tensor(\"Reshape:0\", shape=(1, 2), dtype=int64) (1, 2) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Check keypoints process03: Tensor(\"PyFunc:1\", dtype=float32, device=/job:localhost/replica:0/task:0)\n",
      "Check keypoints process04: Tensor(\"Reshape_1:0\", shape=(2,), dtype=float32)\n",
      "Check augment :Y True\n",
      "Check ds cache[True] and augment[True]\n",
      "Check cache in memory:Y True\n",
      "Check augment :N False\n",
      "Check ds cache[True] and augment[False]\n",
      "best_model_name: ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "base_model.trainable :  False\n",
      "\n",
      "[20210302-1343-33] Learning rate for epoch 1 is 0.00047855067532509565\n",
      "Epoch 1/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 106.7198\n",
      "Epoch 00001: val_loss improved from inf to 79.50526, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 5s 267ms/step - loss: 105.7887 - val_loss: 79.5053\n",
      "\n",
      "[20210302-1343-49] Learning rate for epoch 2 is 0.00047855067532509565\n",
      "Epoch 2/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 42.3148\n",
      "Epoch 00002: val_loss improved from 79.50526 to 26.38083, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 41.8088 - val_loss: 26.3808\n",
      "\n",
      "[20210302-1343-53] Learning rate for epoch 3 is 0.00047855067532509565\n",
      "Epoch 3/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 24.0435\n",
      "Epoch 00003: val_loss improved from 26.38083 to 23.25711, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 7s 350ms/step - loss: 24.0435 - val_loss: 23.2571\n",
      "\n",
      "[20210302-1344-04] Learning rate for epoch 4 is 0.00047855067532509565\n",
      "Epoch 4/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 20.8504\n",
      "Epoch 00004: val_loss improved from 23.25711 to 23.16884, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 20.6331 - val_loss: 23.1688\n",
      "\n",
      "[20210302-1344-08] Learning rate for epoch 5 is 0.00047855067532509565\n",
      "Epoch 5/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 19.6808\n",
      "Epoch 00005: val_loss improved from 23.16884 to 21.95838, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 19.6961 - val_loss: 21.9584\n",
      "\n",
      "[20210302-1344-12] Learning rate for epoch 6 is 0.00047855067532509565\n",
      "Epoch 6/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 18.5880\n",
      "Epoch 00006: val_loss did not improve from 21.95838\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 18.4915 - val_loss: 22.4012\n",
      "\n",
      "[20210302-1344-16] Learning rate for epoch 7 is 0.00047855067532509565\n",
      "Epoch 7/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 18.6455\n",
      "Epoch 00007: val_loss improved from 21.95838 to 21.45913, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 18.6088 - val_loss: 21.4591\n",
      "\n",
      "[20210302-1344-20] Learning rate for epoch 8 is 0.00047855067532509565\n",
      "Epoch 8/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 17.8200\n",
      "Epoch 00008: val_loss improved from 21.45913 to 20.73295, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 17.8358 - val_loss: 20.7329\n",
      "\n",
      "[20210302-1344-25] Learning rate for epoch 9 is 0.00047855067532509565\n",
      "Epoch 9/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 17.7343\n",
      "Epoch 00009: val_loss improved from 20.73295 to 19.98963, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 17.6686 - val_loss: 19.9896\n",
      "\n",
      "[20210302-1344-29] Learning rate for epoch 10 is 0.00047855067532509565\n",
      "Epoch 10/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 17.6778\n",
      "Epoch 00010: val_loss improved from 19.98963 to 19.14248, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 17.6778 - val_loss: 19.1425\n",
      "\n",
      "[20210302-1344-34] Learning rate for epoch 11 is 0.00047855067532509565\n",
      "Epoch 11/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 17.3410\n",
      "Epoch 00011: val_loss improved from 19.14248 to 18.58719, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 17.3365 - val_loss: 18.5872\n",
      "\n",
      "[20210302-1344-38] Learning rate for epoch 12 is 0.00047855067532509565\n",
      "Epoch 12/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 17.7871\n",
      "Epoch 00012: val_loss improved from 18.58719 to 17.98035, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 59ms/step - loss: 17.7394 - val_loss: 17.9804\n",
      "\n",
      "[20210302-1344-42] Learning rate for epoch 13 is 0.00047855067532509565\n",
      "Epoch 13/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 17.5589\n",
      "Epoch 00013: val_loss improved from 17.98035 to 17.68154, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 17.5571 - val_loss: 17.6815\n",
      "\n",
      "[20210302-1344-47] Learning rate for epoch 14 is 0.00047855067532509565\n",
      "Epoch 14/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.9890\n",
      "Epoch 00014: val_loss did not improve from 17.68154\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.9890 - val_loss: 17.7651\n",
      "\n",
      "[20210302-1344-51] Learning rate for epoch 15 is 0.00047855067532509565\n",
      "Epoch 15/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 17.5523\n",
      "Epoch 00015: val_loss improved from 17.68154 to 16.45967, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 17.4553 - val_loss: 16.4597\n",
      "\n",
      "[20210302-1344-55] Learning rate for epoch 16 is 0.00047855067532509565\n",
      "Epoch 16/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 17.2150\n",
      "Epoch 00016: val_loss improved from 16.45967 to 16.03600, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 17.2150 - val_loss: 16.0360\n",
      "\n",
      "[20210302-1344-59] Learning rate for epoch 17 is 0.00047855067532509565\n",
      "Epoch 17/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.9975\n",
      "Epoch 00017: val_loss improved from 16.03600 to 15.88214, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.9395 - val_loss: 15.8821\n",
      "\n",
      "[20210302-1345-04] Learning rate for epoch 18 is 0.00047855067532509565\n",
      "Epoch 18/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 17.4936\n",
      "Epoch 00018: val_loss improved from 15.88214 to 15.58506, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 17.3461 - val_loss: 15.5851\n",
      "\n",
      "[20210302-1345-08] Learning rate for epoch 19 is 0.00047855067532509565\n",
      "Epoch 19/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.4912\n",
      "Epoch 00019: val_loss improved from 15.58506 to 15.57158, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 16.4912 - val_loss: 15.5716\n",
      "\n",
      "[20210302-1345-13] Learning rate for epoch 20 is 0.00047855067532509565\n",
      "Epoch 20/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.9106\n",
      "Epoch 00020: val_loss improved from 15.57158 to 14.28598, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 16.8565 - val_loss: 14.2860\n",
      "\n",
      "[20210302-1345-17] Learning rate for epoch 21 is 0.00047855067532509565\n",
      "Epoch 21/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.4846\n",
      "Epoch 00021: val_loss improved from 14.28598 to 14.26994, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 16.4846 - val_loss: 14.2699\n",
      "\n",
      "[20210302-1345-21] Learning rate for epoch 22 is 0.00047855067532509565\n",
      "Epoch 22/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.8532\n",
      "Epoch 00022: val_loss improved from 14.26994 to 13.99227, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.9028 - val_loss: 13.9923\n",
      "\n",
      "[20210302-1345-26] Learning rate for epoch 23 is 0.00047855067532509565\n",
      "Epoch 23/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 16.5516\n",
      "Epoch 00023: val_loss improved from 13.99227 to 13.66928, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 16.5950 - val_loss: 13.6693\n",
      "\n",
      "[20210302-1345-30] Learning rate for epoch 24 is 0.00047855067532509565\n",
      "Epoch 24/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.2894\n",
      "Epoch 00024: val_loss improved from 13.66928 to 13.43386, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 16.3088 - val_loss: 13.4339\n",
      "\n",
      "[20210302-1345-34] Learning rate for epoch 25 is 0.00047855067532509565\n",
      "Epoch 25/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 16.6870\n",
      "Epoch 00025: val_loss improved from 13.43386 to 12.82929, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 16.7035 - val_loss: 12.8293\n",
      "\n",
      "[20210302-1345-38] Learning rate for epoch 26 is 0.00047855067532509565\n",
      "Epoch 26/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.4999\n",
      "Epoch 00026: val_loss improved from 12.82929 to 12.31756, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.4999 - val_loss: 12.3176\n",
      "\n",
      "[20210302-1345-43] Learning rate for epoch 27 is 0.00047855067532509565\n",
      "Epoch 27/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.3105\n",
      "Epoch 00027: val_loss improved from 12.31756 to 11.89154, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.3105 - val_loss: 11.8915\n",
      "\n",
      "[20210302-1345-47] Learning rate for epoch 28 is 0.00047855067532509565\n",
      "Epoch 28/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.9941\n",
      "Epoch 00028: val_loss improved from 11.89154 to 11.15692, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.1013 - val_loss: 11.1569\n",
      "\n",
      "[20210302-1345-51] Learning rate for epoch 29 is 0.00047855067532509565\n",
      "Epoch 29/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.5706\n",
      "Epoch 00029: val_loss improved from 11.15692 to 10.98696, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 16.5706 - val_loss: 10.9870\n",
      "\n",
      "[20210302-1345-56] Learning rate for epoch 30 is 0.00047855067532509565\n",
      "Epoch 30/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.6248\n",
      "Epoch 00030: val_loss did not improve from 10.98696\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.5376 - val_loss: 12.0277\n",
      "\n",
      "[20210302-1345-59] Learning rate for epoch 31 is 0.00047855067532509565\n",
      "Epoch 31/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 16.6761\n",
      "Epoch 00031: val_loss improved from 10.98696 to 10.79789, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 16.7045 - val_loss: 10.7979\n",
      "\n",
      "[20210302-1346-04] Learning rate for epoch 32 is 0.00047855067532509565\n",
      "Epoch 32/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.2932\n",
      "Epoch 00032: val_loss did not improve from 10.79789\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.2334 - val_loss: 10.8026\n",
      "\n",
      "[20210302-1346-08] Learning rate for epoch 33 is 0.00047855067532509565\n",
      "Epoch 33/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.3999\n",
      "Epoch 00033: val_loss did not improve from 10.79789\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.3114 - val_loss: 10.9127\n",
      "\n",
      "[20210302-1346-12] Learning rate for epoch 34 is 0.00047855067532509565\n",
      "Epoch 34/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.9941\n",
      "Epoch 00034: val_loss did not improve from 10.79789\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.9941 - val_loss: 11.2000\n",
      "\n",
      "[20210302-1346-16] Learning rate for epoch 35 is 0.00047855067532509565\n",
      "Epoch 35/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.5955\n",
      "Epoch 00035: val_loss improved from 10.79789 to 10.36755, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.5075 - val_loss: 10.3676\n",
      "\n",
      "[20210302-1346-20] Learning rate for epoch 36 is 0.00047855067532509565\n",
      "Epoch 36/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 16.3095\n",
      "Epoch 00036: val_loss improved from 10.36755 to 10.21521, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 16.5557 - val_loss: 10.2152\n",
      "\n",
      "[20210302-1346-24] Learning rate for epoch 37 is 0.00047855067532509565\n",
      "Epoch 37/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.9395\n",
      "Epoch 00037: val_loss did not improve from 10.21521\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.8960 - val_loss: 11.1361\n",
      "\n",
      "[20210302-1346-28] Learning rate for epoch 38 is 0.00047855067532509565\n",
      "Epoch 38/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.5677\n",
      "Epoch 00038: val_loss improved from 10.21521 to 9.99256, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 15.6131 - val_loss: 9.9926\n",
      "\n",
      "[20210302-1346-33] Learning rate for epoch 39 is 0.00047855067532509565\n",
      "Epoch 39/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.1164\n",
      "Epoch 00039: val_loss improved from 9.99256 to 9.68462, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 16.1366 - val_loss: 9.6846\n",
      "\n",
      "[20210302-1346-37] Learning rate for epoch 40 is 0.00047855067532509565\n",
      "Epoch 40/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 16.6150\n",
      "Epoch 00040: val_loss did not improve from 9.68462\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.7997 - val_loss: 9.9726\n",
      "\n",
      "[20210302-1346-41] Learning rate for epoch 41 is 0.00047855067532509565\n",
      "Epoch 41/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.2835\n",
      "Epoch 00041: val_loss did not improve from 9.68462\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.3662 - val_loss: 13.4358\n",
      "\n",
      "[20210302-1346-45] Learning rate for epoch 42 is 0.00047855067532509565\n",
      "Epoch 42/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.1763\n",
      "Epoch 00042: val_loss did not improve from 9.68462\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.1763 - val_loss: 11.3640\n",
      "\n",
      "[20210302-1346-49] Learning rate for epoch 43 is 0.00047855067532509565\n",
      "Epoch 43/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.9974\n",
      "Epoch 00043: val_loss did not improve from 9.68462\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.0900 - val_loss: 10.6691\n",
      "\n",
      "[20210302-1346-53] Learning rate for epoch 44 is 0.00047855067532509565\n",
      "Epoch 44/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 16.0045\n",
      "Epoch 00044: val_loss did not improve from 9.68462\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.9627 - val_loss: 9.8768\n",
      "\n",
      "[20210302-1346-57] Learning rate for epoch 45 is 0.00047855067532509565\n",
      "Epoch 45/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.1491\n",
      "Epoch 00045: val_loss did not improve from 9.68462\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.0924 - val_loss: 10.2534\n",
      "\n",
      "[20210302-1347-01] Learning rate for epoch 46 is 0.00047855067532509565\n",
      "Epoch 46/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.8803\n",
      "Epoch 00046: val_loss did not improve from 9.68462\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 15.7958 - val_loss: 10.9766\n",
      "\n",
      "[20210302-1347-05] Learning rate for epoch 47 is 0.00047855067532509565\n",
      "Epoch 47/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.8548\n",
      "Epoch 00047: val_loss improved from 9.68462 to 9.48283, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 15.7271 - val_loss: 9.4828\n",
      "\n",
      "[20210302-1347-09] Learning rate for epoch 48 is 0.00047855067532509565\n",
      "Epoch 48/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 16.3151\n",
      "Epoch 00048: val_loss did not improve from 9.48283\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.1885 - val_loss: 9.8089\n",
      "\n",
      "[20210302-1347-13] Learning rate for epoch 49 is 0.00047855067532509565\n",
      "Epoch 49/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.2763\n",
      "Epoch 00049: val_loss did not improve from 9.48283\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.1881 - val_loss: 10.6230\n",
      "\n",
      "[20210302-1347-17] Learning rate for epoch 50 is 0.00047855067532509565\n",
      "Epoch 50/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.4392\n",
      "Epoch 00050: val_loss improved from 9.48283 to 8.84067, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 15.5343 - val_loss: 8.8407\n",
      "\n",
      "[20210302-1347-21] Learning rate for epoch 51 is 0.00047855067532509565\n",
      "Epoch 51/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.7485\n",
      "Epoch 00051: val_loss did not improve from 8.84067\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.7098 - val_loss: 9.6207\n",
      "\n",
      "[20210302-1347-25] Learning rate for epoch 52 is 0.00047855067532509565\n",
      "Epoch 52/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.6424\n",
      "Epoch 00052: val_loss did not improve from 8.84067\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.6424 - val_loss: 9.8613\n",
      "\n",
      "[20210302-1347-29] Learning rate for epoch 53 is 0.00047855067532509565\n",
      "Epoch 53/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.6320\n",
      "Epoch 00053: val_loss did not improve from 8.84067\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.6320 - val_loss: 9.1285\n",
      "\n",
      "[20210302-1347-33] Learning rate for epoch 54 is 0.00047855067532509565\n",
      "Epoch 54/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.6738\n",
      "Epoch 00054: val_loss improved from 8.84067 to 8.69909, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 16.6738 - val_loss: 8.6991\n",
      "\n",
      "[20210302-1347-37] Learning rate for epoch 55 is 0.00047855067532509565\n",
      "Epoch 55/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.8984\n",
      "Epoch 00055: val_loss did not improve from 8.69909\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.8984 - val_loss: 8.9094\n",
      "\n",
      "[20210302-1347-41] Learning rate for epoch 56 is 0.00047855067532509565\n",
      "Epoch 56/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.9627\n",
      "Epoch 00056: val_loss did not improve from 8.69909\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.0608 - val_loss: 10.4178\n",
      "\n",
      "[20210302-1347-45] Learning rate for epoch 57 is 0.00047855067532509565\n",
      "Epoch 57/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.5648\n",
      "Epoch 00057: val_loss did not improve from 8.69909\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.7090 - val_loss: 9.8044\n",
      "\n",
      "[20210302-1347-49] Learning rate for epoch 58 is 0.00047855067532509565\n",
      "Epoch 58/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.7146\n",
      "Epoch 00058: val_loss did not improve from 8.69909\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.6999 - val_loss: 10.0256\n",
      "\n",
      "[20210302-1347-53] Learning rate for epoch 59 is 0.00047855067532509565\n",
      "Epoch 59/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 16.1826\n",
      "Epoch 00059: val_loss did not improve from 8.69909\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.1567 - val_loss: 9.2415\n",
      "\n",
      "[20210302-1347-57] Learning rate for epoch 60 is 0.00047855067532509565\n",
      "Epoch 60/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.8736\n",
      "Epoch 00060: val_loss did not improve from 8.69909\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.7813 - val_loss: 9.9667\n",
      "\n",
      "[20210302-1348-01] Learning rate for epoch 61 is 0.00047855067532509565\n",
      "Epoch 61/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 15.9749\n",
      "Epoch 00061: val_loss did not improve from 8.69909\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 15.9071 - val_loss: 11.0074\n",
      "\n",
      "[20210302-1348-05] Learning rate for epoch 62 is 0.00047855067532509565\n",
      "Epoch 62/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 16.1043\n",
      "Epoch 00062: val_loss did not improve from 8.69909\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 16.1129 - val_loss: 9.9837\n",
      "\n",
      "[20210302-1348-09] Learning rate for epoch 63 is 0.00047855067532509565\n",
      "Epoch 63/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.6030\n",
      "Epoch 00063: val_loss did not improve from 8.69909\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.5831 - val_loss: 9.5793\n",
      "\n",
      "[20210302-1348-13] Learning rate for epoch 64 is 0.00047855067532509565\n",
      "Epoch 64/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 15.4656\n",
      "Epoch 00064: val_loss did not improve from 8.69909\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 15.4906 - val_loss: 9.8020\n",
      "\n",
      "[20210302-1348-17] Learning rate for epoch 65 is 0.00047855067532509565\n",
      "Epoch 65/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 15.6635\n",
      "Epoch 00065: val_loss did not improve from 8.69909\n",
      "20/20 [==============================] - 0s 24ms/step - loss: 15.8867 - val_loss: 8.7968\n",
      "\n",
      "[20210302-1348-21] Learning rate for epoch 66 is 0.00047855067532509565\n",
      "Epoch 66/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 15.8409\n",
      "Epoch 00066: val_loss did not improve from 8.69909\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.7895 - val_loss: 9.4160\n",
      "\n",
      "[20210302-1348-25] Learning rate for epoch 67 is 0.00047855067532509565\n",
      "Epoch 67/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.3830\n",
      "Epoch 00067: val_loss did not improve from 8.69909\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.3768 - val_loss: 9.2654\n",
      "\n",
      "[20210302-1348-29] Learning rate for epoch 68 is 0.00047855067532509565\n",
      "Epoch 68/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.3920\n",
      "Epoch 00068: val_loss did not improve from 8.69909\n",
      "20/20 [==============================] - 0s 24ms/step - loss: 16.3349 - val_loss: 10.0523\n",
      "\n",
      "[20210302-1348-33] Learning rate for epoch 69 is 0.00047855067532509565\n",
      "Epoch 69/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.5749\n",
      "Epoch 00069: val_loss did not improve from 8.69909\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.6534 - val_loss: 10.0032\n",
      "\n",
      "[20210302-1348-37] Learning rate for epoch 70 is 0.00047855067532509565\n",
      "Epoch 70/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.3556\n",
      "Epoch 00070: val_loss did not improve from 8.69909\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 15.3812 - val_loss: 9.3767\n",
      "\n",
      "[20210302-1348-41] Learning rate for epoch 71 is 0.00047855067532509565\n",
      "Epoch 71/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.5177\n",
      "Epoch 00071: val_loss did not improve from 8.69909\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.5593 - val_loss: 9.4909\n",
      "\n",
      "[20210302-1348-45] Learning rate for epoch 72 is 0.00047855067532509565\n",
      "Epoch 72/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.8402\n",
      "Epoch 00072: val_loss did not improve from 8.69909\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.6812 - val_loss: 10.2855\n",
      "\n",
      "[20210302-1348-49] Learning rate for epoch 73 is 0.00047855067532509565\n",
      "Epoch 73/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.7865\n",
      "Epoch 00073: val_loss did not improve from 8.69909\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.6270 - val_loss: 9.8850\n",
      "\n",
      "[20210302-1348-53] Learning rate for epoch 74 is 0.00047855067532509565\n",
      "Epoch 74/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.4637\n",
      "Epoch 00074: val_loss did not improve from 8.69909\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.4637 - val_loss: 10.2964\n",
      "\n",
      "*********************************** Start fine tune ***********************************\n",
      "tt: 195, nt:46, total layers:241\n",
      "*********************************** Start fine tune ***********************************\n",
      "Epoch 1/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 24.1608\n",
      "Epoch 00001: val_loss did not improve from 8.69909\n",
      "\n",
      "[20210302-1349-35] Learning rate for epoch 1 is 4.999999987376214e-07\n",
      "20/20 [==============================] - 15s 764ms/step - loss: 24.1608 - val_loss: 15.8308\n",
      "Epoch 2/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 22.7495\n",
      "Epoch 00002: val_loss did not improve from 8.69909\n",
      "\n",
      "[20210302-1349-40] Learning rate for epoch 2 is 5.044450517743826e-05\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 22.7495 - val_loss: 15.2209\n",
      "Epoch 3/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 18.1585\n",
      "Epoch 00003: val_loss did not improve from 8.69909\n",
      "\n",
      "[20210302-1350-01] Learning rate for epoch 3 is 0.00010028902033809572\n",
      "20/20 [==============================] - 17s 848ms/step - loss: 18.1585 - val_loss: 10.7904\n",
      "Epoch 4/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.8703\n",
      "Epoch 00004: val_loss did not improve from 8.69909\n",
      "\n",
      "[20210302-1350-06] Learning rate for epoch 4 is 0.00015003354928921908\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 15.8703 - val_loss: 12.3227\n",
      "Epoch 5/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.4277\n",
      "Epoch 00005: val_loss did not improve from 8.69909\n",
      "\n",
      "[20210302-1350-11] Learning rate for epoch 5 is 0.0001996780774788931\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 14.4277 - val_loss: 11.6178\n",
      "Epoch 6/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.5950\n",
      "Epoch 00006: val_loss improved from 8.69909 to 8.66040, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1350-16] Learning rate for epoch 6 is 0.000249222619459033\n",
      "20/20 [==============================] - 2s 91ms/step - loss: 13.5950 - val_loss: 8.6604\n",
      "Epoch 7/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.3077\n",
      "Epoch 00007: val_loss improved from 8.66040 to 8.26365, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1350-21] Learning rate for epoch 7 is 0.0002986671752296388\n",
      "20/20 [==============================] - 2s 89ms/step - loss: 13.3077 - val_loss: 8.2636\n",
      "Epoch 8/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 12.7637\n",
      "Epoch 00008: val_loss improved from 8.26365 to 7.54512, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1350-27] Learning rate for epoch 8 is 0.00034801175934262574\n",
      "20/20 [==============================] - 2s 89ms/step - loss: 12.7637 - val_loss: 7.5451\n",
      "Epoch 9/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 12.9926\n",
      "Epoch 00009: val_loss improved from 7.54512 to 7.06101, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1350-32] Learning rate for epoch 9 is 0.00039725631359033287\n",
      "20/20 [==============================] - 2s 91ms/step - loss: 12.9926 - val_loss: 7.0610\n",
      "Epoch 10/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 12.4389\n",
      "Epoch 00010: val_loss did not improve from 7.06101\n",
      "\n",
      "[20210302-1350-37] Learning rate for epoch 10 is 0.0004464008961804211\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 12.4389 - val_loss: 10.6442\n",
      "Epoch 11/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.7160\n",
      "Epoch 00011: val_loss did not improve from 7.06101\n",
      "\n",
      "[20210302-1350-42] Learning rate for epoch 11 is 0.0004954455071128905\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 11.7160 - val_loss: 8.5333\n",
      "Epoch 12/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 12.1658\n",
      "Epoch 00012: val_loss did not improve from 7.06101\n",
      "\n",
      "[20210302-1350-47] Learning rate for epoch 12 is 0.0005443901172839105\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 12.1658 - val_loss: 8.2048\n",
      "Epoch 13/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.3770\n",
      "Epoch 00013: val_loss did not improve from 7.06101\n",
      "\n",
      "[20210302-1350-52] Learning rate for epoch 13 is 0.0005932347266934812\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 11.3770 - val_loss: 10.1925\n",
      "Epoch 14/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.0081\n",
      "Epoch 00014: val_loss did not improve from 7.06101\n",
      "\n",
      "[20210302-1350-56] Learning rate for epoch 14 is 0.0006419793353416026\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 11.0081 - val_loss: 14.1076\n",
      "Epoch 15/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.4013\n",
      "Epoch 00015: val_loss improved from 7.06101 to 6.03372, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1351-02] Learning rate for epoch 15 is 0.0006906240014359355\n",
      "20/20 [==============================] - 2s 94ms/step - loss: 10.4013 - val_loss: 6.0337\n",
      "Epoch 16/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.2260\n",
      "Epoch 00016: val_loss improved from 6.03372 to 5.41325, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1351-07] Learning rate for epoch 16 is 0.0007391686085611582\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 10.2260 - val_loss: 5.4133\n",
      "Epoch 17/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.7784\n",
      "Epoch 00017: val_loss improved from 5.41325 to 4.77486, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1351-12] Learning rate for epoch 17 is 0.0007876132731325924\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 9.7784 - val_loss: 4.7749\n",
      "Epoch 18/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.9138\n",
      "Epoch 00018: val_loss improved from 4.77486 to 4.76714, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1351-18] Learning rate for epoch 18 is 0.0008359579369425774\n",
      "20/20 [==============================] - 2s 93ms/step - loss: 8.9138 - val_loss: 4.7671\n",
      "Epoch 19/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.7170\n",
      "Epoch 00019: val_loss did not improve from 4.76714\n",
      "\n",
      "[20210302-1351-23] Learning rate for epoch 19 is 0.000884202599991113\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 8.7170 - val_loss: 5.7458\n",
      "Epoch 20/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.3875\n",
      "Epoch 00020: val_loss did not improve from 4.76714\n",
      "\n",
      "[20210302-1351-28] Learning rate for epoch 20 is 0.0009323473204858601\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 8.3875 - val_loss: 8.8604\n",
      "Epoch 21/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.3841\n",
      "Epoch 00021: val_loss did not improve from 4.76714\n",
      "\n",
      "[20210302-1351-32] Learning rate for epoch 21 is 0.000980391982011497\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 8.3841 - val_loss: 5.1294\n",
      "Epoch 22/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.0991\n",
      "Epoch 00022: val_loss did not improve from 4.76714\n",
      "\n",
      "[20210302-1351-37] Learning rate for epoch 22 is 0.0010283367009833455\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 8.0991 - val_loss: 4.8250\n",
      "Epoch 23/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3675\n",
      "Epoch 00023: val_loss improved from 4.76714 to 4.74051, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1351-43] Learning rate for epoch 23 is 0.0010761814191937447\n",
      "20/20 [==============================] - 2s 92ms/step - loss: 7.3675 - val_loss: 4.7405\n",
      "Epoch 24/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5160\n",
      "Epoch 00024: val_loss improved from 4.74051 to 3.78993, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1351-48] Learning rate for epoch 24 is 0.0011239261366426945\n",
      "20/20 [==============================] - 2s 89ms/step - loss: 7.5160 - val_loss: 3.7899\n",
      "Epoch 25/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5603\n",
      "Epoch 00025: val_loss did not improve from 3.78993\n",
      "\n",
      "[20210302-1351-53] Learning rate for epoch 25 is 0.001171570853330195\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.5603 - val_loss: 4.2696\n",
      "Epoch 26/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.9602\n",
      "Epoch 00026: val_loss did not improve from 3.78993\n",
      "\n",
      "[20210302-1351-58] Learning rate for epoch 26 is 0.001219115569256246\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.9602 - val_loss: 4.4776\n",
      "Epoch 27/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8132\n",
      "Epoch 00027: val_loss did not improve from 3.78993\n",
      "\n",
      "[20210302-1352-02] Learning rate for epoch 27 is 0.0012665604008361697\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.8132 - val_loss: 5.7902\n",
      "Epoch 28/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6343\n",
      "Epoch 00028: val_loss did not improve from 3.78993\n",
      "\n",
      "[20210302-1352-07] Learning rate for epoch 28 is 0.0013139051152393222\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 7.6343 - val_loss: 6.1968\n",
      "Epoch 29/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1912\n",
      "Epoch 00029: val_loss did not improve from 3.78993\n",
      "\n",
      "[20210302-1352-12] Learning rate for epoch 29 is 0.0013611499452963471\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.1912 - val_loss: 4.4957\n",
      "Epoch 30/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0182\n",
      "Epoch 00030: val_loss did not improve from 3.78993\n",
      "\n",
      "[20210302-1352-17] Learning rate for epoch 30 is 0.001408294658176601\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.0182 - val_loss: 4.7189\n",
      "Epoch 31/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7241\n",
      "Epoch 00031: val_loss did not improve from 3.78993\n",
      "\n",
      "[20210302-1352-22] Learning rate for epoch 31 is 0.0014553394867107272\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.7241 - val_loss: 6.0265\n",
      "Epoch 32/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3218\n",
      "Epoch 00032: val_loss did not improve from 3.78993\n",
      "\n",
      "[20210302-1352-27] Learning rate for epoch 32 is 0.0015022843144834042\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.3218 - val_loss: 6.4269\n",
      "Epoch 33/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9562\n",
      "Epoch 00033: val_loss did not improve from 3.78993\n",
      "\n",
      "[20210302-1352-32] Learning rate for epoch 33 is 0.0015491291414946318\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.9562 - val_loss: 19.4965\n",
      "Epoch 34/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3936\n",
      "Epoch 00034: val_loss did not improve from 3.78993\n",
      "\n",
      "[20210302-1352-36] Learning rate for epoch 34 is 0.00159587396774441\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.3936 - val_loss: 286.9810\n",
      "Epoch 35/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3517\n",
      "Epoch 00035: val_loss did not improve from 3.78993\n",
      "\n",
      "[20210302-1352-41] Learning rate for epoch 35 is 0.001642518793232739\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.3517 - val_loss: 184.1582\n",
      "Epoch 36/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1899\n",
      "Epoch 00036: val_loss did not improve from 3.78993\n",
      "\n",
      "[20210302-1352-46] Learning rate for epoch 36 is 0.0016890636179596186\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.1899 - val_loss: 6.0425\n",
      "Epoch 37/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8065\n",
      "Epoch 00037: val_loss did not improve from 3.78993\n",
      "\n",
      "[20210302-1352-51] Learning rate for epoch 37 is 0.0017355084419250488\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 7.8065 - val_loss: 4.2621\n",
      "Epoch 38/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9449\n",
      "Epoch 00038: val_loss did not improve from 3.78993\n",
      "\n",
      "[20210302-1352-56] Learning rate for epoch 38 is 0.0017818533815443516\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.9449 - val_loss: 7.4483\n",
      "Epoch 39/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8937\n",
      "Epoch 00039: val_loss did not improve from 3.78993\n",
      "\n",
      "[20210302-1353-00] Learning rate for epoch 39 is 0.0018280982039868832\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.8937 - val_loss: 4.4626\n",
      "Epoch 40/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5363\n",
      "Epoch 00040: val_loss did not improve from 3.78993\n",
      "\n",
      "[20210302-1353-05] Learning rate for epoch 40 is 0.0018742431420832872\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.5363 - val_loss: 4.1855\n",
      "Epoch 41/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1162\n",
      "Epoch 00041: val_loss did not improve from 3.78993\n",
      "\n",
      "[20210302-1353-10] Learning rate for epoch 41 is 0.0019202879630029202\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.1162 - val_loss: 9.5756\n",
      "Epoch 42/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0283\n",
      "Epoch 00042: val_loss did not improve from 3.78993\n",
      "\n",
      "[20210302-1353-15] Learning rate for epoch 42 is 0.0019662328995764256\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.0283 - val_loss: 8.2298\n",
      "Epoch 43/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9287\n",
      "Epoch 00043: val_loss did not improve from 3.78993\n",
      "\n",
      "[20210302-1353-20] Learning rate for epoch 43 is 0.00201207771897316\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.9287 - val_loss: 6.0773\n",
      "Epoch 44/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5917\n",
      "Epoch 00044: val_loss did not improve from 3.78993\n",
      "\n",
      "[20210302-1353-25] Learning rate for epoch 44 is 0.0020578226540237665\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.5917 - val_loss: 5.7587\n",
      "Epoch 45/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6403\n",
      "Epoch 00045: val_loss did not improve from 3.78993\n",
      "\n",
      "[20210302-1353-30] Learning rate for epoch 45 is 0.0021034677047282457\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.6403 - val_loss: 6.5300\n",
      "Epoch 46/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2046\n",
      "Epoch 00046: val_loss did not improve from 3.78993\n",
      "\n",
      "[20210302-1353-34] Learning rate for epoch 46 is 0.002149012638255954\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.2046 - val_loss: 5.3963\n",
      "Epoch 47/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3710\n",
      "Epoch 00047: val_loss did not improve from 3.78993\n",
      "\n",
      "[20210302-1353-39] Learning rate for epoch 47 is 0.0021944576874375343\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.3710 - val_loss: 8.2539\n",
      "Epoch 48/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1866\n",
      "Epoch 00048: val_loss did not improve from 3.78993\n",
      "\n",
      "[20210302-1353-44] Learning rate for epoch 48 is 0.0022398026194423437\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 7.1866 - val_loss: 7.6617\n",
      "Epoch 49/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3222\n",
      "Epoch 00049: val_loss did not improve from 3.78993\n",
      "\n",
      "[20210302-1353-49] Learning rate for epoch 49 is 0.002285047434270382\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.3222 - val_loss: 16.6737\n",
      "Epoch 50/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0835\n",
      "Epoch 00050: val_loss did not improve from 3.78993\n",
      "\n",
      "[20210302-1353-54] Learning rate for epoch 50 is 0.0023301925975829363\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.0835 - val_loss: 5.8742\n",
      "Epoch 51/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5821\n",
      "Epoch 00051: val_loss did not improve from 3.78993\n",
      "\n",
      "[20210302-1353-59] Learning rate for epoch 51 is 0.002375237410888076\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.5821 - val_loss: 6.4720\n",
      "Epoch 52/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2133\n",
      "Epoch 00052: val_loss improved from 3.78993 to 3.76099, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1354-04] Learning rate for epoch 52 is 0.0024201825726777315\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 7.2133 - val_loss: 3.7610\n",
      "Epoch 53/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6609\n",
      "Epoch 00053: val_loss did not improve from 3.76099\n",
      "\n",
      "[20210302-1354-09] Learning rate for epoch 53 is 0.002465027617290616\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.6609 - val_loss: 4.0126\n",
      "Epoch 54/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8137\n",
      "Epoch 00054: val_loss did not improve from 3.76099\n",
      "\n",
      "[20210302-1354-14] Learning rate for epoch 54 is 0.0025097725447267294\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.8137 - val_loss: 7.2854\n",
      "Epoch 55/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4473\n",
      "Epoch 00055: val_loss did not improve from 3.76099\n",
      "\n",
      "[20210302-1354-19] Learning rate for epoch 55 is 0.0025544175878167152\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.4473 - val_loss: 4.2402\n",
      "Epoch 56/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6603\n",
      "Epoch 00056: val_loss did not improve from 3.76099\n",
      "\n",
      "[20210302-1354-23] Learning rate for epoch 56 is 0.00259896251372993\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.6603 - val_loss: 4.3976\n",
      "Epoch 57/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3172\n",
      "Epoch 00057: val_loss did not improve from 3.76099\n",
      "\n",
      "[20210302-1354-28] Learning rate for epoch 57 is 0.0026434077881276608\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.3172 - val_loss: 4.0490\n",
      "Epoch 58/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2346\n",
      "Epoch 00058: val_loss did not improve from 3.76099\n",
      "\n",
      "[20210302-1354-33] Learning rate for epoch 58 is 0.0026877527125179768\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.2346 - val_loss: 5.2782\n",
      "Epoch 59/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7310\n",
      "Epoch 00059: val_loss did not improve from 3.76099\n",
      "\n",
      "[20210302-1354-38] Learning rate for epoch 59 is 0.0027319977525621653\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.7310 - val_loss: 4.6714\n",
      "Epoch 60/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9914\n",
      "Epoch 00060: val_loss did not improve from 3.76099\n",
      "\n",
      "[20210302-1354-43] Learning rate for epoch 60 is 0.0027761429082602262\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.9914 - val_loss: 5.1543\n",
      "Epoch 61/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0374\n",
      "Epoch 00061: val_loss did not improve from 3.76099\n",
      "\n",
      "[20210302-1354-48] Learning rate for epoch 61 is 0.002820187946781516\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.0374 - val_loss: 4.3674\n",
      "Epoch 62/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5857\n",
      "Epoch 00062: val_loss did not improve from 3.76099\n",
      "\n",
      "[20210302-1354-53] Learning rate for epoch 62 is 0.0028641331009566784\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.5857 - val_loss: 4.0023\n",
      "Epoch 63/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9838\n",
      "Epoch 00063: val_loss did not improve from 3.76099\n",
      "\n",
      "[20210302-1354-57] Learning rate for epoch 63 is 0.0029079781379550695\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.9838 - val_loss: 4.7029\n",
      "Epoch 64/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2068\n",
      "Epoch 00064: val_loss improved from 3.76099 to 3.71325, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1355-03] Learning rate for epoch 64 is 0.002951723290607333\n",
      "20/20 [==============================] - 2s 93ms/step - loss: 6.2068 - val_loss: 3.7132\n",
      "Epoch 65/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6295\n",
      "Epoch 00065: val_loss did not improve from 3.71325\n",
      "\n",
      "[20210302-1355-08] Learning rate for epoch 65 is 0.0029953685589134693\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.6295 - val_loss: 4.1223\n",
      "Epoch 66/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8113\n",
      "Epoch 00066: val_loss did not improve from 3.71325\n",
      "\n",
      "[20210302-1355-13] Learning rate for epoch 66 is 0.0030389137100428343\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.8113 - val_loss: 6.3757\n",
      "Epoch 67/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8495\n",
      "Epoch 00067: val_loss did not improve from 3.71325\n",
      "\n",
      "[20210302-1355-17] Learning rate for epoch 67 is 0.003082358743995428\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.8495 - val_loss: 4.9450\n",
      "Epoch 68/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1732\n",
      "Epoch 00068: val_loss did not improve from 3.71325\n",
      "\n",
      "[20210302-1355-22] Learning rate for epoch 68 is 0.0031257038936018944\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.1732 - val_loss: 3.9232\n",
      "Epoch 69/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5697\n",
      "Epoch 00069: val_loss did not improve from 3.71325\n",
      "\n",
      "[20210302-1355-27] Learning rate for epoch 69 is 0.003168949158862233\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.5697 - val_loss: 14.8259\n",
      "Epoch 70/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1574\n",
      "Epoch 00070: val_loss did not improve from 3.71325\n",
      "\n",
      "[20210302-1355-32] Learning rate for epoch 70 is 0.0032120943069458008\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.1574 - val_loss: 7.4438\n",
      "Epoch 71/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5380\n",
      "Epoch 00071: val_loss did not improve from 3.71325\n",
      "\n",
      "[20210302-1355-37] Learning rate for epoch 71 is 0.003255139570683241\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 6.5380 - val_loss: 5.5927\n",
      "Epoch 72/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6858\n",
      "Epoch 00072: val_loss did not improve from 3.71325\n",
      "\n",
      "[20210302-1355-41] Learning rate for epoch 72 is 0.00329808471724391\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.6858 - val_loss: 6.4751\n",
      "Epoch 73/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4746\n",
      "Epoch 00073: val_loss did not improve from 3.71325\n",
      "\n",
      "[20210302-1355-46] Learning rate for epoch 73 is 0.0033409299794584513\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.4746 - val_loss: 6.0473\n",
      "Epoch 74/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7457\n",
      "Epoch 00074: val_loss did not improve from 3.71325\n",
      "\n",
      "[20210302-1355-51] Learning rate for epoch 74 is 0.0033836751244962215\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.7457 - val_loss: 3.9944\n",
      "Epoch 75/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2623\n",
      "Epoch 00075: val_loss did not improve from 3.71325\n",
      "\n",
      "[20210302-1355-56] Learning rate for epoch 75 is 0.0034263203851878643\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.2623 - val_loss: 4.7784\n",
      "Epoch 76/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8530\n",
      "Epoch 00076: val_loss did not improve from 3.71325\n",
      "\n",
      "[20210302-1356-01] Learning rate for epoch 76 is 0.003468865528702736\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.8530 - val_loss: 56.9365\n",
      "Epoch 77/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3764\n",
      "Epoch 00077: val_loss improved from 3.71325 to 3.70506, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1356-06] Learning rate for epoch 77 is 0.00351131078787148\n",
      "20/20 [==============================] - 2s 87ms/step - loss: 6.3764 - val_loss: 3.7051\n",
      "Epoch 78/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6489\n",
      "Epoch 00078: val_loss did not improve from 3.70506\n",
      "\n",
      "[20210302-1356-11] Learning rate for epoch 78 is 0.0035536561626940966\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.6489 - val_loss: 3.7331\n",
      "Epoch 79/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5687\n",
      "Epoch 00079: val_loss improved from 3.70506 to 3.69720, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1356-16] Learning rate for epoch 79 is 0.003595901420339942\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 6.5687 - val_loss: 3.6972\n",
      "Epoch 80/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4902\n",
      "Epoch 00080: val_loss did not improve from 3.69720\n",
      "\n",
      "[20210302-1356-21] Learning rate for epoch 80 is 0.00363804679363966\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.4902 - val_loss: 4.5216\n",
      "Epoch 81/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4866\n",
      "Epoch 00081: val_loss did not improve from 3.69720\n",
      "\n",
      "[20210302-1356-26] Learning rate for epoch 81 is 0.0036800920497626066\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.4866 - val_loss: 4.5996\n",
      "Epoch 82/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5471\n",
      "Epoch 00082: val_loss did not improve from 3.69720\n",
      "\n",
      "[20210302-1356-31] Learning rate for epoch 82 is 0.003722037188708782\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.5471 - val_loss: 5.7374\n",
      "Epoch 83/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1708\n",
      "Epoch 00083: val_loss did not improve from 3.69720\n",
      "\n",
      "[20210302-1356-36] Learning rate for epoch 83 is 0.003763882676139474\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.1708 - val_loss: 9.0153\n",
      "Epoch 84/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9376\n",
      "Epoch 00084: val_loss did not improve from 3.69720\n",
      "\n",
      "[20210302-1356-40] Learning rate for epoch 84 is 0.0038056280463933945\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.9376 - val_loss: 4.2464\n",
      "Epoch 85/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6549\n",
      "Epoch 00085: val_loss did not improve from 3.69720\n",
      "\n",
      "[20210302-1356-45] Learning rate for epoch 85 is 0.003847273299470544\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.6549 - val_loss: 237.9723\n",
      "Epoch 86/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7971\n",
      "Epoch 00086: val_loss did not improve from 3.69720\n",
      "\n",
      "[20210302-1356-50] Learning rate for epoch 86 is 0.0038888186682015657\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.7971 - val_loss: 36.7357\n",
      "Epoch 87/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6666\n",
      "Epoch 00087: val_loss did not improve from 3.69720\n",
      "\n",
      "[20210302-1356-55] Learning rate for epoch 87 is 0.00393026415258646\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.6666 - val_loss: 142.7038\n",
      "Epoch 88/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3264\n",
      "Epoch 00088: val_loss did not improve from 3.69720\n",
      "\n",
      "[20210302-1357-00] Learning rate for epoch 88 is 0.00397160928696394\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.3264 - val_loss: 9.7840\n",
      "Epoch 89/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5426\n",
      "Epoch 00089: val_loss did not improve from 3.69720\n",
      "\n",
      "[20210302-1357-05] Learning rate for epoch 89 is 0.004012854769825935\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.5426 - val_loss: 4.6215\n",
      "Epoch 90/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2925\n",
      "Epoch 00090: val_loss did not improve from 3.69720\n",
      "\n",
      "[20210302-1357-10] Learning rate for epoch 90 is 0.00405400013551116\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.2925 - val_loss: 13.2841\n",
      "Epoch 91/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3138\n",
      "Epoch 00091: val_loss did not improve from 3.69720\n",
      "\n",
      "[20210302-1357-14] Learning rate for epoch 91 is 0.004095045384019613\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.3138 - val_loss: 3.9130\n",
      "Epoch 92/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1863\n",
      "Epoch 00092: val_loss did not improve from 3.69720\n",
      "\n",
      "[20210302-1357-19] Learning rate for epoch 92 is 0.004135990981012583\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.1863 - val_loss: 4.3073\n",
      "Epoch 93/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6707\n",
      "Epoch 00093: val_loss did not improve from 3.69720\n",
      "\n",
      "[20210302-1357-24] Learning rate for epoch 93 is 0.004176836460828781\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6707 - val_loss: 4.3704\n",
      "Epoch 94/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7020\n",
      "Epoch 00094: val_loss did not improve from 3.69720\n",
      "\n",
      "[20210302-1357-29] Learning rate for epoch 94 is 0.004217581823468208\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.7020 - val_loss: 3.7720\n",
      "Epoch 95/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2767\n",
      "Epoch 00095: val_loss did not improve from 3.69720\n",
      "\n",
      "[20210302-1357-34] Learning rate for epoch 95 is 0.004258227068930864\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.2767 - val_loss: 4.6280\n",
      "Epoch 96/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8333\n",
      "Epoch 00096: val_loss did not improve from 3.69720\n",
      "\n",
      "[20210302-1357-39] Learning rate for epoch 96 is 0.0042987726628780365\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.8333 - val_loss: 4.1671\n",
      "Epoch 97/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3324\n",
      "Epoch 00097: val_loss did not improve from 3.69720\n",
      "\n",
      "[20210302-1357-43] Learning rate for epoch 97 is 0.0043392181396484375\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.3324 - val_loss: 3.7429\n",
      "Epoch 98/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1794\n",
      "Epoch 00098: val_loss did not improve from 3.69720\n",
      "\n",
      "[20210302-1357-48] Learning rate for epoch 98 is 0.004379563499242067\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.1794 - val_loss: 5.1195\n",
      "Epoch 99/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1612\n",
      "Epoch 00099: val_loss did not improve from 3.69720\n",
      "\n",
      "[20210302-1357-53] Learning rate for epoch 99 is 0.004419809207320213\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.1612 - val_loss: 5.0478\n",
      "Epoch 100/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8351\n",
      "Epoch 00100: val_loss did not improve from 3.69720\n",
      "\n",
      "[20210302-1357-58] Learning rate for epoch 100 is 0.004459954332560301\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.8351 - val_loss: 4.5333\n",
      "Epoch 101/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5104\n",
      "Epoch 00101: val_loss did not improve from 3.69720\n",
      "\n",
      "[20210302-1358-03] Learning rate for epoch 101 is 4.4999998749517545e-07\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.5104 - val_loss: 4.1252\n",
      "Epoch 102/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3964\n",
      "Epoch 00102: val_loss improved from 3.69720 to 3.53794, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1358-08] Learning rate for epoch 102 is 0.00018023152370005846\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 6.3964 - val_loss: 3.5379\n",
      "Epoch 103/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7569\n",
      "Epoch 00103: val_loss did not improve from 3.53794\n",
      "\n",
      "[20210302-1358-13] Learning rate for epoch 103 is 0.000359613070031628\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.7569 - val_loss: 3.6450\n",
      "Epoch 104/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8511\n",
      "Epoch 00104: val_loss improved from 3.53794 to 3.36267, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1358-19] Learning rate for epoch 104 is 0.0005385946715250611\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 5.8511 - val_loss: 3.3627\n",
      "Epoch 105/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7489\n",
      "Epoch 00105: val_loss improved from 3.36267 to 3.34923, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1358-24] Learning rate for epoch 105 is 0.0007171762990765274\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 5.7489 - val_loss: 3.3492\n",
      "Epoch 106/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9048\n",
      "Epoch 00106: val_loss did not improve from 3.34923\n",
      "\n",
      "[20210302-1358-29] Learning rate for epoch 106 is 0.0008953579817898571\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.9048 - val_loss: 3.6876\n",
      "Epoch 107/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7314\n",
      "Epoch 00107: val_loss did not improve from 3.34923\n",
      "\n",
      "[20210302-1358-34] Learning rate for epoch 107 is 0.0010731397196650505\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.7314 - val_loss: 4.7233\n",
      "Epoch 108/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3395\n",
      "Epoch 00108: val_loss did not improve from 3.34923\n",
      "\n",
      "[20210302-1358-39] Learning rate for epoch 108 is 0.0012505215127021074\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.3395 - val_loss: 3.7213\n",
      "Epoch 109/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4457\n",
      "Epoch 00109: val_loss did not improve from 3.34923\n",
      "\n",
      "[20210302-1358-43] Learning rate for epoch 109 is 0.001427503302693367\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.4457 - val_loss: 3.5041\n",
      "Epoch 110/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9585\n",
      "Epoch 00110: val_loss did not improve from 3.34923\n",
      "\n",
      "[20210302-1358-48] Learning rate for epoch 110 is 0.0016040850896388292\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.9585 - val_loss: 5.9174\n",
      "Epoch 111/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4226\n",
      "Epoch 00111: val_loss did not improve from 3.34923\n",
      "\n",
      "[20210302-1358-53] Learning rate for epoch 111 is 0.001780266989953816\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.4226 - val_loss: 5.6572\n",
      "Epoch 112/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0086\n",
      "Epoch 00112: val_loss did not improve from 3.34923\n",
      "\n",
      "[20210302-1358-58] Learning rate for epoch 112 is 0.0019560488872230053\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.0086 - val_loss: 3.6909\n",
      "Epoch 113/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6794\n",
      "Epoch 00113: val_loss did not improve from 3.34923\n",
      "\n",
      "[20210302-1359-03] Learning rate for epoch 113 is 0.002131430897861719\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.6794 - val_loss: 3.4900\n",
      "Epoch 114/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5707\n",
      "Epoch 00114: val_loss did not improve from 3.34923\n",
      "\n",
      "[20210302-1359-08] Learning rate for epoch 114 is 0.0023064129054546356\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.5707 - val_loss: 3.5731\n",
      "Epoch 115/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9643\n",
      "Epoch 00115: val_loss did not improve from 3.34923\n",
      "\n",
      "[20210302-1359-13] Learning rate for epoch 115 is 0.0024809949100017548\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.9643 - val_loss: 3.4426\n",
      "Epoch 116/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2552\n",
      "Epoch 00116: val_loss did not improve from 3.34923\n",
      "\n",
      "[20210302-1359-17] Learning rate for epoch 116 is 0.0026551769115030766\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.2552 - val_loss: 4.9083\n",
      "Epoch 117/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2112\n",
      "Epoch 00117: val_loss did not improve from 3.34923\n",
      "\n",
      "[20210302-1359-22] Learning rate for epoch 117 is 0.0028289591427892447\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.2112 - val_loss: 4.0083\n",
      "Epoch 118/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1846\n",
      "Epoch 00118: val_loss did not improve from 3.34923\n",
      "\n",
      "[20210302-1359-27] Learning rate for epoch 118 is 0.0030023413710296154\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.1846 - val_loss: 4.3781\n",
      "Epoch 119/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0320\n",
      "Epoch 00119: val_loss did not improve from 3.34923\n",
      "\n",
      "[20210302-1359-32] Learning rate for epoch 119 is 0.003175323596224189\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.0320 - val_loss: 3.9185\n",
      "Epoch 120/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7745\n",
      "Epoch 00120: val_loss did not improve from 3.34923\n",
      "\n",
      "[20210302-1359-37] Learning rate for epoch 120 is 0.003347905818372965\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.7745 - val_loss: 4.0694\n",
      "Epoch 121/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7004\n",
      "Epoch 00121: val_loss did not improve from 3.34923\n",
      "\n",
      "[20210302-1359-42] Learning rate for epoch 121 is 0.0035200880374759436\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.7004 - val_loss: 3.6976\n",
      "Epoch 122/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3271\n",
      "Epoch 00122: val_loss did not improve from 3.34923\n",
      "\n",
      "[20210302-1359-46] Learning rate for epoch 122 is 0.003691870253533125\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.3271 - val_loss: 4.8707\n",
      "Epoch 123/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2617\n",
      "Epoch 00123: val_loss did not improve from 3.34923\n",
      "\n",
      "[20210302-1359-51] Learning rate for epoch 123 is 0.0038632526993751526\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.2617 - val_loss: 4.4786\n",
      "Epoch 124/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0633\n",
      "Epoch 00124: val_loss did not improve from 3.34923\n",
      "\n",
      "[20210302-1359-56] Learning rate for epoch 124 is 0.004034235142171383\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.0633 - val_loss: 4.2841\n",
      "Epoch 125/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9238\n",
      "Epoch 00125: val_loss did not improve from 3.34923\n",
      "\n",
      "[20210302-1400-01] Learning rate for epoch 125 is 0.004204817581921816\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.9238 - val_loss: 4.9102\n",
      "Epoch 126/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1155\n",
      "Epoch 00126: val_loss did not improve from 3.34923\n",
      "\n",
      "[20210302-1400-06] Learning rate for epoch 126 is 0.0043750000186264515\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.1155 - val_loss: 3.6049\n",
      "Epoch 127/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1364\n",
      "Epoch 00127: val_loss did not improve from 3.34923\n",
      "\n",
      "[20210302-1400-11] Learning rate for epoch 127 is 0.0041952175088226795\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.1364 - val_loss: 4.2698\n",
      "Epoch 128/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9467\n",
      "Epoch 00128: val_loss did not improve from 3.34923\n",
      "\n",
      "[20210302-1400-15] Learning rate for epoch 128 is 0.004015835002064705\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.9467 - val_loss: 4.2089\n",
      "Epoch 129/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3665\n",
      "Epoch 00129: val_loss did not improve from 3.34923\n",
      "\n",
      "[20210302-1400-20] Learning rate for epoch 129 is 0.003836852265521884\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.3665 - val_loss: 4.3881\n",
      "Epoch 130/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9615\n",
      "Epoch 00130: val_loss did not improve from 3.34923\n",
      "\n",
      "[20210302-1400-25] Learning rate for epoch 130 is 0.003658269764855504\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.9615 - val_loss: 5.1462\n",
      "Epoch 131/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0592\n",
      "Epoch 00131: val_loss did not improve from 3.34923\n",
      "\n",
      "[20210302-1400-30] Learning rate for epoch 131 is 0.003480087034404278\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.0592 - val_loss: 3.8586\n",
      "Epoch 132/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9215\n",
      "Epoch 00132: val_loss did not improve from 3.34923\n",
      "\n",
      "[20210302-1400-35] Learning rate for epoch 132 is 0.003302304306998849\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.9215 - val_loss: 3.9102\n",
      "Epoch 133/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6271\n",
      "Epoch 00133: val_loss did not improve from 3.34923\n",
      "\n",
      "[20210302-1400-40] Learning rate for epoch 133 is 0.0031249215826392174\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.6271 - val_loss: 4.1688\n",
      "Epoch 134/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5946\n",
      "Epoch 00134: val_loss did not improve from 3.34923\n",
      "\n",
      "[20210302-1400-45] Learning rate for epoch 134 is 0.0029479386284947395\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 5.5946 - val_loss: 10.3206\n",
      "Epoch 135/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1687\n",
      "Epoch 00135: val_loss did not improve from 3.34923\n",
      "\n",
      "[20210302-1400-49] Learning rate for epoch 135 is 0.0027713559102267027\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.1687 - val_loss: 7.4474\n",
      "Epoch 136/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5618\n",
      "Epoch 00136: val_loss did not improve from 3.34923\n",
      "\n",
      "[20210302-1400-54] Learning rate for epoch 136 is 0.0025951729621738195\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5618 - val_loss: 5.9074\n",
      "Epoch 137/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1176\n",
      "Epoch 00137: val_loss did not improve from 3.34923\n",
      "\n",
      "[20210302-1400-59] Learning rate for epoch 137 is 0.0024193900171667337\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.1176 - val_loss: 5.1370\n",
      "Epoch 138/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1025\n",
      "Epoch 00138: val_loss did not improve from 3.34923\n",
      "\n",
      "[20210302-1401-04] Learning rate for epoch 138 is 0.0022440070752054453\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.1025 - val_loss: 6.1720\n",
      "Epoch 139/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0961\n",
      "Epoch 00139: val_loss did not improve from 3.34923\n",
      "\n",
      "[20210302-1401-09] Learning rate for epoch 139 is 0.002069024136289954\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.0961 - val_loss: 3.5706\n",
      "Epoch 140/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1382\n",
      "Epoch 00140: val_loss did not improve from 3.34923\n",
      "\n",
      "[20210302-1401-14] Learning rate for epoch 140 is 0.0018944410840049386\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.1382 - val_loss: 3.8596\n",
      "Epoch 141/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1110\n",
      "Epoch 00141: val_loss did not improve from 3.34923\n",
      "\n",
      "[20210302-1401-18] Learning rate for epoch 141 is 0.0017202580347657204\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.1110 - val_loss: 4.1704\n",
      "Epoch 142/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4608\n",
      "Epoch 00142: val_loss did not improve from 3.34923\n",
      "\n",
      "[20210302-1401-23] Learning rate for epoch 142 is 0.0015464748721569777\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.4608 - val_loss: 3.8831\n",
      "Epoch 143/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9459\n",
      "Epoch 00143: val_loss did not improve from 3.34923\n",
      "\n",
      "[20210302-1401-28] Learning rate for epoch 143 is 0.0013730917125940323\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.9459 - val_loss: 3.7165\n",
      "Epoch 144/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5319\n",
      "Epoch 00144: val_loss did not improve from 3.34923\n",
      "\n",
      "[20210302-1401-33] Learning rate for epoch 144 is 0.0012001085560768843\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.5319 - val_loss: 3.6366\n",
      "Epoch 145/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8248\n",
      "Epoch 00145: val_loss did not improve from 3.34923\n",
      "\n",
      "[20210302-1401-38] Learning rate for epoch 145 is 0.0010275252861902118\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.8248 - val_loss: 3.5122\n",
      "Epoch 146/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7271\n",
      "Epoch 00146: val_loss did not improve from 3.34923\n",
      "\n",
      "[20210302-1401-43] Learning rate for epoch 146 is 0.0008553420193493366\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.7271 - val_loss: 3.4799\n",
      "Epoch 147/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8283\n",
      "Epoch 00147: val_loss did not improve from 3.34923\n",
      "\n",
      "[20210302-1401-48] Learning rate for epoch 147 is 0.0006835586973465979\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.8283 - val_loss: 3.4898\n",
      "Epoch 148/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4988\n",
      "Epoch 00148: val_loss did not improve from 3.34923\n",
      "\n",
      "[20210302-1401-53] Learning rate for epoch 148 is 0.0005121753201819956\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.4988 - val_loss: 3.5047\n",
      "Epoch 149/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5473\n",
      "Epoch 00149: val_loss did not improve from 3.34923\n",
      "\n",
      "[20210302-1401-57] Learning rate for epoch 149 is 0.00034119191695936024\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.5473 - val_loss: 3.4131\n",
      "Epoch 150/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6136\n",
      "Epoch 00150: val_loss did not improve from 3.34923\n",
      "\n",
      "[20210302-1402-02] Learning rate for epoch 150 is 0.00017060847312677652\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.6136 - val_loss: 3.4099\n",
      "Epoch 151/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7164\n",
      "Epoch 00151: val_loss did not improve from 3.34923\n",
      "\n",
      "[20210302-1402-07] Learning rate for epoch 151 is 4.249999960848072e-07\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.7164 - val_loss: 3.4024\n",
      "Epoch 152/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6725\n",
      "Epoch 00152: val_loss did not improve from 3.34923\n",
      "\n",
      "[20210302-1402-12] Learning rate for epoch 152 is 0.00017020752420648932\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.6725 - val_loss: 3.4045\n",
      "Epoch 153/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5170\n",
      "Epoch 00153: val_loss did not improve from 3.34923\n",
      "\n",
      "[20210302-1402-17] Learning rate for epoch 153 is 0.0003395900712348521\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.5170 - val_loss: 3.3754\n",
      "Epoch 154/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4436\n",
      "Epoch 00154: val_loss did not improve from 3.34923\n",
      "\n",
      "[20210302-1402-22] Learning rate for epoch 154 is 0.0005085726734250784\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.4436 - val_loss: 3.7086\n",
      "Epoch 155/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6783\n",
      "Epoch 00155: val_loss did not improve from 3.34923\n",
      "\n",
      "[20210302-1402-27] Learning rate for epoch 155 is 0.0006771553307771683\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6783 - val_loss: 3.4842\n",
      "Epoch 156/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6918\n",
      "Epoch 00156: val_loss did not improve from 3.34923\n",
      "\n",
      "[20210302-1402-31] Learning rate for epoch 156 is 0.0008453379850834608\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.6918 - val_loss: 3.5321\n",
      "Epoch 157/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8595\n",
      "Epoch 00157: val_loss improved from 3.34923 to 3.33241, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1402-37] Learning rate for epoch 157 is 0.0010131207527592778\n",
      "20/20 [==============================] - 2s 93ms/step - loss: 5.8595 - val_loss: 3.3324\n",
      "Epoch 158/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0554\n",
      "Epoch 00158: val_loss improved from 3.33241 to 3.28413, saving model to ./20210301-225844/heel_K13_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1402-42] Learning rate for epoch 158 is 0.0011805035173892975\n",
      "20/20 [==============================] - 2s 89ms/step - loss: 6.0554 - val_loss: 3.2841\n",
      "Epoch 159/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2058\n",
      "Epoch 00159: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1402-47] Learning rate for epoch 159 is 0.0013474862789735198\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.2058 - val_loss: 3.5172\n",
      "Epoch 160/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4965\n",
      "Epoch 00160: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1402-52] Learning rate for epoch 160 is 0.0015140691539272666\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.4965 - val_loss: 3.8209\n",
      "Epoch 161/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8602\n",
      "Epoch 00161: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1402-57] Learning rate for epoch 161 is 0.001680252025835216\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8602 - val_loss: 3.5165\n",
      "Epoch 162/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7262\n",
      "Epoch 00162: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1403-02] Learning rate for epoch 162 is 0.0018460348946973681\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.7262 - val_loss: 3.6571\n",
      "Epoch 163/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5971\n",
      "Epoch 00163: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1403-06] Learning rate for epoch 163 is 0.0020114178769290447\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5971 - val_loss: 4.6253\n",
      "Epoch 164/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8815\n",
      "Epoch 00164: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1403-11] Learning rate for epoch 164 is 0.0021764009725302458\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.8815 - val_loss: 3.7172\n",
      "Epoch 165/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6090\n",
      "Epoch 00165: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1403-16] Learning rate for epoch 165 is 0.002340983832255006\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.6090 - val_loss: 3.7989\n",
      "Epoch 166/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6873\n",
      "Epoch 00166: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1403-21] Learning rate for epoch 166 is 0.002505166921764612\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.6873 - val_loss: 3.7667\n",
      "Epoch 167/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2416\n",
      "Epoch 00167: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1403-26] Learning rate for epoch 167 is 0.002668950008228421\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.2416 - val_loss: 3.9619\n",
      "Epoch 168/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5930\n",
      "Epoch 00168: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1403-31] Learning rate for epoch 168 is 0.0028323333244770765\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5930 - val_loss: 4.0637\n",
      "Epoch 169/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0826\n",
      "Epoch 00169: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1403-35] Learning rate for epoch 169 is 0.002995316404849291\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.0826 - val_loss: 4.7862\n",
      "Epoch 170/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8218\n",
      "Epoch 00170: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1403-40] Learning rate for epoch 170 is 0.0031578997150063515\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.8218 - val_loss: 6.5628\n",
      "Epoch 171/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9570\n",
      "Epoch 00171: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1403-45] Learning rate for epoch 171 is 0.0033200830221176147\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.9570 - val_loss: 4.1868\n",
      "Epoch 172/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8245\n",
      "Epoch 00172: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1403-50] Learning rate for epoch 172 is 0.0034818663261830807\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.8245 - val_loss: 5.4207\n",
      "Epoch 173/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0724\n",
      "Epoch 00173: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1403-55] Learning rate for epoch 173 is 0.0036432496272027493\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.0724 - val_loss: 4.8392\n",
      "Epoch 174/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1825\n",
      "Epoch 00174: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1404-00] Learning rate for epoch 174 is 0.003804233158007264\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.1825 - val_loss: 4.2586\n",
      "Epoch 175/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0850\n",
      "Epoch 00175: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1404-04] Learning rate for epoch 175 is 0.003964816685765982\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.0850 - val_loss: 5.2553\n",
      "Epoch 176/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8888\n",
      "Epoch 00176: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1404-09] Learning rate for epoch 176 is 0.004124999977648258\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.8888 - val_loss: 5.1512\n",
      "Epoch 177/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7449\n",
      "Epoch 00177: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1404-14] Learning rate for epoch 177 is 0.003955216612666845\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.7449 - val_loss: 4.0144\n",
      "Epoch 178/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6935\n",
      "Epoch 00178: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1404-19] Learning rate for epoch 178 is 0.003785833017900586\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 5.6935 - val_loss: 5.7859\n",
      "Epoch 179/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7959\n",
      "Epoch 00179: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1404-24] Learning rate for epoch 179 is 0.0036168494261801243\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.7959 - val_loss: 5.8015\n",
      "Epoch 180/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4256\n",
      "Epoch 00180: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1404-29] Learning rate for epoch 180 is 0.003448265604674816\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.4256 - val_loss: 4.4020\n",
      "Epoch 181/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3116\n",
      "Epoch 00181: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1404-33] Learning rate for epoch 181 is 0.003280082019045949\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.3116 - val_loss: 3.7283\n",
      "Epoch 182/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6038\n",
      "Epoch 00182: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1404-38] Learning rate for epoch 182 is 0.0031122982036322355\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.6038 - val_loss: 4.4639\n",
      "Epoch 183/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0124\n",
      "Epoch 00183: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1404-43] Learning rate for epoch 183 is 0.002944914624094963\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.0124 - val_loss: 4.4653\n",
      "Epoch 184/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1065\n",
      "Epoch 00184: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1404-48] Learning rate for epoch 184 is 0.0027779308147728443\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.1065 - val_loss: 4.2236\n",
      "Epoch 185/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9245\n",
      "Epoch 00185: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1404-53] Learning rate for epoch 185 is 0.0026113467756658792\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.9245 - val_loss: 4.5704\n",
      "Epoch 186/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9920\n",
      "Epoch 00186: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1404-58] Learning rate for epoch 186 is 0.002445162972435355\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.9920 - val_loss: 3.9536\n",
      "Epoch 187/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9698\n",
      "Epoch 00187: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1405-03] Learning rate for epoch 187 is 0.0022793791722506285\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.9698 - val_loss: 4.1137\n",
      "Epoch 188/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3750\n",
      "Epoch 00188: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1405-07] Learning rate for epoch 188 is 0.0021139951422810555\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.3750 - val_loss: 5.2312\n",
      "Epoch 189/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9412\n",
      "Epoch 00189: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1405-12] Learning rate for epoch 189 is 0.0019490111153572798\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.9412 - val_loss: 3.5317\n",
      "Epoch 190/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5904\n",
      "Epoch 00190: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1405-17] Learning rate for epoch 190 is 0.0017844270914793015\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.5904 - val_loss: 3.4713\n",
      "Epoch 191/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7658\n",
      "Epoch 00191: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1405-22] Learning rate for epoch 191 is 0.0016202429542317986\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.7658 - val_loss: 3.7024\n",
      "Epoch 192/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7211\n",
      "Epoch 00192: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1405-27] Learning rate for epoch 192 is 0.001456458936445415\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.7211 - val_loss: 3.5833\n",
      "Epoch 193/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4070\n",
      "Epoch 00193: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1405-31] Learning rate for epoch 193 is 0.001293074688874185\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.4070 - val_loss: 3.7703\n",
      "Epoch 194/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7768\n",
      "Epoch 00194: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1405-36] Learning rate for epoch 194 is 0.0011300905607640743\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.7768 - val_loss: 3.9038\n",
      "Epoch 195/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7239\n",
      "Epoch 00195: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1405-41] Learning rate for epoch 195 is 0.0009675062610767782\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.7239 - val_loss: 3.9914\n",
      "Epoch 196/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5531\n",
      "Epoch 00196: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1405-46] Learning rate for epoch 196 is 0.0008053220226429403\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.5531 - val_loss: 3.5233\n",
      "Epoch 197/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2813\n",
      "Epoch 00197: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1405-51] Learning rate for epoch 197 is 0.0006435376708395779\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.2813 - val_loss: 3.6139\n",
      "Epoch 198/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3433\n",
      "Epoch 00198: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1405-56] Learning rate for epoch 198 is 0.0004821533220820129\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.3433 - val_loss: 3.6704\n",
      "Epoch 199/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1888\n",
      "Epoch 00199: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1406-01] Learning rate for epoch 199 is 0.0003211689181625843\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.1888 - val_loss: 3.4763\n",
      "Epoch 200/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6287\n",
      "Epoch 00200: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1406-05] Learning rate for epoch 200 is 0.00016058447363320738\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6287 - val_loss: 3.4067\n",
      "Epoch 201/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6065\n",
      "Epoch 00201: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1406-10] Learning rate for epoch 201 is 4.0000000467443897e-07\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.6065 - val_loss: 3.4045\n",
      "Epoch 202/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3356\n",
      "Epoch 00202: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1406-15] Learning rate for epoch 202 is 0.0001601835247129202\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.3356 - val_loss: 3.4152\n",
      "Epoch 203/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5348\n",
      "Epoch 00203: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1406-20] Learning rate for epoch 203 is 0.00031956707243807614\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.5348 - val_loss: 3.4053\n",
      "Epoch 204/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3134\n",
      "Epoch 00204: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1406-25] Learning rate for epoch 204 is 0.00047855067532509565\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.3134 - val_loss: 3.3411\n",
      "Epoch 205/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3573\n",
      "Epoch 00205: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1406-30] Learning rate for epoch 205 is 0.0006371343042701483\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.3573 - val_loss: 3.3496\n",
      "Epoch 206/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2054\n",
      "Epoch 00206: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1406-34] Learning rate for epoch 206 is 0.0007953179883770645\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.2054 - val_loss: 3.5193\n",
      "Epoch 207/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6027\n",
      "Epoch 00207: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1406-39] Learning rate for epoch 207 is 0.0009531017276458442\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.6027 - val_loss: 3.5809\n",
      "Epoch 208/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1622\n",
      "Epoch 00208: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1406-44] Learning rate for epoch 208 is 0.0011104855220764875\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.1622 - val_loss: 3.5395\n",
      "Epoch 209/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3145\n",
      "Epoch 00209: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1406-49] Learning rate for epoch 209 is 0.0012674692552536726\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.3145 - val_loss: 4.4677\n",
      "Epoch 210/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.0726\n",
      "Epoch 00210: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1406-54] Learning rate for epoch 210 is 0.0014240531018003821\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.0726 - val_loss: 3.7689\n",
      "Epoch 211/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6277\n",
      "Epoch 00211: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1406-59] Learning rate for epoch 211 is 0.0015802369453012943\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.6277 - val_loss: 3.3731\n",
      "Epoch 212/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2985\n",
      "Epoch 00212: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1407-04] Learning rate for epoch 212 is 0.001736020902171731\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.2985 - val_loss: 8.7177\n",
      "Epoch 213/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8708\n",
      "Epoch 00213: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1407-08] Learning rate for epoch 213 is 0.0018914048559963703\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.8708 - val_loss: 4.1285\n",
      "Epoch 214/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6792\n",
      "Epoch 00214: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1407-13] Learning rate for epoch 214 is 0.0020463888067752123\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.6792 - val_loss: 12.0380\n",
      "Epoch 215/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8260\n",
      "Epoch 00215: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1407-18] Learning rate for epoch 215 is 0.0022009729873389006\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.8260 - val_loss: 13.9303\n",
      "Epoch 216/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6052\n",
      "Epoch 00216: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1407-23] Learning rate for epoch 216 is 0.002355156932026148\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.6052 - val_loss: 4.7257\n",
      "Epoch 217/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0836\n",
      "Epoch 00217: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1407-28] Learning rate for epoch 217 is 0.0025089411064982414\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.0836 - val_loss: 4.0254\n",
      "Epoch 218/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5329\n",
      "Epoch 00218: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1407-33] Learning rate for epoch 218 is 0.0026623252779245377\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.5329 - val_loss: 4.5030\n",
      "Epoch 219/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6441\n",
      "Epoch 00219: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1407-38] Learning rate for epoch 219 is 0.0028153094463050365\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6441 - val_loss: 4.0887\n",
      "Epoch 220/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0414\n",
      "Epoch 00220: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1407-43] Learning rate for epoch 220 is 0.002967893611639738\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.0414 - val_loss: 3.9999\n",
      "Epoch 221/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6647\n",
      "Epoch 00221: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1407-48] Learning rate for epoch 221 is 0.003120078006759286\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.6647 - val_loss: 5.4206\n",
      "Epoch 222/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6707\n",
      "Epoch 00222: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1407-53] Learning rate for epoch 222 is 0.0032718623988330364\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.6707 - val_loss: 4.0465\n",
      "Epoch 223/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9061\n",
      "Epoch 00223: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1407-58] Learning rate for epoch 223 is 0.0034232467878609896\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 5.9061 - val_loss: 5.0158\n",
      "Epoch 224/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6916\n",
      "Epoch 00224: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1408-02] Learning rate for epoch 224 is 0.0035742311738431454\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.6916 - val_loss: 392.4162\n",
      "Epoch 225/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3669\n",
      "Epoch 00225: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1408-07] Learning rate for epoch 225 is 0.003724815556779504\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.3669 - val_loss: 168.8205\n",
      "Epoch 226/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7961\n",
      "Epoch 00226: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1408-12] Learning rate for epoch 226 is 0.003874999936670065\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.7961 - val_loss: 38.0813\n",
      "Epoch 227/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5184\n",
      "Epoch 00227: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1408-17] Learning rate for epoch 227 is 0.0037152154836803675\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5184 - val_loss: 17.7166\n",
      "Epoch 228/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8136\n",
      "Epoch 00228: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1408-22] Learning rate for epoch 228 is 0.0035558310337364674\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 5.8136 - val_loss: 6.8571\n",
      "Epoch 229/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5335\n",
      "Epoch 00229: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1408-27] Learning rate for epoch 229 is 0.003396846354007721\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.5335 - val_loss: 3.7819\n",
      "Epoch 230/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5764\n",
      "Epoch 00230: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1408-31] Learning rate for epoch 230 is 0.003238261677324772\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 5.5764 - val_loss: 4.2967\n",
      "Epoch 231/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3276\n",
      "Epoch 00231: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1408-36] Learning rate for epoch 231 is 0.00308007700368762\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.3276 - val_loss: 4.6732\n",
      "Epoch 232/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9755\n",
      "Epoch 00232: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1408-41] Learning rate for epoch 232 is 0.002922292333096266\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.9755 - val_loss: 7.7280\n",
      "Epoch 233/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8455\n",
      "Epoch 00233: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1408-46] Learning rate for epoch 233 is 0.002764907432720065\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 5.8455 - val_loss: 3.8188\n",
      "Epoch 234/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4739\n",
      "Epoch 00234: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1408-51] Learning rate for epoch 234 is 0.0026079227682203054\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.4739 - val_loss: 4.8973\n",
      "Epoch 235/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6222\n",
      "Epoch 00235: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1408-56] Learning rate for epoch 235 is 0.0024513378739356995\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.6222 - val_loss: 5.7431\n",
      "Epoch 236/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4383\n",
      "Epoch 00236: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1409-01] Learning rate for epoch 236 is 0.002295152982696891\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.4383 - val_loss: 6.3922\n",
      "Epoch 237/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8118\n",
      "Epoch 00237: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1409-05] Learning rate for epoch 237 is 0.0021393680945038795\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.8118 - val_loss: 5.0151\n",
      "Epoch 238/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0660\n",
      "Epoch 00238: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1409-10] Learning rate for epoch 238 is 0.0019839832093566656\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.0660 - val_loss: 4.4566\n",
      "Epoch 239/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4841\n",
      "Epoch 00239: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1409-15] Learning rate for epoch 239 is 0.0018289980944246054\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 5.4841 - val_loss: 4.3215\n",
      "Epoch 240/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6687\n",
      "Epoch 00240: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1409-20] Learning rate for epoch 240 is 0.0016744130989536643\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.6687 - val_loss: 4.5937\n",
      "Epoch 241/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6397\n",
      "Epoch 00241: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1409-25] Learning rate for epoch 241 is 0.0015202279901131988\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.6397 - val_loss: 3.5404\n",
      "Epoch 242/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.0581\n",
      "Epoch 00242: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1409-30] Learning rate for epoch 242 is 0.0013664428843185306\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.0581 - val_loss: 3.4111\n",
      "Epoch 243/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6396\n",
      "Epoch 00243: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1409-34] Learning rate for epoch 243 is 0.0012130576651543379\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.6396 - val_loss: 3.7619\n",
      "Epoch 244/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5880\n",
      "Epoch 00244: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1409-39] Learning rate for epoch 244 is 0.0010600725654512644\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.5880 - val_loss: 3.7645\n",
      "Epoch 245/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4181\n",
      "Epoch 00245: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1409-44] Learning rate for epoch 245 is 0.0009074872941710055\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.4181 - val_loss: 3.5659\n",
      "Epoch 246/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6771\n",
      "Epoch 00246: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1409-49] Learning rate for epoch 246 is 0.0007553020259365439\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.6771 - val_loss: 3.6861\n",
      "Epoch 247/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 4.9590\n",
      "Epoch 00247: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1409-54] Learning rate for epoch 247 is 0.0006035167025402188\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 4.9590 - val_loss: 3.3850\n",
      "Epoch 248/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 4.8283\n",
      "Epoch 00248: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1409-59] Learning rate for epoch 248 is 0.00045213132398203015\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 4.8283 - val_loss: 3.3554\n",
      "Epoch 249/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8810\n",
      "Epoch 00249: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1410-03] Learning rate for epoch 249 is 0.00030114591936580837\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.8810 - val_loss: 3.3737\n",
      "Epoch 250/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8127\n",
      "Epoch 00250: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1410-08] Learning rate for epoch 250 is 0.00015056047413963825\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.8127 - val_loss: 3.4275\n",
      "Epoch 251/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2688\n",
      "Epoch 00251: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1410-13] Learning rate for epoch 251 is 3.7500001326407073e-07\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.2688 - val_loss: 3.4123\n",
      "Epoch 252/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.0748\n",
      "Epoch 00252: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1410-18] Learning rate for epoch 252 is 0.00015015952521935105\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.0748 - val_loss: 3.4122\n",
      "Epoch 253/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5418\n",
      "Epoch 00253: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1410-23] Learning rate for epoch 253 is 0.0002995440736413002\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.5418 - val_loss: 3.6580\n",
      "Epoch 254/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1464\n",
      "Epoch 00254: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1410-28] Learning rate for epoch 254 is 0.0004485286772251129\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.1464 - val_loss: 3.5226\n",
      "Epoch 255/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4075\n",
      "Epoch 00255: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1410-33] Learning rate for epoch 255 is 0.0005971133359707892\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.4075 - val_loss: 3.4897\n",
      "Epoch 256/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5008\n",
      "Epoch 00256: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1410-38] Learning rate for epoch 256 is 0.0007452979916706681\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.5008 - val_loss: 3.3702\n",
      "Epoch 257/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6456\n",
      "Epoch 00257: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1410-43] Learning rate for epoch 257 is 0.0008930827025324106\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.6456 - val_loss: 3.3711\n",
      "Epoch 258/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5203\n",
      "Epoch 00258: val_loss did not improve from 3.28413\n",
      "\n",
      "[20210302-1410-47] Learning rate for epoch 258 is 0.0010404675267636776\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.5203 - val_loss: 3.4897\n",
      "K= 14\n",
      "Check cache in memory:Y True\n",
      "Check keypoints process01: Tensor(\"args_1:0\", shape=(2,), dtype=int64) (2,) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Check keypoints process02: Tensor(\"Reshape:0\", shape=(1, 2), dtype=int64) (1, 2) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Check keypoints process03: Tensor(\"PyFunc:1\", dtype=float32, device=/job:localhost/replica:0/task:0)\n",
      "Check keypoints process04: Tensor(\"Reshape_1:0\", shape=(2,), dtype=float32)\n",
      "Check augment :Y True\n",
      "Check ds cache[True] and augment[True]\n",
      "Check cache in memory:Y True\n",
      "Check augment :N False\n",
      "Check ds cache[True] and augment[False]\n",
      "best_model_name: ./20210301-225844/heel_K14_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "base_model.trainable :  False\n",
      "\n",
      "[20210302-1410-50] Learning rate for epoch 1 is 0.00047855067532509565\n",
      "Epoch 1/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 106.8703\n",
      "Epoch 00001: val_loss improved from inf to 75.97653, saving model to ./20210301-225844/heel_K14_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 5s 269ms/step - loss: 104.5648 - val_loss: 75.9765\n",
      "\n",
      "[20210302-1411-06] Learning rate for epoch 2 is 0.00047855067532509565\n",
      "Epoch 2/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 41.6715\n",
      "Epoch 00002: val_loss improved from 75.97653 to 29.16439, saving model to ./20210301-225844/heel_K14_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 41.0572 - val_loss: 29.1644\n",
      "\n",
      "[20210302-1411-10] Learning rate for epoch 3 is 0.00047855067532509565\n",
      "Epoch 3/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 23.6163\n",
      "Epoch 00003: val_loss improved from 29.16439 to 24.45958, saving model to ./20210301-225844/heel_K14_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 7s 350ms/step - loss: 23.6163 - val_loss: 24.4596\n",
      "\n",
      "[20210302-1411-21] Learning rate for epoch 4 is 0.00047855067532509565\n",
      "Epoch 4/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 20.5668\n",
      "Epoch 00004: val_loss improved from 24.45958 to 22.30422, saving model to ./20210301-225844/heel_K14_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 20.5668 - val_loss: 22.3042\n",
      "\n",
      "[20210302-1411-25] Learning rate for epoch 5 is 0.00047855067532509565\n",
      "Epoch 5/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 19.8129\n",
      "Epoch 00005: val_loss improved from 22.30422 to 21.88169, saving model to ./20210301-225844/heel_K14_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 19.7415 - val_loss: 21.8817\n",
      "\n",
      "[20210302-1411-29] Learning rate for epoch 6 is 0.00047855067532509565\n",
      "Epoch 6/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 19.0784\n",
      "Epoch 00006: val_loss improved from 21.88169 to 21.38596, saving model to ./20210301-225844/heel_K14_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 19.0784 - val_loss: 21.3860\n",
      "\n",
      "[20210302-1411-34] Learning rate for epoch 7 is 0.00047855067532509565\n",
      "Epoch 7/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 18.2396\n",
      "Epoch 00007: val_loss improved from 21.38596 to 21.34432, saving model to ./20210301-225844/heel_K14_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 18.3556 - val_loss: 21.3443\n",
      "\n",
      "[20210302-1411-38] Learning rate for epoch 8 is 0.00047855067532509565\n",
      "Epoch 8/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 18.6733\n",
      "Epoch 00008: val_loss improved from 21.34432 to 20.27299, saving model to ./20210301-225844/heel_K14_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 18.5172 - val_loss: 20.2730\n",
      "\n",
      "[20210302-1411-42] Learning rate for epoch 9 is 0.00047855067532509565\n",
      "Epoch 9/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 17.6712\n",
      "Epoch 00009: val_loss improved from 20.27299 to 19.74677, saving model to ./20210301-225844/heel_K14_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 17.6648 - val_loss: 19.7468\n",
      "\n",
      "[20210302-1411-47] Learning rate for epoch 10 is 0.00047855067532509565\n",
      "Epoch 10/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 17.7143\n",
      "Epoch 00010: val_loss improved from 19.74677 to 19.22023, saving model to ./20210301-225844/heel_K14_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 17.7749 - val_loss: 19.2202\n",
      "\n",
      "[20210302-1411-51] Learning rate for epoch 11 is 0.00047855067532509565\n",
      "Epoch 11/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 18.1197\n",
      "Epoch 00011: val_loss improved from 19.22023 to 18.61012, saving model to ./20210301-225844/heel_K14_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 55ms/step - loss: 18.1359 - val_loss: 18.6101\n",
      "\n",
      "[20210302-1411-56] Learning rate for epoch 12 is 0.00047855067532509565\n",
      "Epoch 12/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 17.4446\n",
      "Epoch 00012: val_loss improved from 18.61012 to 18.25636, saving model to ./20210301-225844/heel_K14_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 17.4873 - val_loss: 18.2564\n",
      "\n",
      "[20210302-1412-00] Learning rate for epoch 13 is 0.00047855067532509565\n",
      "Epoch 13/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 17.3828\n",
      "Epoch 00013: val_loss improved from 18.25636 to 17.43068, saving model to ./20210301-225844/heel_K14_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 17.3828 - val_loss: 17.4307\n",
      "\n",
      "[20210302-1412-04] Learning rate for epoch 14 is 0.00047855067532509565\n",
      "Epoch 14/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.8017\n",
      "Epoch 00014: val_loss improved from 17.43068 to 16.95222, saving model to ./20210301-225844/heel_K14_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.8017 - val_loss: 16.9522\n",
      "\n",
      "[20210302-1412-09] Learning rate for epoch 15 is 0.00047855067532509565\n",
      "Epoch 15/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 17.2091\n",
      "Epoch 00015: val_loss improved from 16.95222 to 16.91771, saving model to ./20210301-225844/heel_K14_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 17.2090 - val_loss: 16.9177\n",
      "\n",
      "[20210302-1412-13] Learning rate for epoch 16 is 0.00047855067532509565\n",
      "Epoch 16/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 17.2333\n",
      "Epoch 00016: val_loss improved from 16.91771 to 16.34275, saving model to ./20210301-225844/heel_K14_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 17.2333 - val_loss: 16.3427\n",
      "\n",
      "[20210302-1412-17] Learning rate for epoch 17 is 0.00047855067532509565\n",
      "Epoch 17/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.8815\n",
      "Epoch 00017: val_loss improved from 16.34275 to 15.80282, saving model to ./20210301-225844/heel_K14_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.8773 - val_loss: 15.8028\n",
      "\n",
      "[20210302-1412-22] Learning rate for epoch 18 is 0.00047855067532509565\n",
      "Epoch 18/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.8547\n",
      "Epoch 00018: val_loss improved from 15.80282 to 15.43633, saving model to ./20210301-225844/heel_K14_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.8797 - val_loss: 15.4363\n",
      "\n",
      "[20210302-1412-26] Learning rate for epoch 19 is 0.00047855067532509565\n",
      "Epoch 19/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 16.5314\n",
      "Epoch 00019: val_loss improved from 15.43633 to 15.00718, saving model to ./20210301-225844/heel_K14_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 16.6064 - val_loss: 15.0072\n",
      "\n",
      "[20210302-1412-30] Learning rate for epoch 20 is 0.00047855067532509565\n",
      "Epoch 20/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.6602\n",
      "Epoch 00020: val_loss improved from 15.00718 to 13.58405, saving model to ./20210301-225844/heel_K14_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.7426 - val_loss: 13.5841\n",
      "\n",
      "[20210302-1412-35] Learning rate for epoch 21 is 0.00047855067532509565\n",
      "Epoch 21/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 16.5174\n",
      "Epoch 00021: val_loss did not improve from 13.58405\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.3611 - val_loss: 14.0144\n",
      "\n",
      "[20210302-1412-39] Learning rate for epoch 22 is 0.00047855067532509565\n",
      "Epoch 22/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.6291\n",
      "Epoch 00022: val_loss improved from 13.58405 to 13.26540, saving model to ./20210301-225844/heel_K14_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.6291 - val_loss: 13.2654\n",
      "\n",
      "[20210302-1412-43] Learning rate for epoch 23 is 0.00047855067532509565\n",
      "Epoch 23/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 16.5635\n",
      "Epoch 00023: val_loss did not improve from 13.26540\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.5738 - val_loss: 13.3274\n",
      "\n",
      "[20210302-1412-47] Learning rate for epoch 24 is 0.00047855067532509565\n",
      "Epoch 24/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.2152\n",
      "Epoch 00024: val_loss improved from 13.26540 to 13.14495, saving model to ./20210301-225844/heel_K14_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 16.2808 - val_loss: 13.1449\n",
      "\n",
      "[20210302-1412-51] Learning rate for epoch 25 is 0.00047855067532509565\n",
      "Epoch 25/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.5560\n",
      "Epoch 00025: val_loss improved from 13.14495 to 12.37369, saving model to ./20210301-225844/heel_K14_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 16.5560 - val_loss: 12.3737\n",
      "\n",
      "[20210302-1412-55] Learning rate for epoch 26 is 0.00047855067532509565\n",
      "Epoch 26/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.7409\n",
      "Epoch 00026: val_loss improved from 12.37369 to 12.33793, saving model to ./20210301-225844/heel_K14_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 16.7417 - val_loss: 12.3379\n",
      "\n",
      "[20210302-1413-00] Learning rate for epoch 27 is 0.00047855067532509565\n",
      "Epoch 27/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 16.2503\n",
      "Epoch 00027: val_loss improved from 12.33793 to 11.99370, saving model to ./20210301-225844/heel_K14_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.2825 - val_loss: 11.9937\n",
      "\n",
      "[20210302-1413-04] Learning rate for epoch 28 is 0.00047855067532509565\n",
      "Epoch 28/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.9158\n",
      "Epoch 00028: val_loss improved from 11.99370 to 11.82165, saving model to ./20210301-225844/heel_K14_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 16.0173 - val_loss: 11.8217\n",
      "\n",
      "[20210302-1413-08] Learning rate for epoch 29 is 0.00047855067532509565\n",
      "Epoch 29/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 16.4998\n",
      "Epoch 00029: val_loss improved from 11.82165 to 11.39311, saving model to ./20210301-225844/heel_K14_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 16.3997 - val_loss: 11.3931\n",
      "\n",
      "[20210302-1413-13] Learning rate for epoch 30 is 0.00047855067532509565\n",
      "Epoch 30/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 16.6528\n",
      "Epoch 00030: val_loss improved from 11.39311 to 11.27345, saving model to ./20210301-225844/heel_K14_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.5457 - val_loss: 11.2735\n",
      "\n",
      "[20210302-1413-17] Learning rate for epoch 31 is 0.00047855067532509565\n",
      "Epoch 31/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.8370\n",
      "Epoch 00031: val_loss did not improve from 11.27345\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.8370 - val_loss: 12.4265\n",
      "\n",
      "[20210302-1413-21] Learning rate for epoch 32 is 0.00047855067532509565\n",
      "Epoch 32/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 16.1222\n",
      "Epoch 00032: val_loss improved from 11.27345 to 10.38152, saving model to ./20210301-225844/heel_K14_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 15.9355 - val_loss: 10.3815\n",
      "\n",
      "[20210302-1413-25] Learning rate for epoch 33 is 0.00047855067532509565\n",
      "Epoch 33/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 16.3324\n",
      "Epoch 00033: val_loss improved from 10.38152 to 10.32750, saving model to ./20210301-225844/heel_K14_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 16.3602 - val_loss: 10.3275\n",
      "\n",
      "[20210302-1413-30] Learning rate for epoch 34 is 0.00047855067532509565\n",
      "Epoch 34/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.7558\n",
      "Epoch 00034: val_loss did not improve from 10.32750\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.7841 - val_loss: 10.7352\n",
      "\n",
      "[20210302-1413-33] Learning rate for epoch 35 is 0.00047855067532509565\n",
      "Epoch 35/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.8604\n",
      "Epoch 00035: val_loss did not improve from 10.32750\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.8872 - val_loss: 11.5742\n",
      "\n",
      "[20210302-1413-37] Learning rate for epoch 36 is 0.00047855067532509565\n",
      "Epoch 36/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.8155\n",
      "Epoch 00036: val_loss did not improve from 10.32750\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.9030 - val_loss: 10.5549\n",
      "\n",
      "[20210302-1413-41] Learning rate for epoch 37 is 0.00047855067532509565\n",
      "Epoch 37/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.2795\n",
      "Epoch 00037: val_loss did not improve from 10.32750\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.2795 - val_loss: 10.3928\n",
      "\n",
      "[20210302-1413-45] Learning rate for epoch 38 is 0.00047855067532509565\n",
      "Epoch 38/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.7845\n",
      "Epoch 00038: val_loss did not improve from 10.32750\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.9427 - val_loss: 10.6226\n",
      "\n",
      "[20210302-1413-49] Learning rate for epoch 39 is 0.00047855067532509565\n",
      "Epoch 39/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.5815\n",
      "Epoch 00039: val_loss did not improve from 10.32750\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.5815 - val_loss: 10.7481\n",
      "\n",
      "[20210302-1413-53] Learning rate for epoch 40 is 0.00047855067532509565\n",
      "Epoch 40/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 16.4968\n",
      "Epoch 00040: val_loss did not improve from 10.32750\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.4953 - val_loss: 11.1172\n",
      "\n",
      "[20210302-1413-57] Learning rate for epoch 41 is 0.00047855067532509565\n",
      "Epoch 41/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.9218\n",
      "Epoch 00041: val_loss did not improve from 10.32750\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.9218 - val_loss: 10.7861\n",
      "\n",
      "[20210302-1414-01] Learning rate for epoch 42 is 0.00047855067532509565\n",
      "Epoch 42/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.4340\n",
      "Epoch 00042: val_loss did not improve from 10.32750\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.4038 - val_loss: 10.9892\n",
      "\n",
      "[20210302-1414-05] Learning rate for epoch 43 is 0.00047855067532509565\n",
      "Epoch 43/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.7215\n",
      "Epoch 00043: val_loss did not improve from 10.32750\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 15.7701 - val_loss: 10.5229\n",
      "\n",
      "[20210302-1414-09] Learning rate for epoch 44 is 0.00047855067532509565\n",
      "Epoch 44/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 15.3091\n",
      "Epoch 00044: val_loss did not improve from 10.32750\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.6390 - val_loss: 10.9481\n",
      "\n",
      "[20210302-1414-13] Learning rate for epoch 45 is 0.00047855067532509565\n",
      "Epoch 45/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 15.9112\n",
      "Epoch 00045: val_loss did not improve from 10.32750\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 15.8564 - val_loss: 10.6781\n",
      "\n",
      "[20210302-1414-17] Learning rate for epoch 46 is 0.00047855067532509565\n",
      "Epoch 46/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 15.2943\n",
      "Epoch 00046: val_loss did not improve from 10.32750\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.2863 - val_loss: 10.5731\n",
      "\n",
      "[20210302-1414-21] Learning rate for epoch 47 is 0.00047855067532509565\n",
      "Epoch 47/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.0906\n",
      "Epoch 00047: val_loss did not improve from 10.32750\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.1272 - val_loss: 10.3927\n",
      "\n",
      "[20210302-1414-25] Learning rate for epoch 48 is 0.00047855067532509565\n",
      "Epoch 48/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.4658\n",
      "Epoch 00048: val_loss did not improve from 10.32750\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.3878 - val_loss: 10.6487\n",
      "\n",
      "[20210302-1414-29] Learning rate for epoch 49 is 0.00047855067532509565\n",
      "Epoch 49/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.3969\n",
      "Epoch 00049: val_loss did not improve from 10.32750\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.4543 - val_loss: 13.2523\n",
      "\n",
      "[20210302-1414-33] Learning rate for epoch 50 is 0.00047855067532509565\n",
      "Epoch 50/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.4448\n",
      "Epoch 00050: val_loss did not improve from 10.32750\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.4448 - val_loss: 11.0742\n",
      "\n",
      "[20210302-1414-37] Learning rate for epoch 51 is 0.00047855067532509565\n",
      "Epoch 51/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.7412\n",
      "Epoch 00051: val_loss improved from 10.32750 to 10.13334, saving model to ./20210301-225844/heel_K14_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 15.9058 - val_loss: 10.1333\n",
      "\n",
      "[20210302-1414-41] Learning rate for epoch 52 is 0.00047855067532509565\n",
      "Epoch 52/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.3159\n",
      "Epoch 00052: val_loss did not improve from 10.13334\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.2563 - val_loss: 10.4012\n",
      "\n",
      "[20210302-1414-45] Learning rate for epoch 53 is 0.00047855067532509565\n",
      "Epoch 53/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.8475\n",
      "Epoch 00053: val_loss did not improve from 10.13334\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.7989 - val_loss: 10.6766\n",
      "\n",
      "[20210302-1414-49] Learning rate for epoch 54 is 0.00047855067532509565\n",
      "Epoch 54/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.0819\n",
      "Epoch 00054: val_loss did not improve from 10.13334\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.0330 - val_loss: 12.4212\n",
      "\n",
      "[20210302-1414-53] Learning rate for epoch 55 is 0.00047855067532509565\n",
      "Epoch 55/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.0536\n",
      "Epoch 00055: val_loss did not improve from 10.13334\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.9545 - val_loss: 10.6963\n",
      "\n",
      "[20210302-1414-57] Learning rate for epoch 56 is 0.00047855067532509565\n",
      "Epoch 56/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.6811\n",
      "Epoch 00056: val_loss did not improve from 10.13334\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.7761 - val_loss: 10.6239\n",
      "\n",
      "[20210302-1415-01] Learning rate for epoch 57 is 0.00047855067532509565\n",
      "Epoch 57/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.3030\n",
      "Epoch 00057: val_loss did not improve from 10.13334\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.3213 - val_loss: 10.1751\n",
      "\n",
      "[20210302-1415-05] Learning rate for epoch 58 is 0.00047855067532509565\n",
      "Epoch 58/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.6999\n",
      "Epoch 00058: val_loss improved from 10.13334 to 9.62945, saving model to ./20210301-225844/heel_K14_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 15.6999 - val_loss: 9.6294\n",
      "\n",
      "[20210302-1415-10] Learning rate for epoch 59 is 0.00047855067532509565\n",
      "Epoch 59/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.7511\n",
      "Epoch 00059: val_loss did not improve from 9.62945\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.7252 - val_loss: 10.4009\n",
      "\n",
      "[20210302-1415-14] Learning rate for epoch 60 is 0.00047855067532509565\n",
      "Epoch 60/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.9183\n",
      "Epoch 00060: val_loss did not improve from 9.62945\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.8181 - val_loss: 10.2415\n",
      "\n",
      "[20210302-1415-18] Learning rate for epoch 61 is 0.00047855067532509565\n",
      "Epoch 61/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.4196\n",
      "Epoch 00061: val_loss did not improve from 9.62945\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.4634 - val_loss: 9.8521\n",
      "\n",
      "[20210302-1415-22] Learning rate for epoch 62 is 0.00047855067532509565\n",
      "Epoch 62/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 15.7200\n",
      "Epoch 00062: val_loss improved from 9.62945 to 9.62828, saving model to ./20210301-225844/heel_K14_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 15.7802 - val_loss: 9.6283\n",
      "\n",
      "[20210302-1415-26] Learning rate for epoch 63 is 0.00047855067532509565\n",
      "Epoch 63/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.7676\n",
      "Epoch 00063: val_loss did not improve from 9.62828\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.7137 - val_loss: 10.3862\n",
      "\n",
      "[20210302-1415-30] Learning rate for epoch 64 is 0.00047855067532509565\n",
      "Epoch 64/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.4273\n",
      "Epoch 00064: val_loss did not improve from 9.62828\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 15.3629 - val_loss: 10.4601\n",
      "\n",
      "[20210302-1415-34] Learning rate for epoch 65 is 0.00047855067532509565\n",
      "Epoch 65/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.7655\n",
      "Epoch 00065: val_loss did not improve from 9.62828\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.7660 - val_loss: 10.2340\n",
      "\n",
      "[20210302-1415-38] Learning rate for epoch 66 is 0.00047855067532509565\n",
      "Epoch 66/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.1710\n",
      "Epoch 00066: val_loss did not improve from 9.62828\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.1710 - val_loss: 10.5210\n",
      "\n",
      "[20210302-1415-42] Learning rate for epoch 67 is 0.00047855067532509565\n",
      "Epoch 67/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.3449\n",
      "Epoch 00067: val_loss did not improve from 9.62828\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.3337 - val_loss: 10.1086\n",
      "\n",
      "[20210302-1415-46] Learning rate for epoch 68 is 0.00047855067532509565\n",
      "Epoch 68/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.5632\n",
      "Epoch 00068: val_loss did not improve from 9.62828\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.5632 - val_loss: 10.8215\n",
      "\n",
      "[20210302-1415-50] Learning rate for epoch 69 is 0.00047855067532509565\n",
      "Epoch 69/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.6682\n",
      "Epoch 00069: val_loss did not improve from 9.62828\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.7103 - val_loss: 11.4461\n",
      "\n",
      "[20210302-1415-54] Learning rate for epoch 70 is 0.00047855067532509565\n",
      "Epoch 70/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.1328\n",
      "Epoch 00070: val_loss did not improve from 9.62828\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 16.1017 - val_loss: 11.5033\n",
      "\n",
      "[20210302-1415-58] Learning rate for epoch 71 is 0.00047855067532509565\n",
      "Epoch 71/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.1517\n",
      "Epoch 00071: val_loss did not improve from 9.62828\n",
      "20/20 [==============================] - 0s 24ms/step - loss: 15.1983 - val_loss: 10.5630\n",
      "\n",
      "[20210302-1416-02] Learning rate for epoch 72 is 0.00047855067532509565\n",
      "Epoch 72/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.9225\n",
      "Epoch 00072: val_loss did not improve from 9.62828\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.8750 - val_loss: 10.1637\n",
      "\n",
      "[20210302-1416-06] Learning rate for epoch 73 is 0.00047855067532509565\n",
      "Epoch 73/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.4661\n",
      "Epoch 00073: val_loss did not improve from 9.62828\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.5402 - val_loss: 10.1071\n",
      "\n",
      "[20210302-1416-10] Learning rate for epoch 74 is 0.00047855067532509565\n",
      "Epoch 74/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.9557\n",
      "Epoch 00074: val_loss did not improve from 9.62828\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.8894 - val_loss: 10.3397\n",
      "\n",
      "[20210302-1416-14] Learning rate for epoch 75 is 0.00047855067532509565\n",
      "Epoch 75/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.1652\n",
      "Epoch 00075: val_loss did not improve from 9.62828\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.1637 - val_loss: 10.1418\n",
      "\n",
      "[20210302-1416-18] Learning rate for epoch 76 is 0.00047855067532509565\n",
      "Epoch 76/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.4637\n",
      "Epoch 00076: val_loss did not improve from 9.62828\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.4637 - val_loss: 10.2012\n",
      "\n",
      "[20210302-1416-22] Learning rate for epoch 77 is 0.00047855067532509565\n",
      "Epoch 77/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.9990\n",
      "Epoch 00077: val_loss did not improve from 9.62828\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 14.9990 - val_loss: 10.1643\n",
      "\n",
      "[20210302-1416-26] Learning rate for epoch 78 is 0.00047855067532509565\n",
      "Epoch 78/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.5294\n",
      "Epoch 00078: val_loss did not improve from 9.62828\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.5294 - val_loss: 10.2105\n",
      "\n",
      "[20210302-1416-30] Learning rate for epoch 79 is 0.00047855067532509565\n",
      "Epoch 79/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.9967\n",
      "Epoch 00079: val_loss did not improve from 9.62828\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.7930 - val_loss: 12.8755\n",
      "\n",
      "[20210302-1416-34] Learning rate for epoch 80 is 0.00047855067532509565\n",
      "Epoch 80/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.1090\n",
      "Epoch 00080: val_loss did not improve from 9.62828\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.0553 - val_loss: 11.5429\n",
      "\n",
      "[20210302-1416-38] Learning rate for epoch 81 is 0.00047855067532509565\n",
      "Epoch 81/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.9312\n",
      "Epoch 00081: val_loss did not improve from 9.62828\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.8223 - val_loss: 10.9662\n",
      "\n",
      "[20210302-1416-42] Learning rate for epoch 82 is 0.00047855067532509565\n",
      "Epoch 82/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 16.0486\n",
      "Epoch 00082: val_loss did not improve from 9.62828\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.9096 - val_loss: 11.0460\n",
      "\n",
      "*********************************** Start fine tune ***********************************\n",
      "tt: 195, nt:46, total layers:241\n",
      "*********************************** Start fine tune ***********************************\n",
      "Epoch 1/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 24.8310\n",
      "Epoch 00001: val_loss did not improve from 9.62828\n",
      "\n",
      "[20210302-1417-17] Learning rate for epoch 1 is 4.999999987376214e-07\n",
      "20/20 [==============================] - 14s 696ms/step - loss: 24.8310 - val_loss: 15.8591\n",
      "Epoch 2/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 22.4365\n",
      "Epoch 00002: val_loss did not improve from 9.62828\n",
      "\n",
      "[20210302-1417-22] Learning rate for epoch 2 is 5.044450517743826e-05\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 22.4365 - val_loss: 16.7912\n",
      "Epoch 3/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 17.8313\n",
      "Epoch 00003: val_loss did not improve from 9.62828\n",
      "\n",
      "[20210302-1417-41] Learning rate for epoch 3 is 0.00010028902033809572\n",
      "20/20 [==============================] - 16s 793ms/step - loss: 17.8313 - val_loss: 13.6462\n",
      "Epoch 4/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.2379\n",
      "Epoch 00004: val_loss did not improve from 9.62828\n",
      "\n",
      "[20210302-1417-46] Learning rate for epoch 4 is 0.00015003354928921908\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 15.2379 - val_loss: 14.9883\n",
      "Epoch 5/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.5806\n",
      "Epoch 00005: val_loss did not improve from 9.62828\n",
      "\n",
      "[20210302-1417-51] Learning rate for epoch 5 is 0.0001996780774788931\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 14.5806 - val_loss: 12.1540\n",
      "Epoch 6/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.6405\n",
      "Epoch 00006: val_loss did not improve from 9.62828\n",
      "\n",
      "[20210302-1417-56] Learning rate for epoch 6 is 0.000249222619459033\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 13.6405 - val_loss: 11.1728\n",
      "Epoch 7/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 12.7019\n",
      "Epoch 00007: val_loss improved from 9.62828 to 7.59489, saving model to ./20210301-225844/heel_K14_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1418-01] Learning rate for epoch 7 is 0.0002986671752296388\n",
      "20/20 [==============================] - 2s 93ms/step - loss: 12.7019 - val_loss: 7.5949\n",
      "Epoch 8/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.1970\n",
      "Epoch 00008: val_loss did not improve from 7.59489\n",
      "\n",
      "[20210302-1418-06] Learning rate for epoch 8 is 0.00034801175934262574\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 13.1970 - val_loss: 9.2246\n",
      "Epoch 9/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 12.3938\n",
      "Epoch 00009: val_loss did not improve from 7.59489\n",
      "\n",
      "[20210302-1418-11] Learning rate for epoch 9 is 0.00039725631359033287\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 12.3938 - val_loss: 8.5229\n",
      "Epoch 10/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 12.0718\n",
      "Epoch 00010: val_loss did not improve from 7.59489\n",
      "\n",
      "[20210302-1418-15] Learning rate for epoch 10 is 0.0004464008961804211\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 12.0718 - val_loss: 9.1893\n",
      "Epoch 11/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.9362\n",
      "Epoch 00011: val_loss did not improve from 7.59489\n",
      "\n",
      "[20210302-1418-20] Learning rate for epoch 11 is 0.0004954455071128905\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 11.9362 - val_loss: 11.0057\n",
      "Epoch 12/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.5364\n",
      "Epoch 00012: val_loss improved from 7.59489 to 5.21599, saving model to ./20210301-225844/heel_K14_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1418-26] Learning rate for epoch 12 is 0.0005443901172839105\n",
      "20/20 [==============================] - 2s 92ms/step - loss: 11.5364 - val_loss: 5.2160\n",
      "Epoch 13/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.2983\n",
      "Epoch 00013: val_loss did not improve from 5.21599\n",
      "\n",
      "[20210302-1418-31] Learning rate for epoch 13 is 0.0005932347266934812\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 11.2983 - val_loss: 9.3006\n",
      "Epoch 14/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.0727\n",
      "Epoch 00014: val_loss did not improve from 5.21599\n",
      "\n",
      "[20210302-1418-35] Learning rate for epoch 14 is 0.0006419793353416026\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 11.0727 - val_loss: 9.7904\n",
      "Epoch 15/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.4277\n",
      "Epoch 00015: val_loss did not improve from 5.21599\n",
      "\n",
      "[20210302-1418-40] Learning rate for epoch 15 is 0.0006906240014359355\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 10.4277 - val_loss: 8.0857\n",
      "Epoch 16/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.3439\n",
      "Epoch 00016: val_loss did not improve from 5.21599\n",
      "\n",
      "[20210302-1418-45] Learning rate for epoch 16 is 0.0007391686085611582\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 9.3439 - val_loss: 10.1245\n",
      "Epoch 17/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.1688\n",
      "Epoch 00017: val_loss did not improve from 5.21599\n",
      "\n",
      "[20210302-1418-50] Learning rate for epoch 17 is 0.0007876132731325924\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 9.1688 - val_loss: 6.4059\n",
      "Epoch 18/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.7833\n",
      "Epoch 00018: val_loss did not improve from 5.21599\n",
      "\n",
      "[20210302-1418-55] Learning rate for epoch 18 is 0.0008359579369425774\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 8.7833 - val_loss: 6.0782\n",
      "Epoch 19/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.7488\n",
      "Epoch 00019: val_loss improved from 5.21599 to 4.90243, saving model to ./20210301-225844/heel_K14_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1419-00] Learning rate for epoch 19 is 0.000884202599991113\n",
      "20/20 [==============================] - 2s 93ms/step - loss: 8.7488 - val_loss: 4.9024\n",
      "Epoch 20/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.0496\n",
      "Epoch 00020: val_loss improved from 4.90243 to 4.17596, saving model to ./20210301-225844/heel_K14_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1419-06] Learning rate for epoch 20 is 0.0009323473204858601\n",
      "20/20 [==============================] - 2s 92ms/step - loss: 8.0496 - val_loss: 4.1760\n",
      "Epoch 21/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.1622\n",
      "Epoch 00021: val_loss did not improve from 4.17596\n",
      "\n",
      "[20210302-1419-11] Learning rate for epoch 21 is 0.000980391982011497\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 8.1622 - val_loss: 7.0999\n",
      "Epoch 22/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8446\n",
      "Epoch 00022: val_loss did not improve from 4.17596\n",
      "\n",
      "[20210302-1419-15] Learning rate for epoch 22 is 0.0010283367009833455\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 7.8446 - val_loss: 15.0901\n",
      "Epoch 23/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.1011\n",
      "Epoch 00023: val_loss did not improve from 4.17596\n",
      "\n",
      "[20210302-1419-20] Learning rate for epoch 23 is 0.0010761814191937447\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 8.1011 - val_loss: 6.5142\n",
      "Epoch 24/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8994\n",
      "Epoch 00024: val_loss did not improve from 4.17596\n",
      "\n",
      "[20210302-1419-25] Learning rate for epoch 24 is 0.0011239261366426945\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 7.8994 - val_loss: 8.8788\n",
      "Epoch 25/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.3668\n",
      "Epoch 00025: val_loss did not improve from 4.17596\n",
      "\n",
      "[20210302-1419-30] Learning rate for epoch 25 is 0.001171570853330195\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 8.3668 - val_loss: 6.5341\n",
      "Epoch 26/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7953\n",
      "Epoch 00026: val_loss did not improve from 4.17596\n",
      "\n",
      "[20210302-1419-35] Learning rate for epoch 26 is 0.001219115569256246\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.7953 - val_loss: 8.8017\n",
      "Epoch 27/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4287\n",
      "Epoch 00027: val_loss did not improve from 4.17596\n",
      "\n",
      "[20210302-1419-40] Learning rate for epoch 27 is 0.0012665604008361697\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 7.4287 - val_loss: 37.3115\n",
      "Epoch 28/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6525\n",
      "Epoch 00028: val_loss did not improve from 4.17596\n",
      "\n",
      "[20210302-1419-45] Learning rate for epoch 28 is 0.0013139051152393222\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.6525 - val_loss: 19.3780\n",
      "Epoch 29/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.1645\n",
      "Epoch 00029: val_loss did not improve from 4.17596\n",
      "\n",
      "[20210302-1419-49] Learning rate for epoch 29 is 0.0013611499452963471\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 8.1645 - val_loss: 133.4251\n",
      "Epoch 30/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5052\n",
      "Epoch 00030: val_loss did not improve from 4.17596\n",
      "\n",
      "[20210302-1419-54] Learning rate for epoch 30 is 0.001408294658176601\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.5052 - val_loss: 30.3458\n",
      "Epoch 31/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3435\n",
      "Epoch 00031: val_loss did not improve from 4.17596\n",
      "\n",
      "[20210302-1419-59] Learning rate for epoch 31 is 0.0014553394867107272\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.3435 - val_loss: 23.4350\n",
      "Epoch 32/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2682\n",
      "Epoch 00032: val_loss did not improve from 4.17596\n",
      "\n",
      "[20210302-1420-04] Learning rate for epoch 32 is 0.0015022843144834042\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.2682 - val_loss: 89.5461\n",
      "Epoch 33/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8293\n",
      "Epoch 00033: val_loss did not improve from 4.17596\n",
      "\n",
      "[20210302-1420-09] Learning rate for epoch 33 is 0.0015491291414946318\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.8293 - val_loss: 20.9792\n",
      "Epoch 34/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.9662\n",
      "Epoch 00034: val_loss did not improve from 4.17596\n",
      "\n",
      "[20210302-1420-14] Learning rate for epoch 34 is 0.00159587396774441\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.9662 - val_loss: 10.4725\n",
      "Epoch 35/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7119\n",
      "Epoch 00035: val_loss did not improve from 4.17596\n",
      "\n",
      "[20210302-1420-19] Learning rate for epoch 35 is 0.001642518793232739\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.7119 - val_loss: 11.8807\n",
      "Epoch 36/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0818\n",
      "Epoch 00036: val_loss did not improve from 4.17596\n",
      "\n",
      "[20210302-1420-23] Learning rate for epoch 36 is 0.0016890636179596186\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.0818 - val_loss: 14.2442\n",
      "Epoch 37/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9981\n",
      "Epoch 00037: val_loss did not improve from 4.17596\n",
      "\n",
      "[20210302-1420-28] Learning rate for epoch 37 is 0.0017355084419250488\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.9981 - val_loss: 26.5691\n",
      "Epoch 38/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5156\n",
      "Epoch 00038: val_loss did not improve from 4.17596\n",
      "\n",
      "[20210302-1420-33] Learning rate for epoch 38 is 0.0017818533815443516\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.5156 - val_loss: 29.5549\n",
      "Epoch 39/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2568\n",
      "Epoch 00039: val_loss did not improve from 4.17596\n",
      "\n",
      "[20210302-1420-38] Learning rate for epoch 39 is 0.0018280982039868832\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.2568 - val_loss: 27.3498\n",
      "Epoch 40/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.6250\n",
      "Epoch 00040: val_loss did not improve from 4.17596\n",
      "\n",
      "[20210302-1420-43] Learning rate for epoch 40 is 0.0018742431420832872\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.6250 - val_loss: 19.9730\n",
      "Epoch 41/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3636\n",
      "Epoch 00041: val_loss did not improve from 4.17596\n",
      "\n",
      "[20210302-1420-47] Learning rate for epoch 41 is 0.0019202879630029202\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.3636 - val_loss: 12.6980\n",
      "Epoch 42/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7938\n",
      "Epoch 00042: val_loss did not improve from 4.17596\n",
      "\n",
      "[20210302-1420-52] Learning rate for epoch 42 is 0.0019662328995764256\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.7938 - val_loss: 10.0642\n",
      "Epoch 43/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4666\n",
      "Epoch 00043: val_loss did not improve from 4.17596\n",
      "\n",
      "[20210302-1420-57] Learning rate for epoch 43 is 0.00201207771897316\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 7.4666 - val_loss: 4.8191\n",
      "Epoch 44/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5452\n",
      "Epoch 00044: val_loss did not improve from 4.17596\n",
      "\n",
      "[20210302-1421-02] Learning rate for epoch 44 is 0.0020578226540237665\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.5452 - val_loss: 4.5447\n",
      "Epoch 45/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4466\n",
      "Epoch 00045: val_loss did not improve from 4.17596\n",
      "\n",
      "[20210302-1421-07] Learning rate for epoch 45 is 0.0021034677047282457\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 7.4466 - val_loss: 6.6021\n",
      "Epoch 46/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0546\n",
      "Epoch 00046: val_loss did not improve from 4.17596\n",
      "\n",
      "[20210302-1421-12] Learning rate for epoch 46 is 0.002149012638255954\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.0546 - val_loss: 7.1225\n",
      "Epoch 47/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8258\n",
      "Epoch 00047: val_loss did not improve from 4.17596\n",
      "\n",
      "[20210302-1421-17] Learning rate for epoch 47 is 0.0021944576874375343\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.8258 - val_loss: 6.5868\n",
      "Epoch 48/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8241\n",
      "Epoch 00048: val_loss did not improve from 4.17596\n",
      "\n",
      "[20210302-1421-21] Learning rate for epoch 48 is 0.0022398026194423437\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.8241 - val_loss: 6.2294\n",
      "Epoch 49/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8810\n",
      "Epoch 00049: val_loss did not improve from 4.17596\n",
      "\n",
      "[20210302-1421-26] Learning rate for epoch 49 is 0.002285047434270382\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.8810 - val_loss: 6.1746\n",
      "Epoch 50/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7597\n",
      "Epoch 00050: val_loss did not improve from 4.17596\n",
      "\n",
      "[20210302-1421-31] Learning rate for epoch 50 is 0.0023301925975829363\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.7597 - val_loss: 8.2264\n",
      "Epoch 51/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8309\n",
      "Epoch 00051: val_loss did not improve from 4.17596\n",
      "\n",
      "[20210302-1421-36] Learning rate for epoch 51 is 0.002375237410888076\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.8309 - val_loss: 11.6494\n",
      "Epoch 52/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4306\n",
      "Epoch 00052: val_loss did not improve from 4.17596\n",
      "\n",
      "[20210302-1421-41] Learning rate for epoch 52 is 0.0024201825726777315\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.4306 - val_loss: 27.2945\n",
      "Epoch 53/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4889\n",
      "Epoch 00053: val_loss did not improve from 4.17596\n",
      "\n",
      "[20210302-1421-46] Learning rate for epoch 53 is 0.002465027617290616\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.4889 - val_loss: 50.1767\n",
      "Epoch 54/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2451\n",
      "Epoch 00054: val_loss did not improve from 4.17596\n",
      "\n",
      "[20210302-1421-50] Learning rate for epoch 54 is 0.0025097725447267294\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.2451 - val_loss: 24.9259\n",
      "Epoch 55/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5485\n",
      "Epoch 00055: val_loss did not improve from 4.17596\n",
      "\n",
      "[20210302-1421-55] Learning rate for epoch 55 is 0.0025544175878167152\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.5485 - val_loss: 18.5086\n",
      "Epoch 56/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8008\n",
      "Epoch 00056: val_loss did not improve from 4.17596\n",
      "\n",
      "[20210302-1422-00] Learning rate for epoch 56 is 0.00259896251372993\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.8008 - val_loss: 10.5669\n",
      "Epoch 57/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1569\n",
      "Epoch 00057: val_loss did not improve from 4.17596\n",
      "\n",
      "[20210302-1422-05] Learning rate for epoch 57 is 0.0026434077881276608\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.1569 - val_loss: 5.6000\n",
      "Epoch 58/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2891\n",
      "Epoch 00058: val_loss did not improve from 4.17596\n",
      "\n",
      "[20210302-1422-10] Learning rate for epoch 58 is 0.0026877527125179768\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.2891 - val_loss: 4.6175\n",
      "Epoch 59/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7639\n",
      "Epoch 00059: val_loss improved from 4.17596 to 3.63888, saving model to ./20210301-225844/heel_K14_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1422-15] Learning rate for epoch 59 is 0.0027319977525621653\n",
      "20/20 [==============================] - 2s 91ms/step - loss: 6.7639 - val_loss: 3.6389\n",
      "Epoch 60/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0797\n",
      "Epoch 00060: val_loss did not improve from 3.63888\n",
      "\n",
      "[20210302-1422-20] Learning rate for epoch 60 is 0.0027761429082602262\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.0797 - val_loss: 3.9768\n",
      "Epoch 61/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4775\n",
      "Epoch 00061: val_loss did not improve from 3.63888\n",
      "\n",
      "[20210302-1422-25] Learning rate for epoch 61 is 0.002820187946781516\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.4775 - val_loss: 6.5647\n",
      "Epoch 62/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1190\n",
      "Epoch 00062: val_loss did not improve from 3.63888\n",
      "\n",
      "[20210302-1422-30] Learning rate for epoch 62 is 0.0028641331009566784\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 7.1190 - val_loss: 5.5978\n",
      "Epoch 63/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9743\n",
      "Epoch 00063: val_loss did not improve from 3.63888\n",
      "\n",
      "[20210302-1422-35] Learning rate for epoch 63 is 0.0029079781379550695\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.9743 - val_loss: 3.9382\n",
      "Epoch 64/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8214\n",
      "Epoch 00064: val_loss did not improve from 3.63888\n",
      "\n",
      "[20210302-1422-39] Learning rate for epoch 64 is 0.002951723290607333\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.8214 - val_loss: 3.7201\n",
      "Epoch 65/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5816\n",
      "Epoch 00065: val_loss did not improve from 3.63888\n",
      "\n",
      "[20210302-1422-44] Learning rate for epoch 65 is 0.0029953685589134693\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.5816 - val_loss: 3.7663\n",
      "Epoch 66/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1126\n",
      "Epoch 00066: val_loss did not improve from 3.63888\n",
      "\n",
      "[20210302-1422-49] Learning rate for epoch 66 is 0.0030389137100428343\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.1126 - val_loss: 5.2403\n",
      "Epoch 67/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9982\n",
      "Epoch 00067: val_loss did not improve from 3.63888\n",
      "\n",
      "[20210302-1422-54] Learning rate for epoch 67 is 0.003082358743995428\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.9982 - val_loss: 4.4108\n",
      "Epoch 68/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0156\n",
      "Epoch 00068: val_loss did not improve from 3.63888\n",
      "\n",
      "[20210302-1422-59] Learning rate for epoch 68 is 0.0031257038936018944\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.0156 - val_loss: 4.0910\n",
      "Epoch 69/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8180\n",
      "Epoch 00069: val_loss did not improve from 3.63888\n",
      "\n",
      "[20210302-1423-04] Learning rate for epoch 69 is 0.003168949158862233\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.8180 - val_loss: 7.5199\n",
      "Epoch 70/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0536\n",
      "Epoch 00070: val_loss did not improve from 3.63888\n",
      "\n",
      "[20210302-1423-08] Learning rate for epoch 70 is 0.0032120943069458008\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.0536 - val_loss: 5.1811\n",
      "Epoch 71/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7073\n",
      "Epoch 00071: val_loss did not improve from 3.63888\n",
      "\n",
      "[20210302-1423-13] Learning rate for epoch 71 is 0.003255139570683241\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.7073 - val_loss: 10.6626\n",
      "Epoch 72/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3521\n",
      "Epoch 00072: val_loss did not improve from 3.63888\n",
      "\n",
      "[20210302-1423-18] Learning rate for epoch 72 is 0.00329808471724391\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.3521 - val_loss: 13.4124\n",
      "Epoch 73/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.3912\n",
      "Epoch 00073: val_loss did not improve from 3.63888\n",
      "\n",
      "[20210302-1423-23] Learning rate for epoch 73 is 0.0033409299794584513\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.3912 - val_loss: 11.0356\n",
      "Epoch 74/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9973\n",
      "Epoch 00074: val_loss did not improve from 3.63888\n",
      "\n",
      "[20210302-1423-28] Learning rate for epoch 74 is 0.0033836751244962215\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.9973 - val_loss: 7.2380\n",
      "Epoch 75/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1323\n",
      "Epoch 00075: val_loss did not improve from 3.63888\n",
      "\n",
      "[20210302-1423-32] Learning rate for epoch 75 is 0.0034263203851878643\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.1323 - val_loss: 106.7694\n",
      "Epoch 76/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4498\n",
      "Epoch 00076: val_loss did not improve from 3.63888\n",
      "\n",
      "[20210302-1423-37] Learning rate for epoch 76 is 0.003468865528702736\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.4498 - val_loss: 197.4146\n",
      "Epoch 77/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.1488\n",
      "Epoch 00077: val_loss did not improve from 3.63888\n",
      "\n",
      "[20210302-1423-42] Learning rate for epoch 77 is 0.00351131078787148\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.1488 - val_loss: 7.4171\n",
      "Epoch 78/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8675\n",
      "Epoch 00078: val_loss did not improve from 3.63888\n",
      "\n",
      "[20210302-1423-47] Learning rate for epoch 78 is 0.0035536561626940966\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.8675 - val_loss: 8.0537\n",
      "Epoch 79/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.9452\n",
      "Epoch 00079: val_loss did not improve from 3.63888\n",
      "\n",
      "[20210302-1423-52] Learning rate for epoch 79 is 0.003595901420339942\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.9452 - val_loss: 8.8567\n",
      "Epoch 80/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6204\n",
      "Epoch 00080: val_loss did not improve from 3.63888\n",
      "\n",
      "[20210302-1423-57] Learning rate for epoch 80 is 0.00363804679363966\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.6204 - val_loss: 8.5917\n",
      "Epoch 81/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6982\n",
      "Epoch 00081: val_loss did not improve from 3.63888\n",
      "\n",
      "[20210302-1424-01] Learning rate for epoch 81 is 0.0036800920497626066\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.6982 - val_loss: 5.6812\n",
      "Epoch 82/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2106\n",
      "Epoch 00082: val_loss did not improve from 3.63888\n",
      "\n",
      "[20210302-1424-06] Learning rate for epoch 82 is 0.003722037188708782\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.2106 - val_loss: 4.5929\n",
      "Epoch 83/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3926\n",
      "Epoch 00083: val_loss did not improve from 3.63888\n",
      "\n",
      "[20210302-1424-11] Learning rate for epoch 83 is 0.003763882676139474\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.3926 - val_loss: 4.0748\n",
      "Epoch 84/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.0119\n",
      "Epoch 00084: val_loss did not improve from 3.63888\n",
      "\n",
      "[20210302-1424-16] Learning rate for epoch 84 is 0.0038056280463933945\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.0119 - val_loss: 11.0116\n",
      "Epoch 85/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.2169\n",
      "Epoch 00085: val_loss did not improve from 3.63888\n",
      "\n",
      "[20210302-1424-21] Learning rate for epoch 85 is 0.003847273299470544\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 7.2169 - val_loss: 4.8978\n",
      "Epoch 86/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3471\n",
      "Epoch 00086: val_loss did not improve from 3.63888\n",
      "\n",
      "[20210302-1424-26] Learning rate for epoch 86 is 0.0038888186682015657\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.3471 - val_loss: 6.2591\n",
      "Epoch 87/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5475\n",
      "Epoch 00087: val_loss did not improve from 3.63888\n",
      "\n",
      "[20210302-1424-31] Learning rate for epoch 87 is 0.00393026415258646\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 7.5475 - val_loss: 4.4688\n",
      "Epoch 88/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6636\n",
      "Epoch 00088: val_loss did not improve from 3.63888\n",
      "\n",
      "[20210302-1424-35] Learning rate for epoch 88 is 0.00397160928696394\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.6636 - val_loss: 14.1607\n",
      "Epoch 89/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8153\n",
      "Epoch 00089: val_loss did not improve from 3.63888\n",
      "\n",
      "[20210302-1424-40] Learning rate for epoch 89 is 0.004012854769825935\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.8153 - val_loss: 1958.6206\n",
      "Epoch 90/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5302\n",
      "Epoch 00090: val_loss did not improve from 3.63888\n",
      "\n",
      "[20210302-1424-45] Learning rate for epoch 90 is 0.00405400013551116\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.5302 - val_loss: 247.7777\n",
      "Epoch 91/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8509\n",
      "Epoch 00091: val_loss did not improve from 3.63888\n",
      "\n",
      "[20210302-1424-50] Learning rate for epoch 91 is 0.004095045384019613\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.8509 - val_loss: 22.2575\n",
      "Epoch 92/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6093\n",
      "Epoch 00092: val_loss did not improve from 3.63888\n",
      "\n",
      "[20210302-1424-55] Learning rate for epoch 92 is 0.004135990981012583\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.6093 - val_loss: 6.9135\n",
      "Epoch 93/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3461\n",
      "Epoch 00093: val_loss did not improve from 3.63888\n",
      "\n",
      "[20210302-1425-00] Learning rate for epoch 93 is 0.004176836460828781\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.3461 - val_loss: 5.7330\n",
      "Epoch 94/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4585\n",
      "Epoch 00094: val_loss did not improve from 3.63888\n",
      "\n",
      "[20210302-1425-04] Learning rate for epoch 94 is 0.004217581823468208\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.4585 - val_loss: 4.8125\n",
      "Epoch 95/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2062\n",
      "Epoch 00095: val_loss did not improve from 3.63888\n",
      "\n",
      "[20210302-1425-09] Learning rate for epoch 95 is 0.004258227068930864\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.2062 - val_loss: 5.2223\n",
      "Epoch 96/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5067\n",
      "Epoch 00096: val_loss did not improve from 3.63888\n",
      "\n",
      "[20210302-1425-14] Learning rate for epoch 96 is 0.0042987726628780365\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.5067 - val_loss: 4.3519\n",
      "Epoch 97/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2517\n",
      "Epoch 00097: val_loss improved from 3.63888 to 3.52427, saving model to ./20210301-225844/heel_K14_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1425-20] Learning rate for epoch 97 is 0.0043392181396484375\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 6.2517 - val_loss: 3.5243\n",
      "Epoch 98/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5008\n",
      "Epoch 00098: val_loss did not improve from 3.52427\n",
      "\n",
      "[20210302-1425-25] Learning rate for epoch 98 is 0.004379563499242067\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.5008 - val_loss: 4.9669\n",
      "Epoch 99/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7245\n",
      "Epoch 00099: val_loss did not improve from 3.52427\n",
      "\n",
      "[20210302-1425-30] Learning rate for epoch 99 is 0.004419809207320213\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.7245 - val_loss: 4.4520\n",
      "Epoch 100/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3680\n",
      "Epoch 00100: val_loss did not improve from 3.52427\n",
      "\n",
      "[20210302-1425-34] Learning rate for epoch 100 is 0.004459954332560301\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 6.3680 - val_loss: 4.7271\n",
      "Epoch 101/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7943\n",
      "Epoch 00101: val_loss did not improve from 3.52427\n",
      "\n",
      "[20210302-1425-39] Learning rate for epoch 101 is 4.4999998749517545e-07\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.7943 - val_loss: 4.3306\n",
      "Epoch 102/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3878\n",
      "Epoch 00102: val_loss did not improve from 3.52427\n",
      "\n",
      "[20210302-1425-44] Learning rate for epoch 102 is 0.00018023152370005846\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.3878 - val_loss: 3.6945\n",
      "Epoch 103/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3466\n",
      "Epoch 00103: val_loss did not improve from 3.52427\n",
      "\n",
      "[20210302-1425-49] Learning rate for epoch 103 is 0.000359613070031628\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.3466 - val_loss: 3.5963\n",
      "Epoch 104/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0931\n",
      "Epoch 00104: val_loss improved from 3.52427 to 3.27253, saving model to ./20210301-225844/heel_K14_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1425-54] Learning rate for epoch 104 is 0.0005385946715250611\n",
      "20/20 [==============================] - 2s 87ms/step - loss: 6.0931 - val_loss: 3.2725\n",
      "Epoch 105/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6846\n",
      "Epoch 00105: val_loss improved from 3.27253 to 3.26944, saving model to ./20210301-225844/heel_K14_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1426-00] Learning rate for epoch 105 is 0.0007171762990765274\n",
      "20/20 [==============================] - 2s 93ms/step - loss: 5.6846 - val_loss: 3.2694\n",
      "Epoch 106/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2408\n",
      "Epoch 00106: val_loss did not improve from 3.26944\n",
      "\n",
      "[20210302-1426-05] Learning rate for epoch 106 is 0.0008953579817898571\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.2408 - val_loss: 3.3000\n",
      "Epoch 107/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.7054\n",
      "Epoch 00107: val_loss did not improve from 3.26944\n",
      "\n",
      "[20210302-1426-10] Learning rate for epoch 107 is 0.0010731397196650505\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.7054 - val_loss: 4.1308\n",
      "Epoch 108/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0835\n",
      "Epoch 00108: val_loss did not improve from 3.26944\n",
      "\n",
      "[20210302-1426-14] Learning rate for epoch 108 is 0.0012505215127021074\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.0835 - val_loss: 3.6021\n",
      "Epoch 109/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9944\n",
      "Epoch 00109: val_loss did not improve from 3.26944\n",
      "\n",
      "[20210302-1426-19] Learning rate for epoch 109 is 0.001427503302693367\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.9944 - val_loss: 3.4862\n",
      "Epoch 110/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1438\n",
      "Epoch 00110: val_loss did not improve from 3.26944\n",
      "\n",
      "[20210302-1426-24] Learning rate for epoch 110 is 0.0016040850896388292\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.1438 - val_loss: 3.6098\n",
      "Epoch 111/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1130\n",
      "Epoch 00111: val_loss improved from 3.26944 to 3.24211, saving model to ./20210301-225844/heel_K14_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1426-29] Learning rate for epoch 111 is 0.001780266989953816\n",
      "20/20 [==============================] - 2s 89ms/step - loss: 6.1130 - val_loss: 3.2421\n",
      "Epoch 112/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9092\n",
      "Epoch 00112: val_loss did not improve from 3.24211\n",
      "\n",
      "[20210302-1426-34] Learning rate for epoch 112 is 0.0019560488872230053\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.9092 - val_loss: 3.6739\n",
      "Epoch 113/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7854\n",
      "Epoch 00113: val_loss did not improve from 3.24211\n",
      "\n",
      "[20210302-1426-39] Learning rate for epoch 113 is 0.002131430897861719\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.7854 - val_loss: 3.4172\n",
      "Epoch 114/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1729\n",
      "Epoch 00114: val_loss did not improve from 3.24211\n",
      "\n",
      "[20210302-1426-44] Learning rate for epoch 114 is 0.0023064129054546356\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.1729 - val_loss: 3.4280\n",
      "Epoch 115/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5652\n",
      "Epoch 00115: val_loss did not improve from 3.24211\n",
      "\n",
      "[20210302-1426-49] Learning rate for epoch 115 is 0.0024809949100017548\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.5652 - val_loss: 3.9635\n",
      "Epoch 116/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6734\n",
      "Epoch 00116: val_loss did not improve from 3.24211\n",
      "\n",
      "[20210302-1426-54] Learning rate for epoch 116 is 0.0026551769115030766\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.6734 - val_loss: 3.5423\n",
      "Epoch 117/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0567\n",
      "Epoch 00117: val_loss did not improve from 3.24211\n",
      "\n",
      "[20210302-1426-58] Learning rate for epoch 117 is 0.0028289591427892447\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.0567 - val_loss: 3.7244\n",
      "Epoch 118/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5113\n",
      "Epoch 00118: val_loss did not improve from 3.24211\n",
      "\n",
      "[20210302-1427-03] Learning rate for epoch 118 is 0.0030023413710296154\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.5113 - val_loss: 3.9296\n",
      "Epoch 119/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6893\n",
      "Epoch 00119: val_loss did not improve from 3.24211\n",
      "\n",
      "[20210302-1427-08] Learning rate for epoch 119 is 0.003175323596224189\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.6893 - val_loss: 3.3413\n",
      "Epoch 120/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8381\n",
      "Epoch 00120: val_loss did not improve from 3.24211\n",
      "\n",
      "[20210302-1427-13] Learning rate for epoch 120 is 0.003347905818372965\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8381 - val_loss: 4.3790\n",
      "Epoch 121/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4685\n",
      "Epoch 00121: val_loss did not improve from 3.24211\n",
      "\n",
      "[20210302-1427-18] Learning rate for epoch 121 is 0.0035200880374759436\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.4685 - val_loss: 4.6908\n",
      "Epoch 122/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5190\n",
      "Epoch 00122: val_loss did not improve from 3.24211\n",
      "\n",
      "[20210302-1427-23] Learning rate for epoch 122 is 0.003691870253533125\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 6.5190 - val_loss: 5.0233\n",
      "Epoch 123/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2955\n",
      "Epoch 00123: val_loss did not improve from 3.24211\n",
      "\n",
      "[20210302-1427-28] Learning rate for epoch 123 is 0.0038632526993751526\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.2955 - val_loss: 5.0389\n",
      "Epoch 124/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2330\n",
      "Epoch 00124: val_loss did not improve from 3.24211\n",
      "\n",
      "[20210302-1427-33] Learning rate for epoch 124 is 0.004034235142171383\n",
      "20/20 [==============================] - 1s 68ms/step - loss: 6.2330 - val_loss: 4.2823\n",
      "Epoch 125/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8411\n",
      "Epoch 00125: val_loss did not improve from 3.24211\n",
      "\n",
      "[20210302-1427-38] Learning rate for epoch 125 is 0.004204817581921816\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 5.8411 - val_loss: 4.5911\n",
      "Epoch 126/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4132\n",
      "Epoch 00126: val_loss did not improve from 3.24211\n",
      "\n",
      "[20210302-1427-44] Learning rate for epoch 126 is 0.0043750000186264515\n",
      "20/20 [==============================] - 1s 69ms/step - loss: 6.4132 - val_loss: 4.0383\n",
      "Epoch 127/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1717\n",
      "Epoch 00127: val_loss did not improve from 3.24211\n",
      "\n",
      "[20210302-1427-49] Learning rate for epoch 127 is 0.0041952175088226795\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 6.1717 - val_loss: 5.8131\n",
      "Epoch 128/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8321\n",
      "Epoch 00128: val_loss did not improve from 3.24211\n",
      "\n",
      "[20210302-1427-54] Learning rate for epoch 128 is 0.004015835002064705\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 6.8321 - val_loss: 5.0035\n",
      "Epoch 129/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4358\n",
      "Epoch 00129: val_loss did not improve from 3.24211\n",
      "\n",
      "[20210302-1427-59] Learning rate for epoch 129 is 0.003836852265521884\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 6.4358 - val_loss: 8.0054\n",
      "Epoch 130/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9381\n",
      "Epoch 00130: val_loss did not improve from 3.24211\n",
      "\n",
      "[20210302-1428-04] Learning rate for epoch 130 is 0.003658269764855504\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.9381 - val_loss: 5.3032\n",
      "Epoch 131/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2410\n",
      "Epoch 00131: val_loss did not improve from 3.24211\n",
      "\n",
      "[20210302-1428-09] Learning rate for epoch 131 is 0.003480087034404278\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 6.2410 - val_loss: 4.5750\n",
      "Epoch 132/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4974\n",
      "Epoch 00132: val_loss did not improve from 3.24211\n",
      "\n",
      "[20210302-1428-15] Learning rate for epoch 132 is 0.003302304306998849\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 6.4974 - val_loss: 3.2644\n",
      "Epoch 133/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9966\n",
      "Epoch 00133: val_loss did not improve from 3.24211\n",
      "\n",
      "[20210302-1428-20] Learning rate for epoch 133 is 0.0031249215826392174\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 5.9966 - val_loss: 3.7835\n",
      "Epoch 134/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.6646\n",
      "Epoch 00134: val_loss did not improve from 3.24211\n",
      "\n",
      "[20210302-1428-25] Learning rate for epoch 134 is 0.0029479386284947395\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 6.6646 - val_loss: 5.1463\n",
      "Epoch 135/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.4437\n",
      "Epoch 00135: val_loss did not improve from 3.24211\n",
      "\n",
      "[20210302-1428-30] Learning rate for epoch 135 is 0.0027713559102267027\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.4437 - val_loss: 4.3202\n",
      "Epoch 136/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9309\n",
      "Epoch 00136: val_loss did not improve from 3.24211\n",
      "\n",
      "[20210302-1428-35] Learning rate for epoch 136 is 0.0025951729621738195\n",
      "20/20 [==============================] - 1s 68ms/step - loss: 5.9309 - val_loss: 4.3112\n",
      "Epoch 137/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8710\n",
      "Epoch 00137: val_loss did not improve from 3.24211\n",
      "\n",
      "[20210302-1428-41] Learning rate for epoch 137 is 0.0024193900171667337\n",
      "20/20 [==============================] - 1s 68ms/step - loss: 5.8710 - val_loss: 4.0454\n",
      "Epoch 138/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9673\n",
      "Epoch 00138: val_loss did not improve from 3.24211\n",
      "\n",
      "[20210302-1428-46] Learning rate for epoch 138 is 0.0022440070752054453\n",
      "20/20 [==============================] - 1s 68ms/step - loss: 5.9673 - val_loss: 4.0488\n",
      "Epoch 139/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9362\n",
      "Epoch 00139: val_loss did not improve from 3.24211\n",
      "\n",
      "[20210302-1428-51] Learning rate for epoch 139 is 0.002069024136289954\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 5.9362 - val_loss: 3.5172\n",
      "Epoch 140/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1388\n",
      "Epoch 00140: val_loss did not improve from 3.24211\n",
      "\n",
      "[20210302-1428-56] Learning rate for epoch 140 is 0.0018944410840049386\n",
      "20/20 [==============================] - 1s 68ms/step - loss: 6.1388 - val_loss: 4.1931\n",
      "Epoch 141/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0423\n",
      "Epoch 00141: val_loss did not improve from 3.24211\n",
      "\n",
      "[20210302-1429-02] Learning rate for epoch 141 is 0.0017202580347657204\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 6.0423 - val_loss: 3.7962\n",
      "Epoch 142/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5052\n",
      "Epoch 00142: val_loss did not improve from 3.24211\n",
      "\n",
      "[20210302-1429-07] Learning rate for epoch 142 is 0.0015464748721569777\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 5.5052 - val_loss: 3.8342\n",
      "Epoch 143/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0522\n",
      "Epoch 00143: val_loss did not improve from 3.24211\n",
      "\n",
      "[20210302-1429-12] Learning rate for epoch 143 is 0.0013730917125940323\n",
      "20/20 [==============================] - 1s 68ms/step - loss: 6.0522 - val_loss: 3.4223\n",
      "Epoch 144/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2047\n",
      "Epoch 00144: val_loss did not improve from 3.24211\n",
      "\n",
      "[20210302-1429-18] Learning rate for epoch 144 is 0.0012001085560768843\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 6.2047 - val_loss: 3.8027\n",
      "Epoch 145/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8978\n",
      "Epoch 00145: val_loss did not improve from 3.24211\n",
      "\n",
      "[20210302-1429-23] Learning rate for epoch 145 is 0.0010275252861902118\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 5.8978 - val_loss: 3.2641\n",
      "Epoch 146/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8671\n",
      "Epoch 00146: val_loss did not improve from 3.24211\n",
      "\n",
      "[20210302-1429-28] Learning rate for epoch 146 is 0.0008553420193493366\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 5.8671 - val_loss: 3.4585\n",
      "Epoch 147/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6887\n",
      "Epoch 00147: val_loss did not improve from 3.24211\n",
      "\n",
      "[20210302-1429-33] Learning rate for epoch 147 is 0.0006835586973465979\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 5.6887 - val_loss: 3.4832\n",
      "Epoch 148/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5028\n",
      "Epoch 00148: val_loss did not improve from 3.24211\n",
      "\n",
      "[20210302-1429-39] Learning rate for epoch 148 is 0.0005121753201819956\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 5.5028 - val_loss: 3.4486\n",
      "Epoch 149/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8900\n",
      "Epoch 00149: val_loss did not improve from 3.24211\n",
      "\n",
      "[20210302-1429-44] Learning rate for epoch 149 is 0.00034119191695936024\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.8900 - val_loss: 3.3722\n",
      "Epoch 150/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8110\n",
      "Epoch 00150: val_loss did not improve from 3.24211\n",
      "\n",
      "[20210302-1429-48] Learning rate for epoch 150 is 0.00017060847312677652\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.8110 - val_loss: 3.2847\n",
      "Epoch 151/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7353\n",
      "Epoch 00151: val_loss did not improve from 3.24211\n",
      "\n",
      "[20210302-1429-53] Learning rate for epoch 151 is 4.249999960848072e-07\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.7353 - val_loss: 3.2886\n",
      "Epoch 152/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8890\n",
      "Epoch 00152: val_loss did not improve from 3.24211\n",
      "\n",
      "[20210302-1429-58] Learning rate for epoch 152 is 0.00017020752420648932\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.8890 - val_loss: 3.3078\n",
      "Epoch 153/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4327\n",
      "Epoch 00153: val_loss did not improve from 3.24211\n",
      "\n",
      "[20210302-1430-03] Learning rate for epoch 153 is 0.0003395900712348521\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.4327 - val_loss: 3.2987\n",
      "Epoch 154/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0144\n",
      "Epoch 00154: val_loss did not improve from 3.24211\n",
      "\n",
      "[20210302-1430-08] Learning rate for epoch 154 is 0.0005085726734250784\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.0144 - val_loss: 3.2727\n",
      "Epoch 155/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4809\n",
      "Epoch 00155: val_loss did not improve from 3.24211\n",
      "\n",
      "[20210302-1430-13] Learning rate for epoch 155 is 0.0006771553307771683\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.4809 - val_loss: 3.2424\n",
      "Epoch 156/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8924\n",
      "Epoch 00156: val_loss did not improve from 3.24211\n",
      "\n",
      "[20210302-1430-18] Learning rate for epoch 156 is 0.0008453379850834608\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8924 - val_loss: 3.2933\n",
      "Epoch 157/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8164\n",
      "Epoch 00157: val_loss did not improve from 3.24211\n",
      "\n",
      "[20210302-1430-22] Learning rate for epoch 157 is 0.0010131207527592778\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.8164 - val_loss: 3.2810\n",
      "Epoch 158/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9615\n",
      "Epoch 00158: val_loss did not improve from 3.24211\n",
      "\n",
      "[20210302-1430-27] Learning rate for epoch 158 is 0.0011805035173892975\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.9615 - val_loss: 3.3287\n",
      "Epoch 159/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4858\n",
      "Epoch 00159: val_loss did not improve from 3.24211\n",
      "\n",
      "[20210302-1430-32] Learning rate for epoch 159 is 0.0013474862789735198\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.4858 - val_loss: 3.6914\n",
      "Epoch 160/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7488\n",
      "Epoch 00160: val_loss improved from 3.24211 to 3.20260, saving model to ./20210301-225844/heel_K14_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1430-38] Learning rate for epoch 160 is 0.0015140691539272666\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 5.7488 - val_loss: 3.2026\n",
      "Epoch 161/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5265\n",
      "Epoch 00161: val_loss did not improve from 3.20260\n",
      "\n",
      "[20210302-1430-42] Learning rate for epoch 161 is 0.001680252025835216\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.5265 - val_loss: 4.1005\n",
      "Epoch 162/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7829\n",
      "Epoch 00162: val_loss did not improve from 3.20260\n",
      "\n",
      "[20210302-1430-47] Learning rate for epoch 162 is 0.0018460348946973681\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.7829 - val_loss: 3.8282\n",
      "Epoch 163/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0072\n",
      "Epoch 00163: val_loss did not improve from 3.20260\n",
      "\n",
      "[20210302-1430-52] Learning rate for epoch 163 is 0.0020114178769290447\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.0072 - val_loss: 4.5060\n",
      "Epoch 164/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0436\n",
      "Epoch 00164: val_loss did not improve from 3.20260\n",
      "\n",
      "[20210302-1430-57] Learning rate for epoch 164 is 0.0021764009725302458\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.0436 - val_loss: 4.2804\n",
      "Epoch 165/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8414\n",
      "Epoch 00165: val_loss did not improve from 3.20260\n",
      "\n",
      "[20210302-1431-02] Learning rate for epoch 165 is 0.002340983832255006\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.8414 - val_loss: 3.3892\n",
      "Epoch 166/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5908\n",
      "Epoch 00166: val_loss did not improve from 3.20260\n",
      "\n",
      "[20210302-1431-06] Learning rate for epoch 166 is 0.002505166921764612\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.5908 - val_loss: 3.7600\n",
      "Epoch 167/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0809\n",
      "Epoch 00167: val_loss did not improve from 3.20260\n",
      "\n",
      "[20210302-1431-11] Learning rate for epoch 167 is 0.002668950008228421\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.0809 - val_loss: 3.3121\n",
      "Epoch 168/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0834\n",
      "Epoch 00168: val_loss did not improve from 3.20260\n",
      "\n",
      "[20210302-1431-16] Learning rate for epoch 168 is 0.0028323333244770765\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 6.0834 - val_loss: 4.1041\n",
      "Epoch 169/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0439\n",
      "Epoch 00169: val_loss did not improve from 3.20260\n",
      "\n",
      "[20210302-1431-21] Learning rate for epoch 169 is 0.002995316404849291\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.0439 - val_loss: 3.5847\n",
      "Epoch 170/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0343\n",
      "Epoch 00170: val_loss did not improve from 3.20260\n",
      "\n",
      "[20210302-1431-26] Learning rate for epoch 170 is 0.0031578997150063515\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.0343 - val_loss: 4.6959\n",
      "Epoch 171/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1601\n",
      "Epoch 00171: val_loss did not improve from 3.20260\n",
      "\n",
      "[20210302-1431-31] Learning rate for epoch 171 is 0.0033200830221176147\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.1601 - val_loss: 4.4496\n",
      "Epoch 172/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8898\n",
      "Epoch 00172: val_loss did not improve from 3.20260\n",
      "\n",
      "[20210302-1431-35] Learning rate for epoch 172 is 0.0034818663261830807\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.8898 - val_loss: 4.2075\n",
      "Epoch 173/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6888\n",
      "Epoch 00173: val_loss did not improve from 3.20260\n",
      "\n",
      "[20210302-1431-40] Learning rate for epoch 173 is 0.0036432496272027493\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6888 - val_loss: 4.0537\n",
      "Epoch 174/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9781\n",
      "Epoch 00174: val_loss did not improve from 3.20260\n",
      "\n",
      "[20210302-1431-45] Learning rate for epoch 174 is 0.003804233158007264\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.9781 - val_loss: 4.7323\n",
      "Epoch 175/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7645\n",
      "Epoch 00175: val_loss did not improve from 3.20260\n",
      "\n",
      "[20210302-1431-50] Learning rate for epoch 175 is 0.003964816685765982\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.7645 - val_loss: 3.4799\n",
      "Epoch 176/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9751\n",
      "Epoch 00176: val_loss did not improve from 3.20260\n",
      "\n",
      "[20210302-1431-55] Learning rate for epoch 176 is 0.004124999977648258\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.9751 - val_loss: 4.8234\n",
      "Epoch 177/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9217\n",
      "Epoch 00177: val_loss did not improve from 3.20260\n",
      "\n",
      "[20210302-1432-00] Learning rate for epoch 177 is 0.003955216612666845\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.9217 - val_loss: 3.7816\n",
      "Epoch 178/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3792\n",
      "Epoch 00178: val_loss did not improve from 3.20260\n",
      "\n",
      "[20210302-1432-05] Learning rate for epoch 178 is 0.003785833017900586\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.3792 - val_loss: 4.3400\n",
      "Epoch 179/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8567\n",
      "Epoch 00179: val_loss did not improve from 3.20260\n",
      "\n",
      "[20210302-1432-09] Learning rate for epoch 179 is 0.0036168494261801243\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.8567 - val_loss: 4.5631\n",
      "Epoch 180/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6860\n",
      "Epoch 00180: val_loss did not improve from 3.20260\n",
      "\n",
      "[20210302-1432-14] Learning rate for epoch 180 is 0.003448265604674816\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6860 - val_loss: 3.5872\n",
      "Epoch 181/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7421\n",
      "Epoch 00181: val_loss did not improve from 3.20260\n",
      "\n",
      "[20210302-1432-19] Learning rate for epoch 181 is 0.003280082019045949\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.7421 - val_loss: 3.5788\n",
      "Epoch 182/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8148\n",
      "Epoch 00182: val_loss did not improve from 3.20260\n",
      "\n",
      "[20210302-1432-24] Learning rate for epoch 182 is 0.0031122982036322355\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.8148 - val_loss: 3.5194\n",
      "Epoch 183/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1442\n",
      "Epoch 00183: val_loss did not improve from 3.20260\n",
      "\n",
      "[20210302-1432-29] Learning rate for epoch 183 is 0.002944914624094963\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.1442 - val_loss: 3.5326\n",
      "Epoch 184/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9565\n",
      "Epoch 00184: val_loss did not improve from 3.20260\n",
      "\n",
      "[20210302-1432-34] Learning rate for epoch 184 is 0.0027779308147728443\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.9565 - val_loss: 3.6327\n",
      "Epoch 185/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6553\n",
      "Epoch 00185: val_loss did not improve from 3.20260\n",
      "\n",
      "[20210302-1432-38] Learning rate for epoch 185 is 0.0026113467756658792\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.6553 - val_loss: 3.4743\n",
      "Epoch 186/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7310\n",
      "Epoch 00186: val_loss did not improve from 3.20260\n",
      "\n",
      "[20210302-1432-43] Learning rate for epoch 186 is 0.002445162972435355\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.7310 - val_loss: 3.7389\n",
      "Epoch 187/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4635\n",
      "Epoch 00187: val_loss did not improve from 3.20260\n",
      "\n",
      "[20210302-1432-48] Learning rate for epoch 187 is 0.0022793791722506285\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.4635 - val_loss: 3.4435\n",
      "Epoch 188/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5072\n",
      "Epoch 00188: val_loss did not improve from 3.20260\n",
      "\n",
      "[20210302-1432-53] Learning rate for epoch 188 is 0.0021139951422810555\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5072 - val_loss: 3.5140\n",
      "Epoch 189/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5807\n",
      "Epoch 00189: val_loss did not improve from 3.20260\n",
      "\n",
      "[20210302-1432-58] Learning rate for epoch 189 is 0.0019490111153572798\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.5807 - val_loss: 3.7517\n",
      "Epoch 190/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1508\n",
      "Epoch 00190: val_loss did not improve from 3.20260\n",
      "\n",
      "[20210302-1433-02] Learning rate for epoch 190 is 0.0017844270914793015\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.1508 - val_loss: 4.3169\n",
      "Epoch 191/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8735\n",
      "Epoch 00191: val_loss did not improve from 3.20260\n",
      "\n",
      "[20210302-1433-07] Learning rate for epoch 191 is 0.0016202429542317986\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.8735 - val_loss: 3.3005\n",
      "Epoch 192/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4223\n",
      "Epoch 00192: val_loss did not improve from 3.20260\n",
      "\n",
      "[20210302-1433-12] Learning rate for epoch 192 is 0.001456458936445415\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.4223 - val_loss: 3.5844\n",
      "Epoch 193/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4124\n",
      "Epoch 00193: val_loss did not improve from 3.20260\n",
      "\n",
      "[20210302-1433-17] Learning rate for epoch 193 is 0.001293074688874185\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.4124 - val_loss: 3.7332\n",
      "Epoch 194/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7282\n",
      "Epoch 00194: val_loss did not improve from 3.20260\n",
      "\n",
      "[20210302-1433-22] Learning rate for epoch 194 is 0.0011300905607640743\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 5.7282 - val_loss: 3.5332\n",
      "Epoch 195/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7797\n",
      "Epoch 00195: val_loss did not improve from 3.20260\n",
      "\n",
      "[20210302-1433-27] Learning rate for epoch 195 is 0.0009675062610767782\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.7797 - val_loss: 3.5609\n",
      "Epoch 196/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5084\n",
      "Epoch 00196: val_loss did not improve from 3.20260\n",
      "\n",
      "[20210302-1433-31] Learning rate for epoch 196 is 0.0008053220226429403\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.5084 - val_loss: 3.4667\n",
      "Epoch 197/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1760\n",
      "Epoch 00197: val_loss improved from 3.20260 to 3.19944, saving model to ./20210301-225844/heel_K14_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1433-37] Learning rate for epoch 197 is 0.0006435376708395779\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 5.1760 - val_loss: 3.1994\n",
      "Epoch 198/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2996\n",
      "Epoch 00198: val_loss did not improve from 3.19944\n",
      "\n",
      "[20210302-1433-42] Learning rate for epoch 198 is 0.0004821533220820129\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.2996 - val_loss: 3.2581\n",
      "Epoch 199/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6095\n",
      "Epoch 00199: val_loss did not improve from 3.19944\n",
      "\n",
      "[20210302-1433-46] Learning rate for epoch 199 is 0.0003211689181625843\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6095 - val_loss: 3.3287\n",
      "Epoch 200/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9916\n",
      "Epoch 00200: val_loss did not improve from 3.19944\n",
      "\n",
      "[20210302-1433-51] Learning rate for epoch 200 is 0.00016058447363320738\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.9916 - val_loss: 3.3245\n",
      "Epoch 201/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4307\n",
      "Epoch 00201: val_loss did not improve from 3.19944\n",
      "\n",
      "[20210302-1433-56] Learning rate for epoch 201 is 4.0000000467443897e-07\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.4307 - val_loss: 3.3189\n",
      "Epoch 202/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6483\n",
      "Epoch 00202: val_loss did not improve from 3.19944\n",
      "\n",
      "[20210302-1434-01] Learning rate for epoch 202 is 0.0001601835247129202\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.6483 - val_loss: 3.3127\n",
      "Epoch 203/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7929\n",
      "Epoch 00203: val_loss did not improve from 3.19944\n",
      "\n",
      "[20210302-1434-06] Learning rate for epoch 203 is 0.00031956707243807614\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.7929 - val_loss: 3.3224\n",
      "Epoch 204/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4347\n",
      "Epoch 00204: val_loss did not improve from 3.19944\n",
      "\n",
      "[20210302-1434-11] Learning rate for epoch 204 is 0.00047855067532509565\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.4347 - val_loss: 3.3353\n",
      "Epoch 205/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3831\n",
      "Epoch 00205: val_loss did not improve from 3.19944\n",
      "\n",
      "[20210302-1434-15] Learning rate for epoch 205 is 0.0006371343042701483\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.3831 - val_loss: 3.3643\n",
      "Epoch 206/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4390\n",
      "Epoch 00206: val_loss did not improve from 3.19944\n",
      "\n",
      "[20210302-1434-20] Learning rate for epoch 206 is 0.0007953179883770645\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.4390 - val_loss: 3.2999\n",
      "Epoch 207/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5904\n",
      "Epoch 00207: val_loss did not improve from 3.19944\n",
      "\n",
      "[20210302-1434-25] Learning rate for epoch 207 is 0.0009531017276458442\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5904 - val_loss: 3.7423\n",
      "Epoch 208/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7022\n",
      "Epoch 00208: val_loss did not improve from 3.19944\n",
      "\n",
      "[20210302-1434-30] Learning rate for epoch 208 is 0.0011104855220764875\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.7022 - val_loss: 3.6887\n",
      "Epoch 209/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8823\n",
      "Epoch 00209: val_loss did not improve from 3.19944\n",
      "\n",
      "[20210302-1434-35] Learning rate for epoch 209 is 0.0012674692552536726\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8823 - val_loss: 3.4017\n",
      "Epoch 210/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4147\n",
      "Epoch 00210: val_loss did not improve from 3.19944\n",
      "\n",
      "[20210302-1434-39] Learning rate for epoch 210 is 0.0014240531018003821\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.4147 - val_loss: 3.5999\n",
      "Epoch 211/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4594\n",
      "Epoch 00211: val_loss did not improve from 3.19944\n",
      "\n",
      "[20210302-1434-44] Learning rate for epoch 211 is 0.0015802369453012943\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.4594 - val_loss: 3.4559\n",
      "Epoch 212/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0036\n",
      "Epoch 00212: val_loss did not improve from 3.19944\n",
      "\n",
      "[20210302-1434-49] Learning rate for epoch 212 is 0.001736020902171731\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 6.0036 - val_loss: 3.3860\n",
      "Epoch 213/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8003\n",
      "Epoch 00213: val_loss did not improve from 3.19944\n",
      "\n",
      "[20210302-1434-54] Learning rate for epoch 213 is 0.0018914048559963703\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8003 - val_loss: 3.7759\n",
      "Epoch 214/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5447\n",
      "Epoch 00214: val_loss did not improve from 3.19944\n",
      "\n",
      "[20210302-1434-59] Learning rate for epoch 214 is 0.0020463888067752123\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.5447 - val_loss: 3.5708\n",
      "Epoch 215/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4516\n",
      "Epoch 00215: val_loss did not improve from 3.19944\n",
      "\n",
      "[20210302-1435-04] Learning rate for epoch 215 is 0.0022009729873389006\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 5.4516 - val_loss: 3.6693\n",
      "Epoch 216/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4349\n",
      "Epoch 00216: val_loss did not improve from 3.19944\n",
      "\n",
      "[20210302-1435-09] Learning rate for epoch 216 is 0.002355156932026148\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.4349 - val_loss: 3.8156\n",
      "Epoch 217/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0473\n",
      "Epoch 00217: val_loss did not improve from 3.19944\n",
      "\n",
      "[20210302-1435-14] Learning rate for epoch 217 is 0.0025089411064982414\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.0473 - val_loss: 3.7270\n",
      "Epoch 218/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9518\n",
      "Epoch 00218: val_loss did not improve from 3.19944\n",
      "\n",
      "[20210302-1435-18] Learning rate for epoch 218 is 0.0026623252779245377\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.9518 - val_loss: 4.6093\n",
      "Epoch 219/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0417\n",
      "Epoch 00219: val_loss did not improve from 3.19944\n",
      "\n",
      "[20210302-1435-30] Learning rate for epoch 219 is 0.0028153094463050365\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.0417 - val_loss: 3.4464\n",
      "Epoch 220/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0404\n",
      "Epoch 00220: val_loss did not improve from 3.19944\n",
      "\n",
      "[20210302-1435-34] Learning rate for epoch 220 is 0.002967893611639738\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.0404 - val_loss: 1524.5592\n",
      "Epoch 221/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7905\n",
      "Epoch 00221: val_loss did not improve from 3.19944\n",
      "\n",
      "[20210302-1435-39] Learning rate for epoch 221 is 0.003120078006759286\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.7905 - val_loss: 1082.2662\n",
      "Epoch 222/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7943\n",
      "Epoch 00222: val_loss did not improve from 3.19944\n",
      "\n",
      "[20210302-1435-44] Learning rate for epoch 222 is 0.0032718623988330364\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.7943 - val_loss: 19222.4395\n",
      "Epoch 223/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4011\n",
      "Epoch 00223: val_loss did not improve from 3.19944\n",
      "\n",
      "[20210302-1435-49] Learning rate for epoch 223 is 0.0034232467878609896\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.4011 - val_loss: 1749.9548\n",
      "Epoch 224/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3838\n",
      "Epoch 00224: val_loss did not improve from 3.19944\n",
      "\n",
      "[20210302-1435-54] Learning rate for epoch 224 is 0.0035742311738431454\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.3838 - val_loss: 359.1869\n",
      "Epoch 225/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2554\n",
      "Epoch 00225: val_loss did not improve from 3.19944\n",
      "\n",
      "[20210302-1435-59] Learning rate for epoch 225 is 0.003724815556779504\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.2554 - val_loss: 13.9423\n",
      "Epoch 226/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3926\n",
      "Epoch 00226: val_loss did not improve from 3.19944\n",
      "\n",
      "[20210302-1436-04] Learning rate for epoch 226 is 0.003874999936670065\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.3926 - val_loss: 6.3675\n",
      "Epoch 227/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5666\n",
      "Epoch 00227: val_loss did not improve from 3.19944\n",
      "\n",
      "[20210302-1436-08] Learning rate for epoch 227 is 0.0037152154836803675\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5666 - val_loss: 4.1361\n",
      "Epoch 228/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8635\n",
      "Epoch 00228: val_loss did not improve from 3.19944\n",
      "\n",
      "[20210302-1436-13] Learning rate for epoch 228 is 0.0035558310337364674\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 5.8635 - val_loss: 5.4229\n",
      "Epoch 229/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3167\n",
      "Epoch 00229: val_loss did not improve from 3.19944\n",
      "\n",
      "[20210302-1436-18] Learning rate for epoch 229 is 0.003396846354007721\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.3167 - val_loss: 4.0439\n",
      "Epoch 230/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8321\n",
      "Epoch 00230: val_loss did not improve from 3.19944\n",
      "\n",
      "[20210302-1436-23] Learning rate for epoch 230 is 0.003238261677324772\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.8321 - val_loss: 5.3066\n",
      "Epoch 231/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6265\n",
      "Epoch 00231: val_loss did not improve from 3.19944\n",
      "\n",
      "[20210302-1436-28] Learning rate for epoch 231 is 0.00308007700368762\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.6265 - val_loss: 4.1450\n",
      "Epoch 232/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4311\n",
      "Epoch 00232: val_loss did not improve from 3.19944\n",
      "\n",
      "[20210302-1436-33] Learning rate for epoch 232 is 0.002922292333096266\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.4311 - val_loss: 4.9270\n",
      "Epoch 233/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7637\n",
      "Epoch 00233: val_loss did not improve from 3.19944\n",
      "\n",
      "[20210302-1436-37] Learning rate for epoch 233 is 0.002764907432720065\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 5.7637 - val_loss: 3.7263\n",
      "Epoch 234/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6722\n",
      "Epoch 00234: val_loss did not improve from 3.19944\n",
      "\n",
      "[20210302-1436-42] Learning rate for epoch 234 is 0.0026079227682203054\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 5.6722 - val_loss: 4.0282\n",
      "Epoch 235/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9392\n",
      "Epoch 00235: val_loss did not improve from 3.19944\n",
      "\n",
      "[20210302-1436-47] Learning rate for epoch 235 is 0.0024513378739356995\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.9392 - val_loss: 4.1209\n",
      "Epoch 236/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6326\n",
      "Epoch 00236: val_loss did not improve from 3.19944\n",
      "\n",
      "[20210302-1436-52] Learning rate for epoch 236 is 0.002295152982696891\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.6326 - val_loss: 3.9569\n",
      "Epoch 237/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6968\n",
      "Epoch 00237: val_loss did not improve from 3.19944\n",
      "\n",
      "[20210302-1436-57] Learning rate for epoch 237 is 0.0021393680945038795\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.6968 - val_loss: 4.7897\n",
      "Epoch 238/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0244\n",
      "Epoch 00238: val_loss did not improve from 3.19944\n",
      "\n",
      "[20210302-1437-02] Learning rate for epoch 238 is 0.0019839832093566656\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.0244 - val_loss: 4.8288\n",
      "Epoch 239/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4296\n",
      "Epoch 00239: val_loss did not improve from 3.19944\n",
      "\n",
      "[20210302-1437-07] Learning rate for epoch 239 is 0.0018289980944246054\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.4296 - val_loss: 3.5246\n",
      "Epoch 240/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6292\n",
      "Epoch 00240: val_loss did not improve from 3.19944\n",
      "\n",
      "[20210302-1437-11] Learning rate for epoch 240 is 0.0016744130989536643\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.6292 - val_loss: 3.8727\n",
      "Epoch 241/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2305\n",
      "Epoch 00241: val_loss did not improve from 3.19944\n",
      "\n",
      "[20210302-1437-16] Learning rate for epoch 241 is 0.0015202279901131988\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.2305 - val_loss: 4.2854\n",
      "Epoch 242/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6466\n",
      "Epoch 00242: val_loss did not improve from 3.19944\n",
      "\n",
      "[20210302-1437-21] Learning rate for epoch 242 is 0.0013664428843185306\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.6466 - val_loss: 3.9660\n",
      "Epoch 243/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3117\n",
      "Epoch 00243: val_loss did not improve from 3.19944\n",
      "\n",
      "[20210302-1437-26] Learning rate for epoch 243 is 0.0012130576651543379\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.3117 - val_loss: 3.3013\n",
      "Epoch 244/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1687\n",
      "Epoch 00244: val_loss did not improve from 3.19944\n",
      "\n",
      "[20210302-1437-31] Learning rate for epoch 244 is 0.0010600725654512644\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.1687 - val_loss: 3.2971\n",
      "Epoch 245/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3260\n",
      "Epoch 00245: val_loss did not improve from 3.19944\n",
      "\n",
      "[20210302-1437-36] Learning rate for epoch 245 is 0.0009074872941710055\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.3260 - val_loss: 3.4069\n",
      "Epoch 246/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0690\n",
      "Epoch 00246: val_loss did not improve from 3.19944\n",
      "\n",
      "[20210302-1437-40] Learning rate for epoch 246 is 0.0007553020259365439\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.0690 - val_loss: 3.4660\n",
      "Epoch 247/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4942\n",
      "Epoch 00247: val_loss did not improve from 3.19944\n",
      "\n",
      "[20210302-1437-45] Learning rate for epoch 247 is 0.0006035167025402188\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.4942 - val_loss: 3.5256\n",
      "Epoch 248/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 4.8948\n",
      "Epoch 00248: val_loss did not improve from 3.19944\n",
      "\n",
      "[20210302-1437-50] Learning rate for epoch 248 is 0.00045213132398203015\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 4.8948 - val_loss: 3.4504\n",
      "Epoch 249/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4738\n",
      "Epoch 00249: val_loss did not improve from 3.19944\n",
      "\n",
      "[20210302-1437-55] Learning rate for epoch 249 is 0.00030114591936580837\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.4738 - val_loss: 3.2324\n",
      "Epoch 250/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4878\n",
      "Epoch 00250: val_loss improved from 3.19944 to 3.16639, saving model to ./20210301-225844/heel_K14_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1438-00] Learning rate for epoch 250 is 0.00015056047413963825\n",
      "20/20 [==============================] - 2s 92ms/step - loss: 5.4878 - val_loss: 3.1664\n",
      "Epoch 251/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1309\n",
      "Epoch 00251: val_loss improved from 3.16639 to 3.16354, saving model to ./20210301-225844/heel_K14_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1438-06] Learning rate for epoch 251 is 3.7500001326407073e-07\n",
      "20/20 [==============================] - 2s 92ms/step - loss: 5.1309 - val_loss: 3.1635\n",
      "Epoch 252/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1311\n",
      "Epoch 00252: val_loss improved from 3.16354 to 3.16084, saving model to ./20210301-225844/heel_K14_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1438-11] Learning rate for epoch 252 is 0.00015015952521935105\n",
      "20/20 [==============================] - 2s 92ms/step - loss: 5.1311 - val_loss: 3.1608\n",
      "Epoch 253/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2858\n",
      "Epoch 00253: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1438-16] Learning rate for epoch 253 is 0.0002995440736413002\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 5.2858 - val_loss: 3.2158\n",
      "Epoch 254/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8156\n",
      "Epoch 00254: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1438-21] Learning rate for epoch 254 is 0.0004485286772251129\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.8156 - val_loss: 3.2264\n",
      "Epoch 255/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3314\n",
      "Epoch 00255: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1438-26] Learning rate for epoch 255 is 0.0005971133359707892\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.3314 - val_loss: 3.2473\n",
      "Epoch 256/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1595\n",
      "Epoch 00256: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1438-31] Learning rate for epoch 256 is 0.0007452979916706681\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.1595 - val_loss: 3.3052\n",
      "Epoch 257/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5323\n",
      "Epoch 00257: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1438-35] Learning rate for epoch 257 is 0.0008930827025324106\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.5323 - val_loss: 3.6370\n",
      "Epoch 258/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5581\n",
      "Epoch 00258: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1438-40] Learning rate for epoch 258 is 0.0010404675267636776\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.5581 - val_loss: 3.2989\n",
      "Epoch 259/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2530\n",
      "Epoch 00259: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1438-45] Learning rate for epoch 259 is 0.0011874522315338254\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.2530 - val_loss: 3.3916\n",
      "Epoch 260/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4199\n",
      "Epoch 00260: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1438-50] Learning rate for epoch 260 is 0.0013340371660888195\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.4199 - val_loss: 4.0285\n",
      "Epoch 261/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7796\n",
      "Epoch 00261: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1438-55] Learning rate for epoch 261 is 0.0014802219811826944\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.7796 - val_loss: 3.3461\n",
      "Epoch 262/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5508\n",
      "Epoch 00262: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1439-00] Learning rate for epoch 262 is 0.0016260069096460938\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5508 - val_loss: 3.9043\n",
      "Epoch 263/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8590\n",
      "Epoch 00263: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1439-05] Learning rate for epoch 263 is 0.001771391835063696\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.8590 - val_loss: 3.3149\n",
      "Epoch 264/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2686\n",
      "Epoch 00264: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1439-09] Learning rate for epoch 264 is 0.0019163768738508224\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.2686 - val_loss: 3.4887\n",
      "Epoch 265/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2482\n",
      "Epoch 00265: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1439-14] Learning rate for epoch 265 is 0.0020609619095921516\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.2482 - val_loss: 3.6804\n",
      "Epoch 266/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9694\n",
      "Epoch 00266: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1439-19] Learning rate for epoch 266 is 0.0022051469422876835\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.9694 - val_loss: 3.8018\n",
      "Epoch 267/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4625\n",
      "Epoch 00267: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1439-24] Learning rate for epoch 267 is 0.0023489322047680616\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.4625 - val_loss: 3.9592\n",
      "Epoch 268/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6732\n",
      "Epoch 00268: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1439-29] Learning rate for epoch 268 is 0.002492317231371999\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6732 - val_loss: 5.1878\n",
      "Epoch 269/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.0157\n",
      "Epoch 00269: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1439-34] Learning rate for epoch 269 is 0.0026353024877607822\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.0157 - val_loss: 3.5166\n",
      "Epoch 270/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8162\n",
      "Epoch 00270: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1439-38] Learning rate for epoch 270 is 0.0027778877411037683\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.8162 - val_loss: 3.6051\n",
      "Epoch 271/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2188\n",
      "Epoch 00271: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1439-43] Learning rate for epoch 271 is 0.002920072991400957\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.2188 - val_loss: 3.7465\n",
      "Epoch 272/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3842\n",
      "Epoch 00272: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1439-48] Learning rate for epoch 272 is 0.0030618582386523485\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.3842 - val_loss: 3.8899\n",
      "Epoch 273/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3524\n",
      "Epoch 00273: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1439-53] Learning rate for epoch 273 is 0.0032032437156885862\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.3524 - val_loss: 4.1204\n",
      "Epoch 274/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.5029\n",
      "Epoch 00274: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1439-58] Learning rate for epoch 274 is 0.0033442291896790266\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 6.5029 - val_loss: 6.5582\n",
      "Epoch 275/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.2332\n",
      "Epoch 00275: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1440-03] Learning rate for epoch 275 is 0.003484814427793026\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 6.2332 - val_loss: 4.0264\n",
      "Epoch 276/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4819\n",
      "Epoch 00276: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1440-07] Learning rate for epoch 276 is 0.0036249998956918716\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.4819 - val_loss: 5.0652\n",
      "Epoch 277/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4801\n",
      "Epoch 00277: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1440-12] Learning rate for epoch 277 is 0.0034752145875245333\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.4801 - val_loss: 4.9009\n",
      "Epoch 278/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5856\n",
      "Epoch 00278: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1440-17] Learning rate for epoch 278 is 0.003325828816741705\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5856 - val_loss: 3.9443\n",
      "Epoch 279/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2812\n",
      "Epoch 00279: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1440-22] Learning rate for epoch 279 is 0.0031768432818353176\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.2812 - val_loss: 3.9112\n",
      "Epoch 280/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3406\n",
      "Epoch 00280: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1440-27] Learning rate for epoch 280 is 0.0030282577499747276\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.3406 - val_loss: 4.0653\n",
      "Epoch 281/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4179\n",
      "Epoch 00281: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1440-32] Learning rate for epoch 281 is 0.0028800719883292913\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.4179 - val_loss: 4.9003\n",
      "Epoch 282/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6141\n",
      "Epoch 00282: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1440-36] Learning rate for epoch 282 is 0.0027322862297296524\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.6141 - val_loss: 7.9056\n",
      "Epoch 283/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4345\n",
      "Epoch 00283: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1440-41] Learning rate for epoch 283 is 0.002584900474175811\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.4345 - val_loss: 6.7371\n",
      "Epoch 284/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7189\n",
      "Epoch 00284: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1440-46] Learning rate for epoch 284 is 0.0024379147216677666\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.7189 - val_loss: 4.3158\n",
      "Epoch 285/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5060\n",
      "Epoch 00285: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1440-51] Learning rate for epoch 285 is 0.0022913289722055197\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5060 - val_loss: 4.9645\n",
      "Epoch 286/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7028\n",
      "Epoch 00286: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1440-56] Learning rate for epoch 286 is 0.0021451429929584265\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.7028 - val_loss: 3.8744\n",
      "Epoch 287/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6700\n",
      "Epoch 00287: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1441-01] Learning rate for epoch 287 is 0.0019993570167571306\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.6700 - val_loss: 3.7600\n",
      "Epoch 288/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2544\n",
      "Epoch 00288: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1441-06] Learning rate for epoch 288 is 0.001853971160016954\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.2544 - val_loss: 3.7012\n",
      "Epoch 289/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 4.9627\n",
      "Epoch 00289: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1441-10] Learning rate for epoch 289 is 0.001708985073491931\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 4.9627 - val_loss: 3.5347\n",
      "Epoch 290/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3563\n",
      "Epoch 00290: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1441-15] Learning rate for epoch 290 is 0.0015643991064280272\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.3563 - val_loss: 3.4037\n",
      "Epoch 291/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6003\n",
      "Epoch 00291: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1441-20] Learning rate for epoch 291 is 0.0014202130259945989\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.6003 - val_loss: 4.1494\n",
      "Epoch 292/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1878\n",
      "Epoch 00292: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1441-25] Learning rate for epoch 292 is 0.001276426832191646\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.1878 - val_loss: 3.3282\n",
      "Epoch 293/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1527\n",
      "Epoch 00293: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1441-30] Learning rate for epoch 293 is 0.0011330407578498125\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.1527 - val_loss: 3.3992\n",
      "Epoch 294/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4459\n",
      "Epoch 00294: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1441-35] Learning rate for epoch 294 is 0.0009900545701384544\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.4459 - val_loss: 3.5362\n",
      "Epoch 295/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2039\n",
      "Epoch 00295: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1441-40] Learning rate for epoch 295 is 0.0008474682690575719\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.2039 - val_loss: 3.9486\n",
      "Epoch 296/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7291\n",
      "Epoch 00296: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1441-44] Learning rate for epoch 296 is 0.0007052819710224867\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.7291 - val_loss: 3.3938\n",
      "Epoch 297/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5312\n",
      "Epoch 00297: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1441-49] Learning rate for epoch 297 is 0.0005634956760331988\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.5312 - val_loss: 3.9152\n",
      "Epoch 298/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4164\n",
      "Epoch 00298: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1441-54] Learning rate for epoch 298 is 0.0004221093258820474\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.4164 - val_loss: 3.4970\n",
      "Epoch 299/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1196\n",
      "Epoch 00299: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1441-59] Learning rate for epoch 299 is 0.00028112292056903243\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.1196 - val_loss: 3.5590\n",
      "Epoch 300/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.0095\n",
      "Epoch 00300: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1442-04] Learning rate for epoch 300 is 0.0001405364746460691\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.0095 - val_loss: 3.5251\n",
      "Epoch 301/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 4.9806\n",
      "Epoch 00301: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1442-08] Learning rate for epoch 301 is 3.4999999343199306e-07\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 4.9806 - val_loss: 3.5396\n",
      "Epoch 302/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 4.9106\n",
      "Epoch 00302: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1442-13] Learning rate for epoch 302 is 0.00014013552572578192\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 4.9106 - val_loss: 3.4270\n",
      "Epoch 303/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 4.7686\n",
      "Epoch 00303: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1442-18] Learning rate for epoch 303 is 0.00027952107484452426\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 4.7686 - val_loss: 3.3523\n",
      "Epoch 304/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6661\n",
      "Epoch 00304: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1442-23] Learning rate for epoch 304 is 0.0004185066791251302\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.6661 - val_loss: 3.3262\n",
      "Epoch 305/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3558\n",
      "Epoch 00305: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1442-28] Learning rate for epoch 305 is 0.0005570923094637692\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.3558 - val_loss: 3.7630\n",
      "Epoch 306/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 4.9589\n",
      "Epoch 00306: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1442-33] Learning rate for epoch 306 is 0.0006952779949642718\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 4.9589 - val_loss: 3.4186\n",
      "Epoch 307/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3035\n",
      "Epoch 00307: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1442-37] Learning rate for epoch 307 is 0.0008330637356266379\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.3035 - val_loss: 3.5458\n",
      "Epoch 308/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2942\n",
      "Epoch 00308: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1442-42] Learning rate for epoch 308 is 0.0009704494732432067\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.2942 - val_loss: 3.6612\n",
      "Epoch 309/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5963\n",
      "Epoch 00309: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1442-47] Learning rate for epoch 309 is 0.0011074353242293\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.5963 - val_loss: 3.6556\n",
      "Epoch 310/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3932\n",
      "Epoch 00310: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1442-52] Learning rate for epoch 310 is 0.001244021113961935\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.3932 - val_loss: 4.6113\n",
      "Epoch 311/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3594\n",
      "Epoch 00311: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1442-57] Learning rate for epoch 311 is 0.0013802070170640945\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.3594 - val_loss: 3.8916\n",
      "Epoch 312/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4657\n",
      "Epoch 00312: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1443-02] Learning rate for epoch 312 is 0.0015159929171204567\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.4657 - val_loss: 3.9472\n",
      "Epoch 313/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1607\n",
      "Epoch 00313: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1443-06] Learning rate for epoch 313 is 0.0016513789305463433\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.1607 - val_loss: 3.6471\n",
      "Epoch 314/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5537\n",
      "Epoch 00314: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1443-11] Learning rate for epoch 314 is 0.0017863648245111108\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.5537 - val_loss: 3.7405\n",
      "Epoch 315/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3453\n",
      "Epoch 00315: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1443-16] Learning rate for epoch 315 is 0.0019209509482607245\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.3453 - val_loss: 4.4339\n",
      "Epoch 316/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9070\n",
      "Epoch 00316: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1443-21] Learning rate for epoch 316 is 0.002055136952549219\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 5.9070 - val_loss: 3.7889\n",
      "Epoch 317/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1813\n",
      "Epoch 00317: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1443-26] Learning rate for epoch 317 is 0.002188923070207238\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.1813 - val_loss: 4.1758\n",
      "Epoch 318/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4328\n",
      "Epoch 00318: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1443-31] Learning rate for epoch 318 is 0.00232230918481946\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.4328 - val_loss: 3.6623\n",
      "Epoch 319/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4932\n",
      "Epoch 00319: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1443-35] Learning rate for epoch 319 is 0.002455295529216528\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.4932 - val_loss: 3.9836\n",
      "Epoch 320/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.7197\n",
      "Epoch 00320: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1443-40] Learning rate for epoch 320 is 0.002587881637737155\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.7197 - val_loss: 5.7228\n",
      "Epoch 321/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6691\n",
      "Epoch 00321: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1443-45] Learning rate for epoch 321 is 0.0027200679760426283\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.6691 - val_loss: 5.2649\n",
      "Epoch 322/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4922\n",
      "Epoch 00322: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1443-50] Learning rate for epoch 322 is 0.0028518543113023043\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.4922 - val_loss: 4.1933\n",
      "Epoch 323/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6099\n",
      "Epoch 00323: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1443-55] Learning rate for epoch 323 is 0.002983240643516183\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.6099 - val_loss: 3.9691\n",
      "Epoch 324/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6646\n",
      "Epoch 00324: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1444-00] Learning rate for epoch 324 is 0.003114226972684264\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.6646 - val_loss: 4.9415\n",
      "Epoch 325/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.1995\n",
      "Epoch 00325: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1444-05] Learning rate for epoch 325 is 0.0032448135316371918\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 6.1995 - val_loss: 4.6465\n",
      "Epoch 326/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6690\n",
      "Epoch 00326: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1444-10] Learning rate for epoch 326 is 0.003375000087544322\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.6690 - val_loss: 4.3717\n",
      "Epoch 327/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8577\n",
      "Epoch 00327: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1444-14] Learning rate for epoch 327 is 0.0032352134585380554\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.8577 - val_loss: 4.6717\n",
      "Epoch 328/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9128\n",
      "Epoch 00328: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1444-19] Learning rate for epoch 328 is 0.003095826832577586\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.9128 - val_loss: 4.9677\n",
      "Epoch 329/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.3918\n",
      "Epoch 00329: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1444-24] Learning rate for epoch 329 is 0.0029568402096629143\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 6.3918 - val_loss: 7.1831\n",
      "Epoch 330/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.9933\n",
      "Epoch 00330: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1444-29] Learning rate for epoch 330 is 0.0028182535897940397\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.9933 - val_loss: 5.5305\n",
      "Epoch 331/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5491\n",
      "Epoch 00331: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1444-34] Learning rate for epoch 331 is 0.0026800669729709625\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 5.5491 - val_loss: 4.7235\n",
      "Epoch 332/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4985\n",
      "Epoch 00332: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1444-39] Learning rate for epoch 332 is 0.0025422803591936827\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.4985 - val_loss: 3.5877\n",
      "Epoch 333/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8024\n",
      "Epoch 00333: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1444-44] Learning rate for epoch 333 is 0.0024048935156315565\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.8024 - val_loss: 3.9535\n",
      "Epoch 334/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5328\n",
      "Epoch 00334: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1444-48] Learning rate for epoch 334 is 0.0022679066751152277\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.5328 - val_loss: 4.3730\n",
      "Epoch 335/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6014\n",
      "Epoch 00335: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1444-53] Learning rate for epoch 335 is 0.0021313198376446962\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.6014 - val_loss: 4.3210\n",
      "Epoch 336/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3848\n",
      "Epoch 00336: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1444-58] Learning rate for epoch 336 is 0.001995133003219962\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.3848 - val_loss: 4.4796\n",
      "Epoch 337/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.6032\n",
      "Epoch 00337: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1445-03] Learning rate for epoch 337 is 0.0018593460554257035\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.6032 - val_loss: 5.9416\n",
      "Epoch 338/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 6.8067\n",
      "Epoch 00338: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1445-08] Learning rate for epoch 338 is 0.0017239591106772423\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 6.8067 - val_loss: 8.7900\n",
      "Epoch 339/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5324\n",
      "Epoch 00339: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1445-13] Learning rate for epoch 339 is 0.0015889721689745784\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.5324 - val_loss: 3.8242\n",
      "Epoch 340/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3621\n",
      "Epoch 00340: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1445-18] Learning rate for epoch 340 is 0.00145438511390239\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.3621 - val_loss: 4.5603\n",
      "Epoch 341/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2811\n",
      "Epoch 00341: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1445-23] Learning rate for epoch 341 is 0.0013201979454606771\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.2811 - val_loss: 3.8137\n",
      "Epoch 342/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4558\n",
      "Epoch 00342: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1445-27] Learning rate for epoch 342 is 0.0011864108964800835\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.4558 - val_loss: 4.1076\n",
      "Epoch 343/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.5080\n",
      "Epoch 00343: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1445-32] Learning rate for epoch 343 is 0.0010530237341299653\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 5.5080 - val_loss: 3.7450\n",
      "Epoch 344/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2849\n",
      "Epoch 00344: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1445-37] Learning rate for epoch 344 is 0.0009200365166179836\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.2849 - val_loss: 3.6630\n",
      "Epoch 345/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.2407\n",
      "Epoch 00345: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1445-42] Learning rate for epoch 345 is 0.0007874493021517992\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.2407 - val_loss: 3.5837\n",
      "Epoch 346/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.1397\n",
      "Epoch 00346: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1445-47] Learning rate for epoch 346 is 0.0006552619743160903\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.1397 - val_loss: 3.4664\n",
      "Epoch 347/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3138\n",
      "Epoch 00347: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1445-51] Learning rate for epoch 347 is 0.0005234747077338398\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.3138 - val_loss: 3.5539\n",
      "Epoch 348/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3542\n",
      "Epoch 00348: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1445-56] Learning rate for epoch 348 is 0.0003920873277820647\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.3542 - val_loss: 3.4750\n",
      "Epoch 349/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4482\n",
      "Epoch 00349: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1446-01] Learning rate for epoch 349 is 0.0002610999217722565\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 5.4482 - val_loss: 3.4895\n",
      "Epoch 350/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.0287\n",
      "Epoch 00350: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1446-06] Learning rate for epoch 350 is 0.00013051247515249997\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 5.0287 - val_loss: 3.4987\n",
      "Epoch 351/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.3746\n",
      "Epoch 00351: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1446-11] Learning rate for epoch 351 is 3.250000020216248e-07\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 5.3746 - val_loss: 3.4707\n",
      "Epoch 352/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.0007\n",
      "Epoch 00352: val_loss did not improve from 3.16084\n",
      "\n",
      "[20210302-1446-15] Learning rate for epoch 352 is 0.00013011152623221278\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 5.0007 - val_loss: 3.4727\n",
      "K= 15\n",
      "Check cache in memory:Y True\n",
      "Check keypoints process01: Tensor(\"args_1:0\", shape=(2,), dtype=int64) (2,) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Check keypoints process02: Tensor(\"Reshape:0\", shape=(1, 2), dtype=int64) (1, 2) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Check keypoints process03: Tensor(\"PyFunc:1\", dtype=float32, device=/job:localhost/replica:0/task:0)\n",
      "Check keypoints process04: Tensor(\"Reshape_1:0\", shape=(2,), dtype=float32)\n",
      "Check augment :Y True\n",
      "Check ds cache[True] and augment[True]\n",
      "Check cache in memory:Y True\n",
      "Check augment :N False\n",
      "Check ds cache[True] and augment[False]\n",
      "best_model_name: ./20210301-225844/heel_K15_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "base_model.trainable :  False\n",
      "\n",
      "[20210302-1446-18] Learning rate for epoch 1 is 0.00047855067532509565\n",
      "Epoch 1/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 106.7304\n",
      "Epoch 00001: val_loss improved from inf to 77.13185, saving model to ./20210301-225844/heel_K15_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 5s 267ms/step - loss: 104.4445 - val_loss: 77.1319\n",
      "\n",
      "[20210302-1446-34] Learning rate for epoch 2 is 0.00047855067532509565\n",
      "Epoch 2/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 41.3442\n",
      "Epoch 00002: val_loss improved from 77.13185 to 25.09061, saving model to ./20210301-225844/heel_K15_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 40.7517 - val_loss: 25.0906\n",
      "\n",
      "[20210302-1446-39] Learning rate for epoch 3 is 0.00047855067532509565\n",
      "Epoch 3/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 23.1826\n",
      "Epoch 00003: val_loss improved from 25.09061 to 23.32869, saving model to ./20210301-225844/heel_K15_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 8s 409ms/step - loss: 23.1826 - val_loss: 23.3287\n",
      "\n",
      "[20210302-1446-50] Learning rate for epoch 4 is 0.00047855067532509565\n",
      "Epoch 4/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 20.9703\n",
      "Epoch 00004: val_loss improved from 23.32869 to 22.25252, saving model to ./20210301-225844/heel_K15_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 20.9519 - val_loss: 22.2525\n",
      "\n",
      "[20210302-1446-55] Learning rate for epoch 5 is 0.00047855067532509565\n",
      "Epoch 5/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 19.5061\n",
      "Epoch 00005: val_loss improved from 22.25252 to 21.63883, saving model to ./20210301-225844/heel_K15_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 19.5628 - val_loss: 21.6388\n",
      "\n",
      "[20210302-1446-59] Learning rate for epoch 6 is 0.00047855067532509565\n",
      "Epoch 6/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 19.3447\n",
      "Epoch 00006: val_loss improved from 21.63883 to 21.17675, saving model to ./20210301-225844/heel_K15_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 19.1933 - val_loss: 21.1767\n",
      "\n",
      "[20210302-1447-03] Learning rate for epoch 7 is 0.00047855067532509565\n",
      "Epoch 7/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 18.7323\n",
      "Epoch 00007: val_loss improved from 21.17675 to 20.40689, saving model to ./20210301-225844/heel_K15_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 18.8426 - val_loss: 20.4069\n",
      "\n",
      "[20210302-1447-08] Learning rate for epoch 8 is 0.00047855067532509565\n",
      "Epoch 8/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 18.4771\n",
      "Epoch 00008: val_loss improved from 20.40689 to 19.65855, saving model to ./20210301-225844/heel_K15_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 18.3638 - val_loss: 19.6586\n",
      "\n",
      "[20210302-1447-12] Learning rate for epoch 9 is 0.00047855067532509565\n",
      "Epoch 9/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 17.3717\n",
      "Epoch 00009: val_loss did not improve from 19.65855\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 17.3717 - val_loss: 19.7340\n",
      "\n",
      "[20210302-1447-16] Learning rate for epoch 10 is 0.00047855067532509565\n",
      "Epoch 10/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 17.1266\n",
      "Epoch 00010: val_loss improved from 19.65855 to 18.87660, saving model to ./20210301-225844/heel_K15_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 17.1126 - val_loss: 18.8766\n",
      "\n",
      "[20210302-1447-20] Learning rate for epoch 11 is 0.00047855067532509565\n",
      "Epoch 11/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 17.6118\n",
      "Epoch 00011: val_loss improved from 18.87660 to 18.78854, saving model to ./20210301-225844/heel_K15_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 17.6118 - val_loss: 18.7885\n",
      "\n",
      "[20210302-1447-25] Learning rate for epoch 12 is 0.00047855067532509565\n",
      "Epoch 12/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 17.6887\n",
      "Epoch 00012: val_loss improved from 18.78854 to 18.33394, saving model to ./20210301-225844/heel_K15_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 17.6225 - val_loss: 18.3339\n",
      "\n",
      "[20210302-1447-29] Learning rate for epoch 13 is 0.00047855067532509565\n",
      "Epoch 13/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 17.1139\n",
      "Epoch 00013: val_loss improved from 18.33394 to 17.56190, saving model to ./20210301-225844/heel_K15_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 17.1139 - val_loss: 17.5619\n",
      "\n",
      "[20210302-1447-33] Learning rate for epoch 14 is 0.00047855067532509565\n",
      "Epoch 14/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 17.2672\n",
      "Epoch 00014: val_loss did not improve from 17.56190\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 17.2864 - val_loss: 17.5801\n",
      "\n",
      "[20210302-1447-37] Learning rate for epoch 15 is 0.00047855067532509565\n",
      "Epoch 15/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.5601\n",
      "Epoch 00015: val_loss improved from 17.56190 to 16.76633, saving model to ./20210301-225844/heel_K15_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.5371 - val_loss: 16.7663\n",
      "\n",
      "[20210302-1447-42] Learning rate for epoch 16 is 0.00047855067532509565\n",
      "Epoch 16/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.7671\n",
      "Epoch 00016: val_loss improved from 16.76633 to 16.74655, saving model to ./20210301-225844/heel_K15_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 16.7671 - val_loss: 16.7465\n",
      "\n",
      "[20210302-1447-46] Learning rate for epoch 17 is 0.00047855067532509565\n",
      "Epoch 17/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 17.5776\n",
      "Epoch 00017: val_loss improved from 16.74655 to 16.13910, saving model to ./20210301-225844/heel_K15_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 17.3664 - val_loss: 16.1391\n",
      "\n",
      "[20210302-1447-50] Learning rate for epoch 18 is 0.00047855067532509565\n",
      "Epoch 18/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 17.1050\n",
      "Epoch 00018: val_loss did not improve from 16.13910\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 17.1050 - val_loss: 16.4260\n",
      "\n",
      "[20210302-1447-54] Learning rate for epoch 19 is 0.00047855067532509565\n",
      "Epoch 19/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 16.4773\n",
      "Epoch 00019: val_loss improved from 16.13910 to 15.76502, saving model to ./20210301-225844/heel_K15_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 16.4176 - val_loss: 15.7650\n",
      "\n",
      "[20210302-1447-58] Learning rate for epoch 20 is 0.00047855067532509565\n",
      "Epoch 20/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.4259\n",
      "Epoch 00020: val_loss improved from 15.76502 to 15.34092, saving model to ./20210301-225844/heel_K15_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.5117 - val_loss: 15.3409\n",
      "\n",
      "[20210302-1448-03] Learning rate for epoch 21 is 0.00047855067532509565\n",
      "Epoch 21/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.8203\n",
      "Epoch 00021: val_loss improved from 15.34092 to 14.53760, saving model to ./20210301-225844/heel_K15_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 16.8203 - val_loss: 14.5376\n",
      "\n",
      "[20210302-1448-07] Learning rate for epoch 22 is 0.00047855067532509565\n",
      "Epoch 22/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.0278\n",
      "Epoch 00022: val_loss improved from 14.53760 to 14.11552, saving model to ./20210301-225844/heel_K15_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.0278 - val_loss: 14.1155\n",
      "\n",
      "[20210302-1448-11] Learning rate for epoch 23 is 0.00047855067532509565\n",
      "Epoch 23/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.3778\n",
      "Epoch 00023: val_loss did not improve from 14.11552\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.4051 - val_loss: 14.7293\n",
      "\n",
      "[20210302-1448-15] Learning rate for epoch 24 is 0.00047855067532509565\n",
      "Epoch 24/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.7901\n",
      "Epoch 00024: val_loss improved from 14.11552 to 13.42774, saving model to ./20210301-225844/heel_K15_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 16.8300 - val_loss: 13.4277\n",
      "\n",
      "[20210302-1448-19] Learning rate for epoch 25 is 0.00047855067532509565\n",
      "Epoch 25/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 16.5380\n",
      "Epoch 00025: val_loss improved from 13.42774 to 13.01430, saving model to ./20210301-225844/heel_K15_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 16.6092 - val_loss: 13.0143\n",
      "\n",
      "[20210302-1448-24] Learning rate for epoch 26 is 0.00047855067532509565\n",
      "Epoch 26/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 16.4713\n",
      "Epoch 00026: val_loss did not improve from 13.01430\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 16.5120 - val_loss: 13.8368\n",
      "\n",
      "[20210302-1448-28] Learning rate for epoch 27 is 0.00047855067532509565\n",
      "Epoch 27/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.3951\n",
      "Epoch 00027: val_loss improved from 13.01430 to 12.08428, saving model to ./20210301-225844/heel_K15_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.4536 - val_loss: 12.0843\n",
      "\n",
      "[20210302-1448-32] Learning rate for epoch 28 is 0.00047855067532509565\n",
      "Epoch 28/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.8595\n",
      "Epoch 00028: val_loss did not improve from 12.08428\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.8595 - val_loss: 12.3366\n",
      "\n",
      "[20210302-1448-36] Learning rate for epoch 29 is 0.00047855067532509565\n",
      "Epoch 29/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.6247\n",
      "Epoch 00029: val_loss improved from 12.08428 to 11.98481, saving model to ./20210301-225844/heel_K15_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 16.5611 - val_loss: 11.9848\n",
      "\n",
      "[20210302-1448-40] Learning rate for epoch 30 is 0.00047855067532509565\n",
      "Epoch 30/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 16.2249\n",
      "Epoch 00030: val_loss did not improve from 11.98481\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.3899 - val_loss: 12.1623\n",
      "\n",
      "[20210302-1448-44] Learning rate for epoch 31 is 0.00047855067532509565\n",
      "Epoch 31/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.1177\n",
      "Epoch 00031: val_loss improved from 11.98481 to 10.91471, saving model to ./20210301-225844/heel_K15_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 16.0883 - val_loss: 10.9147\n",
      "\n",
      "[20210302-1448-49] Learning rate for epoch 32 is 0.00047855067532509565\n",
      "Epoch 32/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.8545\n",
      "Epoch 00032: val_loss improved from 10.91471 to 10.85811, saving model to ./20210301-225844/heel_K15_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 15.9429 - val_loss: 10.8581\n",
      "\n",
      "[20210302-1448-53] Learning rate for epoch 33 is 0.00047855067532509565\n",
      "Epoch 33/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.2698\n",
      "Epoch 00033: val_loss improved from 10.85811 to 10.67231, saving model to ./20210301-225844/heel_K15_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 16.2693 - val_loss: 10.6723\n",
      "\n",
      "[20210302-1448-57] Learning rate for epoch 34 is 0.00047855067532509565\n",
      "Epoch 34/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 16.6468\n",
      "Epoch 00034: val_loss did not improve from 10.67231\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.5734 - val_loss: 11.1069\n",
      "\n",
      "[20210302-1449-01] Learning rate for epoch 35 is 0.00047855067532509565\n",
      "Epoch 35/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.4732\n",
      "Epoch 00035: val_loss did not improve from 10.67231\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.5999 - val_loss: 11.0685\n",
      "\n",
      "[20210302-1449-05] Learning rate for epoch 36 is 0.00047855067532509565\n",
      "Epoch 36/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.9069\n",
      "Epoch 00036: val_loss did not improve from 10.67231\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 15.8522 - val_loss: 11.8593\n",
      "\n",
      "[20210302-1449-09] Learning rate for epoch 37 is 0.00047855067532509565\n",
      "Epoch 37/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.1964\n",
      "Epoch 00037: val_loss did not improve from 10.67231\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.2095 - val_loss: 10.9664\n",
      "\n",
      "[20210302-1449-13] Learning rate for epoch 38 is 0.00047855067532509565\n",
      "Epoch 38/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 16.2446\n",
      "Epoch 00038: val_loss did not improve from 10.67231\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 16.2029 - val_loss: 10.7630\n",
      "\n",
      "[20210302-1449-17] Learning rate for epoch 39 is 0.00047855067532509565\n",
      "Epoch 39/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.2838\n",
      "Epoch 00039: val_loss did not improve from 10.67231\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 16.3249 - val_loss: 10.7855\n",
      "\n",
      "[20210302-1449-21] Learning rate for epoch 40 is 0.00047855067532509565\n",
      "Epoch 40/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 16.3892\n",
      "Epoch 00040: val_loss did not improve from 10.67231\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 16.2840 - val_loss: 10.8228\n",
      "\n",
      "[20210302-1449-25] Learning rate for epoch 41 is 0.00047855067532509565\n",
      "Epoch 41/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.1543\n",
      "Epoch 00041: val_loss did not improve from 10.67231\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.2320 - val_loss: 11.6999\n",
      "\n",
      "[20210302-1449-29] Learning rate for epoch 42 is 0.00047855067532509565\n",
      "Epoch 42/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.8866\n",
      "Epoch 00042: val_loss did not improve from 10.67231\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 16.0311 - val_loss: 11.2466\n",
      "\n",
      "[20210302-1449-33] Learning rate for epoch 43 is 0.00047855067532509565\n",
      "Epoch 43/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.8083\n",
      "Epoch 00043: val_loss did not improve from 10.67231\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.8322 - val_loss: 10.9034\n",
      "\n",
      "[20210302-1449-37] Learning rate for epoch 44 is 0.00047855067532509565\n",
      "Epoch 44/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 16.2442\n",
      "Epoch 00044: val_loss did not improve from 10.67231\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 16.0866 - val_loss: 10.6761\n",
      "\n",
      "[20210302-1449-41] Learning rate for epoch 45 is 0.00047855067532509565\n",
      "Epoch 45/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.2819\n",
      "Epoch 00045: val_loss improved from 10.67231 to 10.23253, saving model to ./20210301-225844/heel_K15_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 15.4278 - val_loss: 10.2325\n",
      "\n",
      "[20210302-1449-45] Learning rate for epoch 46 is 0.00047855067532509565\n",
      "Epoch 46/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.9355\n",
      "Epoch 00046: val_loss did not improve from 10.23253\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 15.9116 - val_loss: 11.3032\n",
      "\n",
      "[20210302-1449-49] Learning rate for epoch 47 is 0.00047855067532509565\n",
      "Epoch 47/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.7915\n",
      "Epoch 00047: val_loss did not improve from 10.23253\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.7915 - val_loss: 10.9936\n",
      "\n",
      "[20210302-1449-53] Learning rate for epoch 48 is 0.00047855067532509565\n",
      "Epoch 48/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.5138\n",
      "Epoch 00048: val_loss did not improve from 10.23253\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.3951 - val_loss: 11.1389\n",
      "\n",
      "[20210302-1449-57] Learning rate for epoch 49 is 0.00047855067532509565\n",
      "Epoch 49/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 15.5323\n",
      "Epoch 00049: val_loss did not improve from 10.23253\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.5569 - val_loss: 11.7205\n",
      "\n",
      "[20210302-1450-01] Learning rate for epoch 50 is 0.00047855067532509565\n",
      "Epoch 50/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 15.8818\n",
      "Epoch 00050: val_loss did not improve from 10.23253\n",
      "20/20 [==============================] - 0s 24ms/step - loss: 15.8062 - val_loss: 11.2389\n",
      "\n",
      "[20210302-1450-05] Learning rate for epoch 51 is 0.00047855067532509565\n",
      "Epoch 51/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 16.5219\n",
      "Epoch 00051: val_loss did not improve from 10.23253\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.5219 - val_loss: 11.2111\n",
      "\n",
      "[20210302-1450-09] Learning rate for epoch 52 is 0.00047855067532509565\n",
      "Epoch 52/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.2695\n",
      "Epoch 00052: val_loss did not improve from 10.23253\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 16.3031 - val_loss: 11.4240\n",
      "\n",
      "[20210302-1450-13] Learning rate for epoch 53 is 0.00047855067532509565\n",
      "Epoch 53/500\n",
      "18/20 [==========================>...] - ETA: 0s - loss: 16.2382\n",
      "Epoch 00053: val_loss did not improve from 10.23253\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 16.2281 - val_loss: 13.0232\n",
      "\n",
      "[20210302-1450-17] Learning rate for epoch 54 is 0.00047855067532509565\n",
      "Epoch 54/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 16.1804\n",
      "Epoch 00054: val_loss did not improve from 10.23253\n",
      "20/20 [==============================] - 0s 24ms/step - loss: 16.2360 - val_loss: 12.3162\n",
      "\n",
      "[20210302-1450-21] Learning rate for epoch 55 is 0.00047855067532509565\n",
      "Epoch 55/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 14.8693\n",
      "Epoch 00055: val_loss did not improve from 10.23253\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 14.8037 - val_loss: 10.9194\n",
      "\n",
      "[20210302-1450-25] Learning rate for epoch 56 is 0.00047855067532509565\n",
      "Epoch 56/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.7337\n",
      "Epoch 00056: val_loss did not improve from 10.23253\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.8017 - val_loss: 11.7810\n",
      "\n",
      "[20210302-1450-29] Learning rate for epoch 57 is 0.00047855067532509565\n",
      "Epoch 57/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.7394\n",
      "Epoch 00057: val_loss did not improve from 10.23253\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 15.6880 - val_loss: 11.0493\n",
      "\n",
      "[20210302-1450-32] Learning rate for epoch 58 is 0.00047855067532509565\n",
      "Epoch 58/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.4067\n",
      "Epoch 00058: val_loss did not improve from 10.23253\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.4307 - val_loss: 10.5257\n",
      "\n",
      "[20210302-1450-36] Learning rate for epoch 59 is 0.00047855067532509565\n",
      "Epoch 59/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.3858\n",
      "Epoch 00059: val_loss did not improve from 10.23253\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.5020 - val_loss: 11.0784\n",
      "\n",
      "[20210302-1450-40] Learning rate for epoch 60 is 0.00047855067532509565\n",
      "Epoch 60/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.8583\n",
      "Epoch 00060: val_loss did not improve from 10.23253\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.8540 - val_loss: 11.5277\n",
      "\n",
      "[20210302-1450-44] Learning rate for epoch 61 is 0.00047855067532509565\n",
      "Epoch 61/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.2743\n",
      "Epoch 00061: val_loss did not improve from 10.23253\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.2997 - val_loss: 10.6685\n",
      "\n",
      "[20210302-1450-48] Learning rate for epoch 62 is 0.00047855067532509565\n",
      "Epoch 62/500\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.5503\n",
      "Epoch 00062: val_loss did not improve from 10.23253\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.5503 - val_loss: 10.4625\n",
      "\n",
      "[20210302-1450-52] Learning rate for epoch 63 is 0.00047855067532509565\n",
      "Epoch 63/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.9623\n",
      "Epoch 00063: val_loss did not improve from 10.23253\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.9971 - val_loss: 10.8180\n",
      "\n",
      "[20210302-1450-56] Learning rate for epoch 64 is 0.00047855067532509565\n",
      "Epoch 64/500\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 15.9460\n",
      "Epoch 00064: val_loss did not improve from 10.23253\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 15.8654 - val_loss: 10.3808\n",
      "\n",
      "[20210302-1451-00] Learning rate for epoch 65 is 0.00047855067532509565\n",
      "Epoch 65/500\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 15.9990\n",
      "Epoch 00065: val_loss did not improve from 10.23253\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 15.8551 - val_loss: 10.4412\n",
      "\n",
      "*********************************** Start fine tune ***********************************\n",
      "tt: 195, nt:46, total layers:241\n",
      "*********************************** Start fine tune ***********************************\n",
      "Epoch 1/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 25.2586\n",
      "Epoch 00001: val_loss did not improve from 10.23253\n",
      "\n",
      "[20210302-1451-34] Learning rate for epoch 1 is 4.999999987376214e-07\n",
      "20/20 [==============================] - 14s 690ms/step - loss: 25.2586 - val_loss: 16.2943\n",
      "Epoch 2/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 22.9916\n",
      "Epoch 00002: val_loss did not improve from 10.23253\n",
      "\n",
      "[20210302-1451-39] Learning rate for epoch 2 is 5.044450517743826e-05\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 22.9916 - val_loss: 16.4672\n",
      "Epoch 3/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 17.7176\n",
      "Epoch 00003: val_loss did not improve from 10.23253\n",
      "\n",
      "[20210302-1452-00] Learning rate for epoch 3 is 0.00010028902033809572\n",
      "20/20 [==============================] - 18s 884ms/step - loss: 17.7176 - val_loss: 12.0129\n",
      "Epoch 4/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 15.2908\n",
      "Epoch 00004: val_loss improved from 10.23253 to 9.87987, saving model to ./20210301-225844/heel_K15_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1452-06] Learning rate for epoch 4 is 0.00015003354928921908\n",
      "20/20 [==============================] - 2s 91ms/step - loss: 15.2908 - val_loss: 9.8799\n",
      "Epoch 5/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 14.1088\n",
      "Epoch 00005: val_loss did not improve from 9.87987\n",
      "\n",
      "[20210302-1452-11] Learning rate for epoch 5 is 0.0001996780774788931\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 14.1088 - val_loss: 11.2579\n",
      "Epoch 6/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.5927\n",
      "Epoch 00006: val_loss improved from 9.87987 to 9.08543, saving model to ./20210301-225844/heel_K15_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1452-16] Learning rate for epoch 6 is 0.000249222619459033\n",
      "20/20 [==============================] - 2s 93ms/step - loss: 13.5927 - val_loss: 9.0854\n",
      "Epoch 7/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 13.4171\n",
      "Epoch 00007: val_loss improved from 9.08543 to 8.88101, saving model to ./20210301-225844/heel_K15_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1452-21] Learning rate for epoch 7 is 0.0002986671752296388\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 13.4171 - val_loss: 8.8810\n",
      "Epoch 8/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 12.9904\n",
      "Epoch 00008: val_loss did not improve from 8.88101\n",
      "\n",
      "[20210302-1452-26] Learning rate for epoch 8 is 0.00034801175934262574\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 12.9904 - val_loss: 11.8684\n",
      "Epoch 9/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 12.7892\n",
      "Epoch 00009: val_loss did not improve from 8.88101\n",
      "\n",
      "[20210302-1452-31] Learning rate for epoch 9 is 0.00039725631359033287\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 12.7892 - val_loss: 10.0230\n",
      "Epoch 10/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 12.5654\n",
      "Epoch 00010: val_loss did not improve from 8.88101\n",
      "\n",
      "[20210302-1452-36] Learning rate for epoch 10 is 0.0004464008961804211\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 12.5654 - val_loss: 14.8362\n",
      "Epoch 11/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.8370\n",
      "Epoch 00011: val_loss did not improve from 8.88101\n",
      "\n",
      "[20210302-1452-41] Learning rate for epoch 11 is 0.0004954455071128905\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 11.8370 - val_loss: 11.4176\n",
      "Epoch 12/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.8343\n",
      "Epoch 00012: val_loss did not improve from 8.88101\n",
      "\n",
      "[20210302-1452-46] Learning rate for epoch 12 is 0.0005443901172839105\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 11.8343 - val_loss: 9.8204\n",
      "Epoch 13/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.3324\n",
      "Epoch 00013: val_loss improved from 8.88101 to 7.19611, saving model to ./20210301-225844/heel_K15_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1452-51] Learning rate for epoch 13 is 0.0005932347266934812\n",
      "20/20 [==============================] - 2s 91ms/step - loss: 11.3324 - val_loss: 7.1961\n",
      "Epoch 14/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 11.0705\n",
      "Epoch 00014: val_loss improved from 7.19611 to 5.58827, saving model to ./20210301-225844/heel_K15_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1452-56] Learning rate for epoch 14 is 0.0006419793353416026\n",
      "20/20 [==============================] - 2s 87ms/step - loss: 11.0705 - val_loss: 5.5883\n",
      "Epoch 15/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 10.1310\n",
      "Epoch 00015: val_loss improved from 5.58827 to 4.63480, saving model to ./20210301-225844/heel_K15_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1453-02] Learning rate for epoch 15 is 0.0006906240014359355\n",
      "20/20 [==============================] - 2s 89ms/step - loss: 10.1310 - val_loss: 4.6348\n",
      "Epoch 16/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 9.8748\n",
      "Epoch 00016: val_loss improved from 4.63480 to 4.34146, saving model to ./20210301-225844/heel_K15_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1453-07] Learning rate for epoch 16 is 0.0007391686085611582\n",
      "20/20 [==============================] - 2s 93ms/step - loss: 9.8748 - val_loss: 4.3415\n",
      "Epoch 17/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.5958\n",
      "Epoch 00017: val_loss did not improve from 4.34146\n",
      "\n",
      "[20210302-1453-12] Learning rate for epoch 17 is 0.0007876132731325924\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 8.5958 - val_loss: 5.8950\n",
      "Epoch 18/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.6115\n",
      "Epoch 00018: val_loss did not improve from 4.34146\n",
      "\n",
      "[20210302-1453-17] Learning rate for epoch 18 is 0.0008359579369425774\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 8.6115 - val_loss: 7.5484\n",
      "Epoch 19/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.4828\n",
      "Epoch 00019: val_loss did not improve from 4.34146\n",
      "\n",
      "[20210302-1453-21] Learning rate for epoch 19 is 0.000884202599991113\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 8.4828 - val_loss: 14.2689\n",
      "Epoch 20/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.5656\n",
      "Epoch 00020: val_loss did not improve from 4.34146\n",
      "\n",
      "[20210302-1453-26] Learning rate for epoch 20 is 0.0009323473204858601\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 8.5656 - val_loss: 7.3362\n",
      "Epoch 21/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.4988\n",
      "Epoch 00021: val_loss did not improve from 4.34146\n",
      "\n",
      "[20210302-1453-31] Learning rate for epoch 21 is 0.000980391982011497\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 8.4988 - val_loss: 4.5770\n",
      "Epoch 22/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.3098\n",
      "Epoch 00022: val_loss did not improve from 4.34146\n",
      "\n",
      "[20210302-1453-36] Learning rate for epoch 22 is 0.0010283367009833455\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 8.3098 - val_loss: 7.7178\n",
      "Epoch 23/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5682\n",
      "Epoch 00023: val_loss improved from 4.34146 to 4.26758, saving model to ./20210301-225844/heel_K15_EfficientNetB0_bs64_w120_best_val_loss.h5\n",
      "\n",
      "[20210302-1453-41] Learning rate for epoch 23 is 0.0010761814191937447\n",
      "20/20 [==============================] - 2s 92ms/step - loss: 7.5682 - val_loss: 4.2676\n",
      "Epoch 24/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7371\n",
      "Epoch 00024: val_loss did not improve from 4.26758\n",
      "\n",
      "[20210302-1453-46] Learning rate for epoch 24 is 0.0011239261366426945\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.7371 - val_loss: 4.7089\n",
      "Epoch 25/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.2863\n",
      "Epoch 00025: val_loss did not improve from 4.26758\n",
      "\n",
      "[20210302-1453-51] Learning rate for epoch 25 is 0.001171570853330195\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 8.2863 - val_loss: 5.1591\n",
      "Epoch 26/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 8.1821\n",
      "Epoch 00026: val_loss did not improve from 4.26758\n",
      "\n",
      "[20210302-1453-56] Learning rate for epoch 26 is 0.001219115569256246\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 8.1821 - val_loss: 5.9773\n",
      "Epoch 27/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.4340\n",
      "Epoch 00027: val_loss did not improve from 4.26758\n",
      "\n",
      "[20210302-1454-01] Learning rate for epoch 27 is 0.0012665604008361697\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 7.4340 - val_loss: 5.4398\n",
      "Epoch 28/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.5872\n",
      "Epoch 00028: val_loss did not improve from 4.26758\n",
      "\n",
      "[20210302-1454-06] Learning rate for epoch 28 is 0.0013139051152393222\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 7.5872 - val_loss: 4.2955\n",
      "Epoch 29/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.9815\n",
      "Epoch 00029: val_loss did not improve from 4.26758\n",
      "\n",
      "[20210302-1454-10] Learning rate for epoch 29 is 0.0013611499452963471\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 7.9815 - val_loss: 7.1836\n",
      "Epoch 30/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.8069\n",
      "Epoch 00030: val_loss did not improve from 4.26758\n",
      "\n",
      "[20210302-1454-15] Learning rate for epoch 30 is 0.001408294658176601\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 7.8069 - val_loss: 12.3985\n",
      "Epoch 31/1000\n",
      "20/20 [==============================] - ETA: 0s - loss: 7.7462\n",
      "Epoch 00031: val_loss did not improve from 4.26758\n",
      "\n",
      "[20210302-1454-20] Learning rate for epoch 31 is 0.0014553394867107272\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 7.7462 - val_loss: 10.0798\n",
      "Epoch 32/1000\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# big K = 5 (fold 0 ~ 4) \n",
    "# KFlodNum = 5 # follow Toe's K.\n",
    "\n",
    "\n",
    "\n",
    "history_heel = []\n",
    "history_heel_finetune = []\n",
    "\n",
    "#above until 'train_ds_map_toe now' to 'train_ds_map_toe_s', 'valid_ds_map_toe_s'\n",
    "for k in range(KFlodNum):\n",
    "    \n",
    "    \n",
    "    # Split data to train/valid with K-Fold #\n",
    "    print(\"K=\", k)\n",
    "#     # Toe split\n",
    "#     train_ds_map_toe_s, valid_ds_map_toe_s = get_KFold_ds(train_ds_map_toe, K=k)\n",
    "    \n",
    "#     # Toe ds_pre\n",
    "#     train_ds_pre_toe_s = configure_for_performance_cache_train(train_ds_map_toe_s, augment=True)\n",
    "#     valid_ds_pre_toe_s = configure_for_performance_cache_val(valid_ds_map_toe_s)\n",
    "    \n",
    "    \n",
    "    # heel split\n",
    "    train_ds_map_heel_s, valid_ds_map_heel_s = get_KFold_ds(train_ds_map_heel, K=k)\n",
    "    # Heel ds_pre\n",
    "    train_ds_pre_heel_s = configure_for_performance_cache_train_AToe(train_ds_map_heel_s, augment=True)\n",
    "    valid_ds_pre_heel_s = configure_for_performance_cache_val(valid_ds_map_heel_s)\n",
    "    \n",
    "    \n",
    "    # Train K-Model with transfer learnling #\n",
    "    \n",
    "    # Toe model, TL\n",
    "    #th = 'toe'\n",
    "    th = 'heel'\n",
    "    best_model_name = get_best_model_name(th, K=str(k))\n",
    "    best_model_save = tf.keras.callbacks.ModelCheckpoint(filepath=best_model_name, \n",
    "                                 save_best_only = True, \n",
    "                                 save_weights_only = False,\n",
    "                                 monitor = monitor, \n",
    "                                 mode = 'auto', verbose = 1)\n",
    "    callbacks_heel_tl = [\n",
    "                    #     tensorboard_callback,\n",
    "                        best_model_save,\n",
    "                        tf.keras.callbacks.EarlyStopping(monitor=monitor, patience=20), #patience=step_size or ep_num\n",
    "                    #     lr_reduceonplateau,\n",
    "                        tf.keras.callbacks.LearningRateScheduler(lrdump),#lrdump, decay or lrfn or lrfn2. clr\n",
    "                        PrintLRtoe()\n",
    "                        ]\n",
    "    callbacks_heel_fn = [\n",
    "                    #     tensorboard_callback,\n",
    "                        best_model_save,\n",
    "                        tf.keras.callbacks.EarlyStopping(monitor=monitor, patience=100), #patience=step_size or ep_num\n",
    "                    #     lr_reduceonplateau,\n",
    "                        tf.keras.callbacks.LearningRateScheduler(clr3),#lrdump, decay or lrfn or lrfn2. clr, CosineDecayCLRWarmUp, CosineDecayCLRWarmUpLSW\n",
    "                        PrintLRheel()\n",
    "                    ]\n",
    "    print('best_model_name:', best_model_name)\n",
    "\n",
    "\n",
    "    top_dropout_rate = 0.4 #less dp rate, say 0.1, train_loss will lower than val_loss\n",
    "    drop_connect_rate = 0.4 #for efnet This parameter serves as a toggle for extra regularization in finetuning, but does not affect loaded weights.\n",
    "    outputnum = 2\n",
    "    with strategy.scope():\n",
    "        model_heel = build_efn_model(outputnum, top_dropout_rate, drop_connect_rate)\n",
    "    # fit the model on all data\n",
    "    hist = model_heel.fit(train_ds_pre_heel_s, \n",
    "                          verbose=1, \n",
    "                          epochs=ep_num_transf, \n",
    "                          validation_data=valid_ds_pre_heel_s, \n",
    "                          callbacks=callbacks_heel_tl)#, validation_split=0.1)\n",
    "    history_heel.append(hist)\n",
    "    \n",
    "      \n",
    "    # Train K-Model with fine tune #\n",
    "    \n",
    "    # Toe model, FT\n",
    "    unfreeze_model(model_heel)\n",
    "    count_model_trainOrNot_layers(model_heel)\n",
    "    # fit the model on all data\n",
    "    hist = model_heel.fit(train_ds_pre_heel_s, \n",
    "                          verbose=1, \n",
    "                          epochs=ep_num, \n",
    "                          validation_data=valid_ds_pre_heel_s, \n",
    "                          callbacks=callbacks_heel_fn)#, validation_split=0.1)\n",
    "    history_heel_finetune.append(hist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ED sum\n",
    "def get_valloss(his_v_l):  \n",
    "    return np.min(his_v_l), np.argmin(his_v_l)\n",
    "\n",
    "# t_vl = []\n",
    "h_vl = []\n",
    "for k in range(KFlodNum):\n",
    "    print(f'K:{k}')\n",
    "#     t_v, _ = get_valloss(history_toe_finetune[k].history['val_loss'])\n",
    "    h_v, _ = get_valloss(history_heel_finetune[k].history['val_loss'])\n",
    "    \n",
    "#     t_vl.append(t_v)\n",
    "    h_vl.append(h_v)\n",
    "\n",
    "# t_vl = np.mean(t_vl, axis=0)\n",
    "# h_vl = np.mean(h_vl, axis=0)\n",
    "# print(f'{round(t_vl,5)} + {round(h_vl,5)} = {round(t_vl + h_vl,5)}')\n",
    "\n",
    "# t_vl\n",
    "h_vl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(f'{log_dir_name}/heel_FNED.txt', h_vl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum plot losses heel-tl\n",
    "\n",
    "plt.figure(figsize=(25, 10))\n",
    "\n",
    "for k in range(KFlodNum):\n",
    "\n",
    "    plt.plot(history_heel[k].history['loss'])\n",
    "    plt.plot(history_heel[k].history['val_loss'])\n",
    "\n",
    "    \n",
    "plt.title('K-model ed loss heel-TL')\n",
    "plt.ylabel('ed loss'), plt.ylim(5, 50)# for too large loss\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['k0 loss', 'k0 val_loss', 'k1 loss', 'k1 val_loss', 'k2 loss', 'k2 val_loss', \n",
    "            'k3 loss', 'k3 val_loss', 'k4 loss', 'k4 val_loss'], loc='upper left') \n",
    "# plt.show()\n",
    "\n",
    "# save plot : comment plo.show in jupyter notebook.\n",
    "plt.savefig(f'{log_dir_name}/{log_dir_name}_heel_Ksum_TL.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Single plot loss heel-tl\n",
    "\n",
    "for k in range(KFlodNum):\n",
    "    plt.figure(figsize=(25, 10))\n",
    "    plt.plot(history_heel[k].history['loss'])\n",
    "    plt.plot(history_heel[k].history['val_loss'])\n",
    "    plt.title('K-model ed loss heel-TL')\n",
    "    plt.ylabel('ed loss'), plt.ylim(5, 80)# for too large loss\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['loss', 'val_loss'], loc='upper left') \n",
    "    # plt.show()\n",
    "\n",
    "    # save plot : comment plo.show in jupyter notebook.\n",
    "    def get_valloss(his_v_l):   \n",
    "        return np.min(his_v_l), np.argmin(his_v_l)\n",
    "\n",
    "    vl, ep = get_valloss(history_heel[k].history['val_loss'])\n",
    "    plt.savefig(f'{log_dir_name}/{log_dir_name}_heel_K{k}_TL_clr_ed{round(vl,4)}@{ep}.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the loos the model trained.\n",
    "\n",
    "def get_valloss(his_v_l):  \n",
    "    return np.min(his_v_l), np.argmin(his_v_l)\n",
    "\n",
    "# t_vl = []\n",
    "h_vl = []\n",
    "for k in range(KFlodNum):\n",
    "    print(f'K:{k}')\n",
    "#     t_v, _ = get_valloss(history_toe_finetune[k].history['val_loss'])\n",
    "    h_v, _ = get_valloss(history_heel_finetune[k].history['val_loss'])\n",
    "    \n",
    "    h_vl.append(h_v)\n",
    "    \n",
    "# for different scales (different Y-axes)\n",
    "# fig, ax1 = plt.subplots()\n",
    "fig, ax1 = plt.subplots(figsize=(25, 10))\n",
    "\n",
    "# nice to have this colorful tip.\n",
    "color = 'tab:red'\n",
    "\n",
    "ax1.set_title('[ heel_finetune ] \\n ED loss')\n",
    "\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.set_ylabel('ed loss', color=color)\n",
    "\n",
    "for k in range(KFlodNum):\n",
    "\n",
    "    plt.plot(history_heel_finetune[k].history['loss'])\n",
    "    plt.plot(history_heel_finetune[k].history['val_loss'])\n",
    "\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "# ax1.legend(['loss', 'val_loss'], loc='upper center') # legend may ocvered by next ax ploting. moved to end.\n",
    "ax1.legend(['k0 loss', 'k0 val_loss', 'k1 loss', 'k1 val_loss', 'k2 loss', 'k2 val_loss', \n",
    "            'k3 loss', 'k3 val_loss', 'k4 loss', 'k4 val_loss'], loc='upper right') \n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('learning rate', color=color)\n",
    "ax2.plot(history_heel_finetune[0].history['lr'], color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.legend(['lr'], loc='upper right') \n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# # save plot : comment plo.show in jupyter notebook.\n",
    "# def get_valloss(his_v_l):   \n",
    "#     return np.min(his_v_l), np.argmin(his_v_l)\n",
    "\n",
    "# vl, ep = get_valloss(history_toe_finetune.history['val_loss'])\n",
    "\n",
    "\n",
    "# t_vl = np.mean(t_vl, axis=0)\n",
    "h_vl = np.mean(h_vl, axis=0)\n",
    "plt.savefig(f'{log_dir_name}/{log_dir_name}_heel_ft_Ksum-clr_ed{round(h_vl,4)}.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum plot losses heel-ft\n",
    "\n",
    "plt.figure(figsize=(25, 10))\n",
    "\n",
    "for k in range(KFlodNum):\n",
    "\n",
    "    plt.plot(history_heel_finetune[k].history['loss'])\n",
    "    plt.plot(history_heel_finetune[k].history['val_loss'])\n",
    "\n",
    "    \n",
    "plt.title('K-model ed loss heel-FT')\n",
    "plt.ylabel('ed loss'), plt.ylim(2, 20)# for too large loss\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['k0 loss', 'k0 val_loss', 'k1 loss', 'k1 val_loss', 'k2 loss', 'k2 val_loss', \n",
    "            'k3 loss', 'k3 val_loss', 'k4 loss', 'k4 val_loss'], loc='upper left') \n",
    "# plt.show()\n",
    "\n",
    "# save plot : comment plo.show in jupyter notebook.\n",
    "plt.savefig(f'{log_dir_name}/{log_dir_name}_heel_Ksum_FT.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Single plot loss heel-FT\n",
    "\n",
    "for k in range(KFlodNum):\n",
    "    plt.figure(figsize=(25, 10))\n",
    "    plt.plot(history_heel_finetune[k].history['loss'])\n",
    "    plt.plot(history_heel_finetune[k].history['val_loss'])\n",
    "    plt.title('K-model ed loss heel-FT')\n",
    "    plt.ylabel('ed loss'), plt.ylim(2, 20)# for too large loss\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['loss', 'val_loss'], loc='upper left') \n",
    "    # plt.show()\n",
    "\n",
    "    # save plot : comment plo.show in jupyter notebook.\n",
    "    def get_valloss(his_v_l):   \n",
    "        return np.min(his_v_l), np.argmin(his_v_l)\n",
    "\n",
    "    vl, ep = get_valloss(history_heel_finetune[k].history['val_loss'])\n",
    "    plt.savefig(f'{log_dir_name}/{log_dir_name}_heel_K{k}_FT_clr_ed{round(vl,4)}@{ep}.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ploting your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model_toe.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # show after modl.fit\n",
    "# model_toe.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check metrics the model have.\n",
    "# history_toe.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.utils.plot_model(model_toe, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from tensorflow.keras.utils import plot_model\n",
    "# plot_model(model_toe, to_file='model_toe_conv_layer_blocks.png', show_shapes=True)\n",
    "# from IPython.display import Image\n",
    "# Image(filename='model_toe_conv_layer_blocks.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## show pp pred\n",
    "\n",
    "* we can switch toe/hell by comment it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EFN Toe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it_valid_ds_pre_toe_s = iter(valid_ds_pre_toe_s)\n",
    "# # it_valid_ds_pre_heel_s = iter(valid_ds_pre_heel_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # image_batch, label_batch = next(valid_ds_pre_toe_s)\n",
    "\n",
    "# image_batch, label_batch = next(it_valid_ds_pre_toe_s)\n",
    "# # image_batch, label_batch = next(it_valid_ds_pre_heel_s)\n",
    "\n",
    "\n",
    "# pred = model_toe.predict_on_batch(image_batch) #predictions\n",
    "# # pred = model.predict_on_batch(image_batch) #Simple 2D CNN model predictions\n",
    "\n",
    "# plt.figure(figsize=(20, 20))\n",
    "# for i in range(64):\n",
    "#     ax = plt.subplot(8, 8, i + 1)\n",
    "#     plt.imshow(image_batch[i])\n",
    "#     plt.title(label_batch[i].numpy())\n",
    "#     plt.axis(\"off\")\n",
    "    \n",
    "#     # note: y_offset_toe for ds image\n",
    "    \n",
    "#     #ground truth\n",
    "#     plt.plot(label_batch[i].numpy()[0], label_batch[i].numpy()[1], 'r+', markersize=15, mew=2)\n",
    "\n",
    "#     #pred\n",
    "#     plt.plot(pred[i][0], pred[i][1], 'k+', markersize=15, mew=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #test_ds一次做完即可不用分batch\n",
    "# neg = label_batch - pred\n",
    "# neg[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.abs(neg)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neg_abs = np.abs(neg)\n",
    "# neg_abs.mean(axis=0)#所有x 所有y個別平均  neg.mean(axis=0)#所有x 所有y個別平均"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ED 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # y_pred = neg_abs.mean(axis=0)\n",
    "# ed_metric_2d([0,0], [neg_abs.mean(axis=0)]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EFN Heel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # it_valid_ds_pre_toe_s = iter(valid_ds_pre_toe_s)\n",
    "# it_valid_ds_pre_heel_s = iter(valid_ds_pre_heel_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # image_batch, label_batch = next(valid_ds_pre_toe_s)\n",
    "\n",
    "# # image_batch, label_batch = next(it_valid_ds_pre_toe_s)\n",
    "# image_batch, label_batch = next(it_valid_ds_pre_heel_s)\n",
    "\n",
    "\n",
    "# pred = model_heel.predict_on_batch(image_batch) #predictions\n",
    "# # pred = model.predict_on_batch(image_batch) #Simple 2D CNN model predictions\n",
    "\n",
    "# plt.figure(figsize=(20, 20))\n",
    "# for i in range(64):\n",
    "#     ax = plt.subplot(8, 8, i + 1)\n",
    "#     plt.imshow(image_batch[i])\n",
    "#     plt.title(label_batch[i].numpy())\n",
    "#     plt.axis(\"off\")\n",
    "    \n",
    "#     # note: y_offset_toe for ds image\n",
    "    \n",
    "#     #ground truth\n",
    "#     plt.plot(label_batch[i].numpy()[0], label_batch[i].numpy()[1], 'r+', markersize=15, mew=2)\n",
    "\n",
    "#     #pred\n",
    "#     plt.plot(pred[i][0], pred[i][1], 'k+', markersize=15, mew=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_batch[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #test_ds一次做完即可不用分batch\n",
    "# neg = label_batch - pred\n",
    "# neg[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.abs(neg)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neg_abs = np.abs(neg)\n",
    "# neg_abs.mean(axis=0)#所有x 所有y個別平均  neg.mean(axis=0)#所有x 所有y個別平均"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ED 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # y_pred = neg_abs.mean(axis=0)\n",
    "# ed_metric_2d([0,0], [neg_abs.mean(axis=0)]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merg Toe/Heel model and predict the Test data at once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TEST DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = 'test_submission.csv'\n",
    "df_ts = pd.read_csv(ts)\n",
    "df_ts.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ts['images']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataframe\n",
    "list_ds_test = tf.data.Dataset.from_tensor_slices(df_ts['images'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_ds_test)#.shape() #take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check The type specification of an element of this dataset.\n",
    "list_ds_test.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in list_ds_test.take(5):\n",
    "    print(f'take test sample: {f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST DS: Process TEST path to image tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST-DS: re-used from train/val-ds\n",
    "\n",
    "im_test = 'test_images/'\n",
    "\n",
    "'''\n",
    "def get_label(file_path):\n",
    "    # convert the path to a list of path components\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    # The second to last is the class-directory\n",
    "    boolen = parts[-2] == class_names\n",
    "    #one_hot_num = np.array(boolen, dtype=np.int) not works should use tf.x repalced.\n",
    "    one_hot_num = tf.dtypes.cast(boolen, tf.int64)\n",
    "    one_num = tf.argmax(one_hot_num)\n",
    "    print('one_num:', one_num)\n",
    "    # Integer encode the label\n",
    "    return one_num\n",
    "\n",
    "def decode_img(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    # resize the image to the desired size\n",
    "#     return tf.image.resize(img, [img_height, img_width])# augment 已經resize過一次了 但這邊不先做會比較慢\n",
    "    return tf.cast(tf.image.resize(img, [img_height, img_width]), tf.uint8)# 避免float over at augment\n",
    "'''\n",
    "\n",
    "#\n",
    "# map list to ds, Toe part.\n",
    "#\n",
    "\n",
    "def decode_crop_png_toe_test(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.io.decode_png(img, channels=3)\n",
    "    # crop the toe from top-left corner [image, offset_height y1, offset_width x1, target_height, target_width]\n",
    "    y1=y_offset_toe;    x1=0;    h=img_height;    w=img_width # not the pp location\n",
    "    img = tf.image.crop_to_bounding_box(img, int(y1), int(x1), h, w)\n",
    "    #img = tf.image.crop_to_bounding_box(img, int(y1), int(x1), int(y2)-int(y1), int(x2)-int(x1))\n",
    "    # resize the image to the desired size\n",
    "    return img\n",
    "\n",
    "def process_path_toe_test(file_name):\n",
    "    file_path = im_test + file_name\n",
    "    #label = get_label(file_path)\n",
    "    # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)#can read the byte string paths b'image_0001.png'\n",
    "    img = decode_crop_png_toe_test(img)\n",
    "    return img, file_name\n",
    "\n",
    "#\n",
    "# map list to ds, Heel part.\n",
    "#\n",
    "\n",
    "def decode_crop_png_heel_test(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.io.decode_png(img, channels=3)\n",
    "    # crop the toe from top-left corner [image, offset_height y1, offset_width x1, target_height, target_width]\n",
    "    y2=y_offset_heel;    x2=0;    h=img_height;    w=img_width # not the pp location\n",
    "    img = tf.image.crop_to_bounding_box(img, int(y2), int(x2), h, w)\n",
    "    #img = tf.image.crop_to_bounding_box(img, int(y1), int(x1), int(y2)-int(y1), int(x2)-int(x1))\n",
    "    # resize the image to the desired size\n",
    "    return img\n",
    "\n",
    "def process_path_heel_test(file_name):\n",
    "    file_path = im_test + file_name\n",
    "    #label = get_label(file_path)\n",
    "    # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)#can read the byte string paths b'image_0001.png'\n",
    "    img = decode_crop_png_heel_test(img)\n",
    "    return img, file_name\n",
    "\n",
    "\n",
    "#\n",
    "# test how to put parameters to map\n",
    "#\n",
    "\n",
    "def t_ds_map(file_path,x1,y1,x2,y2):\n",
    "#     img = get_img('train/images/' + str(file_path))\n",
    "#     print(file_path)\n",
    "    return file_path,x1,y1,x2,y2 #img, [x1,y1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Toe ds\n",
    "test_ds_map_toe = list_ds_test.map(process_path_toe_test, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "# TEST Heel ds\n",
    "test_ds_map_heel = list_ds_test.map(process_path_heel_test, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, file_name in test_ds_map_toe.take(5):\n",
    "    print(f'take sample: {img.shape} {file_name}')\n",
    "    \n",
    "# print('f', f.dtype)\n",
    "# print('xy', xy.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare TEST_ds_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_for_performance_cache_test(ds, cache=True):\n",
    "\n",
    "    if cache:\n",
    "        ds = ds.cache()\n",
    "        print(\"Check cache in memory:Y\", cache)\n",
    "    else:\n",
    "        print(\"Check cache in memory:N\", cache)\n",
    "        \n",
    "#     if augment:\n",
    "# #         ds = ds.map(data_augment, num_parallel_calls=AUTOTUNE)\n",
    "#         ds = ds.map(AA, num_parallel_calls=AUTOTUNE)\n",
    "# #         ds = ds.map(RA, num_parallel_calls=AUTOTUNE)\n",
    "#         print(\"Check augment :Y\", augment)\n",
    "#     else:\n",
    "#         print(\"Check augment :N\", augment)\n",
    "    \n",
    "#     #ds = ds.repeat()#TODO:2020-12-14: test\n",
    "#     ds = ds.shuffle(buffer_size=MULTI_BATCH_SIZE*2) # (buffer_size=MULTI_BATCH_SIZE*5) 6sec,buffer_size=1000 take few sec. or buffer_size=image_count <- take too long # each take ds take 30~45 sec, TODO!!\n",
    "#     ds = ds.shuffle(len(list_ds), reshuffle_each_iteration=False) #todo: move to ds_pre. see https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle\n",
    "    ds = ds.batch(1000)# 1k for foot test images #MULTI_BATCH_SIZE for multi-GPUs\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE) #buffer_size=AUTOTUNE seem no speed improve\n",
    "    \n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prepare the ds properties (cache, augment, bs, shuffle, prefetch, etc.) for better performance.\n",
    "\"\"\"\n",
    "# TEST Toe ds_pre\n",
    "test_ds_pre_toe = configure_for_performance_cache_test(test_ds_map_toe)\n",
    "\n",
    "# TEST Heel ds_pre\n",
    "test_ds_pre_heel = configure_for_performance_cache_test(test_ds_map_heel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Best-K-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if K-models are in last time frame\n",
    "# best_model_name = get_best_model_name(th, K=str(k))\n",
    "\n",
    "predictions_toe = []\n",
    "predictions_heel = []\n",
    "\n",
    "for k in range(KFlodNum):\n",
    "    print(f'K:{k}')\n",
    "    best_model_toe_name = get_best_model_name('toe', K=str(k))\n",
    "    best_model_heel_name = get_best_model_name('heel', K=str(k))\n",
    "\n",
    "# # if models are in last time frame\n",
    "# best_model_toe_name = get_best_model_name('toe')\n",
    "# best_model_heel_name = get_best_model_name('heel')\n",
    "\n",
    "# # if toe/heel are in different time frame\n",
    "# best_model_toe_name = '20210118-212454/toe_EfficientNetB0_bs64_w120_best_val_loss.h5'#6.3318 @e393\n",
    "# best_model_heel_name = '20210122-084854/heel_EfficientNetB0_bs64_w120_best_val_loss.h5'#3.27979@152\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(best_model_toe_name)\n",
    "    print(best_model_heel_name)\n",
    "    # log_dir_name + '/' + 'leaf-2020-12-01-EfficientNetB7_top-layer50_lr_lrfn_val-acc.8352_wh512_e37.h5'\n",
    "\n",
    "    best_model_toe = tf.keras.models.load_model(best_model_toe_name,compile=False)\n",
    "    best_model_heel = tf.keras.models.load_model(best_model_heel_name,compile=False)\n",
    "    \n",
    "    best_model_toe.compile(optimizer = tf.keras.optimizers.Adam(),#RMSprop , Adam\n",
    "                loss=ed_metric_2d_mean)#, ed_loss ed_metric_2d ed_metric_2d_mean            \n",
    "                #metrics=['mae', 'accuracy'])\n",
    "    best_model_heel.compile(optimizer = tf.keras.optimizers.Adam(),#RMSprop , Adam\n",
    "                loss=ed_metric_2d_mean)#, ed_loss ed_metric_2d ed_metric_2d_mean            \n",
    "                #metrics=['mae', 'accuracy'])\n",
    "        \n",
    "        \n",
    "    pred_toe = best_model_toe.predict(test_ds_pre_toe)\n",
    "    pred_toe[:,1] = pred_toe[:,1] + y_offset_toe\n",
    "    predictions_toe.append(pred_toe)\n",
    "    \n",
    "    pred_heel = best_model_heel.predict(test_ds_pre_heel)\n",
    "    pred_heel[:,1] = pred_heel[:,1] + y_offset_heel\n",
    "    predictions_heel.append(pred_heel)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions_toe[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "# make sure we got the k-pred as k models.\n",
    "for i, _ in enumerate(predictions_toe):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(predictions_toe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_toe[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_toe[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_heel[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_heel[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean the k-predictions\n",
    "k_predictions_toe = np.mean(predictions_toe, axis=0)\n",
    "k_predictions_toe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(k_predictions_toe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean the k-predictions\n",
    "k_predictions_heel = np.mean(predictions_heel, axis=0)\n",
    "k_predictions_heel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge toe/hell pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_th = np.append(k_predictions_toe, k_predictions_heel, axis=1)#左右接\n",
    "predictions_th.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_name = np.expand_dims(df_ts['images'], axis=1)\n",
    "images_name.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_merge = np.append(images_name, predictions_th, axis=1)#左右接\n",
    "predictions_merge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.DataFrame(predictions_merge)\n",
    "df_submission.columns = ['images','x1','y1','x2','y2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submi_name = CSVNAME + '.' + log_dir_name +'.csv'\n",
    "\n",
    "df_submission.to_csv(submi_name, index=False)\n",
    "print('Save {} as submission CSV.'.format(submi_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K:0\n",
      "K:1\n",
      "K:2\n",
      "K:3\n",
      "K:4\n",
      "K:5\n",
      "K:6\n",
      "K:7\n",
      "K:8\n",
      "K:9\n",
      "K:10\n",
      "K:11\n",
      "K:12\n",
      "K:13\n",
      "K:14\n",
      "K:15\n",
      "K:16\n",
      "K:17\n",
      "K:18\n",
      "K:19\n",
      "5.19652 + 3.17747 = 8.37399\n"
     ]
    }
   ],
   "source": [
    "#ED sum\n",
    "def get_valloss(his_v_l):  \n",
    "    return np.min(his_v_l), np.argmin(his_v_l)\n",
    "\n",
    "t_vl = []\n",
    "h_vl = []\n",
    "for k in range(KFlodNum):\n",
    "    print(f'K:{k}')\n",
    "    t_v, _ = get_valloss(history_toe_finetune[k].history['val_loss'])\n",
    "    h_v, _ = get_valloss(history_heel_finetune[k].history['val_loss'])\n",
    "    \n",
    "    t_vl.append(t_v)\n",
    "    h_vl.append(h_v)\n",
    "\n",
    "t_vl = np.mean(t_vl, axis=0)\n",
    "h_vl = np.mean(h_vl, axis=0)\n",
    "print(f'{round(t_vl,5)} + {round(h_vl,5)} = {round(t_vl + h_vl,5)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K134520210224-114845.csv\n",
    "# 5.63922 + 3.34466 = 8.98389 LB:8.4890610"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_vl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_vl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_timer.toc() #Time elapsed since t.tic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compile the model separately afterwards. to load model with custom loss function\n",
    "\n",
    "* https://github.com/tensorflow/tensorflow/issues/32348"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model_toe.compile(optimizer = tf.keras.optimizers.Adam(),#RMSprop , Adam\n",
    "#                 loss=ed_metric_2d_mean)#, ed_loss ed_metric_2d ed_metric_2d_mean            \n",
    "#                 #metrics=['mae', 'accuracy'])\n",
    "# best_model_heel.compile(optimizer = tf.keras.optimizers.Adam(),#RMSprop , Adam\n",
    "#                 loss=ed_metric_2d_mean)#, ed_loss ed_metric_2d ed_metric_2d_mean            \n",
    "#                 #metrics=['mae', 'accuracy'])\n",
    "\n",
    "# best_model_toe.compile(optimizer = tf.keras.optimizers.Adam(),#RMSprop , Adam\n",
    "#                 loss=tf.keras.losses.MeanSquaredError())#, ed_loss ed_metric_2d ed_metric_2d_mean            \n",
    "#                 #metrics=['mae', 'accuracy'])\n",
    "# best_model_heel.compile(optimizer = tf.keras.optimizers.Adam(),#RMSprop , Adam\n",
    "#                 loss=tf.keras.losses.MeanSquaredError())#, ed_loss ed_metric_2d ed_metric_2d_mean            \n",
    "#                 #metrics=['mae', 'accuracy'])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # inference all test_ds once\n",
    "# predictions_toe = best_model_toe.predict(test_ds_pre_toe)\n",
    "# predictions_toe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offset Toe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_toe[:,1] = predictions_toe[:,1] + y_offset_toe\n",
    "\n",
    "# # for [0,1]\n",
    "# # predictions_toe[:,0] = predictions_toe[:,0]*120\n",
    "# # predictions_toe[:,1] = predictions_toe[:,1]*120 + y_offset_toe\n",
    "\n",
    "# # # for [-1,1]\n",
    "# # # for re-scale back xy \n",
    "# # # return img, [(x1-60)/60,((y1-y_offset_toe)-60)/60]#normalized [-1,1] \n",
    "# # # return img, [(x2-60)/60,((y2-y_offset_heel)-60)/60]#normalized [-1,1] \n",
    "# # predictions_toe[:,0] = (predictions_toe[:,0]*60)+60\n",
    "# # predictions_toe[:,1] = (predictions_toe[:,1]*60)+60 + y_offset_toe\n",
    "\n",
    "# predictions_toe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # inference all test_ds once\n",
    "# predictions_heel = best_model_heel.predict(test_ds_pre_heel)\n",
    "# predictions_heel.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offset Heel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_heel[:,1] = predictions_heel[:,1] + y_offset_heel\n",
    "\n",
    "# # for [0,1]\n",
    "# # predictions_heel[:,0] = predictions_heel[:,0]*120\n",
    "# # predictions_heel[:,1] = predictions_heel[:,1]*120 + y_offset_heel\n",
    "\n",
    "# # # for [-1,1]\n",
    "# # predictions_heel[:,0] = (predictions_heel[:,0]*60)+60\n",
    "# # predictions_heel[:,1] = (predictions_heel[:,1]*60)+60 + y_offset_heel\n",
    "\n",
    "# predictions_heel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge toe/hell pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_th = np.append(predictions_toe, predictions_heel, axis=1)#左右接\n",
    "# predictions_th.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images_name = np.expand_dims(df_ts['images'], axis=1)\n",
    "# images_name.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_merge = np.append(images_name, predictions_th, axis=1)#左右接\n",
    "# predictions_merge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_submission = pd.DataFrame(predictions_merge)\n",
    "# df_submission.columns = ['images','x1','y1','x2','y2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submi_name = '0000_ft_' + log_dir_name +'.csv'\n",
    "# # submi_name = 'Bth_clr3_2690_XYnorm[0-1]_' + log_dir_name +'.csv'\n",
    "# df_submission.to_csv(submi_name, index=False)\n",
    "# print('Save {} as submission CSV.'.format(submi_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bth_clr3_2690_ed_findtune_20210202-141718.csv\n",
    "\n",
    "#toe.9.9/heel.4.4 109 trainable LB:9.3411759 比heel保持top-20略高0.04 (9.3084957)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #ED sum\n",
    "# def get_valloss(his_v_l):  \n",
    "#     return np.min(his_v_l), np.argmin(his_v_l)\n",
    "\n",
    "# t_vl, _ = get_valloss(history_toe_finetune.history['val_loss'])\n",
    "# h_vl, _ = get_valloss(history_heel_finetune.history['val_loss'])\n",
    "\n",
    "# print(f'{round(t_vl,5)} + {round(h_vl,5)} = {round(t_vl + h_vl,5)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# best_model_name = './cop_' + model_name + '_bs' + str(BATCH_SIZE) + '_w' + str(img_width) + '_e' + str(ep_num) + '_' + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + '_.h5'\n",
    "# # model.save(best_model_name)\n",
    "# print(\"Save model: \", best_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "multi output model:\n",
    "https://navoshta.com/end-to-end-deep-learning/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
