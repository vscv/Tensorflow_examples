{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4b01191",
   "metadata": {},
   "source": [
    "## Keypoint Detection with Transfer Learning\n",
    "\n",
    "    https://keras.io/examples/vision/keypoint_detection/\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dc4b3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!sudo pip install -q -U imgaug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91597488",
   "metadata": {},
   "source": [
    "### Data collection\n",
    "The StanfordExtra dataset contains 12,000 images of dogs together with keypoints and segmentation maps. It is developed from the Stanford dogs dataset. It can be downloaded with the command below:\n",
    "\n",
    "Annotations are provided as a single JSON file in the StanfordExtra dataset and one needs to fill this form to get access to it. The authors explicitly instruct users not to share the JSON file, and this example respects this wish: you should obtain the JSON file yourself. (填好申請表格)\n",
    "\n",
    "The JSON file is expected to be locally available as stanfordextra_v12.zip.\n",
    "\n",
    "After the files are downloaded, we can extract the archives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac984805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget -q http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar\n",
    "# 800MB take an hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5801a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar xf images.tar\n",
    "!unzip -qq ~/stanfordextra_v12.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8691e6",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f44396",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from imgaug.augmentables.kps import KeypointsOnImage\n",
    "from imgaug.augmentables.kps import Keypoint\n",
    "import imgaug.augmenters as iaa\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960af5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab60fd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5\n",
    "NUM_KEYPOINTS = 24 * 2  # 24 pairs each having x and y coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e783b2",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "    In this example, the keys we are interested in are:\n",
    "\n",
    "    img_path\n",
    "    joints\n",
    "    \n",
    "    There are a total of 24 entries present inside joints. Each entry has 3 values:\n",
    "\n",
    "        x-coordinate\n",
    "        y-coordinate\n",
    "        visibility flag of the keypoints (1 indicates visibility and 0 indicates non-visibility)\n",
    "        \n",
    "    As we can see joints contain multiple [0, 0, 0] entries which denote that those keypoints were not labeled. In this example, we will consider both non-visible as well as unlabeled keypoints in order to allow mini-batch learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beb444b",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_DIR = \"Images\"\n",
    "JSON = \"StanfordExtra_V12/StanfordExtra_v12.json\"\n",
    "KEYPOINT_DEF = (\n",
    "    \"https://github.com/benjiebob/StanfordExtra/raw/master/keypoint_definitions.csv\"\n",
    ")\n",
    "\n",
    "# Load the ground-truth annotations.\n",
    "with open(JSON) as infile:\n",
    "    json_data = json.load(infile)\n",
    "\n",
    "# Set up a dictionary, mapping all the ground-truth information\n",
    "# with respect to the path of the image.\n",
    "json_dict = {i[\"img_path\"]: i for i in json_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd2b8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the metdata definition file and preview it.\n",
    "keypoint_def = pd.read_csv(KEYPOINT_DEF)\n",
    "keypoint_def.head()\n",
    "\n",
    "# Extract the colours and labels.\n",
    "colours = keypoint_def[\"Hex colour\"].values.tolist()\n",
    "colours = [\"#\" + colour for colour in colours]\n",
    "labels = keypoint_def[\"Name\"].values.tolist()\n",
    "\n",
    "# Utility for reading an image and for getting its annotations.\n",
    "def get_dog(name):\n",
    "    data = json_dict[name]\n",
    "    img_data = plt.imread(os.path.join(IMG_DIR, data[\"img_path\"]))\n",
    "    # If the image is RGBA convert it to RGB.\n",
    "    if img_data.shape[-1] == 4:\n",
    "        img_data = img_data.astype(np.uint8)\n",
    "        img_data = Image.fromarray(img_data)\n",
    "        img_data = np.array(img_data.convert(\"RGB\"))\n",
    "    data[\"img_data\"] = img_data\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea92b1cc",
   "metadata": {},
   "source": [
    "### Visualize data\n",
    "        Now, we write a utility function to visualize the images and their keypoints.\n",
    "        \n",
    "        The plots show that we have images of non-uniform sizes, which is expected in most real-world scenarios. However, if we resize these images to have a uniform shape (for instance (224 x 224)) their ground-truth annotations will also be affected. The same applies if we apply any geometric transformation (horizontal flip, for e.g.) to an image. Fortunately, imgaug provides utilities that can handle this issue. In the next section, we will write a data generator inheriting the [keras.utils.Sequence](/api/utils/python_utils#sequence-class) class that applies data augmentation on batches of data using imgaug.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762c3263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parts of this code come from here:\n",
    "# https://github.com/benjiebob/StanfordExtra/blob/master/demo.ipynb\n",
    "def visualize_keypoints(images, keypoints):\n",
    "    fig, axes = plt.subplots(nrows=len(images), ncols=2, figsize=(16, 12))\n",
    "    [ax.axis(\"off\") for ax in np.ravel(axes)]\n",
    "\n",
    "    for (ax_orig, ax_all), image, current_keypoint in zip(axes, images, keypoints):\n",
    "        ax_orig.imshow(image)\n",
    "        ax_all.imshow(image)\n",
    "\n",
    "        # If the keypoints were formed by `imgaug` then the coordinates need\n",
    "        # to be iterated differently.\n",
    "        if isinstance(current_keypoint, KeypointsOnImage):\n",
    "            for idx, kp in enumerate(current_keypoint.keypoints):\n",
    "                ax_all.scatter(\n",
    "                    [kp.x], [kp.y], c=colours[idx], marker=\"x\", s=50, linewidths=5\n",
    "                )\n",
    "        else:\n",
    "            current_keypoint = np.array(current_keypoint)\n",
    "            # Since the last entry is the visibility flag, we discard it.\n",
    "            current_keypoint = current_keypoint[:, :2]\n",
    "            for idx, (x, y) in enumerate(current_keypoint):\n",
    "                ax_all.scatter([x], [y], c=colours[idx], marker=\"x\", s=50, linewidths=5)\n",
    "\n",
    "    plt.tight_layout(pad=2.0)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Select four samples randomly for visualization.\n",
    "samples = list(json_dict.keys())\n",
    "num_samples = 4\n",
    "selected_samples = np.random.choice(samples, num_samples, replace=False)\n",
    "\n",
    "images, keypoints = [], []\n",
    "\n",
    "for sample in selected_samples:\n",
    "    data = get_dog(sample)\n",
    "    image = data[\"img_data\"]\n",
    "    keypoint = data[\"joints\"]\n",
    "\n",
    "    images.append(image)\n",
    "    keypoints.append(keypoint)\n",
    "\n",
    "visualize_keypoints(images, keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116f085e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b6b2f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
