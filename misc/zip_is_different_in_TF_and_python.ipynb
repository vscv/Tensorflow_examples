{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea4cb83a",
   "metadata": {},
   "source": [
    "# zip_is_different_in_TF_and_python\n",
    "\n",
    "* The Q source\n",
    "\n",
    "https://stackoverflow.com/questions/75244920/split-my-dataset-in-train-validation-using-mapdataset-in-python\n",
    "\n",
    "\n",
    "Hi everyone I'm facing an issue after that I elaborate images and labels. To create an unique dataset I use the zip function. After the elaboration both images and labels are 18k and it's correct but when I call the zip(image,labels), items become 563. Here some code to let you to understand:\n",
    "\n",
    "```Python\n",
    "# Map the load_and_preprocess_image function over the dataset of image paths\n",
    "images = image_paths.map(load_and_preprocess_image)\n",
    "# Map the extract_label function over the dataset of image paths\n",
    "labels = image_paths.map(extract_label)    \n",
    "# Zip the labels and images together to create a dataset of (image, label) pairs\n",
    "#HERE SOMETHING STRANGE HAPPENS\n",
    "data = tf.data.Dataset.zip((images,labels))\n",
    "# Shuffle and batch the data\n",
    "data = data.shuffle(buffer_size=1000).batch(32)\n",
    "# Split the data into train and test sets\n",
    "data = data.shuffle(buffer_size=len(data))\n",
    "# Convert the dataset into a collection of data\n",
    "num_train = int(0.8 * len(data))\n",
    "train_data = image_paths.take(num_train)\n",
    "val_data = image_paths.skip(num_train)\n",
    "```\n",
    "\n",
    "I cannot see where is the error. Can you help me plese? Thanks\n",
    "\n",
    "I'd like to have a dataset of 18k images,labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b47470",
   "metadata": {},
   "source": [
    "## my ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da1a27f",
   "metadata": {},
   "source": [
    "`tf.data.Dataset.zip` is not like Python's `zip`. The `tf.data.Dataset.zip`'s input is `tf datasets`, not the elements of the `tf dataset`. You may check the [official doc][1] for detail. \n",
    "\n",
    "In your case, combine the image and label processing in one map function and return both:\n",
    "\n",
    "    # load_and_preprocess_image_and_label\n",
    "    def load_and_preprocess_image_and_label(image_path):\n",
    "        \"\"\" load image and label then some operations \"\"\"\n",
    "        return image, label\n",
    "    \n",
    "    # Map the load_and_preprocess_image function over the dataset of image/label paths\n",
    "    train_list = tf.data.Dataset.list_files(str(PATH / 'train/*.jpg'))\n",
    "    data = train_list.map(load_and_preprocess_image_and_label,\n",
    "                                      num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "  [1]: https://www.tensorflow.org/api_docs/python/tf/data/Dataset#zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4afabae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eed7fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04e21e71",
   "metadata": {},
   "source": [
    "## refernce\n",
    "\n",
    "https://blog.csdn.net/xinjieyuan/article/details/103749167\n",
    "\n",
    "```dataset.zip()与zip()```\n",
    "这个函数和python中的zip()相当的不一样，切勿被迷惑了双眼。首先tensorflow中的zip接受的数据是dataset\n",
    "作用：通过将给定的数据集压缩在一起创建一个“数据集”。\n",
    "使用过程举例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d9c9fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-26 20:52:06.445946: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e51f4e5",
   "metadata": {},
   "source": [
    "### 1. tf.data.dataset input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9d3cbc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.data.Dataset.range(1,4) # [ 1, 2, 3 ]\n",
    "b = tf.data.Dataset.range(4,7) # [ 4, 5, 6 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cb6d2df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<RangeDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "55b8d7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.dataset a= [1, 2, 3]\n",
      "tf.dataset b= [4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "print(\"tf.dataset a=\", list(a.as_numpy_iterator()))\n",
    "print(\"tf.dataset b=\", list(b.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "11063c50",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid `datasets`. `datasets` is expected to be a (nested) structure of `tf.data.Dataset` objects. Python `list` is not supported and you should use `tuple` instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [74], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc=\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlist\u001b[39m(c\u001b[38;5;241m.\u001b[39mas_numpy_iterator()))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/dataset_ops.py:1264\u001b[0m, in \u001b[0;36mDatasetV2.zip\u001b[0;34m(datasets, name)\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m   1219\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mzip\u001b[39m(datasets, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1220\u001b[0m   \u001b[38;5;124;03m\"\"\"Creates a `Dataset` by zipping together the given datasets.\u001b[39;00m\n\u001b[1;32m   1221\u001b[0m \n\u001b[1;32m   1222\u001b[0m \u001b[38;5;124;03m  This method has similar semantics to the built-in `zip()` function\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;124;03m    Dataset: A `Dataset`.\u001b[39;00m\n\u001b[1;32m   1263\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1264\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mZipDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/dataset_ops.py:4841\u001b[0m, in \u001b[0;36mZipDataset.__init__\u001b[0;34m(self, datasets, name)\u001b[0m\n\u001b[1;32m   4839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ds, DatasetV2):\n\u001b[1;32m   4840\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ds, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m-> 4841\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid `datasets`. `datasets` is expected to be a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4842\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(nested) structure of `tf.data.Dataset` objects. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4843\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPython `list` is not supported and you should use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4844\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tuple` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4845\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4846\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid `datasets`. `datasets` is expected to be a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4847\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(nested) structure of `tf.data.Dataset` objects \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4848\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut encountered object of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(ds)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid `datasets`. `datasets` is expected to be a (nested) structure of `tf.data.Dataset` objects. Python `list` is not supported and you should use `tuple` instead."
     ]
    }
   ],
   "source": [
    "c = tf.data.Dataset.zip((a, b))\n",
    "print(\"c=\", list(c.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7548981d",
   "metadata": {},
   "source": [
    "### 2. python range input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e6536f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = range(1,4) # [ 1, 2, 3 ]\n",
    "# b = range(4,7) # [ 4, 5, 6 ]\n",
    "\n",
    "a = [ 1, 2, 3 ]\n",
    "b = [ 4, 5, 6 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6a2020b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a= [1, 2, 3]\n",
      "b= [4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "# for i in a:\n",
    "#     print(i)\n",
    "\n",
    "print(\"a=\", list(a))\n",
    "print(\"b=\", list(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a3d72933",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = zip((a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "94d18e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([1, 2, 3],)\n",
      "([4, 5, 6],)\n"
     ]
    }
   ],
   "source": [
    "for x in c:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9406d1",
   "metadata": {},
   "source": [
    "#### 這是？？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9386b84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = zip(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3474968e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4)\n",
      "(2, 5)\n",
      "(3, 6)\n"
     ]
    }
   ],
   "source": [
    "for x in c:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1fe663a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = tf.data.Dataset.zip((a, b))\n",
    "# print(\"c=\", list(c.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88c8fb3",
   "metadata": {},
   "source": [
    "## tf.data.Dataset.zip from TF offical doc\n",
    "https://www.tensorflow.org/api_docs/python/tf/data/Dataset#zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59f58dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a= [1, 2, 3]\n",
      "b= [4, 5, 6]\n",
      "ds= [(1, 4), (2, 5), (3, 6)]\n"
     ]
    }
   ],
   "source": [
    "# The nested structure of the `datasets` argument determines the\n",
    "# structure of elements in the resulting dataset.\n",
    "a = tf.data.Dataset.range(1, 4)  # ==> [ 1, 2, 3 ]\n",
    "b = tf.data.Dataset.range(4, 7)  # ==> [ 4, 5, 6 ]\n",
    "ds = tf.data.Dataset.zip((a, b))\n",
    "\n",
    "print(\"a=\", list(a.as_numpy_iterator()))\n",
    "print(\"b=\", list(b.as_numpy_iterator()))\n",
    "print(\"ds=\", list(ds.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c12c886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 1), (5, 2), (6, 3)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = tf.data.Dataset.zip((b, a))\n",
    "list(ds.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4885d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4, array([7, 8]))\n",
      "(2, 5, array([ 9, 10]))\n",
      "(3, 6, array([11, 12]))\n"
     ]
    }
   ],
   "source": [
    "# The `datasets` argument may contain an arbitrary number of datasets.\n",
    "c = tf.data.Dataset.range(7, 13).batch(2)  # ==> [ [7, 8],\n",
    "                                           #       [9, 10],\n",
    "                                           #       [11, 12] ]\n",
    "ds = tf.data.Dataset.zip((a, b, c))\n",
    "for element in ds.as_numpy_iterator():\n",
    "  print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eec70ad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 13), (2, 14)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The number of elements in the resulting dataset is the same as\n",
    "# the size of the smallest dataset in `datasets`.\n",
    "d = tf.data.Dataset.range(13, 15)  # ==> [ 13, 14 ]\n",
    "ds = tf.data.Dataset.zip((a, d))\n",
    "list(ds.as_numpy_iterator())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4bed3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
