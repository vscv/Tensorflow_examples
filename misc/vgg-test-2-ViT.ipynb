{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f647a0a7",
   "metadata": {},
   "source": [
    "### VGG not work!!!!\n",
    "\n",
    "\n",
    "12/08\n",
    "12/09\n",
    "12/10\n",
    "VGG16/16 seems not works? train fail at step-2/epoch-1,\n",
    "tensorflow-21.06-tf2-py3:tf25odocrpp2111 2nd steps faile out (tf2.5)\n",
    "tensorflow-21.08-tf2-py3:latest 可以跑了(tf2.5) 差異在cuda版本  \n",
    "\n",
    "\n",
    "12/10\n",
    "12/11\n",
    "Let test other SOTA models (ViT, BiT, ConvMixer...)\n",
    "ViT:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c1f10a",
   "metadata": {},
   "source": [
    "### 1. ENV\n",
    "#### <font color=#FF6600>[ENV] Turn off the error from twcc's AMP issue</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1e0602b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export TF_ENABLE_AUTO_MIXED_PRECISION=0\n",
    "\n",
    "#no work\n",
    "!export TF_FORCE_GPU_ALLOW_GROWTH=1\n",
    "# !export drop_remainder=False\n",
    "\n",
    "# !export TF_ENABLE_AUTO_MIXED_PRECISION=1\n",
    "# !export TF_ENABLE_AUTO_MIXED_PRECISION_GRAPH_REWRITE=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e8cae132",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !sh install_env.sh\n",
    "# !pip install -U --quiet vit-keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907ce98a",
   "metadata": {},
   "source": [
    "#### <font color=#FF6600>[ENV] Moduls importing</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "55e5cb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# set log need before import tf\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "os.environ[\"AUTOGRAPH_VERBOSITY\"] = \"0\"\n",
    "\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# ViT (tf.hub version)\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# # ViT (vit-keras)\n",
    "# from vit_keras import vit\n",
    "# import tensorflow_addons as tfa\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import errno\n",
    "import json\n",
    "\n",
    "\n",
    "# albumentations\n",
    "from functools import partial\n",
    "import albumentations as A\n",
    "\n",
    "# RandAugment, AutoAugment\n",
    "from augment import RandAugment,AutoAugment\n",
    "\n",
    "\n",
    "from pytictoc import TicToc\n",
    "\n",
    "t = TicToc() #create instance of class\n",
    "\n",
    "t.tic() #Start time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a30f93de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf: 2.5.0 \n",
      "cv2: 4.5.3 \n",
      "np: 1.19.5 \n",
      "pd: 1.3.2 \n",
      "matplotlib: 3.4.3\n",
      "Elapsed time is 0.007991 seconds.\n"
     ]
    }
   ],
   "source": [
    "print(f'tf: {tf.__version__} \\ncv2: {cv2.__version__} \\nnp: {np.__version__} \\npd: {pd.__version__} \\nmatplotlib: {matplotlib.__version__}')\n",
    "t.toc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de78dc4",
   "metadata": {},
   "source": [
    "#### <font color=#FF6600>[ENV] Parameters</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "359c3f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:4', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:5', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:6', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:7', device_type='GPU')] \n",
      "\n",
      "[LogicalDevice(name='/device:GPU:0', device_type='GPU'), LogicalDevice(name='/device:GPU:1', device_type='GPU'), LogicalDevice(name='/device:GPU:2', device_type='GPU'), LogicalDevice(name='/device:GPU:3', device_type='GPU'), LogicalDevice(name='/device:GPU:4', device_type='GPU'), LogicalDevice(name='/device:GPU:5', device_type='GPU'), LogicalDevice(name='/device:GPU:6', device_type='GPU'), LogicalDevice(name='/device:GPU:7', device_type='GPU')] \n",
      "\n",
      "Num GPUs Available:  8\n",
      "Num logical_gpus  : 8\n",
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3', '/job:localhost/replica:0/task:0/device:GPU:4', '/job:localhost/replica:0/task:0/device:GPU:5', '/job:localhost/replica:0/task:0/device:GPU:6', '/job:localhost/replica:0/task:0/device:GPU:7')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3', '/job:localhost/replica:0/task:0/device:GPU:4', '/job:localhost/replica:0/task:0/device:GPU:5', '/job:localhost/replica:0/task:0/device:GPU:6', '/job:localhost/replica:0/task:0/device:GPU:7')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of REPLICAS: 8\n",
      "\n",
      "BATCH_SIZE: 4, MULTI_BATCH_SIZE: 32\n"
     ]
    }
   ],
   "source": [
    "# Image size\n",
    "BATCH_SIZE = 4 #2 # 8# 32 #64 #64:512*8 OOM, B7+bs8:RecvAsync is cancelled\n",
    "img_height = 512 #600 #512 #120\n",
    "img_width = 512 #600 #512 #120\n",
    "\n",
    "patience_1 = 5\n",
    "patience_2 = 10\n",
    "\n",
    "# 自動調節tf.data管道\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# visible/logical device (able to be used)\n",
    "print(tf.config.experimental.list_physical_devices('GPU'),'\\n')\n",
    "print(tf.config.experimental.list_logical_devices('GPU'),'\\n')\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num logical_gpus  :\", len(tf.config.experimental.list_logical_devices('GPU')))\n",
    "\n",
    "# tf MirroredStrategy seting\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "REPLICAS = strategy.num_replicas_in_sync\n",
    "print('\\nNumber of REPLICAS: {}\\n'.format(REPLICAS))\n",
    "\n",
    "\n",
    "MULTI_BATCH_SIZE = BATCH_SIZE * strategy.num_replicas_in_sync\n",
    "print('BATCH_SIZE: {}, MULTI_BATCH_SIZE: {}'.format(BATCH_SIZE, MULTI_BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c956004",
   "metadata": {},
   "source": [
    "### 2. Dataset (DS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5878217f",
   "metadata": {},
   "source": [
    "#### <font color=orange>[DS] Create the training dataset W/ croped</font>\n",
    "#### <font color=#00FF00>[DS] Create the training dataset W/ croped</font>\n",
    "\n",
    "    label_num_to_disease_map.json    {\n",
    "    \"0\": \"Cassava Bacterial Blight (CBB)\", \n",
    "    \"1\": \"Cassava Brown Streak Disease (CBSD)\", \n",
    "    \"2\": \"Cassava Green Mottle (CGM)\", \n",
    "    \"3\": \"Cassava Mosaic Disease (CMD)\", \n",
    "    \"4\": \"Healthy\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9819bb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = ['CBB', \n",
    "           'CBSD', \n",
    "           'CGM', \n",
    "           'CMD', \n",
    "           'Healthy']\n",
    "LABELS = {\"0\": \"CBB\", \n",
    "          \"1\": \"CBSD\", \n",
    "          \"2\": \"CGM\", \n",
    "          \"3\": \"CMD\", \n",
    "          \"4\": \"Healthy\"}\n",
    "\n",
    "data_dir = '/home/u3148947/.keras/datasets/leaf/'\n",
    "# leaf_dir = 'leaf/leaf_labels/'\n",
    "leaf_dir = '/home/u3148947/.keras/datasets/leaf/train_images/'\n",
    "\n",
    "df_train = pd.read_csv(data_dir + '/train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8b9e07f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 CBB\n",
      "1 CBSD\n",
      "2 CGM\n",
      "3 CMD\n",
      "4 Healthy\n",
      "[('0', 'CBB'), ('1', 'CBSD'), ('2', 'CGM'), ('3', 'CMD'), ('4', 'Healthy')]\n",
      "0 CBB\n",
      "1 CBSD\n",
      "2 CGM\n",
      "3 CMD\n",
      "4 Healthy\n"
     ]
    }
   ],
   "source": [
    "# check lables\n",
    "for i in range(5):\n",
    "    print(i, CLASSES[i])\n",
    "print([(i,l) for i,l in zip(LABELS.keys(), LABELS.values())])\n",
    "\n",
    "for i,l in zip(LABELS.keys(), LABELS.values()):\n",
    "    print(i,l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c31ee5e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21397"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cb56ee13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000015157.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000201771.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100042118.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000723321.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000812911.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image_id  label\n",
       "0  1000015157.jpg      0\n",
       "1  1000201771.jpg      3\n",
       "2   100042118.jpg      1\n",
       "3  1000723321.jpg      1\n",
       "4  1000812911.jpg      3"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "03f302f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and reset index\n",
    "# fixed shuffle for compare later, random_state=42\n",
    "df_train = df_train.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ce46e04d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2615227158.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1277648239.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2305895487.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>336299725.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1951270318.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image_id  label\n",
       "0  2615227158.jpg      4\n",
       "1  1277648239.jpg      3\n",
       "2  2305895487.jpg      3\n",
       "3   336299725.jpg      2\n",
       "4  1951270318.jpg      2"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a18b061f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "image_id    21397\n",
       "label       21397\n",
       "dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d6625c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       image_id\n",
      "label          \n",
      "0          1087\n",
      "1          2189\n",
      "2          2386\n",
      "3         13158\n",
      "4          2577\n"
     ]
    }
   ],
   "source": [
    "freq = df_train.groupby(['label']).count() \n",
    "print(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8fd4d2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3    13158\n",
      "4     2577\n",
      "2     2386\n",
      "1     2189\n",
      "0     1087\n",
      "Name: label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get no key freq\n",
    "freq = df_train['label'].value_counts() \n",
    "ax = freq.plot.bar(x='image_id', y='label', rot=0) #no key so x y no matter.\n",
    "print(freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0979fe4",
   "metadata": {},
   "source": [
    "\n",
    "#### <font color=#00FF00>[DS] Create tf.dataset (DS)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "01cf6c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataframe\n",
    "list_ds = tf.data.Dataset.from_tensor_slices((df_train['image_id'], df_train['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "78177e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Python iterator\n",
    "\n",
    "it_list_ds = iter(list_ds) # Make sure iter ds only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "29560658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'2615227158.jpg' 4\n",
      "b'1277648239.jpg' 3\n",
      "b'2305895487.jpg' 3\n",
      "b'336299725.jpg' 2\n"
     ]
    }
   ],
   "source": [
    "# using iter and consuming its elements using next: every print different image name.\n",
    "\n",
    "for i in range(4):\n",
    "    image_id, label = next(it_list_ds)\n",
    "    print(image_id.numpy(), label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "35968d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# map list to ds.\n",
    "#\n",
    "def process_path_label(image_id, label):\n",
    "    file_path = leaf_dir + image_id\n",
    "    # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)#can read the byte string paths b'image_0001.png'\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    img = tf.cast(tf.image.resize(img, [img_height, img_width]), tf.uint8) # for resize the training image for faster checing!\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "90275a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leaf train ds\n",
    "train_ds_map = list_ds.map(process_path_label, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78e69a3",
   "metadata": {},
   "source": [
    "#### <font color=#00FF00>[DS] Split TVT</font>\n",
    "train/val/test with ratio 7. 1.5 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f85f1dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val size: 3209\n",
      "total size: 21397\n",
      "\n",
      "train 14979\n",
      "valid 3209\n",
      "test 3209\n"
     ]
    }
   ],
   "source": [
    "# split TVT train/val/test 7 1.5 1.5\n",
    "\n",
    "\n",
    "val_size = int(tf.data.experimental.cardinality(train_ds_map).numpy() * 0.15)\n",
    "# val_size = int(tf.data.experimental.cardinality(train_ds_map_toe).numpy() * 0.1)#no help\n",
    "\n",
    "print(\"val size:\", val_size)\n",
    "\n",
    "train_ds_map_s = train_ds_map.skip(val_size+val_size)\n",
    "temp_s = train_ds_map.take(val_size+val_size)\n",
    "\n",
    "valid_ds_map_s = temp_s.take(val_size)\n",
    "test_ds_map_s = temp_s.skip(val_size)\n",
    "\n",
    "print(\"total size:\", len(train_ds_map))\n",
    "print(\"\\ntrain\", tf.data.experimental.cardinality(train_ds_map_s).numpy())\n",
    "print(\"valid\", tf.data.experimental.cardinality(valid_ds_map_s).numpy())\n",
    "print(\"test\", tf.data.experimental.cardinality(test_ds_map_s).numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9a101754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create iterator once\n",
    "# iter_map = iter(train_ds_map_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0f059603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # get new image every next time\n",
    "# image, label = next(iter_map)\n",
    "\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# plt.imshow(image)\n",
    "# plt.title(f'Image dtype: {image.dtype} and the range: {image.numpy().min()} to {image.numpy().max()}, shape:{image.shape}')\n",
    "\n",
    "# print(f'Lable: {label}')\n",
    "\n",
    "# \"\"\"\n",
    "# png\n",
    "# Lable: 4\n",
    "# CPU times: user 18.8 s, sys: 934 ms, total: 19.7 s\n",
    "# Wall time: 36.6 s\n",
    "\n",
    "# jpg\n",
    "# Lable: 4\n",
    "# CPU times: user 13.4 s, sys: 370 ms, total: 13.8 s\n",
    "# Wall time: 3.57 s\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64e1b28",
   "metadata": {},
   "source": [
    "#### <font color=#00FF00>[DS] Augmentation and performance cache pipeline</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6f9dde77",
   "metadata": {},
   "outputs": [],
   "source": [
    "## AA, auto aug test\n",
    "\n",
    "# image = tf.io.read_file(str(CMD[1]))\n",
    "# image = tf.image.decode_jpeg(image, channels=3)\n",
    "\n",
    "\n",
    "def AA(image, label):\n",
    "    Auto_Aug = AutoAugment()\n",
    "#     auto_img = Auto_Aug.distort(image)\n",
    "    return Auto_Aug.distort(image), label\n",
    "\n",
    "\n",
    "def RA(image, label):\n",
    "    Rand_Aug = RandAugment()\n",
    "#     auto_img = Auto_Aug.distort(image)\n",
    "    return Rand_Aug.distort(image), label\n",
    "\n",
    "\n",
    "## DS performance cache\n",
    "\n",
    "def configure_for_performance_cache(ds, cache=True, augment=False):  \n",
    "    \"\"\"#TODO: need to check the parse logic of ds.cache.\n",
    "    if cache:\n",
    "        print(\"Check cache-f1 to file:\", cache)\n",
    "        if isinstance(cache, str):\n",
    "            ds = ds.cache(cache)\n",
    "            print(\"Check cache-f2 to file:\", cache)\n",
    "    else:\n",
    "        ds = ds.cache()\n",
    "        print(\"Check cache in memory:\", cache)\n",
    "    \"\"\"    \n",
    "    if cache:\n",
    "        ds = ds.cache()\n",
    "        print(\"Check cache in memory:Y\", cache)\n",
    "    else:\n",
    "        print(\"Check cache in memory:N\", cache)\n",
    "        \n",
    "    if augment:\n",
    "#         ds = ds.map(data_augment, num_parallel_calls=AUTOTUNE)\n",
    "        ds = ds.map(AA, num_parallel_calls=AUTOTUNE)\n",
    "#         ds = ds.map(RA, num_parallel_calls=AUTOTUNE)\n",
    "        print(\"Check augment :Y\", augment)\n",
    "    else:\n",
    "        print(\"Check augment :N\", augment)\n",
    "    \n",
    "    #ds = ds.repeat()#TODO:2020-12-14: test\n",
    "    ds = ds.shuffle(buffer_size=MULTI_BATCH_SIZE, reshuffle_each_iteration=True) #buffer_size=MULTI_BATCH_SIZE*2 10sec. # (buffer_size=MULTI_BATCH_SIZE*5) ~10sec,buffer_size=1000 take few sec. or buffer_size=image_count <- take too long # each take ds take 30~45 sec, TODO!!\n",
    "    \"\"\"Note: While large buffer_sizes shuffle more thoroughly, they can take a lot of memory, and \n",
    "        significant time to fill. Consider using Dataset.interleave across files if this becomes a problem.\"\"\"\n",
    "    \n",
    "    ds = ds.batch(MULTI_BATCH_SIZE)#MULTI_BATCH_SIZE for multi-GPUs\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE) #buffer_size=AUTOTUNE seem no speed improve\n",
    "    \n",
    "    print(\"Check ds cache[{}] and augment[{}]\".format(cache, augment))\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "645f5b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check cache in memory:Y True\n",
      "Check augment :Y True\n",
      "Check ds cache[True] and augment[True]\n",
      "Check cache in memory:Y True\n",
      "Check augment :N False\n",
      "Check ds cache[True] and augment[False]\n",
      "Check cache in memory:Y True\n",
      "Check augment :N False\n",
      "Check ds cache[True] and augment[False]\n"
     ]
    }
   ],
   "source": [
    "augment=True#False #aug, AA, RA, ADA\n",
    "\n",
    "train_ds_pre = configure_for_performance_cache(train_ds_map_s, cache=True, augment=augment)\n",
    "valid_ds_pre = configure_for_performance_cache(valid_ds_map_s)\n",
    "test_ds_pre = configure_for_performance_cache(test_ds_map_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "edd416b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# print('AUTOTUNE=', AUTOTUNE)\n",
    "\n",
    "# import math\n",
    "# col_row = math.sqrt(int(BATCH_SIZE))\n",
    "\n",
    "# plt.figure(figsize=(20, 20))\n",
    "# for images, labels in train_ds_pre.take(1):\n",
    "#     print('batch * multi:', len(labels))\n",
    "#     for i in range(16):\n",
    "#         ax = plt.subplot(4, 4, i + 1)\n",
    "#         plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "#         plt.title(f'labels:{labels[i]}, {CLASSES[labels[i]]}')\n",
    "#         plt.axis(\"off\")\n",
    "        \n",
    "# #why take so long\n",
    "# \"\"\"\n",
    "# tf.io.decode_png\n",
    "# AUTOTUNE= -1\n",
    "# batch * multi: 64\n",
    "# CPU times: user 9min 52s, sys: 23.5 s, total: 10min 15s\n",
    "# Wall time: 5min 10s\n",
    "\n",
    "# tf.io.decode_jpeg\n",
    "# AUTOTUNE= -1\n",
    "# batch * multi: 64\n",
    "# CPU times: user 9min 40s, sys: 22.6 s, total: 10min 2s\n",
    "# Wall time: 3min 39s\n",
    "\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "15034605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# print('AUTOTUNE=', AUTOTUNE)\n",
    "# # too long\n",
    "\n",
    "# #J 新版：tf2.3\n",
    "\n",
    "# image_batch, label_batch = next(iter(train_ds_pre))\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# plt.imshow(image_batch[1].numpy().astype(\"uint8\"))\n",
    "# # image_batch[1]\n",
    "\n",
    "# print(f'Image dtype: {image_batch[1].dtype} and the range: {image_batch[1].numpy().min()} to {image_batch[1].numpy().max()}')\n",
    "\n",
    "# \"\"\"tf.cast(uint8) will hard fit source to 0-255\"\"\"\n",
    "\n",
    "# \"\"\"AUTOTUNE= -1\n",
    "# Image dtype: <dtype: 'uint8'> and the range: 0 to 255\n",
    "# CPU times: user 8min 56s, sys: 5.43 s, total: 9min 1s\n",
    "# Wall time: 3min 28s\n",
    "\n",
    "# AUTOTUNE= -1\n",
    "# Image dtype: <dtype: 'uint8'> and the range: 0 to 255\n",
    "# CPU times: user 8min 59s, sys: 5.81 s, total: 9min 5s\n",
    "# Wall time: 3min 29s\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a8dbe932",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.tic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "62775704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create it_ds once\n",
    "# it_train_ds_pre_toe_s = iter(train_ds_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f2ebfb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# image_batch, label_batch = next(it_train_ds_pre_toe_s)\n",
    "\n",
    "# fig = plt.figure(figsize=(8, 8))\n",
    "# print('batch * multi:', len(label_batch), ', MULTI_BATCH_SIZE=', MULTI_BATCH_SIZE)\n",
    "# for i in range(16):\n",
    "#     ax = plt.subplot(4, 4, i + 1)\n",
    "#     plt.imshow(image_batch[i].numpy().astype(\"uint8\"))\n",
    "#     plt.title(label_batch[i].numpy())\n",
    "#     plt.axis(\"off\")\n",
    "   \n",
    "#     # 2021-11-05\n",
    "#     # Check jpg, png agter ds is [0-255]\n",
    "#     print(' [min,max]:', image_batch[i].numpy().min() , image_batch[i].numpy().max()) \n",
    "    \n",
    "#     print(f'Check lables: {label_batch[i]}')\n",
    "\n",
    "# \"\"\"CPU times: user 8min 54s, sys: 4.8 s, total: 8min 58s\n",
    "# Wall time: 3min 27s\n",
    "\n",
    "# CPU times: user 9min 2s, sys: 5.46 s, total: 9min 7s\n",
    "# Wall time: 3min 30s\n",
    "\n",
    "# ds.shuffle buffer_sizes  = MULTI_BATCH_SIZE\n",
    "# CPU times: user 22 s, sys: 552 ms, total: 22.6 s\n",
    "# Wall time: 7.75 s\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d7cc3a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time is 0.027763 seconds.\n"
     ]
    }
   ],
   "source": [
    "t.toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "977aa38e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'跑完整個ds需要3min33s\\n100%|██████████| 235/235 [03:33<00:00,  1.10it/s]\\nnum_bs= 235\\nElapsed time is 213.461381 seconds.\\nCPU times: user 9min 1s, sys: 8.04 s, total: 9min 9s\\nWall time: 3min 33s\\n'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%time\n",
    "# t.tic()\n",
    "# num_bs=0\n",
    "# for bs in tqdm(train_ds_pre):\n",
    "# #     print(type(bs))\n",
    "# #     print(\".\",end=\"\")\n",
    "# #     print(len(bs))\n",
    "#     print([len(a) for a in bs])\n",
    "#     num_bs += 1\n",
    "# print(\"num_bs=\", num_bs)\n",
    "# t.toc()\n",
    "\n",
    "\n",
    "\"\"\"跑完整個ds需要3min33s\n",
    "100%|██████████| 235/235 [03:33<00:00,  1.10it/s]\n",
    "num_bs= 235\n",
    "Elapsed time is 213.461381 seconds.\n",
    "CPU times: user 9min 1s, sys: 8.04 s, total: 9min 9s\n",
    "Wall time: 3min 33s\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ff2175",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8744c6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cc3f44",
   "metadata": {},
   "source": [
    "### 4. Models\n",
    "\n",
    "#### <font color=\"yellow\"> [Models] </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ad9a668c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT_b8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "aps: module 'tensorflow.keras.applications' from '/usr/local/lib/python3.8/dist-packages/tensorflow/keras/applications/__init__.py\n",
    "vim /usr/local/lib/python3.8/dist-packages/tensorflow/keras/applications/__init__.py\n",
    "\n",
    "可以實現由動態字串變數載入特定的基本模型<但是太多處理>還不如直接表列每項寫出來的清楚簡單!!!!\n",
    "\n",
    "Model_List = [\"Xception\", \"ResNet50\", \"ResNet101\", \"ResNet152\", \"InceptionV3\", \"MobileNet\", \"MobileNetV2\", \n",
    "\"DenseNet121\",\"DenseNet169\",\"DenseNet201\",\n",
    "\"NASNetMobile\",\"NASNetLarge\", \n",
    "\"EfficientNetB0\",\n",
    "\"EfficientNetB1\", #13\n",
    "\"EfficientNetB3\",\n",
    "\"EfficientNetB5\", #15\n",
    "\"EfficientNetB7\",\n",
    "]\n",
    "\"\"\"\n",
    "import importlib \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def build_efn_model(model_name, outputnum, top_dropout_rate, drop_connect_rate):\n",
    "    \n",
    "    # move to Top \n",
    "    inputs = tf.keras.Input(shape=(img_height, img_width, 3)) #shape=(120, 120, 3), img_height, img_width shape=(img_height, img_width, 3)\n",
    "    \n",
    "    # EfficientNetB@# #\n",
    "#     # OK efn\n",
    "#     if model_name.startswith('EfficientNetB'):# == \"EfficientNetB0\":\n",
    "#         root_m_name = 'efficientnet'\n",
    "#         fullnameofmodel = \"tensorflow.keras.applications.\" + root_m_name #model_name #model_name.lower()\n",
    "#         model = importlib.import_module(fullnameofmodel)\n",
    "#         BaseCnn = getattr(model,model_name)       \n",
    "#         base_model = BaseCnn(include_top=False, weights=\"imagenet\", input_shape=(120,120,3),drop_connect_rate=drop_connect_rate) #{'imagenet', None}\n",
    "    # shorter version of OK efn\n",
    "    if model_name.startswith('EfficientNetB'):\n",
    "        \"\"\"For EfficientNet, input preprocessing is included as part of the model (as a Rescaling layer).\"\"\"\n",
    "        BaseCnn = getattr(importlib.import_module(\"tensorflow.keras.applications.efficientnet\"), model_name)       \n",
    "        base_model = BaseCnn(include_top=False, weights=\"imagenet\", drop_connect_rate=drop_connect_rate) #{'imagenet', None}\n",
    "        # NO extra rescale need, efn already include the scaling inside the model\n",
    "        rescaling_input = inputs\n",
    "        \n",
    "    # Xception #\n",
    "    \"\"\"When run in \"tf\" mode it actuallly expect the input to be uint8 between 0 and 255 and scales it to the range from -1.0 to 1.0. \n",
    "    Check the docstring and the source code.\"\"\" #NOT TRUE\n",
    "    \"\"\" For Xception, call tf.keras.applications.xception.preprocess_input on your inputs before passing them to the model. \n",
    "    xception.preprocess_input will scale input pixels between -1 and 1.\"\"\"\n",
    "    if model_name.startswith('Xception'):\n",
    "        BaseCnn = getattr(importlib.import_module(\"tensorflow.python.keras.applications.xception\"), model_name)     \n",
    "        base_model = BaseCnn(include_top=False, weights=\"imagenet\") #{'imagenet', None}\n",
    "        \n",
    "        rescaling_input = tf.keras.applications.xception.preprocess_input(inputs)\n",
    "\n",
    "    # ResNet50 ResNet101 ResNet152 #\n",
    "    if model_name.startswith('ResNet'):\n",
    "        \"\"\"For ResNet, call tf.keras.applications.resnet.preprocess_input on your inputs before passing them to the model. \n",
    "        resnet.preprocess_input will convert the input images from RGB to BGR, then will zero-center each color channel with \n",
    "        respect to the ImageNet dataset, without scaling.\"\"\"\n",
    "        BaseCnn = getattr(importlib.import_module(\"tensorflow.python.keras.applications.resnet\"), model_name)     \n",
    "        base_model = BaseCnn(include_top=False, weights=\"imagenet\") #{'imagenet', None}    \n",
    "        \n",
    "        rescaling_input = tf.keras.applications.resnet.preprocess_input(inputs)\n",
    "        \n",
    "    # InceptionV3 #\n",
    "    if model_name.startswith('InceptionV3'):\n",
    "        \"\"\"For InceptionV3, call tf.keras.applications.inception_v3.preprocess_input on your inputs before passing them to the model. \n",
    "        inception_v3.preprocess_input will scale input pixels between -1 and 1.\"\"\"\n",
    "        BaseCnn = getattr(importlib.import_module(\"tensorflow.python.keras.applications.inception_v3\"), model_name)     \n",
    "        base_model = BaseCnn(include_top=False, weights=\"imagenet\") #{'imagenet', None}    \n",
    "        \n",
    "        rescaling_input = tf.keras.applications.inception_v3.preprocess_input(inputs)        \n",
    "\n",
    "    # MobileNet #\n",
    "    if model_name.endswith('MobileNet'):\n",
    "        \"\"\" For MobileNet, call tf.keras.applications.mobilenet.preprocess_input on your inputs before passing them to the model. \n",
    "        mobilenet.preprocess_input will scale input pixels between -1 and 1.\"\"\"\n",
    "        BaseCnn = getattr(importlib.import_module(\"tensorflow.python.keras.applications.mobilenet\"), model_name)     \n",
    "        base_model = BaseCnn(include_top=False, weights=\"imagenet\") #{'imagenet', None}    \n",
    "        \n",
    "        rescaling_input = tf.keras.applications.mobilenet.preprocess_input(inputs)             \n",
    "        \n",
    "    # MobileNetV2 #\n",
    "    if model_name.startswith('MobileNetV2'):\n",
    "        \"\"\" For MobileNetV2, call tf.keras.applications.mobilenet_v2.preprocess_input on your inputs before passing them to the model. \n",
    "        mobilenet_v2.preprocess_input will scale input pixels between -1 and 1.\"\"\"\n",
    "        BaseCnn = getattr(importlib.import_module(\"tensorflow.python.keras.applications.mobilenet_v2\"), model_name)     \n",
    "        base_model = BaseCnn(include_top=False, weights=\"imagenet\") #{'imagenet', None}    \n",
    "        \n",
    "        rescaling_input = tf.keras.applications.mobilenet_v2.preprocess_input(inputs)            \n",
    "        \n",
    "    # DenseNet121 DenseNet169 DenseNet201 #\n",
    "    if model_name.startswith('DenseNet'):\n",
    "        \"\"\" For DenseNet, call tf.keras.applications.densenet.preprocess_input on your inputs before passing them to the model.\"\"\"\n",
    "        BaseCnn = getattr(importlib.import_module(\"tensorflow.python.keras.applications.densenet\"), model_name)     \n",
    "        base_model = BaseCnn(include_top=False, weights=\"imagenet\") #{'imagenet', None}    \n",
    "        \n",
    "        rescaling_input = tf.keras.applications.densenet.preprocess_input(inputs)            \n",
    "        \n",
    "    # NASNet: NASNetMobile #\n",
    "    if model_name.startswith('NASNetMobile'):\n",
    "        \"\"\"For NASNet, call tf.keras.applications.nasnet.preprocess_input on your inputs before passing them to the model.\"\"\"\n",
    "        \"\"\"Optional shape tuple, only to be specified if include_top is False (otherwise the input shape has to be (224, 224, 3)\n",
    "        for NASNetMobile It should have exactly 3 inputs channels, and width and height should be no smaller than 32. E.g. (224, 224, 3) \n",
    "        would be one valid value. For loading imagenet weights, input_shape should be (224, 224, 3)\"\"\"\n",
    "        \n",
    "        \"\"\"otherwise the input shape has to be (331, 331, 3) for NASNetLarge. It should have exactly 3 inputs channels, and width and height should \n",
    "        be no smaller than 32. E.g. (224, 224, 3) would be one valid value.  For loading imagenet weights, input_shape should be (331, 331, 3)\"\"\"\n",
    "        \"\"\" NASNetMobile imagenet with 224: ted 10.x/5.x \n",
    "            NASNetMobile None with 120: ted 12.x/4.x,  seem no different at fine tune phase. \"\"\"\n",
    "        \n",
    "        \"\"\"pre-set use inputs = tf.keras.Input([None, None, 3]) to fake run build mode to get the model weight.  Then, run\n",
    "        on normal build with specific input size without imagenet-weight, and reload the weight by hand.\n",
    "        Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/nasnet/nasnet_mobile_no_top.h5\n",
    "        Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/nasnet/NASNet-large-no-top.h5\n",
    "        https://github.com/keras-team/keras-applications/issues/78\n",
    "        \"\"\"\n",
    "        \"\"\"Very large model, NASNetLarge take 8xM parameters, take 800~300 sec for one epoch.\n",
    "        Epoch 00015: val_accuracy did not improve from 0.87036\n",
    "        [306.6111526489258] of epoch 15\n",
    "        CPU times: user 7h 30min 14s, sys: 32min 2s, total: 8h 2min 16s\n",
    "        Wall time: 1h 26min 32s\n",
    "        \n",
    "        NASNetMobile: but loss: nan after epcoh 1, need reduce the lr!!!!!!!!!!!!!!!!!!!!!!!!!!!!!![12/14]Fixed by seprating mobile/large to two functions.\n",
    "        Epoch 00011: val_accuracy did not improve from 0.64444\n",
    "        [124.91568398475647] of epoch 11\n",
    "        CPU times: user 3h 19min 49s, sys: 3min 41s, total: 3h 23min 31s\n",
    "        Wall time: 29min 27s\n",
    "        \"\"\"\n",
    "        # Pre download the model first for it weight later we need to reload it.\n",
    "#         inputs = tf.keras.Input([None, None, 3])\n",
    "\n",
    "        rescaling_input = tf.keras.applications.nasnet.preprocess_input(inputs)     \n",
    "\n",
    "        BaseCnn = getattr(importlib.import_module(\"tensorflow.python.keras.applications.nasnet\"), model_name)     \n",
    "#         base_model = BaseCnn(include_top=False, weights=\"imagenet\") #{'imagenet', None}    \n",
    "#         base_model = BaseCnn(include_top=False, weights='imagenet', input_shape=(img_height, img_width, 3)) #{'imagenet', None} for set input to 120x120    \n",
    "#         base_model = BaseCnn(include_top=False, weights='imagenet') #{'imagenet', None} for set input to 120x120    \n",
    "\n",
    "        # load weight by hand.\n",
    "        base_model = BaseCnn(include_top=False, weights=None, input_shape=(img_height, img_width, 3))\n",
    "        base_model.load_weights('/home/u3148947/.keras/models/nasnet_mobile_no_top.h5')\n",
    "    \n",
    "#         inputs = tf.keras.layers.Resizing(224, 224) # tf >= 2.6.0, but currnet TWCC newest 21.08 is tf=2.5.0\n",
    "#         i = tf.compat.v1.keras.layers.experimental.preprocessing.Resizing(224, 224)(inputs)\n",
    "#         x = tf.cast(i, tf.float32)\n",
    "\n",
    "\n",
    "    # NASNet: NASNetLarge #\n",
    "    if model_name.startswith('NASNetLarge'):\n",
    "        rescaling_input = tf.keras.applications.nasnet.preprocess_input(inputs)     \n",
    "\n",
    "        BaseCnn = getattr(importlib.import_module(\"tensorflow.python.keras.applications.nasnet\"), model_name)    \n",
    "        \n",
    "        # load weight by hand.\n",
    "        base_model = BaseCnn(include_top=False, weights=None, input_shape=(img_height, img_width, 3))\n",
    "        base_model.load_weights('/home/u3148947/.keras/models/nasnet_large_no_top.h5')        \n",
    "        \n",
    "        \n",
    "        \n",
    "    # VGG16 #\n",
    "    \"\"\"VGG16 not train even with None or Imagenet. pooling= is not the factor\"\"\"\n",
    "    if model_name.startswith('VGG16'):\n",
    "        \"\"\"For VGG16, call tf.keras.applications.vgg16.preprocess_input on your inputs before passing them to the model. \n",
    "        vgg16.preprocess_input will convert the input images from RGB to BGR, then will zero-center each color channel \n",
    "        with respect to the ImageNet dataset, without scaling.\"\"\"\n",
    "#         BaseCnn = getattr(importlib.import_module(\"tensorflow.python.keras.applications.vgg16\"), model_name)     \n",
    "#         base_model = BaseCnn(include_top=False, weights='imagenet') #{'imagenet', None}\n",
    "        \n",
    "#         base_model = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
    "        \n",
    "#         rescaling_input = tf.keras.applications.vgg16.preprocess_input(inputs)  \n",
    "\n",
    "#         base_model = tf.keras.applications.vgg16.VGG16(weights='imagenet', include_top=False, input_tensor=rescaling_input)\n",
    "\n",
    "#        base_model = tf.keras.applications.vgg16.VGG16(include_top=False, weights='imagenet', input_shape=(img_height, img_width, 3))\n",
    "\n",
    "\n",
    "        base_model = tf.keras.applications.VGG16(include_top=False, weights='imagenet', input_tensor=inputs)\n",
    "\n",
    "\n",
    "\n",
    "    # VGG19 #\n",
    "    if model_name.startswith('VGG19'):\n",
    "        \"\"\"For VGG19, call tf.keras.applications.vgg19.preprocess_input on your inputs before passing them to the model. \n",
    "        vgg16.preprocess_input will convert the input images from RGB to BGR, then will zero-center each color channel \n",
    "        with respect to the ImageNet dataset, without scaling.\"\"\"\n",
    "        \n",
    "        BaseCnn = getattr(importlib.import_module(\"tensorflow.python.keras.applications.vgg19\"), model_name)     \n",
    "        base_model = BaseCnn(include_top=False, weights=\"imagenet\") #{'imagenet', None}    \n",
    "        \n",
    "        rescaling_input = tf.keras.applications.vgg16.preprocess_input(inputs)  \n",
    "        \n",
    "\n",
    "    # ViT # tf.hub version [Waiting for twcc update CCS image for version 21.11]\n",
    "    if model_name.startswith('ViT'):\n",
    "        \"\"\"Inputs to the model must:\n",
    "            1.be four dimensional Tensors of the shape (batch_size, height, width, num_channels). Note that the model expects images with channels_last property. num_channels must be 3.\n",
    "            2.be resized to 224x224 resolution.\n",
    "            3.have pixel values in the range [-1, 1].\n",
    "        \"\"\"\n",
    "        \"\"\"ValueError: Unknown SavedObject type: None\n",
    "        but work in wth tf2.6.0, tf2.7.0\n",
    "\n",
    "        \"\"\"\n",
    "        if model_name.startswith('ViT_b8'):\n",
    "            handle=\"https://tfhub.dev/sayakpaul/vit_b8_fe/1\"\n",
    "        if model_name.startswith('ViT_s16'):\n",
    "            handle=\"https://tfhub.dev/sayakpaul/vit_s16_fe/1\"\n",
    "        num_classes=5\n",
    "        \n",
    "        # ViT model as a layer\n",
    "        hub_layer = hub.KerasLayer(\"https://tfhub.dev/sayakpaul/vit_b8_fe/1\", trainable=True)\n",
    "        model = tf.keras.Sequential([\n",
    "                                        inputs,\n",
    "                                        hub_layer,\n",
    "                                        keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "                                        ])\n",
    " \n",
    " \n",
    "    \n",
    "#     # ViT # ViT-keras, but seems need more epoch to train.\n",
    "#     if model_name.startswith('ViT'):\n",
    "#         \"\"\"For ViT (vit-keras)\n",
    "#         There are models pre-trained on imagenet21k for the following architectures: ViT-B/16, ViT-B/32, ViT-L/16, ViT-L/32 and ViT-H/14. \n",
    "#         There are also the same models pre-trained on imagenet21k and fine-tuned on imagenet2012.\n",
    "        \n",
    "#         pip install -U --quiet vit-keras # for imagenet21k pre-trained weight.\n",
    "#         pip install -U tensorflow-addons # for scratch\n",
    "#         \"\"\"\n",
    "#         \"\"\"base_model = vit.vit_b32\n",
    "#         Downloading data from https://github.com/faustomorales/vit-keras/releases/download/dl/ViT-B_32_imagenet21k+imagenet2012.npz\n",
    "        \n",
    "#         base_model = vit.vit_b16\n",
    "#         Downloading data from https://github.com/faustomorales/vit-keras/releases/download/dl/ViT-B_16_imagenet21k+imagenet2012.npz\n",
    "#         \"\"\"\n",
    "#         \"\"\"\n",
    "#         ValueError: Input 0 of layer global_average_pooling2d is incompatible with the layer: expected ndim=4, found ndim=2. Full shape received: (None, 768)\n",
    "#         seems need use Flatten() to capture laster feature output.\n",
    "#         \"\"\"\n",
    "#         \"\"\"AssertionError: image_size must be a multiple of patch_size\n",
    "#         600 / 16 = 37.5\n",
    "#         512 / 16 = 32, only works\n",
    "#         \"\"\"\n",
    "#         from vit_keras import vit\n",
    "#         #import tensorflow_addons as tfa\n",
    "        \n",
    "#         #vit_b16 vit_b32  vit_L16 vit_L32  \n",
    "#         base_model = vit.vit_b16(\n",
    "#             image_size = img_width,\n",
    "#             activation = 'softmax',\n",
    "#             pretrained = True, #True,\n",
    "#             include_top = True,\n",
    "#             pretrained_top = False,\n",
    "#             classes = 5)\n",
    "        \n",
    "#         #rescaling_input = inputs\n",
    "        \n",
    "        \n",
    "        \n",
    "#     # template #\n",
    "#     if model_name.startswith(''):\n",
    "#         \"\"\"For.\"\"\"\n",
    "#         BaseCnn = getattr(importlib.import_module(\"tensorflow.python.keras.applications.\"), model_name)     \n",
    "#         base_model = BaseCnn(include_top=False, weights=\"imagenet\", input_shape=(120,120,3)) #{'imagenet', None}    \n",
    "        \n",
    "#         rescaling_input = tf.keras.applications.inception_v3.preprocess_input(inputs)  \n",
    "\n",
    "\n",
    "#     # template #\n",
    "#     if model_name.startswith(''):\n",
    "#         \"\"\"For.\"\"\"\n",
    "#         BaseCnn = getattr(importlib.import_module(\"tensorflow.python.keras.applications.\"), model_name)     \n",
    "#         base_model = BaseCnn(include_top=False, weights=\"imagenet\", input_shape=(120,120,3)) #{'imagenet', None}    \n",
    "        \n",
    "#         rescaling_input = tf.keras.applications.inception_v3.preprocess_input(inputs)  \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "# # move to Top \n",
    "#     # How to add training=False in base_model create\n",
    "#     #inputs = tf.keras.Input(shape=(120, 120, 3))\n",
    "#     #rescal = rescaling_input()(inputs)\n",
    "#     #b_m_output = base_model(inputs, training=False)\n",
    "    \n",
    "#     if model_name.startswith('ViT'):\n",
    "# #         out = tf.keras.layers.Flatten()(base_model.output)\n",
    "# #         out = tf.keras.layers.BatchNormalization()(out)\n",
    "# #         out = tf.keras.layers.Dense(11, activation = tfa.activations.gelu)(out)\n",
    "# #         out = tf.keras.layers.BatchNormalization()(out)\n",
    "# #         outputs = tf.keras.layers.Dense(5, 'softmax')(out)\n",
    "# #         model = tf.keras.Model(base_model.input, outputs, name=model_name)\n",
    "\n",
    "#         \"\"\"#vit_b16 vit_b32  vit_L16 vit_L32, just model.compile like tf.keras?!\n",
    "#         \"\"\"\n",
    "#         base_model = vit.vit_b16(\n",
    "#             image_size = img_width,\n",
    "#             activation = 'sigmoid',\n",
    "#             pretrained = True, #True,\n",
    "#             include_top = True,\n",
    "#             pretrained_top = False,\n",
    "#             classes = 5)\n",
    "#         model = base_model\n",
    "        \n",
    "#     else:\n",
    "        \n",
    "    \n",
    "    # ViT # tf.hub version [Waiting for twcc update CCS image for version 21.11]\n",
    "    if model_name.startswith('ViT'):\n",
    "        \"\"\"ViT was loaded above already.\"\"\"\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "        # Freeze the pretrained weights\n",
    "        base_model.trainable = False\n",
    "        print(\"base_model.trainable : \", base_model.trainable)\n",
    "        b_m_output = base_model(rescaling_input, training=False)\n",
    "\n",
    "        # Rebuild top       \n",
    "        gap2d = tf.keras.layers.GlobalAveragePooling2D()(b_m_output) #(base_model.output)\n",
    "        #BNL = tf.keras.layers.BatchNormalization()(gap2d) #tood: remove#\n",
    "        dropout = tf.keras.layers.Dropout(top_dropout_rate)(gap2d)#tood: remove# J add dropout, for flood 0.2 is ok. for leaf 0.4 is better.\n",
    "        #outputs = tf.keras.layers.Dense(outputnum)(dropout)# remove activation for regression output (to default, the linear), , activation = 'relu' no help\n",
    "        outputs = tf.keras.layers.Dense(5, activation=\"softmax\")(dropout)#todo: activation=\"softmax\", default is \"linear\" activation: a(x) = x\n",
    "\n",
    "\n",
    "        # Compile new model\n",
    "        model = tf.keras.Model(inputs, outputs, name=model_name)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),#RMSprop , Adam, SGD Adadelta(learning_rate=0.001), if set lr_callback the learning_rate=0.001 will not effeced.\n",
    "                loss=tf.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "    return model, base_model\n",
    "\n",
    "\n",
    "# Pick a model #\n",
    "Model_List = [\"Xception\", \"ResNet50\", \"ResNet101\", \"ResNet152\", \"InceptionV3\", \"MobileNet\", \"MobileNetV2\", # 0-6 \n",
    "\"DenseNet121\",\"DenseNet169\",\"DenseNet201\", # 7 8 9\n",
    "\"NASNetMobile\",\"NASNetLarge\", # 10 11 (hard code of size!224 331!)\n",
    "\"EfficientNetB0\", #12\n",
    "\"EfficientNetB1\", #13\n",
    "\"EfficientNetB3\",\n",
    "\"EfficientNetB5\", #15\n",
    "\"EfficientNetB7\", #16\n",
    "'VGG16', # 17 #train fail\n",
    "'VGG19', # 18\n",
    "'ViT_b8', #19 Vision Transformer\n",
    "'BiT', #20 BigTransfer\n",
    "'ConvMixer', #21\n",
    "]\n",
    "\n",
    "model_name = Model_List[19]\n",
    "# print(Model_List[1:3])\n",
    "print(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c085f10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5877c641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2d79e86",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\"> [Models] Train misc. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2f261742",
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor = 'val_accuracy' #'val_loss' 'val_accuracy' if use ed_loss it still the loss here.\n",
    "\n",
    "def mk_log_dir(log_dir_name):\n",
    "    try:\n",
    "        os.makedirs(log_dir_name)\n",
    "    except OSError as e:\n",
    "        print(\"This log dir exist.\")\n",
    "        if e.errno != errno.EEXIST:\n",
    "            raise ValueError(\"we got problem.\")\n",
    "\n",
    "def get_best_model_name(th):\n",
    "    return './' + log_dir_name + '/' + th + '_' + model_name + '_bs' + str(BATCH_SIZE) + '_w' + str(img_width) + '_best_' + monitor + '.h5'\n",
    "\n",
    "# th = 'toe'\n",
    "# th = 'heel'\n",
    "\n",
    "th = \"stage1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9f3a7d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This log dir exist.\n",
      "best_model_name: ./TrainSaveDir/stage1_ViT_b8_bs4_w512_best_val_accuracy.h5\n"
     ]
    }
   ],
   "source": [
    "# use once at the time\n",
    "# log_dir_name = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir_name = \"TrainSaveDir\"\n",
    "\n",
    "mk_log_dir(log_dir_name)\n",
    "\n",
    "best_model_name = get_best_model_name(th)\n",
    "\n",
    "best_model_save = tf.keras.callbacks.ModelCheckpoint(filepath=best_model_name, \n",
    "                             save_best_only = True, \n",
    "                             save_weights_only = False,\n",
    "                             monitor = monitor, \n",
    "                             mode = 'auto', verbose = 1)\n",
    "print('best_model_name:', best_model_name)\n",
    "\n",
    "logdir = log_dir_name + \"/logs/toe/\"\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947f7dba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38d480e3",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\"> [Models] Learning Rate Scheduler </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "59b347c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc350641730>]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"dump lr\n",
    "\"\"\"\n",
    "ep_num_transf = 500\n",
    "\n",
    "\n",
    "\n",
    "def lrdump(epoch):\n",
    "    \n",
    "    #step_size = 100\n",
    "    lr_max = 0.004\n",
    "    lr_min = 0.00001\n",
    "    lr_start = 0.00001\n",
    "\n",
    "    lr_init_ep = 0\n",
    "    lr_ramp_ep = 50\n",
    "    lr_sus_ep  = 0 #10\n",
    "    lr_decay   = 0.95\n",
    "\n",
    "    \n",
    "    # warm up\n",
    "    if epoch < lr_init_ep:\n",
    "        lr = (lr_max - lr_min) / lr_ramp_ep * epoch + lr_min    \n",
    "        \n",
    "    elif lr_init_ep -1 < epoch < lr_ramp_ep:\n",
    "        lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "\n",
    "    elif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "        lr = lr_max\n",
    "\n",
    "    else:\n",
    "        lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
    "\n",
    "    return lr\n",
    "\n",
    "rng = [i for i in range(ep_num_transf)]\n",
    "y = [lrdump(x) for x in rng]\n",
    "sns.set(style='darkgrid')\n",
    "fig, ax = plt.subplots(figsize=(20, 6))\n",
    "plt.plot(rng, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "62dfb9e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' [LR] Stepwise warmup cosine decay restart.\\ntodo\\n'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" [LR] Stepwise warmup cosine decay restart.\n",
    "todo\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# def lr_step_WCDR(step):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8385fdae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-05 ~ 0.004\n"
     ]
    }
   ],
   "source": [
    "print('{} ~ {}'.format(min(y), max(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0317b23f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t 1e-05\n",
      "\n",
      "1\t 8.980000000000001e-05\n",
      "\n",
      "2\t 0.00016960000000000003\n",
      "\n",
      "3\t 0.00024940000000000004\n",
      "\n",
      "4\t 0.0003292000000000001\n",
      "\n",
      "5\t 0.0004090000000000001\n",
      "\n",
      "6\t 0.0004888000000000001\n",
      "\n",
      "7\t 0.0005686000000000002\n",
      "\n",
      "8\t 0.0006484000000000001\n",
      "\n",
      "9\t 0.0007282000000000001\n",
      "\n",
      "10\t 0.0008080000000000001\n",
      "\n",
      "11\t 0.0008878000000000002\n",
      "\n",
      "12\t 0.0009676000000000002\n",
      "\n",
      "13\t 0.0010474000000000002\n",
      "\n",
      "14\t 0.0011272000000000003\n",
      "\n",
      "15\t 0.0012070000000000002\n",
      "\n",
      "16\t 0.0012868000000000003\n",
      "\n",
      "17\t 0.0013666000000000004\n",
      "\n",
      "18\t 0.0014464000000000002\n",
      "\n",
      "19\t 0.0015262000000000003\n",
      "\n",
      "20\t 0.0016060000000000002\n",
      "\n",
      "21\t 0.0016858000000000003\n",
      "\n",
      "22\t 0.0017656000000000004\n",
      "\n",
      "23\t 0.0018454000000000003\n",
      "\n",
      "24\t 0.0019252000000000004\n",
      "\n",
      "25\t 0.0020050000000000003\n",
      "\n",
      "26\t 0.0020848000000000004\n",
      "\n",
      "27\t 0.0021646000000000005\n",
      "\n",
      "28\t 0.0022444000000000006\n",
      "\n",
      "29\t 0.0023242000000000007\n",
      "\n",
      "30\t 0.0024040000000000003\n",
      "\n",
      "31\t 0.0024838000000000004\n",
      "\n",
      "32\t 0.0025636000000000005\n",
      "\n",
      "33\t 0.0026434000000000006\n",
      "\n",
      "34\t 0.0027232000000000007\n",
      "\n",
      "35\t 0.0028030000000000004\n",
      "\n",
      "36\t 0.0028828000000000005\n",
      "\n",
      "37\t 0.0029626000000000006\n",
      "\n",
      "38\t 0.0030424000000000007\n",
      "\n",
      "39\t 0.0031222000000000008\n",
      "\n",
      "40\t 0.0032020000000000004\n",
      "\n",
      "41\t 0.0032818000000000005\n",
      "\n",
      "42\t 0.0033616000000000006\n",
      "\n",
      "43\t 0.0034414000000000007\n",
      "\n",
      "44\t 0.003521200000000001\n",
      "\n",
      "45\t 0.003601000000000001\n",
      "\n",
      "46\t 0.0036808000000000006\n",
      "\n",
      "47\t 0.0037606000000000007\n",
      "\n",
      "48\t 0.0038404000000000008\n",
      "\n",
      "49\t 0.0039202\n",
      "\n",
      "50\t 0.004\n",
      "\n",
      "51\t 0.0038005000000000005\n",
      "\n",
      "52\t 0.003610975\n",
      "\n",
      "53\t 0.00343092625\n",
      "\n",
      "54\t 0.0032598799374999998\n",
      "\n",
      "55\t 0.0030973859406249996\n",
      "\n",
      "56\t 0.0029430166435937496\n",
      "\n",
      "57\t 0.0027963658114140617\n",
      "\n",
      "58\t 0.002657047520843359\n",
      "\n",
      "59\t 0.0025246951448011905\n",
      "\n",
      "60\t 0.002398960387561131\n",
      "\n",
      "61\t 0.0022795123681830743\n",
      "\n",
      "62\t 0.0021660367497739206\n",
      "\n",
      "63\t 0.0020582349122852243\n",
      "\n",
      "64\t 0.001955823166670963\n",
      "\n",
      "65\t 0.001858532008337415\n",
      "\n",
      "66\t 0.001766105407920544\n",
      "\n",
      "67\t 0.0016783001375245167\n",
      "\n",
      "68\t 0.001594885130648291\n",
      "\n",
      "69\t 0.0015156408741158762\n",
      "\n",
      "70\t 0.0014403588304100824\n",
      "\n",
      "71\t 0.0013688408888895783\n",
      "\n",
      "72\t 0.0013008988444450991\n",
      "\n",
      "73\t 0.0012363539022228443\n",
      "\n",
      "74\t 0.0011750362071117019\n",
      "\n",
      "75\t 0.001116784396756117\n",
      "\n",
      "76\t 0.0010614451769183108\n",
      "\n",
      "77\t 0.0010088729180723953\n",
      "\n",
      "78\t 0.0009589292721687754\n",
      "\n",
      "79\t 0.0009114828085603367\n",
      "\n",
      "80\t 0.0008664086681323198\n",
      "\n",
      "81\t 0.0008235882347257038\n",
      "\n",
      "82\t 0.0007829088229894186\n",
      "\n",
      "83\t 0.0007442633818399477\n",
      "\n",
      "84\t 0.0007075502127479502\n",
      "\n",
      "85\t 0.0006726727021105527\n",
      "\n",
      "86\t 0.000639539067005025\n",
      "\n",
      "87\t 0.0006080621136547738\n",
      "\n",
      "88\t 0.000578159007972035\n",
      "\n",
      "89\t 0.0005497510575734332\n",
      "\n",
      "90\t 0.0005227635046947616\n",
      "\n",
      "91\t 0.0004971253294600235\n",
      "\n",
      "92\t 0.0004727690629870223\n",
      "\n",
      "93\t 0.00044963060983767116\n",
      "\n",
      "94\t 0.0004276490793457876\n",
      "\n",
      "95\t 0.00040676662537849815\n",
      "\n",
      "96\t 0.0003869282941095732\n",
      "\n",
      "97\t 0.0003680818794040946\n",
      "\n",
      "98\t 0.00035017778543388977\n",
      "\n",
      "99\t 0.00033316889616219535\n",
      "\n",
      "100\t 0.0003170104513540855\n",
      "\n",
      "101\t 0.00030165992878638127\n",
      "\n",
      "102\t 0.00028707693234706217\n",
      "\n",
      "103\t 0.0002732230857297091\n",
      "\n",
      "104\t 0.00026006193144322357\n",
      "\n",
      "105\t 0.0002475588348710624\n",
      "\n",
      "106\t 0.00023568089312750923\n",
      "\n",
      "107\t 0.00022439684847113376\n",
      "\n",
      "108\t 0.00021367700604757705\n",
      "\n",
      "109\t 0.00020349315574519822\n",
      "\n",
      "110\t 0.0001938184979579383\n",
      "\n",
      "111\t 0.00018462757306004137\n",
      "\n",
      "112\t 0.0001758961944070393\n",
      "\n",
      "113\t 0.00016760138468668731\n",
      "\n",
      "114\t 0.00015972131545235294\n",
      "\n",
      "115\t 0.0001522352496797353\n",
      "\n",
      "116\t 0.00014512348719574852\n",
      "\n",
      "117\t 0.00013836731283596107\n",
      "\n",
      "118\t 0.00013194894719416303\n",
      "\n",
      "119\t 0.00012585149983445487\n",
      "\n",
      "120\t 0.00012005892484273212\n",
      "\n",
      "121\t 0.00011455597860059551\n",
      "\n",
      "122\t 0.00010932817967056573\n",
      "\n",
      "123\t 0.00010436177068703745\n",
      "\n",
      "124\t 9.964368215268557e-05\n",
      "\n",
      "125\t 9.516149804505128e-05\n",
      "\n",
      "126\t 9.090342314279871e-05\n",
      "\n",
      "127\t 8.685825198565877e-05\n",
      "\n",
      "128\t 8.301533938637583e-05\n",
      "\n",
      "129\t 7.936457241705704e-05\n",
      "\n",
      "130\t 7.589634379620419e-05\n",
      "\n",
      "131\t 7.260152660639397e-05\n",
      "\n",
      "132\t 6.947145027607428e-05\n",
      "\n",
      "133\t 6.649787776227055e-05\n",
      "\n",
      "134\t 6.367298387415703e-05\n",
      "\n",
      "135\t 6.098933468044917e-05\n",
      "\n",
      "136\t 5.8439867946426706e-05\n",
      "\n",
      "137\t 5.601787454910537e-05\n",
      "\n",
      "138\t 5.37169808216501e-05\n",
      "\n",
      "139\t 5.15311317805676e-05\n",
      "\n",
      "140\t 4.945457519153921e-05\n",
      "\n",
      "141\t 4.748184643196225e-05\n",
      "\n",
      "142\t 4.560775411036414e-05\n",
      "\n",
      "143\t 4.382736640484593e-05\n",
      "\n",
      "144\t 4.213599808460363e-05\n",
      "\n",
      "145\t 4.052919818037344e-05\n",
      "\n",
      "146\t 3.900273827135477e-05\n",
      "\n",
      "147\t 3.7552601357787035e-05\n",
      "\n",
      "148\t 3.617497128989768e-05\n",
      "\n",
      "149\t 3.486622272540279e-05\n",
      "\n",
      "150\t 3.3622911589132655e-05\n",
      "\n",
      "151\t 3.244176600967602e-05\n",
      "\n",
      "152\t 3.131967770919222e-05\n",
      "\n",
      "153\t 3.0253693823732608e-05\n",
      "\n",
      "154\t 2.9241009132545977e-05\n",
      "\n",
      "155\t 2.8278958675918675e-05\n",
      "\n",
      "156\t 2.736501074212274e-05\n",
      "\n",
      "157\t 2.6496760205016604e-05\n",
      "\n",
      "158\t 2.5671922194765774e-05\n",
      "\n",
      "159\t 2.4888326085027486e-05\n",
      "\n",
      "160\t 2.4143909780776108e-05\n",
      "\n",
      "161\t 2.34367142917373e-05\n",
      "\n",
      "162\t 2.2764878577150438e-05\n",
      "\n",
      "163\t 2.2126634648292915e-05\n",
      "\n",
      "164\t 2.1520302915878267e-05\n",
      "\n",
      "165\t 2.0944287770084354e-05\n",
      "\n",
      "166\t 2.0397073381580138e-05\n",
      "\n",
      "167\t 1.987721971250113e-05\n",
      "\n",
      "168\t 1.9383358726876072e-05\n",
      "\n",
      "169\t 1.8914190790532267e-05\n",
      "\n",
      "170\t 1.8468481251005653e-05\n",
      "\n",
      "171\t 1.8045057188455375e-05\n",
      "\n",
      "172\t 1.7642804329032602e-05\n",
      "\n",
      "173\t 1.7260664112580972e-05\n",
      "\n",
      "174\t 1.6897630906951923e-05\n",
      "\n",
      "175\t 1.6552749361604327e-05\n",
      "\n",
      "176\t 1.622511189352411e-05\n",
      "\n",
      "177\t 1.5913856298847904e-05\n",
      "\n",
      "178\t 1.561816348390551e-05\n",
      "\n",
      "179\t 1.5337255309710235e-05\n",
      "\n",
      "180\t 1.5070392544224722e-05\n",
      "\n",
      "181\t 1.4816872917013485e-05\n",
      "\n",
      "182\t 1.4576029271162812e-05\n",
      "\n",
      "183\t 1.4347227807604669e-05\n",
      "\n",
      "184\t 1.4129866417224437e-05\n",
      "\n",
      "185\t 1.3923373096363214e-05\n",
      "\n",
      "186\t 1.3727204441545054e-05\n",
      "\n",
      "187\t 1.35408442194678e-05\n",
      "\n",
      "188\t 1.336380200849441e-05\n",
      "\n",
      "189\t 1.319561190806969e-05\n",
      "\n",
      "190\t 1.3035831312666207e-05\n",
      "\n",
      "191\t 1.2884039747032895e-05\n",
      "\n",
      "192\t 1.273983775968125e-05\n",
      "\n",
      "193\t 1.2602845871697188e-05\n",
      "\n",
      "194\t 1.2472703578112328e-05\n",
      "\n",
      "195\t 1.2349068399206712e-05\n",
      "\n",
      "196\t 1.2231614979246376e-05\n",
      "\n",
      "197\t 1.2120034230284057e-05\n",
      "\n",
      "198\t 1.2014032518769855e-05\n",
      "\n",
      "199\t 1.1913330892831362e-05\n",
      "\n",
      "200\t 1.1817664348189793e-05\n",
      "\n",
      "201\t 1.1726781130780304e-05\n",
      "\n",
      "202\t 1.1640442074241289e-05\n",
      "\n",
      "203\t 1.1558419970529224e-05\n",
      "\n",
      "204\t 1.1480498972002763e-05\n",
      "\n",
      "205\t 1.1406474023402625e-05\n",
      "\n",
      "206\t 1.1336150322232494e-05\n",
      "\n",
      "207\t 1.1269342806120869e-05\n",
      "\n",
      "208\t 1.1205875665814824e-05\n",
      "\n",
      "209\t 1.1145581882524084e-05\n",
      "\n",
      "210\t 1.108830278839788e-05\n",
      "\n",
      "211\t 1.1033887648977986e-05\n",
      "\n",
      "212\t 1.0982193266529087e-05\n",
      "\n",
      "213\t 1.0933083603202632e-05\n",
      "\n",
      "214\t 1.08864294230425e-05\n",
      "\n",
      "215\t 1.0842107951890376e-05\n",
      "\n",
      "216\t 1.0800002554295857e-05\n",
      "\n",
      "217\t 1.0760002426581063e-05\n",
      "\n",
      "218\t 1.0722002305252011e-05\n",
      "\n",
      "219\t 1.068590218998941e-05\n",
      "\n",
      "220\t 1.065160708048994e-05\n",
      "\n",
      "221\t 1.0619026726465443e-05\n",
      "\n",
      "222\t 1.058807539014217e-05\n",
      "\n",
      "223\t 1.0558671620635062e-05\n",
      "\n",
      "224\t 1.053073803960331e-05\n",
      "\n",
      "225\t 1.0504201137623143e-05\n",
      "\n",
      "226\t 1.0478991080741987e-05\n",
      "\n",
      "227\t 1.0455041526704886e-05\n",
      "\n",
      "228\t 1.0432289450369642e-05\n",
      "\n",
      "229\t 1.041067497785116e-05\n",
      "\n",
      "230\t 1.0390141228958602e-05\n",
      "\n",
      "231\t 1.0370634167510672e-05\n",
      "\n",
      "232\t 1.0352102459135138e-05\n",
      "\n",
      "233\t 1.0334497336178382e-05\n",
      "\n",
      "234\t 1.0317772469369464e-05\n",
      "\n",
      "235\t 1.030188384590099e-05\n",
      "\n",
      "236\t 1.0286789653605941e-05\n",
      "\n",
      "237\t 1.0272450170925643e-05\n",
      "\n",
      "238\t 1.0258827662379361e-05\n",
      "\n",
      "239\t 1.0245886279260393e-05\n",
      "\n",
      "240\t 1.0233591965297373e-05\n",
      "\n",
      "241\t 1.0221912367032506e-05\n",
      "\n",
      "242\t 1.021081674868088e-05\n",
      "\n",
      "243\t 1.0200275911246836e-05\n",
      "\n",
      "244\t 1.0190262115684495e-05\n",
      "\n",
      "245\t 1.018074900990027e-05\n",
      "\n",
      "246\t 1.0171711559405256e-05\n",
      "\n",
      "247\t 1.0163125981434993e-05\n",
      "\n",
      "248\t 1.0154969682363244e-05\n",
      "\n",
      "249\t 1.014722119824508e-05\n",
      "\n",
      "250\t 1.0139860138332828e-05\n",
      "\n",
      "251\t 1.0132867131416187e-05\n",
      "\n",
      "252\t 1.0126223774845377e-05\n",
      "\n",
      "253\t 1.0119912586103108e-05\n",
      "\n",
      "254\t 1.0113916956797953e-05\n",
      "\n",
      "255\t 1.0108221108958054e-05\n",
      "\n",
      "256\t 1.0102810053510153e-05\n",
      "\n",
      "257\t 1.0097669550834645e-05\n",
      "\n",
      "258\t 1.0092786073292913e-05\n",
      "\n",
      "259\t 1.0088146769628266e-05\n",
      "\n",
      "260\t 1.0083739431146854e-05\n",
      "\n",
      "261\t 1.007955245958951e-05\n",
      "\n",
      "262\t 1.0075574836610035e-05\n",
      "\n",
      "263\t 1.0071796094779534e-05\n",
      "\n",
      "264\t 1.0068206290040557e-05\n",
      "\n",
      "265\t 1.006479597553853e-05\n",
      "\n",
      "266\t 1.0061556176761604e-05\n",
      "\n",
      "267\t 1.0058478367923523e-05\n",
      "\n",
      "268\t 1.0055554449527346e-05\n",
      "\n",
      "269\t 1.005277672705098e-05\n",
      "\n",
      "270\t 1.0050137890698431e-05\n",
      "\n",
      "271\t 1.0047630996163508e-05\n",
      "\n",
      "272\t 1.0045249446355334e-05\n",
      "\n",
      "273\t 1.0042986974037567e-05\n",
      "\n",
      "274\t 1.0040837625335689e-05\n",
      "\n",
      "275\t 1.0038795744068904e-05\n",
      "\n",
      "276\t 1.0036855956865459e-05\n",
      "\n",
      "277\t 1.0035013159022186e-05\n",
      "\n",
      "278\t 1.0033262501071077e-05\n",
      "\n",
      "279\t 1.0031599376017523e-05\n",
      "\n",
      "280\t 1.0030019407216647e-05\n",
      "\n",
      "281\t 1.0028518436855814e-05\n",
      "\n",
      "282\t 1.0027092515013024e-05\n",
      "\n",
      "283\t 1.0025737889262372e-05\n",
      "\n",
      "284\t 1.0024450994799255e-05\n",
      "\n",
      "285\t 1.0023228445059292e-05\n",
      "\n",
      "286\t 1.0022067022806326e-05\n",
      "\n",
      "287\t 1.002096367166601e-05\n",
      "\n",
      "288\t 1.001991548808271e-05\n",
      "\n",
      "289\t 1.0018919713678574e-05\n",
      "\n",
      "290\t 1.0017973727994646e-05\n",
      "\n",
      "291\t 1.0017075041594914e-05\n",
      "\n",
      "292\t 1.0016221289515168e-05\n",
      "\n",
      "293\t 1.001541022503941e-05\n",
      "\n",
      "294\t 1.001463971378744e-05\n",
      "\n",
      "295\t 1.0013907728098068e-05\n",
      "\n",
      "296\t 1.0013212341693164e-05\n",
      "\n",
      "297\t 1.0012551724608506e-05\n",
      "\n",
      "298\t 1.001192413837808e-05\n",
      "\n",
      "299\t 1.0011327931459177e-05\n",
      "\n",
      "300\t 1.0010761534886218e-05\n",
      "\n",
      "301\t 1.0010223458141907e-05\n",
      "\n",
      "302\t 1.0009712285234812e-05\n",
      "\n",
      "303\t 1.0009226670973072e-05\n",
      "\n",
      "304\t 1.0008765337424418e-05\n",
      "\n",
      "305\t 1.0008327070553197e-05\n",
      "\n",
      "306\t 1.0007910717025537e-05\n",
      "\n",
      "307\t 1.000751518117426e-05\n",
      "\n",
      "308\t 1.0007139422115547e-05\n",
      "\n",
      "309\t 1.000678245100977e-05\n",
      "\n",
      "310\t 1.000644332845928e-05\n",
      "\n",
      "311\t 1.0006121162036317e-05\n",
      "\n",
      "312\t 1.0005815103934501e-05\n",
      "\n",
      "313\t 1.0005524348737776e-05\n",
      "\n",
      "314\t 1.0005248131300888e-05\n",
      "\n",
      "315\t 1.0004985724735844e-05\n",
      "\n",
      "316\t 1.0004736438499051e-05\n",
      "\n",
      "317\t 1.00044996165741e-05\n",
      "\n",
      "318\t 1.0004274635745394e-05\n",
      "\n",
      "319\t 1.0004060903958124e-05\n",
      "\n",
      "320\t 1.0003857858760218e-05\n",
      "\n",
      "321\t 1.0003664965822207e-05\n",
      "\n",
      "322\t 1.0003481717531096e-05\n",
      "\n",
      "323\t 1.0003307631654543e-05\n",
      "\n",
      "324\t 1.0003142250071815e-05\n",
      "\n",
      "325\t 1.0002985137568225e-05\n",
      "\n",
      "326\t 1.0002835880689813e-05\n",
      "\n",
      "327\t 1.0002694086655323e-05\n",
      "\n",
      "328\t 1.0002559382322557e-05\n",
      "\n",
      "329\t 1.0002431413206428e-05\n",
      "\n",
      "330\t 1.0002309842546107e-05\n",
      "\n",
      "331\t 1.0002194350418802e-05\n",
      "\n",
      "332\t 1.0002084632897861e-05\n",
      "\n",
      "333\t 1.0001980401252969e-05\n",
      "\n",
      "334\t 1.000188138119032e-05\n",
      "\n",
      "335\t 1.0001787312130805e-05\n",
      "\n",
      "336\t 1.0001697946524265e-05\n",
      "\n",
      "337\t 1.0001613049198052e-05\n",
      "\n",
      "338\t 1.0001532396738149e-05\n",
      "\n",
      "339\t 1.0001455776901241e-05\n",
      "\n",
      "340\t 1.0001382988056179e-05\n",
      "\n",
      "341\t 1.000131383865337e-05\n",
      "\n",
      "342\t 1.0001248146720702e-05\n",
      "\n",
      "343\t 1.0001185739384667e-05\n",
      "\n",
      "344\t 1.0001126452415433e-05\n",
      "\n",
      "345\t 1.0001070129794661e-05\n",
      "\n",
      "346\t 1.0001016623304929e-05\n",
      "\n",
      "347\t 1.0000965792139682e-05\n",
      "\n",
      "348\t 1.0000917502532699e-05\n",
      "\n",
      "349\t 1.0000871627406064e-05\n",
      "\n",
      "350\t 1.000082804603576e-05\n",
      "\n",
      "351\t 1.0000786643733972e-05\n",
      "\n",
      "352\t 1.0000747311547274e-05\n",
      "\n",
      "353\t 1.000070994596991e-05\n",
      "\n",
      "354\t 1.0000674448671415e-05\n",
      "\n",
      "355\t 1.0000640726237844e-05\n",
      "\n",
      "356\t 1.0000608689925951e-05\n",
      "\n",
      "357\t 1.0000578255429655e-05\n",
      "\n",
      "358\t 1.000054934265817e-05\n",
      "\n",
      "359\t 1.0000521875525263e-05\n",
      "\n",
      "360\t 1.0000495781748999e-05\n",
      "\n",
      "361\t 1.000047099266155e-05\n",
      "\n",
      "362\t 1.0000447443028472e-05\n",
      "\n",
      "363\t 1.0000425070877049e-05\n",
      "\n",
      "364\t 1.0000403817333197e-05\n",
      "\n",
      "365\t 1.0000383626466537e-05\n",
      "\n",
      "366\t 1.000036444514321e-05\n",
      "\n",
      "367\t 1.000034622288605e-05\n",
      "\n",
      "368\t 1.0000328911741748e-05\n",
      "\n",
      "369\t 1.000031246615466e-05\n",
      "\n",
      "370\t 1.0000296842846926e-05\n",
      "\n",
      "371\t 1.0000282000704581e-05\n",
      "\n",
      "372\t 1.0000267900669351e-05\n",
      "\n",
      "373\t 1.0000254505635885e-05\n",
      "\n",
      "374\t 1.000024178035409e-05\n",
      "\n",
      "375\t 1.0000229691336385e-05\n",
      "\n",
      "376\t 1.0000218206769566e-05\n",
      "\n",
      "377\t 1.0000207296431088e-05\n",
      "\n",
      "378\t 1.0000196931609533e-05\n",
      "\n",
      "379\t 1.0000187085029058e-05\n",
      "\n",
      "380\t 1.0000177730777604e-05\n",
      "\n",
      "381\t 1.0000168844238723e-05\n",
      "\n",
      "382\t 1.0000160402026787e-05\n",
      "\n",
      "383\t 1.0000152381925448e-05\n",
      "\n",
      "384\t 1.0000144762829176e-05\n",
      "\n",
      "385\t 1.0000137524687717e-05\n",
      "\n",
      "386\t 1.0000130648453332e-05\n",
      "\n",
      "387\t 1.0000124116030664e-05\n",
      "\n",
      "388\t 1.0000117910229131e-05\n",
      "\n",
      "389\t 1.0000112014717676e-05\n",
      "\n",
      "390\t 1.0000106413981792e-05\n",
      "\n",
      "391\t 1.0000101093282702e-05\n",
      "\n",
      "392\t 1.0000096038618567e-05\n",
      "\n",
      "393\t 1.000009123668764e-05\n",
      "\n",
      "394\t 1.0000086674853256e-05\n",
      "\n",
      "395\t 1.0000082341110593e-05\n",
      "\n",
      "396\t 1.0000078224055064e-05\n",
      "\n",
      "397\t 1.0000074312852312e-05\n",
      "\n",
      "398\t 1.0000070597209696e-05\n",
      "\n",
      "399\t 1.0000067067349212e-05\n",
      "\n",
      "400\t 1.000006371398175e-05\n",
      "\n",
      "401\t 1.0000060528282664e-05\n",
      "\n",
      "402\t 1.0000057501868529e-05\n",
      "\n",
      "403\t 1.0000054626775103e-05\n",
      "\n",
      "404\t 1.0000051895436349e-05\n",
      "\n",
      "405\t 1.0000049300664531e-05\n",
      "\n",
      "406\t 1.0000046835631304e-05\n",
      "\n",
      "407\t 1.0000044493849739e-05\n",
      "\n",
      "408\t 1.0000042269157252e-05\n",
      "\n",
      "409\t 1.000004015569939e-05\n",
      "\n",
      "410\t 1.000003814791442e-05\n",
      "\n",
      "411\t 1.00000362405187e-05\n",
      "\n",
      "412\t 1.0000034428492764e-05\n",
      "\n",
      "413\t 1.0000032707068125e-05\n",
      "\n",
      "414\t 1.000003107171472e-05\n",
      "\n",
      "415\t 1.0000029518128983e-05\n",
      "\n",
      "416\t 1.0000028042222535e-05\n",
      "\n",
      "417\t 1.0000026640111408e-05\n",
      "\n",
      "418\t 1.0000025308105838e-05\n",
      "\n",
      "419\t 1.0000024042700545e-05\n",
      "\n",
      "420\t 1.0000022840565518e-05\n",
      "\n",
      "421\t 1.0000021698537243e-05\n",
      "\n",
      "422\t 1.0000020613610381e-05\n",
      "\n",
      "423\t 1.0000019582929861e-05\n",
      "\n",
      "424\t 1.0000018603783369e-05\n",
      "\n",
      "425\t 1.00000176735942e-05\n",
      "\n",
      "426\t 1.000001678991449e-05\n",
      "\n",
      "427\t 1.0000015950418765e-05\n",
      "\n",
      "428\t 1.0000015152897828e-05\n",
      "\n",
      "429\t 1.0000014395252937e-05\n",
      "\n",
      "430\t 1.000001367549029e-05\n",
      "\n",
      "431\t 1.0000012991715775e-05\n",
      "\n",
      "432\t 1.0000012342129986e-05\n",
      "\n",
      "433\t 1.0000011725023487e-05\n",
      "\n",
      "434\t 1.0000011138772312e-05\n",
      "\n",
      "435\t 1.0000010581833697e-05\n",
      "\n",
      "436\t 1.0000010052742012e-05\n",
      "\n",
      "437\t 1.0000009550104911e-05\n",
      "\n",
      "438\t 1.0000009072599667e-05\n",
      "\n",
      "439\t 1.0000008618969683e-05\n",
      "\n",
      "440\t 1.0000008188021198e-05\n",
      "\n",
      "441\t 1.0000007778620139e-05\n",
      "\n",
      "442\t 1.0000007389689132e-05\n",
      "\n",
      "443\t 1.0000007020204675e-05\n",
      "\n",
      "444\t 1.0000006669194442e-05\n",
      "\n",
      "445\t 1.000000633573472e-05\n",
      "\n",
      "446\t 1.0000006018947984e-05\n",
      "\n",
      "447\t 1.0000005718000584e-05\n",
      "\n",
      "448\t 1.0000005432100556e-05\n",
      "\n",
      "449\t 1.0000005160495528e-05\n",
      "\n",
      "450\t 1.000000490247075e-05\n",
      "\n",
      "451\t 1.0000004657347213e-05\n",
      "\n",
      "452\t 1.0000004424479853e-05\n",
      "\n",
      "453\t 1.000000420325586e-05\n",
      "\n",
      "454\t 1.0000003993093068e-05\n",
      "\n",
      "455\t 1.0000003793438414e-05\n",
      "\n",
      "456\t 1.0000003603766493e-05\n",
      "\n",
      "457\t 1.0000003423578169e-05\n",
      "\n",
      "458\t 1.0000003252399261e-05\n",
      "\n",
      "459\t 1.0000003089779298e-05\n",
      "\n",
      "460\t 1.0000002935290333e-05\n",
      "\n",
      "461\t 1.0000002788525816e-05\n",
      "\n",
      "462\t 1.0000002649099526e-05\n",
      "\n",
      "463\t 1.0000002516644549e-05\n",
      "\n",
      "464\t 1.0000002390812321e-05\n",
      "\n",
      "465\t 1.0000002271271706e-05\n",
      "\n",
      "466\t 1.000000215770812e-05\n",
      "\n",
      "467\t 1.0000002049822715e-05\n",
      "\n",
      "468\t 1.0000001947331579e-05\n",
      "\n",
      "469\t 1.0000001849965e-05\n",
      "\n",
      "470\t 1.000000175746675e-05\n",
      "\n",
      "471\t 1.0000001669593413e-05\n",
      "\n",
      "472\t 1.0000001586113742e-05\n",
      "\n",
      "473\t 1.0000001506808054e-05\n",
      "\n",
      "474\t 1.0000001431467652e-05\n",
      "\n",
      "475\t 1.0000001359894269e-05\n",
      "\n",
      "476\t 1.0000001291899556e-05\n",
      "\n",
      "477\t 1.0000001227304578e-05\n",
      "\n",
      "478\t 1.000000116593935e-05\n",
      "\n",
      "479\t 1.0000001107642382e-05\n",
      "\n",
      "480\t 1.0000001052260263e-05\n",
      "\n",
      "481\t 1.000000099964725e-05\n",
      "\n",
      "482\t 1.0000000949664888e-05\n",
      "\n",
      "483\t 1.0000000902181644e-05\n",
      "\n",
      "484\t 1.000000085707256e-05\n",
      "\n",
      "485\t 1.0000000814218932e-05\n",
      "\n",
      "486\t 1.0000000773507986e-05\n",
      "\n",
      "487\t 1.0000000734832587e-05\n",
      "\n",
      "488\t 1.0000000698090958e-05\n",
      "\n",
      "489\t 1.000000066318641e-05\n",
      "\n",
      "490\t 1.000000063002709e-05\n",
      "\n",
      "491\t 1.0000000598525735e-05\n",
      "\n",
      "492\t 1.0000000568599449e-05\n",
      "\n",
      "493\t 1.0000000540169476e-05\n",
      "\n",
      "494\t 1.0000000513161002e-05\n",
      "\n",
      "495\t 1.0000000487502953e-05\n",
      "\n",
      "496\t 1.0000000463127804e-05\n",
      "\n",
      "497\t 1.0000000439971414e-05\n",
      "\n",
      "498\t 1.0000000417972844e-05\n",
      "\n",
      "499\t 1.0000000397074201e-05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for e, lr in zip(rng,y):\n",
    "    print('{}\\t {}\\n'.format(e, lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b6b443",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\"> [Models] Callback </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "37046958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2021-11-25 #\n",
    "#TODO: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback 改寫為callbacks.on_epoch_begin(epoch) 並加入datetime方便在訓練時觀察時間\n",
    "class PrintLR(tf.keras.callbacks.Callback):\n",
    "#     def on_epoch_begin(self, epoch, logs=None):\n",
    "#         self.epoch=epoch\n",
    "#         print(f\"第 {epoch} 執行週期開始...\")\n",
    "#     def on_epoch_end(self, epoch, logs=None):\n",
    "#         print('\\nLearning rate for epoch {} is {}'.format(epoch + 1, model.optimizer.lr.numpy()))\n",
    "\n",
    "\n",
    "#     def on_epoch_begin(self, epoch, logs=None):\n",
    "#         print(f'[{datetime.now()}] Learning rate for epoch {epoch + 1} is {model.optimizer.lr.numpy()}')\n",
    "        \n",
    "    # time of epoch\n",
    "    def on_train_begin(self,  epoch, logs=None):\n",
    "        self.times = []\n",
    "    def on_epoch_begin(self,  epoch, logs=None):\n",
    "        self.epoch_time_start = time.time()\n",
    "        print(f'[{datetime.now()}] Learning rate for epoch {epoch + 1} is {model_toe.optimizer.lr.numpy()}')\n",
    "#         s_time = time.time()\n",
    "#         print(\"start\")\n",
    "    def on_epoch_end(self,  epoch, logs=None):\n",
    "        wall_time = time.time() - self.epoch_time_start\n",
    "        self.times.append(wall_time)\n",
    "        #print(f'[{self.times}] of epoch {epoch + 1}')\n",
    "        print(f'[{wall_time}] of epoch {epoch + 1}')\n",
    "        \n",
    "callback_lr_time = PrintLR() #return a object of callback, not use the Classs PrintLR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "169e8678",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "#     tensorboard_callback,\n",
    "    best_model_save,\n",
    "    tf.keras.callbacks.EarlyStopping(monitor=monitor, patience=patience_1), #patience=step_size or ep_num\n",
    "#     lr_reduceonplateau,\n",
    "    tf.keras.callbacks.LearningRateScheduler(lrdump),#lrdump, decay or lrfn or lrfn2. clr\n",
    "    callback_lr_time\n",
    "#     tensorboard_callback,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1034f59",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\"> [Models] Build models </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f3ac37ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_<lambda>_8011) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_<lambda>_4100) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown SavedObject type: None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_300/3230577155.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Model building/compiling need to be within `strategy.scope()`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmodel_toe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_efn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_dropout_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_connect_rate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# for efnet, Xincept\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_300/198783443.py\u001b[0m in \u001b[0;36mbuild_efn_model\u001b[0;34m(model_name, outputnum, top_dropout_rate, drop_connect_rate)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;31m# ViT model as a layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mhub_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://tfhub.dev/sayakpaul/vit_b8_fe/1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         model = tf.keras.Sequential([\n\u001b[1;32m    172\u001b[0m                                         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, handle, trainable, arguments, _sentinel, tags, signature, signature_outputs_as_dict, output_key, output_shape, load_options, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_training_argument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_has_training_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_hub_module_v1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_is_hub_module_v1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36mload_module\u001b[0;34m(handle, tags, load_options)\u001b[0m\n\u001b[1;32m    447\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Expected before TF2.4.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0mset_load_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodule_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset_load_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow_hub/module_v2.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(handle, tags, options)\u001b[0m\n\u001b[1;32m    104\u001b[0m         module_path, tags=tags, options=options)\n\u001b[1;32m    105\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m   \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_hub_module_v1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_hub_module_v1\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(export_dir, tags, options)\u001b[0m\n\u001b[1;32m    867\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mdon\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mt\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0ma\u001b[0m \u001b[0mMetaGraph\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mSavedModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   \"\"\"\n\u001b[0;32m--> 869\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mload_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"root\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload_internal\u001b[0;34m(export_dir, tags, options, loader_cls, filters)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 905\u001b[0;31m         loader = loader_cls(object_graph_proto, saved_model_proto, export_dir,\n\u001b[0m\u001b[1;32m    906\u001b[0m                             ckpt_options, filters)\n\u001b[1;32m    907\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, object_graph_proto, saved_model_proto, export_dir, ckpt_options, filters)\u001b[0m\n\u001b[1;32m    160\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_concrete_functions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_WrapperFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcrete_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restore_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36m_load_all\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    256\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_load_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;34m\"\"\"Loads all nodes and functions from the SavedModel and their edges.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_edges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;31m# TODO(b/124045874): There are limitations with functions whose captures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36m_load_nodes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;31m# interface.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m       \u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msetter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m       \u001b[0mnodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m       \u001b[0mnode_setters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36m_recreate\u001b[0;34m(self, proto, node_id)\u001b[0m\n\u001b[1;32m    552\u001b[0m     \u001b[0mkind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWhichOneof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"kind\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkind\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfactory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown SavedObject type: %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfactory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown SavedObject type: None"
     ]
    }
   ],
   "source": [
    "top_dropout_rate = 0.8 #less dp rate, say 0.1, train_loss will lower than val_loss\n",
    "drop_connect_rate = 0.9 #for efnet This parameter serves as a toggle for extra regularization in finetuning, but does not affect loaded weights.\n",
    "outputnum = 2\n",
    "\n",
    "with strategy.scope():\n",
    "    # Model building/compiling need to be within `strategy.scope()`.\n",
    "    model_toe, base_model = build_efn_model(model_name, outputnum, top_dropout_rate, drop_connect_rate) # for efnet, Xincept\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df420e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model_toe.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9496799",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = 0\n",
    "nt = 0\n",
    "for layer in model_toe.layers:\n",
    "    if layer.trainable:\n",
    "        tt +=1\n",
    "        print(f'{layer.name}')\n",
    "    else:\n",
    "        nt +=1\n",
    "print(f'tt: {tt}, nt:{nt}')\n",
    "\n",
    "def count_model_trainOrNot_layers(model, printlayers=False):\n",
    "    tt = 0\n",
    "    nt = 0\n",
    "    for layer in model.layers:\n",
    "        if layer.trainable:\n",
    "            tt +=1\n",
    "            if printlayers:\n",
    "                print(f'{layer.name}')\n",
    "        else:\n",
    "            nt +=1\n",
    "    print(f'tt: {tt}, nt:{nt}, total layers:{tt+nt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e5f688",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_model_trainOrNot_layers(model_toe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d816db",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45fb13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_toe.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe423af2",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\"> [Models] Train top layers (transfer learning)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5734286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # fit the model on all data\n",
    "# history_toe = model_toe.fit(train_ds_pre, \n",
    "#                       verbose=1, \n",
    "#                       epochs=5, #ep_num_transf, \n",
    "#                       validation_data=valid_ds_pre, \n",
    "#                       callbacks=callbacks)#, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb76d85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83219901",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d8dd54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056818dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7edd6e24",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (4013923208.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_6246/4013923208.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    看來網誌版本的也不能訓練，應該是跟tf ccs版本有關。\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "[2021-12-10]\n",
    "看來網誌版本的也不能訓練，應該是跟tf ccs版本有關。\n",
    "\n",
    "tensorflow-21.06-tf2-py3:tf25odocrpp2111 2nd steps faile out\n",
    "    \n",
    "\n",
    "tensorflow-21.08-tf2-py3:latest 可以跑了    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b70cd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419db4f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d0f8de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16eb8802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be860933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EFntB7 \n",
    "# inputs = 600*600, 8 gpu, bs4x8\n",
    "\n",
    "# TL seems not work!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88cd5d4",
   "metadata": {},
   "source": [
    "EFntB7 \n",
    "inputs = 512*512, 8 gpu, bs32x8\n",
    "\n",
    "Epoch 00012: val_accuracy improved from 0.73917 to 0.74478, saving model to ./TrainSaveDir/stage1_EfficientNetB7_bs32_w512_best_val_accuracy.h5\n",
    "[40.45903658866882] of epoch 12\n",
    "Epoch 13/500\n",
    "[2021-12-07 13:03:08.241426] Learning rate for epoch 13 is 0.0009675999754108489\n",
    "59/59 [==============================] - 39s 618ms/step - loss: 0.7936 - accuracy: 0.7024 - val_loss: 0.6922 - val_accuracy: 0.7445\n",
    "\n",
    "Epoch 00013: val_accuracy did not improve from 0.74478\n",
    "[38.982457876205444] of epoch 13\n",
    "CPU times: user 1h 30min 59s, sys: 9min 19s, total: 1h 40min 19s\n",
    "Wall time: 13min 11s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20393995",
   "metadata": {},
   "source": [
    "MBnetV2\n",
    "DS, inputs = 512*512, 8 gpu, bs32x8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17038e5",
   "metadata": {},
   "source": [
    "MBnetV2\n",
    "DS, inputs = 512*512, 8 gpu, bs64x8\n",
    "\n",
    "\n",
    "Epoch 00032: val_accuracy did not improve from 0.72608\n",
    "[34.99274802207947] of epoch 32\n",
    "CPU times: user 3h 1min 32s, sys: 8min 26s, total: 3h 9min 59s\n",
    "Wall time: 21min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6108ebc8",
   "metadata": {},
   "source": [
    "MBnetV2\n",
    "DS, inputs = 120*120\n",
    "\n",
    "Epoch 00016: val_accuracy did not improve from 0.68027\n",
    "[11.85263991355896] of epoch 16\n",
    "Epoch 17/500\n",
    "235/235 [==============================] - 12s 44ms/step - loss: 1.1775 - accuracy: 0.6076 - val_loss: 0.9085 - val_accuracy: 0.6768\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653236cf",
   "metadata": {},
   "source": [
    "### Fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3efb8737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unfreeze_model(model, base_model):\n",
    "#     # We unfreeze the top 20 layers while leaving BatchNorm layers frozen\n",
    "#     for layer in model.layers[-20:]:\n",
    "#         if not isinstance(layer, layers.BatchNormalization):\n",
    "#             layer.trainable = True\n",
    "\n",
    "#     model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),#RMSprop , Adam, SGD Adadelta(learning_rate=0.001), if set lr_callback the learning_rate=0.001 will not effeced.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     fine_tune_at = 20 #10 #241 #20\n",
    "#     print('[Note] Now create model fine tuneing at Top-{} layers!'.format(fine_tune_at))\n",
    "#     for layer in model_toe.layers[-fine_tune_at:]:\n",
    "#         if not isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "#             layer.trainable = True\n",
    "\n",
    "\n",
    "#\n",
    "#'block7a_expand_conv'20 'block6c_expand_conv'50 'block6a_expand_conv'79 'block5b_expand_conv'109 'block4a_expand_conv' 166 \n",
    "#\n",
    "\n",
    "    # Set All layers trainable first\n",
    "    #model.trainable = True #範例似乎是指 base_model.trainable = True 而不是新建model！！！！！！(2021-11-08)\n",
    "    base_model.trainable = True\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Then, set layers NOT trainable below specific layer.\n",
    "#     set_trainable = False\n",
    "#     for layer in model.layers:\n",
    "#         if layer.name == 'block5b_expand_conv': \n",
    "#             set_trainable = True\n",
    "#         if set_trainable:\n",
    "#             layer.trainable = True\n",
    "#         else:\n",
    "#             layer.trainable = False\n",
    "\n",
    "#     model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),#RMSprop , Adam, SGD Adadelta(learning_rate=0.001), if set lr_callback the learning_rate=0.001 will not effeced.\n",
    "#                     loss=ed_metric_2d_mean)\n",
    "    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),#RMSprop , Adam, SGD Adadelta(learning_rate=0.001), if set lr_callback the learning_rate=0.001 will not effeced.\n",
    "                loss=tf.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                 metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c2511f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "# unfreeze for finetune the toe model  \n",
    "unfreeze_model(model_toe,base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "55f67772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tt: 20, nt:0, total layers:20\n"
     ]
    }
   ],
   "source": [
    "count_model_trainOrNot_layers(model_toe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "42bf0575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_name_s2: ./TrainSaveDir/stage2_ViT_bs4_w512_best_val_accuracy.h5\n"
     ]
    }
   ],
   "source": [
    "th = 'stage2'\n",
    "best_model_name_s2 = get_best_model_name(th)\n",
    "\n",
    "best_model_save_s2 = tf.keras.callbacks.ModelCheckpoint(filepath=best_model_name_s2, \n",
    "                             save_best_only = True, \n",
    "                             save_weights_only = False,\n",
    "                             monitor = monitor, \n",
    "                             mode = 'auto', verbose = 1)\n",
    "print('best_model_name_s2:', best_model_name_s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "742d7141",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_reduceonplateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=monitor, factor=0.1, patience=5, verbose=1, \n",
    "    mode='auto', min_delta=0.0001, cooldown=0, min_lr=1e-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e9a4761c",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_s2 = [\n",
    "#     tensorboard_callback,\n",
    "    best_model_save_s2,\n",
    "    tf.keras.callbacks.EarlyStopping(monitor=monitor, patience=patience_2), #patience=step_size or ep_num\n",
    "#     lr_reduceonplateau,\n",
    "    tf.keras.callbacks.LearningRateScheduler(lrdump),#lrdump, decay or lrfn or lrfn2. clr\n",
    "    callback_lr_time\n",
    "#     tensorboard_callback,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "79f04e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "[2021-12-11 22:18:06.130088] Learning rate for epoch 1 is 9.999999747378752e-06\n",
      "INFO:tensorflow:batch_all_reduce: 200 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 200 all-reduces with algorithm = nccl, num_packs = 1\n",
      "469/469 [==============================] - 349s 445ms/step - loss: 1.1247 - accuracy: 0.6129 - val_loss: 1.0050 - val_accuracy: 0.6447\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.64475, saving model to ./TrainSaveDir/stage2_ViT_bs4_w512_best_val_accuracy.h5\n",
      "[349.88900542259216] of epoch 1\n",
      "Epoch 2/100\n",
      "[2021-12-11 22:23:58.026102] Learning rate for epoch 2 is 8.97999998414889e-05\n",
      "469/469 [==============================] - 172s 357ms/step - loss: 1.0013 - accuracy: 0.6404 - val_loss: 0.8300 - val_accuracy: 0.7055\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.64475 to 0.70552, saving model to ./TrainSaveDir/stage2_ViT_bs4_w512_best_val_accuracy.h5\n",
      "[173.1159906387329] of epoch 2\n",
      "Epoch 3/100\n",
      "[2021-12-11 22:26:53.071600] Learning rate for epoch 3 is 0.00016959999629762024\n",
      "469/469 [==============================] - 185s 384ms/step - loss: 0.9449 - accuracy: 0.6609 - val_loss: 0.7982 - val_accuracy: 0.7074\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.70552 to 0.70739, saving model to ./TrainSaveDir/stage2_ViT_bs4_w512_best_val_accuracy.h5\n",
      "[185.84552526474] of epoch 3\n",
      "Epoch 4/100\n",
      "[2021-12-11 22:30:00.861874] Learning rate for epoch 4 is 0.0002494000073056668\n",
      "469/469 [==============================] - 171s 356ms/step - loss: 0.9002 - accuracy: 0.6729 - val_loss: 0.7952 - val_accuracy: 0.7096\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.70739 to 0.70957, saving model to ./TrainSaveDir/stage2_ViT_bs4_w512_best_val_accuracy.h5\n",
      "[172.4185483455658] of epoch 4\n",
      "Epoch 5/100\n",
      "[2021-12-11 22:32:55.289044] Learning rate for epoch 5 is 0.00032920000376179814\n",
      "469/469 [==============================] - 172s 358ms/step - loss: 0.8986 - accuracy: 0.6750 - val_loss: 0.7703 - val_accuracy: 0.7236\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.70957 to 0.72359, saving model to ./TrainSaveDir/stage2_ViT_bs4_w512_best_val_accuracy.h5\n",
      "[173.1874885559082] of epoch 5\n",
      "Epoch 6/100\n",
      "[2021-12-11 22:35:50.390644] Learning rate for epoch 6 is 0.0004090000002179295\n",
      "469/469 [==============================] - ETA: 0s - loss: 0.8795 - accuracy: 0.6776"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1212\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m                 steps_per_execution=self._steps_per_execution)\n\u001b[0;32m-> 1214\u001b[0;31m           val_logs = self.evaluate(\n\u001b[0m\u001b[1;32m   1215\u001b[0m               \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m               \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1487\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1489\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1490\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1491\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    922\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3021\u001b[0m       (graph_function,\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3023\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# fit the model on all data\n",
    "history_toe_finetune = model_toe.fit(train_ds_pre, \n",
    "                      verbose=1, \n",
    "                      epochs= 100,#ep_num_transf, \n",
    "                      validation_data=valid_ds_pre, \n",
    "                      callbacks=callbacks_s2)#, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24e1799",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384f8fe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fee18c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530daa4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42032769",
   "metadata": {},
   "outputs": [],
   "source": [
    "for different DS: 800*600 ->> 512resize/600resize/512crop/600crop\n",
    "\n",
    "EFntB7 \n",
    "inputs = 600*600, 8 gpu, bs4x8\n",
    "\n",
    "Epoch 00014: val_accuracy did not improve from 0.88252\n",
    "[205.34342765808105] of epoch 14\n",
    "CPU times: user 5h 35min 30s, sys: 20min 23s, total: 5h 55min 53s\n",
    "Wall time: 56min 51s\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4ab570",
   "metadata": {},
   "outputs": [],
   "source": [
    "ResNet152 inputs = 512*512, 8 gpu, bs4x8\n",
    "\n",
    "Epoch 00011: val_accuracy did not improve from 0.83640\n",
    "[134.74419260025024] of epoch 11\n",
    "CPU times: user 2h 26min 57s, sys: 4min 11s, total: 2h 31min 9s\n",
    "Wall time: 29min 14s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c950bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xception inputs = 512*512, 8 gpu, bs4x8\n",
    "\n",
    "somehow TL not work??? cause FT train from inf!\n",
    "\n",
    "Epoch 00005: val_accuracy did not improve from 0.83017\n",
    "[79.85516905784607] of epoch 5\n",
    "CPU times: user 48min 24s, sys: 2min 27s, total: 50min 51s\n",
    "Wall time: 8min 13s\n",
    "    \n",
    "Epoch 00018: val_accuracy did not improve from 0.86600\n",
    "[72.12753582000732] of epoch 18\n",
    "CPU times: user 2h 37min 51s, sys: 4min 40s, total: 2h 42min 32s\n",
    "Wall time: 23min 47s    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a2391d",
   "metadata": {},
   "source": [
    "DenseNet201  inputs = 512*512, 8 gpu, bs4x8\n",
    "\n",
    "after 4 mins, it ran. (without TL [stage1])\n",
    "\n",
    "Epoch 00005: val_accuracy did not improve from 0.81739\n",
    "[113.5602593421936] of epoch 5\n",
    "Epoch 6/500\n",
    "[2021-12-07 17:04:56.111606] Learning rate for epoch 6 is 0.0004090000002179295\n",
    "469/469 [==============================] - 113s 237ms/step - loss: 0.6786 - accuracy: 0.7655 - val_loss: 0.6962 - val_accuracy: 0.7918\n",
    "\n",
    "Epoch 00006: val_accuracy did not improve from 0.81739\n",
    "[113.0628821849823] of epoch 6\n",
    "CPU times: user 1h 34min 3s, sys: 2min 42s, total: 1h 36min 46s\n",
    "Wall time: 17min 21s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61684f55",
   "metadata": {},
   "source": [
    "EFntB7 inputs = 512*512, 8 gpu, bs4x8\n",
    "\n",
    "after 5 mins, it ran. (without TL [stage1])\n",
    "\n",
    "[592.2609107494354] of epoch 1\n",
    "[210.42193007469177] of epoch 3\n",
    "Epoch 00004: val_accuracy did not improve from 0.88221\n",
    "[181.73404240608215] of epoch 4\n",
    "\n",
    "Epoch 5/500\n",
    "[2021-12-07 15:54:04.367304] Learning rate for epoch 5 is 0.00032920000376179814\n",
    "469/469 [==============================] - 180s 381ms/step - loss: 0.3675 - accuracy: 0.8766 - val_loss: nan - val_accuracy: 0.0000e+00\n",
    "\n",
    "Epoch 00006: val_accuracy did not improve from 0.88221\n",
    "[181.85825777053833] of epoch 6\n",
    "CPU times: user 2h 12min 22s, sys: 7min 30s, total: 2h 19min 53s\n",
    "Wall time: 25min 43s                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc12760f",
   "metadata": {},
   "source": [
    "EFntB7 inputs = 512*512, 8 gpu, bs2x8\n",
    "\n",
    "after 4 mins, it ran. then 10min epoch 1 val_loss: nan. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759f7da8",
   "metadata": {},
   "source": [
    "EFntB7 inputs = 512*512, 8 gpu, bs16x8 and bs8x8\n",
    "\n",
    "\n",
    "CancelledError:  [_Derived_]RecvAsync is cancelled.\n",
    "\t [[{{node div_no_nan_1/ReadVariableOp_6/_3732}}]] [Op:__inference_train_function_778904]\n",
    "\n",
    "Function call stack:\n",
    "train_function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e612d1",
   "metadata": {},
   "source": [
    "EFntB7 \n",
    "inputs = 512*512, 8 gpu, bs32x8\n",
    "\n",
    "CancelledError:  [_Derived_]RecvAsync is cancelled.\n",
    "\t [[{{node div_no_nan/ReadVariableOp_6/_3768}}]] [Op:__inference_train_function_843519]\n",
    "\n",
    "Function call stack:\n",
    "train_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004d6ddc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5106e36e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "221ef6f7",
   "metadata": {},
   "source": [
    "MBnetV2 8 gpu FT 512x512 BS32 patience5\n",
    "\n",
    "Epoch 00010: val_accuracy did not improve from 0.86974\n",
    "[33.78622841835022] of epoch 10\n",
    "Epoch 11/500\n",
    "[2021-12-07 12:39:57.489634] Learning rate for epoch 11 is 0.0008079999824985862\n",
    "59/59 [==============================] - 34s 542ms/step - loss: 0.5160 - accuracy: 0.8253 - val_loss: 0.4515 - val_accuracy: 0.8672\n",
    "\n",
    "Epoch 00011: val_accuracy did not improve from 0.86974\n",
    "[34.4537308216095] of epoch 11\n",
    "CPU times: user 1h 7min 24s, sys: 2min 38s, total: 1h 10min 2s\n",
    "Wall time: 8min 10s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a414d54",
   "metadata": {},
   "source": [
    "MBnetV2 8 gpu FT 512x512 BS64\n",
    "\n",
    "OOM when allocating tensor with shape[64,96,256,256]\n",
    "if img is int8 (1B) then\n",
    "64x96x256x256 x1B = 402.653MB\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1ff99d",
   "metadata": {},
   "source": [
    "MBnetV2 8 gpu FT 120x120\n",
    "\n",
    "Epoch 00024: val_accuracy did not improve from 0.77470\n",
    "[4.618327617645264] of epoch 24\n",
    "Epoch 25/500\n",
    "[2021-12-07 11:09:18.232609] Learning rate for epoch 25 is 0.0019251999910920858\n",
    "30/30 [==============================] - 5s 102ms/step - loss: 0.7740 - accuracy: 0.7228 - val_loss: 0.8236 - val_accuracy: 0.7018\n",
    "\n",
    "Epoch 00025: val_accuracy did not improve from 0.77470\n",
    "[4.819160223007202] of epoch 25\n",
    "CPU times: user 23min 26s, sys: 1min 10s, total: 24min 37s\n",
    "Wall time: 4min 10s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543ae1e8",
   "metadata": {},
   "source": [
    "MBnetV2 two gpu FT 120x120\n",
    "\n",
    "Epoch 00003: val_accuracy did not improve from 0.74945\n",
    "[9.133079528808594] of epoch 3\n",
    "118/118 [==============================] - 9s 59ms/step - loss: 1.1131 - accuracy: 0.6141 - val_loss: 2.5206 - val_accuracy: 0.1399\n",
    "\n",
    "Epoch 00019: val_accuracy did not improve from 0.74945\n",
    "CPU times: user 15min 1s, sys: 21.1 s, total: 15min 22s\n",
    "Wall time: 3min 52s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4785f1b2",
   "metadata": {},
   "source": [
    "MBnetV2 one gpu FT 120x120\n",
    "Epoch 00012: val_accuracy did not improve from 0.74135\n",
    "[13.294872760772705] of epoch 12\n",
    "CPU times: user 8min 18s, sys: 5.84 s, total: 8min 24s\n",
    "Wall time: 3min 18s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96141cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640cf5dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc42236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2021-11-26\n",
    "# hist save and load test https://stackoverflow.com/questions/41061457\n",
    "\n",
    "\n",
    "history_fine = history_toe_finetune\n",
    "\n",
    "\"\"\"Then history is a dictionary and you can retrieve all desirable values using the keys.\"\"\"\n",
    "# add the epoch timeing\n",
    "history_fine.history['epoch_time_secs'] = callback_lr_time.times\n",
    "\n",
    "np.save('test_history.npy', history_fine.history)\n",
    "history_np_load = np.load('test_history.npy', allow_pickle='TRUE').item()\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"As history.history is a dict, you can convert it as well to a pandas DataFrame object, which can then be saved to suit your needs.\"\"\"\n",
    "# convert the history.history dict to a pandas DataFrame:     \n",
    "hist_df = pd.DataFrame(history_fine.history) \n",
    "\n",
    "# save to json:  \n",
    "hist_json_file = 'test_history.json' \n",
    "with open(hist_json_file, mode='w') as f:\n",
    "    hist_df.to_json(f)\n",
    "# with open('data.json') as f:\n",
    "#     data = json.load(f)\n",
    "with open(hist_json_file) as f:\n",
    "    t_h_json = json.load(f)\n",
    "    \n",
    "# or save to csv: \n",
    "hist_csv_file = 'test_history.csv'\n",
    "with open(hist_csv_file, mode='w') as f:\n",
    "    hist_df.to_csv(f)\n",
    "\n",
    "t_h_csv = pd.read_csv(hist_csv_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29b98a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history_np_load['loss'])\n",
    "print(history_np_load['loss'][3]) # the loss at epoch n+1\n",
    "\n",
    "print(history_np_load['val_accuracy'])\n",
    "print(history_np_load['val_accuracy'][0]) # the loss at epoch n+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb7db24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(type(t_h_json))\n",
    "print(\"train_loss:\", t_h_json[\"loss\"], \"\\n\") #take a sub dict\n",
    "\n",
    "t_h_json # all dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cdce80",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_h_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f725444a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for only fine tune\n",
    "acc = history_fine.history['accuracy']\n",
    "val_acc = history_fine.history['val_accuracy']\n",
    "\n",
    "loss = history_fine.history['loss']\n",
    "val_loss = history_fine.history['val_loss']\n",
    "\n",
    "\n",
    "# for only fine tune\n",
    "fig = plt.figure(figsize=(8, 8), num=model_name)\n",
    "fig.suptitle(model_name + '(top-100 layers)', fontsize=14, fontweight='bold')\n",
    "print(\"base model and training pahse: {}\".format(model_name + '_top-100layer' + '_lr' + str(lr) + '_e' + str(ep_num_transf) ))\n",
    "\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.ylim([0.1, 1])\n",
    "# plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "#           plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.ylim([0, 5.0])\n",
    "# plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "#          plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d64efc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load hist from saved npy\n",
    "\n",
    "# for only fine tune\n",
    "acc = history_np_load['accuracy']\n",
    "val_acc = history_np_load['val_accuracy']\n",
    "\n",
    "loss = history_np_load['loss']\n",
    "val_loss = history_np_load['val_loss']\n",
    "\n",
    "\n",
    "# for only fine tune\n",
    "fig = plt.figure(figsize=(8, 8), num=model_name)\n",
    "fig.suptitle(model_name + '(top-100 layers)', fontsize=14, fontweight='bold')\n",
    "print(\"base model and training pahse: {}\".format(model_name + '_top-100layer' + '_lr' + str(lr) + '_e' + str(ep_num_transf) ))\n",
    "\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.ylim([0.1, 1])\n",
    "# plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "#           plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.ylim([0, 5.0])\n",
    "# plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "#          plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe6e0ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae13ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "loss, accuracy = model_toe.evaluate(test_ds_pre)\n",
    "print('Test accuracy :', accuracy)\n",
    "\n",
    "print(\"count roughly ds size: \", tf.data.experimental.cardinality(test_ds_pre).numpy() * MULTI_BATCH_SIZE)\n",
    "\"\"\"the last model may is nan not best one\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986751be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# current_model_name = 'leaf-2020-12-01-EfficientNetB7_top-layer50_lr_lrfn_val-acc.8352_wh512_e37.h5'\n",
    "model_back = tf.keras.models.load_model(best_model_name_s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ca419e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "loss, accuracy = model_back.evaluate(test_ds_pre)\n",
    "print('Test accuracy :', accuracy)\n",
    "\n",
    "print(\"count roughly ds size: \", tf.data.experimental.cardinality(test_ds_pre).numpy() * MULTI_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db3a3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "efnetB7 epoch 6, val_acc:0.882, test_acc:0.891\n",
    "\n",
    "101/101 [==============================] - 36s 356ms/step - loss: 0.3229 - accuracy: 0.8916\n",
    "Test accuracy : 0.8915550112724304\n",
    "count roughly ds size:  3232\n",
    "CPU times: user 10.1 s, sys: 4.25 s, total: 14.4 s\n",
    "Wall time: 36 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3121e3f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52d54ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56800b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7840b0e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba7946f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa3f2a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56042a97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2088dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8083b789",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bbc8cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37d9294",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c6fd40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd54cef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d579612",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173e8aa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bc04b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c461374",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
